<!doctype html><html lang=ko-kr dir=ltr><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=keywords content="TIL,NLP,BERT"><meta name=description content="구글 BERT의 정석 1"><meta name=author content="minyeamer"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><link rel=canonical href=https://minyeamer.github.io/post/study/2022/2022-07/2022-07-17/><meta name=google-site-verification content="u1tWcHmHUZWfFT1cHaku6sqU-bK40N3WLR-C-4VUWN0"><meta name=naver-site-verification content="6eaf8e9da1a6104780f056f1a7797fe5a3a5a0da"><meta property="og:url" content="https://minyeamer.github.io/post/study/2022/2022-07/2022-07-17/"><meta property="og:site_name" content="Minystory"><meta property="og:title" content="2022-07-17 Log"><meta property="og:description" content="구글 BERT의 정석 1"><meta property="og:locale" content="ko_kr"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-07-17T20:00:00+09:00"><meta property="article:modified_time" content="2022-07-17T20:00:00+09:00"><meta property="article:tag" content="TIL"><meta property="article:tag" content="NLP"><meta property="article:tag" content="BERT"><title>2022-07-17 Log | Minystory</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://minyeamer.github.io/post/study/2022/2022-07/2022-07-17/><link rel=stylesheet href=/book.min.b806db2fcd7252f443c9368f219d473f25aaf586f28798bf9f651f41a7225060.css integrity="sha256-uAbbL81yUvRDyTaPIZ1HPyWq9Ybyh5i/n2UfQaciUGA=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/search-input.min.25901006165b642d19a996db15ba3fb29bba415c2ffcfe138ac0af40df3ee4eb.js integrity="sha256-JZAQBhZbZC0ZqZbbFbo/spu6QVwv/P4TisCvQN8+5Os=" crossorigin=anonymous></script><link rel=preload href=/search-data.min.b0e1b42f927aec8bb7d54995be39485efe34e26282f96f491b8830611aeb6bbf.json as=fetch crossorigin><script>window.SEARCH_DATA_URL="/search-data.min.b0e1b42f927aec8bb7d54995be39485efe34e26282f96f491b8830611aeb6bbf.json"</script><script defer src=/search.min.f30f9834d4764fd9751da64098c954d01085f648ba9ca421a3c97582f8c47253.js integrity="sha256-8w+YNNR2T9l1HaZAmMlU0BCF9ki6nKQho8l1gvjEclM=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V")</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.1/css/all.min.css crossorigin=anonymous><script defer src=/scroll-progress.min.841ade7e507a5f6d59c4e7bf2fe2b2ca034070677ff7957eec55610a024dd776.js integrity="sha256-hBreflB6X21ZxOe/L+KyygNAcGd/95V+7FVhCgJN13Y=" crossorigin=anonymous></script><script defer src=/dark-mode.min.e41c6440ffd9967d6f6a419ff3ce09b862009fe1646ab265f5cb2817d2a508e3.js integrity="sha256-5BxkQP/Zln1vakGf884JuGIAn+FkarJl9csoF9KlCOM=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlightjs-line-numbers.js/2.9.0/highlightjs-line-numbers.min.js></script><script defer src=/copy-code.min.aaeef965f0b4992e55f976edaecb34a89d414e1791caa18c3f4f4376c6d8b5a8.js integrity="sha256-qu75ZfC0mS5V+Xbtrss0qJ1BTheRyqGMP09DdsbYtag=" crossorigin=anonymous></script><script defer src=/toc-highlightjs.093016f0ef312174ad862fdcf5792e88ab5442bd39beecc38d15643f71ab5c31.min integrity="sha256-CTAW8O8xIXSthi/c9XkuiKtUQr05vuzDjRVkP3GrXDE=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-post book-layout-post"><div class=scroll-progress><div class=scroll-progress-bar></div></div><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><div class=sidebar-profile><div class=profile-img-wrap><a href=https://minyeamer.github.io/><img src="https://avatars.githubusercontent.com/u/17109173?v=4" alt=Profile class=profile-img></a></div><div class=sidebar-social><a href=https://github.com/minyeamer target=_blank title=GitHub><i class="fa-brands fa-github"></i>
</a><a href=/categories/ title=Categories><i class="fa-solid fa-folder"></i>
</a><a href=/tags/ title=Tags><i class="fa-solid fa-tags"></i>
</a><button id=dark-mode-toggle class=dark-mode-toggle aria-label="Toggle dark mode">
<i class="fa-solid fa-circle-half-stroke"></i></button></div></div><h2 class=book-brand><a class="flex align-center" href=/><span>Minystory</span></a></h2><div class="book-search hidden"><div class=search-input-container><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/ onkeydown='event.key==="Enter"&&goToSearchPage()'>
<button type=button id=book-search-button class=book-search-btn onclick=goToSearchPage()>
<i class="fa-solid fa-magnifying-glass"></i></button></div><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden");function goToSearchPage(){const t=document.getElementById("book-search-input"),e=t.value.trim();e&&(window.location.href="/search/?q="+encodeURIComponent(e))}</script><div class=book-categories><input type=checkbox class="hidden toggle" id=categories-control checked>
<label for=categories-control class="categories-toggle categories-link"><a href=/categories/><i class="fa-solid fa-folder"></i>
<span>전체</span>
<span class=category-count>(126)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul class=categories-menu id=categories-menu><li><input type=checkbox class="hidden toggle" id=cat-algorithm>
<label for=cat-algorithm class="categories-toggle categories-link"><a href=/categories/algorithm/><i class="fa-solid fa-folder"></i>
Algorithm
<span class=category-count>(51)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/algorithm/baekjoon/><i class="fa-solid fa-file"></i>
Baekjoon
<span class=category-count>(31)</span></a></li><li class=categories-link><a href=/categories/algorithm/leetcode/><i class="fa-solid fa-file"></i>
LeetCode
<span class=category-count>(2)</span></a></li><li class=categories-link><a href=/categories/algorithm/programmers/><i class="fa-solid fa-file"></i>
Programmers
<span class=category-count>(17)</span></a></li><li class=categories-link><a href=/categories/algorithm/references/><i class="fa-solid fa-file"></i>
References
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-blog>
<label for=cat-blog class="categories-toggle categories-link"><a href=/categories/blog/><i class="fa-solid fa-folder"></i>
Blog
<span class=category-count>(5)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/blog/review/><i class="fa-solid fa-file"></i>
Review
<span class=category-count>(1)</span></a></li><li class=categories-link><a href=/categories/blog/tech/><i class="fa-solid fa-file"></i>
Tech
<span class=category-count>(4)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-book>
<label for=cat-book class="categories-toggle categories-link"><a href=/categories/book/><i class="fa-solid fa-folder"></i>
Book
<span class=category-count>(1)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/book/finance/><i class="fa-solid fa-file"></i>
Finance
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-data>
<label for=cat-data class="categories-toggle categories-link"><a href=/categories/data/><i class="fa-solid fa-folder"></i>
Data
<span class=category-count>(4)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/data/crawling/><i class="fa-solid fa-file"></i>
Crawling
<span class=category-count>(4)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-diary>
<label for=cat-diary class="categories-toggle categories-link"><a href=/categories/diary/><i class="fa-solid fa-folder"></i>
Diary
<span class=category-count>(3)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/diary/2022/><i class="fa-solid fa-file"></i>
2022
<span class=category-count>(2)</span></a></li><li class=categories-link><a href=/categories/diary/2023/><i class="fa-solid fa-file"></i>
2023
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-study>
<label for=cat-study class="categories-toggle categories-link"><a href=/categories/study/><i class="fa-solid fa-folder"></i>
Study
<span class=category-count>(61)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/study/2022/><i class="fa-solid fa-file"></i>
2022
<span class=category-count>(16)</span></a></li><li class=categories-link><a href=/categories/study/2023/><i class="fa-solid fa-file"></i>
2023
<span class=category-count>(10)</span></a></li><li class=categories-link><a href=/categories/study/ai-school/><i class="fa-solid fa-file"></i>
AI SCHOOL
<span class=category-count>(34)</span></a></li><li class=categories-link><a href=/categories/study/datacamp/><i class="fa-solid fa-file"></i>
DataCamp
<span class=category-count>(1)</span></a></li></ul></li></ul></div><div class=recent-posts><div class=recent-posts-header><i class="fa-solid fa-clock"></i>
<span>최신글</span></div><ul class=recent-posts-list><li class=recent-post-item><a href=/post/study/2023/2023-04/2023-04-02/ title="2023-04-02 Log"><div class=recent-post-title>2023-04-02 Log</div><div class=recent-post-date><time datetime=2023-04-02>2023.04.02</time></div></a></li><li class=recent-post-item><a href=/post/data/crawling/10000-recipe/ title="[Python] 만개의 레시피 데이터 수집"><div class=recent-post-title>[Python] 만개의 레시피 데이터 수집</div><div class=recent-post-date><time datetime=2023-03-26>2023.03.26</time></div></a></li><li class=recent-post-item><a href=/post/study/2023/2023-03/2023-03-25/ title="2023-03-25 Log"><div class=recent-post-title>2023-03-25 Log</div><div class=recent-post-date><time datetime=2023-03-25>2023.03.25</time></div></a></li><li class=recent-post-item><a href=/post/study/2023/2023-03/2023-03-21/ title="2023-03-21 Log"><div class=recent-post-title>2023-03-21 Log</div><div class=recent-post-date><time datetime=2023-03-21>2023.03.21</time></div></a></li><li class=recent-post-item><a href=/post/study/2023/2023-02/2023-02-19/ title="2023-02-19 Log"><div class=recent-post-title>2023-02-19 Log</div><div class=recent-post-date><time datetime=2023-02-19>2023.02.19</time></div></a></li></ul></div></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><i class="fa-solid fa-bars book-icon" id=menu-icon></i></label><h3><a href=https://minyeamer.github.io/ class=site-title>Minystory</a></h3><label for=toc-control><i class="fa-solid fa-list book-icon" id=toc-icon></i></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#self-attention>Self Attention</a></li><li><a href=#multi-head-attention>Multi-Head Attention</a></li><li><a href=#positional-encoding>Positional Encoding</a></li><li><a href=#feed-forward-network>Feed Forward Network</a></li><li><a href=#인코더-순서>인코더 순서</a></li></ul><ul><li><a href=#masked-multi-head-attention>Masked Multi-Head Attention</a></li><li><a href=#encoder-decoder-attention-layer>Encoder-Decoder Attention Layer</a></li><li><a href=#linear-and-sofmax-layer>Linear and Sofmax Layer</a></li><li><a href=#디코더-순서>디코더 순서</a></li><li><a href=#학습>학습</a></li></ul><ul><li><a href=#사전-학습>사전 학습</a></li><li><a href=#token-embedding>Token Embedding</a></li><li><a href=#segment-embedding>Segment Embedding</a></li><li><a href=#position-embedding>Position Embedding</a></li><li><a href=#입력-데이터>입력 데이터</a></li><li><a href=#wordpiece-tokenizer>WordPiece Tokenizer</a></li><li><a href=#language-modeling>Language Modeling</a></li><li><a href=#masked-language-modeling-mlm>Masked Language Modeling (MLM)</a></li><li><a href=#whole-word-masking-wwm>Whole Word Masking (WWM)</a></li><li><a href=#next-sentence-prediction-nsp>Next Sentence Prediction (NSP)</a></li><li><a href=#사전-학습-절차>사전 학습 절차</a></li></ul><ul><li><a href=#byte-pair-encoding-bpe>Byte Pair Encoding (BPE)</a></li><li><a href=#byte-level-byte-pair-encoding-bbpe>Byte-Level Byte Pair Encoding (BBPE)</a></li><li><a href=#wordpiece>WordPiece</a></li></ul></nav><div class=book-nav><button class=book-nav-btn3 onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=book-nav-btn3 onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
<button class=book-nav-btn3 onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div></aside></header><article class="markdown book-article"><header class=post-header><div class=post-header-category><a href=/categories/study/2022/ class=post-header-category-link>Study/2022</a></div><h1 class=post-header-title>2022-07-17 Log</h1><div class=post-header-date><time datetime=2022-07-17T20:00:00+09:00>2022. 7. 17. 20:00</time></div></header><h1 id=기계번역-과제>기계번역 과제
<a class=anchor href=#%ea%b8%b0%ea%b3%84%eb%b2%88%ec%97%ad-%ea%b3%bc%ec%a0%9c>#</a></h1><ul><li>인코더: 입력 문장의 표현 방법 학습</li><li>디코더: 인코더에서 학습한 표현 결과를 입력받아 사용자가 원하는 문장 생성</li></ul><hr><h1 id=encoder>Encoder
<a class=anchor href=#encoder>#</a></h1><h2 id=self-attention>Self Attention
<a class=anchor href=#self-attention>#</a></h2><ul><li>임베딩: 각각의 단어를 표현하는 벡터값, [문장 길이 x 임베딩 차원]</li><li>쿼리(Q), 키(K), 밸류(V) > 각각의 가중치 행렬을 입력 행렬에 곱해 Q, K, V 행렬 생성</li><li>Q, K, V의 차원 [문장 길이 x 벡터의 차원]</li><li>1단계: Q와 K^T 행렬의 내적 연산, 쿼리 벡터(I)와 키 벡터(I, am, good) 사이의 유사도 계산</li><li>2단계: QK^T 행렬을 키 벡터 차원의 제곱근값($\sqrt{d_k}$)으로 나눈 것, 안정적인 gradient 얻음</li><li>3단계: 소프트맥스 함수를 사용해 비정규화된 형태의 유사도 값을 정규화 (score 행렬)</li><li>4단계: 스코어 행렬에 V 행렬을 곱해 어텐션(Z) 행렬 계산, 어텐션 행렬은 문장의 각 단어와 벡터값 가짐<br>(단어 I의 셀프 어텐션은 각 밸류 벡터값의 가중치 합으로 계산, 단어가 문장 내에 있는 다른 단어와의 연관성)</li></ul><h2 id=multi-head-attention>Multi-Head Attention
<a class=anchor href=#multi-head-attention>#</a></h2><ul><li>문장 내에서 모호한 의미를 가진 단어(it)가 있을 경우,<br>문장의 의미가 잘못 해석될 수 있기 때문에 멀티 헤드 어텐션을 사용한 후 그 결괏값을 더함</li><li>다수의 어텐션 행렬을 구하기 위해 서로 다른 가중치 행렬을 입력 행렬에 곱해 Q, K, V 생성</li><li>다수의 어텐션 행렬을 concatenate하고 새로운 가중치 행렬을 곱해 멀티 헤드 어텐션 결과 도출<br>(concatenate 시 [어텐션 헤드 x h] 크기가 되기 때문에 원래 크기로 만들기 위해 가중치 행렬 곱함)</li></ul><h2 id=positional-encoding>Positional Encoding
<a class=anchor href=#positional-encoding>#</a></h2><ul><li>트랜스포머는 문장 안에 있는 모든 단어를 병렬 형태로 입력</li><li>단어의 순서 정보를 제공하기 위해 문장에서 단어의 위치를 나타내는 인코딩 제공</li><li>위치 인코딩은 사인파 함수를 사용</li><li>입력 임베딩 결과에 위치 인코딩을 합한 후 멀티 헤드 어텐션에 입력</li></ul><h2 id=feed-forward-network>Feed Forward Network
<a class=anchor href=#feed-forward-network>#</a></h2><ul><li>2개의 전결합층(Dense)과 ReLU 활성화 함수로 구성</li><li>add와 norm을 추가해 서브레이어에서 멀티 헤드 어텐션의 입력값과 출력값을 서로 연결</li><li>add와 norm은 레이어 정규화(각 레이어 값이 크게 변화하는 것을 방지해 모델 학습 빠르게)와 잔차 연결</li></ul><h2 id=인코더-순서>인코더 순서
<a class=anchor href=#%ec%9d%b8%ec%bd%94%eb%8d%94-%ec%88%9c%ec%84%9c>#</a></h2><ol><li>입력값은 입력 임베딩으로 변환한 다음 위치 인코딩 추가, 가장 아래 있는 인코더 1의 입력값으로 공급</li><li>인코더 1은 입력값을 받아 멀티 헤드 어텐션의 서브레이어에 값을 보냄, 어텐션 행렬을 결괏값으로 출력</li><li>어텐션 행렬의 값을 다음 서브레이어인 피드포워드 네트워크에 입력, 결괏값 출력</li><li>인코더 1의 출력값을 그 위에 있는 인코더 2에 입력값으로 제공</li><li>인코더 2에서 이전과 동일한 방법 수행, 주어진 문장에 대한 인코더 표현 결과를 출력으로 제공</li></ol><hr><h1 id=decoder>Decoder
<a class=anchor href=#decoder>#</a></h1><ul><li>이전 디코더의 입력값과 인코더의 표현(인코더의 출력값), 2개를 입력 데이터로 받음</li><li>t=1에서 디코더의 입력값은 문장의 시작을 알리는 <sos>를 입력 > 타깃 문장의 첫 번째 단어(Je) 생성</li><li>t=2에서 t-1 디코더에서 생성한 단어(<sos>, Je)를 추가해 문장의 다음 단어 생성</li><li>t=3에서도 동일하게 (<sos>, Je, vais)를 입력받아 다음 단어 생성</li><li>디코더에서 <eos>토큰을 생성할 때 타깃 문장의 생성이 완료</li><li>디코더도 입력값을 바로 입력하지 않고 위치 인코딩을 추가한 값을 출력 임베딩에 더해 입력값으로 사용</li></ul><h2 id=masked-multi-head-attention>Masked Multi-Head Attention
<a class=anchor href=#masked-multi-head-attention>#</a></h2><ul><li>디코더에서 문장을 생성할 때 이전 단계에서 생성한 단어만 입력으로 넣기 때문에,<br>아직 예측하지 않은 오른쪽의 모든 단어를 마스킹해 학습을 진행</li><li>소프트맥스 함수를 적용한 정규화 작업 전에 <sos>오른쪽의 모든 단어를 $-{\infty}$로 마스킹 수행<br>($-{\infty}$는 학습 도중 발산하는 경우가 있기 때문에 실제로는 작은 값 $e^{-9}$으로 지정)</li></ul><h2 id=encoder-decoder-attention-layer>Encoder-Decoder Attention Layer
<a class=anchor href=#encoder-decoder-attention-layer>#</a></h2><ul><li>디코더의 멀티 헤드 어텐션의 입력으로 인코더의 표현값 R과 마스크된 멀티 헤드 어텐션의 결과 M을 받을 때 상호작용 발생</li><li>쿼리, 키, 밸류 행렬을 생성할 때, M을 사용해 Q를 생성, R을 활용해 K, V를 생성<br>(쿼리 행렬은 타깃 문장의 표현을 포함하기 때문에 M을 참조, 키와 밸류 행렬은 입력 문장의 표현을 참조)</li><li>쿼리, 키 행렬 간의 내적 시 타깃 단어 <sos>가 입력 문장의 모든 단어(I, am, good)와 얼마나 유사한지 계산<br>(두 번째 행에서 Je에 대해, 나머지 행에서도 동일한 방법을 적용해 유사도 계산)</li></ul><h2 id=linear-and-sofmax-layer>Linear and Sofmax Layer
<a class=anchor href=#linear-and-sofmax-layer>#</a></h2><ul><li>최상위 디코더에서 얻은 출력 값을 선형 및 소프트맥스 레이어에 전달</li><li>선형 레이어는 vocab 크기와 같은 logit 형태</li><li>logit 값을 확률값으로 변환하고, 디코더에서 가장 높은 확률값을 갖는 인덱스의 단어로 출력</li></ul><h2 id=디코더-순서>디코더 순서
<a class=anchor href=#%eb%94%94%ec%bd%94%eb%8d%94-%ec%88%9c%ec%84%9c>#</a></h2><ol><li>디코더에 대한 입력 문장을 임베딩 행렬로 변환하고 위치 인코딩 정보를 추가해 디코더 1에 입력</li><li>입력을 가져와서 마스크된 멀티 헤드 어텐션 레이어에 보내고, 출력으로 어텐션 행렬 M 반환</li><li>어텐션 행렬 M, 인코딩 표현 R을 입력받아 멀티 헤드 어텐션 레이어에 값을 입력, 새로운 어텐션 행렬 생성</li><li>인코더-디코더 어텐션 레이어에서 출력한 어텐션 행렬을 피드포워드 네트워크에 입력, 디코더의 표현으로 값 출력</li><li>디코더 1의 출력값을 다음 디코더 2의 입력값으로 사용</li><li>디코더 2는 이전과 동일한 방법 수행, 타깃 문장에 대한 디코더 표현 반환</li><li>타깃 문장의 디코더 표현을 선형 및 소프트맥스 레이어에 입력해 최종으로 예측된 단어 얻음</li></ol><h2 id=학습>학습
<a class=anchor href=#%ed%95%99%ec%8a%b5>#</a></h2><ul><li>손실 함수로 cross-entropy를 사용해 분포의 차이를 확인</li><li>옵티마이저로 Adam 사용</li><li>과적합을 방지하기 위해 각 서브레이어 출력에 dropout 적용 (임베딩 및 위치 인코딩 합을 구할 때도 포함)</li></ul><hr><h1 id=bert>BERT
<a class=anchor href=#bert>#</a></h1><ul><li>Word2Vec: 문맥 독립 임베딩, BERT: 문맥 기반 임베딩</li><li>BERT는 인코더-디코더가 있는 트랜스포머 모델에서 인코더만 사용</li><li>BERT-base: L(인코더 레이어)=12, A(어텐션 헤드)=12, H(은닉 유닛)=768</li><li>BERT-large: L=24, A=16, H=1024</li><li>BERT-tiny(L=2, A=2, H=128), BERT-mini(L=4, A=4, H=256) 등</li></ul><h2 id=사전-학습>사전 학습
<a class=anchor href=#%ec%82%ac%ec%a0%84-%ed%95%99%ec%8a%b5>#</a></h2><ul><li>대규모 데이터셋으로 학습된 가중치를 활용해 새로운 태스크에 적용 (find-tuning)</li><li>BERT는 MLM과 NSP 태스크를 이용해 거대한 말뭉치를 기반으로 사전 학습</li></ul><h2 id=token-embedding>Token Embedding
<a class=anchor href=#token-embedding>#</a></h2><ul><li>첫 번째 문장의 시작 부분에 [CLS] 토큰 추가</li><li>모든 문장 끝에 [SEP] 토큰 추가</li></ul><h2 id=segment-embedding>Segment Embedding
<a class=anchor href=#segment-embedding>#</a></h2><ul><li>두 문장을 구별하는데 사용</li><li>[SEP] 토큰과 별도로 두 문장을 구분하기 위해 입력 토큰($E_A,E_B$) 제공</li></ul><h2 id=position-embedding>Position Embedding
<a class=anchor href=#position-embedding>#</a></h2><ul><li>단어(토큰)의 위치에 대한 정보 제공</li></ul><h2 id=입력-데이터>입력 데이터
<a class=anchor href=#%ec%9e%85%eb%a0%a5-%eb%8d%b0%ec%9d%b4%ed%84%b0>#</a></h2><ul><li>토큰 임베딩 + 세그먼트 임베딩 + 위치 임베딩 으로 표현</li></ul><h2 id=wordpiece-tokenizer>WordPiece Tokenizer
<a class=anchor href=#wordpiece-tokenizer>#</a></h2><ul><li>하위 단어 토큰화 알고리즘 기반</li><li>Let us start pretraining the model > <code>[let, us, start, pre, ##train, ##ing, the, model]</code></li><li>단어가 어휘 사전에 있으면 토큰으로 사용, 없으면 하위 단어로 분할해 하위 단어가 어휘 사전에 있는지 확인 (OOV 처리에 효과적)</li></ul><h2 id=language-modeling>Language Modeling
<a class=anchor href=#language-modeling>#</a></h2><ul><li>임의의 문장이 주어지고 단어를 순서대로 보면서 다음 단어를 예측하도록 모델 학습</li><li>자동 회귀 언어 모델링: 전방(좌>우) 예측, 후방(좌&lt;우) 예측, 각 방향(단방향)으로 공백까지 모든 단어를 읽음</li><li>자동 인코딩 언어 모델링: 예측하면서 양방향으로 문장을 읽음</li></ul><h2 id=masked-language-modeling-mlm>Masked Language Modeling (MLM)
<a class=anchor href=#masked-language-modeling-mlm>#</a></h2><ul><li>주어진 입력 문장에서 전체 단어의 15%를 무작위로 마스킹, 마스크된 단어를 예측 (빈칸 채우기 태스크)</li><li>[MASK] 토큰을 사전 학습시킬 경우 파인 튜닝 시 입력에 [MASK] 토큰이 없어 불일치가 발생</li><li>15% 토큰에 대해 80%만 [MASK] 토큰으로 교체, 10%는 임의의 토큰(단어)로 교체, 10%는 변경하지 않음<br>(사전 학습과 파인 튜닝 태스크의 차이를 줄이기 위한 일종의 정규화 작업)</li><li>역전파를 통한 반복 학습을 거치며 최적의 가중치 학습</li></ul><h2 id=whole-word-masking-wwm>Whole Word Masking (WWM)
<a class=anchor href=#whole-word-masking-wwm>#</a></h2><ul><li>WWM 방법에서는 하위 단어가 마스킹되면 해당 하위 단어와 관련된 모든 단어를 마스킹</li><li>하위 단어와 관련된 모든 단어의 마스크 비율이 15%를 초과하면 다른 단어의 마스킹을 무시</li></ul><h2 id=next-sentence-prediction-nsp>Next Sentence Prediction (NSP)
<a class=anchor href=#next-sentence-prediction-nsp>#</a></h2><ul><li>BERT에 두 문장을 입력하고 두 번째 문장이 첫 번째 문장의 다음 문장인지 예측</li><li>B 문장이 A 문장에 이어지만 <code>isNext</code>를 반환하고, 그렇지 않으면 <code>notNext</code>를 반환</li><li>두 문장 사이의 관계를 파악해 질문-응답 및 유사문장탐지와 같은 downstream 태스크에서 유용</li><li>한 문서에서 연속된 두 문장을 <code>isNext</code>로 표시하고, 두 문서에서 각각 문장을 가져와 <code>notNext</code>로 표시</li><li>[CLS] 토큰 표현에 소프트맥스 함수를 사용하고 피드포워드 네트워크에 입력해 두 클래스에 대한 확률값 반환</li><li>[CLS] 토큰은 모든 토큰의 집계 표현을 보유하고 있기 때문에 문장 전체에 대한 표현을 담고 있음</li></ul><h2 id=사전-학습-절차>사전 학습 절차
<a class=anchor href=#%ec%82%ac%ec%a0%84-%ed%95%99%ec%8a%b5-%ec%a0%88%ec%b0%a8>#</a></h2><ul><li>lr = 1e-4, b1 = 0.9, b2 = 0.999<br>초기 모델의 큰 변화를 유도하기 위해 웜업으로 1만 스텝 학습<br>(0에서 1e-4로 선형적으로 학습률 증가, 1만 스탭 후 수렴에 가까워짐에 따라 학습률을 선형적으로 감소)</li><li>dropout 0.1, GELU(가우시안 오차 선형 유닛) 활성화 함수 사용</li></ul><hr><h1 id=하위-단어-토큰화-알고리즘>하위 단어 토큰화 알고리즘
<a class=anchor href=#%ed%95%98%ec%9c%84-%eb%8b%a8%ec%96%b4-%ed%86%a0%ed%81%b0%ed%99%94-%ec%95%8c%ea%b3%a0%eb%a6%ac%ec%a6%98>#</a></h1><h2 id=byte-pair-encoding-bpe>Byte Pair Encoding (BPE)
<a class=anchor href=#byte-pair-encoding-bpe>#</a></h2><ul><li>모든 단어를 문자로 나누고 문자 시퀀스로 만듦</li><li>우선 문자 시퀀스에 있는 고유 문자를 어휘 사전에 추가</li><li>어휘 사전 크기에 도달할 때까지 가장 빈도수가 큰 기호 쌍을 반복적으로 병합해 어휘 사전에 추가</li><li>토큰화 시 어휘 사전에 존재하지 않는 단어는 하위 단어로 나눔, 사전에 없는 개별 문자는 <unk>토큰으로 교체</li></ul><h2 id=byte-level-byte-pair-encoding-bbpe>Byte-Level Byte Pair Encoding (BBPE)
<a class=anchor href=#byte-level-byte-pair-encoding-bbpe>#</a></h2><ul><li>문자 수준 시퀀스 대신 바이트 수준 시퀀스를 사용</li><li>유니코드 문자가 바이트로 변환되어 단일 문자 크기는 1~4 바이트가 됨</li><li>바이트 수준에서 빈번한 쌍을 구분해 어휘 사전을 구축</li><li>다국어 설정에서 유용, OOV 단어 처리에 효과적</li></ul><h2 id=wordpiece>WordPiece
<a class=anchor href=#wordpiece>#</a></h2><ul><li>BPE랑 다르게 빈도수 대신 likelihood를 기준으로 기호 쌍을 병합</li><li>모든 기호 쌍에 대해 언어 모델의 가능도를 확인, 가능도가 가장 높은 기호 쌍을 병합</li></ul></article><div class=book-mobile-nav><button class=book-nav-btn3 onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=book-nav-btn3 onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
<button class=book-nav-btn3 onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class=post-tags><a href=/tags/til/ class=tag>#TIL</a>
<a href=/tags/nlp/ class=tag>#NLP</a>
<a href=/tags/bert/ class=tag>#BERT</a></div><div class=post-navigation><a href=/post/study/2022/2022-07/2022-07-19/ class="post-nav-link post-nav-prev"><span class=post-nav-direction><i class="fa-solid fa-backward"></i> PREV</span>
<span class=post-nav-title>2022-07-19 Log</span>
</a><a href=/post/study/2022/2022-07/2022-07-13/ class="post-nav-link post-nav-next"><span class=post-nav-direction>NEXT <i class="fa-solid fa-forward"></i></span>
<span class=post-nav-title>2022-07-13 Log</span></a></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script><div class=book-comments><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://minyeamer.github.io/post/study/2022/2022-07/2022-07-17/",this.page.identifier="https://minyeamer.github.io/post/study/2022/2022-07/2022-07-17/"};(function(){var e=document,t=e.createElement("script");t.src="https://minyeamer.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})();function reloadDisqus(){window.DISQUS&&DISQUS.reset({reload:!0,config:function(){this.page.url="https://minyeamer.github.io/post/study/2022/2022-07/2022-07-17/",this.page.identifier="https://minyeamer.github.io/post/study/2022/2022-07/2022-07-17/"}})}</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#self-attention>Self Attention</a></li><li><a href=#multi-head-attention>Multi-Head Attention</a></li><li><a href=#positional-encoding>Positional Encoding</a></li><li><a href=#feed-forward-network>Feed Forward Network</a></li><li><a href=#인코더-순서>인코더 순서</a></li></ul><ul><li><a href=#masked-multi-head-attention>Masked Multi-Head Attention</a></li><li><a href=#encoder-decoder-attention-layer>Encoder-Decoder Attention Layer</a></li><li><a href=#linear-and-sofmax-layer>Linear and Sofmax Layer</a></li><li><a href=#디코더-순서>디코더 순서</a></li><li><a href=#학습>학습</a></li></ul><ul><li><a href=#사전-학습>사전 학습</a></li><li><a href=#token-embedding>Token Embedding</a></li><li><a href=#segment-embedding>Segment Embedding</a></li><li><a href=#position-embedding>Position Embedding</a></li><li><a href=#입력-데이터>입력 데이터</a></li><li><a href=#wordpiece-tokenizer>WordPiece Tokenizer</a></li><li><a href=#language-modeling>Language Modeling</a></li><li><a href=#masked-language-modeling-mlm>Masked Language Modeling (MLM)</a></li><li><a href=#whole-word-masking-wwm>Whole Word Masking (WWM)</a></li><li><a href=#next-sentence-prediction-nsp>Next Sentence Prediction (NSP)</a></li><li><a href=#사전-학습-절차>사전 학습 절차</a></li></ul><ul><li><a href=#byte-pair-encoding-bpe>Byte Pair Encoding (BPE)</a></li><li><a href=#byte-level-byte-pair-encoding-bbpe>Byte-Level Byte Pair Encoding (BBPE)</a></li><li><a href=#wordpiece>WordPiece</a></li></ul></nav><div class=book-nav><button class=book-nav-btn3 onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=book-nav-btn3 onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
<button class=book-nav-btn3 onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div></div></aside></main></body></html>