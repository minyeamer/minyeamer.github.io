<!doctype html><html lang=ko-kr dir=ltr><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=keywords content="TIL,NLP,Paper"><meta name=description content="Language models paper reviews"><meta name=author content="minyeamer"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><link rel=canonical href=https://minyeamer.github.io/post/study/2022/2022-08/2022-08-26/><meta name=google-site-verification content="u1tWcHmHUZWfFT1cHaku6sqU-bK40N3WLR-C-4VUWN0"><meta name=naver-site-verification content="6eaf8e9da1a6104780f056f1a7797fe5a3a5a0da"><meta property="og:url" content="https://minyeamer.github.io/post/study/2022/2022-08/2022-08-26/"><meta property="og:site_name" content="Minystory"><meta property="og:title" content="2022-08-26 Log"><meta property="og:description" content="Language models paper reviews"><meta property="og:locale" content="ko_kr"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-08-26T20:00:00+09:00"><meta property="article:modified_time" content="2022-08-26T20:00:00+09:00"><meta property="article:tag" content="TIL"><meta property="article:tag" content="NLP"><meta property="article:tag" content="Paper"><title>2022-08-26 Log | Minystory</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://minyeamer.github.io/post/study/2022/2022-08/2022-08-26/><link rel=stylesheet href=/book.min.b806db2fcd7252f443c9368f219d473f25aaf586f28798bf9f651f41a7225060.css integrity="sha256-uAbbL81yUvRDyTaPIZ1HPyWq9Ybyh5i/n2UfQaciUGA=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/search-input.min.25901006165b642d19a996db15ba3fb29bba415c2ffcfe138ac0af40df3ee4eb.js integrity="sha256-JZAQBhZbZC0ZqZbbFbo/spu6QVwv/P4TisCvQN8+5Os=" crossorigin=anonymous></script><link rel=preload href=/search-data.min.b0e1b42f927aec8bb7d54995be39485efe34e26282f96f491b8830611aeb6bbf.json as=fetch crossorigin><script>window.SEARCH_DATA_URL="/search-data.min.b0e1b42f927aec8bb7d54995be39485efe34e26282f96f491b8830611aeb6bbf.json"</script><script defer src=/search.min.f30f9834d4764fd9751da64098c954d01085f648ba9ca421a3c97582f8c47253.js integrity="sha256-8w+YNNR2T9l1HaZAmMlU0BCF9ki6nKQho8l1gvjEclM=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V")</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.1/css/all.min.css crossorigin=anonymous><script defer src=/scroll-progress.min.841ade7e507a5f6d59c4e7bf2fe2b2ca034070677ff7957eec55610a024dd776.js integrity="sha256-hBreflB6X21ZxOe/L+KyygNAcGd/95V+7FVhCgJN13Y=" crossorigin=anonymous></script><script defer src=/dark-mode.min.e41c6440ffd9967d6f6a419ff3ce09b862009fe1646ab265f5cb2817d2a508e3.js integrity="sha256-5BxkQP/Zln1vakGf884JuGIAn+FkarJl9csoF9KlCOM=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlightjs-line-numbers.js/2.9.0/highlightjs-line-numbers.min.js></script><script defer src=/copy-code.min.aaeef965f0b4992e55f976edaecb34a89d414e1791caa18c3f4f4376c6d8b5a8.js integrity="sha256-qu75ZfC0mS5V+Xbtrss0qJ1BTheRyqGMP09DdsbYtag=" crossorigin=anonymous></script><script defer src=/toc-highlightjs.093016f0ef312174ad862fdcf5792e88ab5442bd39beecc38d15643f71ab5c31.min integrity="sha256-CTAW8O8xIXSthi/c9XkuiKtUQr05vuzDjRVkP3GrXDE=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-post book-layout-post"><div class=scroll-progress><div class=scroll-progress-bar></div></div><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><div class=sidebar-profile><div class=profile-img-wrap><a href=https://minyeamer.github.io/><img src="https://avatars.githubusercontent.com/u/17109173?v=4" alt=Profile class=profile-img></a></div><div class=sidebar-social><a href=https://github.com/minyeamer target=_blank title=GitHub><i class="fa-brands fa-github"></i>
</a><a href=/categories/ title=Categories><i class="fa-solid fa-folder"></i>
</a><a href=/tags/ title=Tags><i class="fa-solid fa-tags"></i>
</a><button id=dark-mode-toggle class=dark-mode-toggle aria-label="Toggle dark mode">
<i class="fa-solid fa-circle-half-stroke"></i></button></div></div><h2 class=book-brand><a class="flex align-center" href=/><span>Minystory</span></a></h2><div class="book-search hidden"><div class=search-input-container><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/ onkeydown='event.key==="Enter"&&goToSearchPage()'>
<button type=button id=book-search-button class=book-search-btn onclick=goToSearchPage()>
<i class="fa-solid fa-magnifying-glass"></i></button></div><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden");function goToSearchPage(){const t=document.getElementById("book-search-input"),e=t.value.trim();e&&(window.location.href="/search/?q="+encodeURIComponent(e))}</script><div class=book-categories><input type=checkbox class="hidden toggle" id=categories-control checked>
<label for=categories-control class="categories-toggle categories-link"><a href=/categories/><i class="fa-solid fa-folder"></i>
<span>전체</span>
<span class=category-count>(126)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul class=categories-menu id=categories-menu><li><input type=checkbox class="hidden toggle" id=cat-algorithm>
<label for=cat-algorithm class="categories-toggle categories-link"><a href=/categories/algorithm/><i class="fa-solid fa-folder"></i>
Algorithm
<span class=category-count>(51)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/algorithm/baekjoon/><i class="fa-solid fa-file"></i>
Baekjoon
<span class=category-count>(31)</span></a></li><li class=categories-link><a href=/categories/algorithm/leetcode/><i class="fa-solid fa-file"></i>
LeetCode
<span class=category-count>(2)</span></a></li><li class=categories-link><a href=/categories/algorithm/programmers/><i class="fa-solid fa-file"></i>
Programmers
<span class=category-count>(17)</span></a></li><li class=categories-link><a href=/categories/algorithm/references/><i class="fa-solid fa-file"></i>
References
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-blog>
<label for=cat-blog class="categories-toggle categories-link"><a href=/categories/blog/><i class="fa-solid fa-folder"></i>
Blog
<span class=category-count>(5)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/blog/review/><i class="fa-solid fa-file"></i>
Review
<span class=category-count>(1)</span></a></li><li class=categories-link><a href=/categories/blog/tech/><i class="fa-solid fa-file"></i>
Tech
<span class=category-count>(4)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-book>
<label for=cat-book class="categories-toggle categories-link"><a href=/categories/book/><i class="fa-solid fa-folder"></i>
Book
<span class=category-count>(1)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/book/finance/><i class="fa-solid fa-file"></i>
Finance
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-data>
<label for=cat-data class="categories-toggle categories-link"><a href=/categories/data/><i class="fa-solid fa-folder"></i>
Data
<span class=category-count>(4)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/data/crawling/><i class="fa-solid fa-file"></i>
Crawling
<span class=category-count>(4)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-diary>
<label for=cat-diary class="categories-toggle categories-link"><a href=/categories/diary/><i class="fa-solid fa-folder"></i>
Diary
<span class=category-count>(3)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/diary/2022/><i class="fa-solid fa-file"></i>
2022
<span class=category-count>(2)</span></a></li><li class=categories-link><a href=/categories/diary/2023/><i class="fa-solid fa-file"></i>
2023
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-study>
<label for=cat-study class="categories-toggle categories-link"><a href=/categories/study/><i class="fa-solid fa-folder"></i>
Study
<span class=category-count>(61)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/study/2022/><i class="fa-solid fa-file"></i>
2022
<span class=category-count>(16)</span></a></li><li class=categories-link><a href=/categories/study/2023/><i class="fa-solid fa-file"></i>
2023
<span class=category-count>(10)</span></a></li><li class=categories-link><a href=/categories/study/ai-school/><i class="fa-solid fa-file"></i>
AI SCHOOL
<span class=category-count>(34)</span></a></li><li class=categories-link><a href=/categories/study/datacamp/><i class="fa-solid fa-file"></i>
DataCamp
<span class=category-count>(1)</span></a></li></ul></li></ul></div><div class=recent-posts><div class=recent-posts-header><i class="fa-solid fa-clock"></i>
<span>최신글</span></div><ul class=recent-posts-list><li class=recent-post-item><a href=/post/study/2023/2023-04/2023-04-02/ title="2023-04-02 Log"><div class=recent-post-title>2023-04-02 Log</div><div class=recent-post-date><time datetime=2023-04-02>2023.04.02</time></div></a></li><li class=recent-post-item><a href=/post/data/crawling/10000-recipe/ title="[Python] 만개의 레시피 데이터 수집"><div class=recent-post-title>[Python] 만개의 레시피 데이터 수집</div><div class=recent-post-date><time datetime=2023-03-26>2023.03.26</time></div></a></li><li class=recent-post-item><a href=/post/study/2023/2023-03/2023-03-25/ title="2023-03-25 Log"><div class=recent-post-title>2023-03-25 Log</div><div class=recent-post-date><time datetime=2023-03-25>2023.03.25</time></div></a></li><li class=recent-post-item><a href=/post/study/2023/2023-03/2023-03-21/ title="2023-03-21 Log"><div class=recent-post-title>2023-03-21 Log</div><div class=recent-post-date><time datetime=2023-03-21>2023.03.21</time></div></a></li><li class=recent-post-item><a href=/post/study/2023/2023-02/2023-02-19/ title="2023-02-19 Log"><div class=recent-post-title>2023-02-19 Log</div><div class=recent-post-date><time datetime=2023-02-19>2023.02.19</time></div></a></li></ul></div></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><i class="fa-solid fa-bars book-icon" id=menu-icon></i></label><h3><a href=https://minyeamer.github.io/ class=site-title>Minystory</a></h3><label for=toc-control></label></div><aside class="hidden clearfix"><nav id=TableOfContents></nav><div class=book-nav><button class=book-nav-btn3 onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=book-nav-btn3 onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
<button class=book-nav-btn3 onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div></aside></header><article class="markdown book-article"><header class=post-header><div class=post-header-category><a href=/categories/study/2022/ class=post-header-category-link>Study/2022</a></div><h1 class=post-header-title>2022-08-26 Log</h1><div class=post-header-date><time datetime=2022-08-26T20:00:00+09:00>2022. 8. 26. 20:00</time></div></header><h1 id=nnlm><a href=https://youtu.be/bvSHJG-Fz3Y>NNLM</a>
<a class=anchor href=#nnlm>#</a></h1><ul><li>Word Embedding: 의미적으로 유사한 단어를 가까운 벡터 공간에 표현</li><li>one-hot vector: 단어의 개수만큼의 공간을 표현, 모든 단어간 유사도가 0에 가까움</li><li>어떤 feature vector 표현이 좋고, sequence들의 probability가 높음을 학습</li><li>chain rule에 기반해 이전 단어에 대한 다음 단어의 확률을 계산,<br>단어의 개수가 많아질수록 과거로 봐야할 단어의 수가 많아지기 때문에,<br>Markov assumption을 적용해 n개의 단어만 참조 (n-grams)</li><li>input이 hidden node를 거치지 않고 output으로 연결되는 skip-connection이 존재할 수 있음 (optional)</li><li>NNLM의 목적은 윈도우 내에서 t번째 단어에 대해 t-1번째 단어의 확률이 극대화되는 것을 학습</li></ul><h1 id=word2vec><a href=https://youtu.be/s2KePv-OxZM>Word2Vec</a>
<a class=anchor href=#word2vec>#</a></h1><ul><li>CBOW: 주변 단어를 가지고 중심 단어를 예측, Skip-gram: 중심 단어를 가지고 주변 단어 예측</li><li>CBOW는 gradient가 각각의 주변 단어에 분산되지만,<br>Skip-gram은 주변 단어의 gradient를 합산하는 방향으로 학습하여 성능이 좋음</li><li>activation function이 존재하지 않는 linear 구조</li><li>Skip-gram의 목적 함수는 t번째 단어에 대해 좌우로 n개의 단어에 대한 확률 합의 최대화</li><li>대상 단어 벡터와 특정 단어 벡터 간 내적을 하고 총합에 softmax를 취함</li><li>빈번한 단어는 한쌍으로 묶고, 빈도가 높은 단어의 학습을 제거해 계산을 간소화</li><li>negative sampling: output 단어에 대한 softmax 값을 계산하기 위해 나머지 모든 것들에 대한 내적값을 계산해야 하는데,<br>k 개의 예시에 대해서만 sampling하여 계산의 효율성을 추구</li></ul><h1 id=fasttext><a href=https://youtu.be/7UA21vg4kKE>FastText</a>
<a class=anchor href=#fasttext>#</a></h1><ul><li>형태론적 feature 정보를 한 단어의 subwork unit, 문자 단위에서 추출하는 기법</li><li>기존 방법론은 모든 단어를 각각의 vector로 표현하는 것에 한계가 있고 (OOV 문제),<br>단어의 내부적 구조를 무시하여 유사한 형태의 단어를 표현하지 못함</li><li>skip-gram 기반에 문자 단위 character n-gram을 활용</li><li>기존 모델의 경우 corpus 대비 context word에 대해서만 학습해 비효율적인 연산이 많은데,<br>negative sampling을 적용해 일정한 확률값으로 뽑인 word를 참고</li><li>공통된느 형태소들에 대해 parameter sharing을 하여 embedding에 반영 (의미 공유)</li><li>단어의 시작과 끝 표현 &lsquo;&lt;&rsquo;, &lsquo;>&lsquo;을 추가하고 n개 문자 단위의 벡터를 사용해 계산하며,<br>n-gram이 커지는 문제를 방지하기 위해 hasing function으로 hash값 계산</li></ul><h1 id=lstm><a href=https://youtu.be/bX6GLbpw-A4>LSTM</a>
<a class=anchor href=#lstm>#</a></h1><ul><li>RNN에서 새로운 가중치는 기존 가중치 W - lr * W에 대한 미분값의 chain rule인데,<br>hidden state가 많아질수록 새로운 weight가 기존 weight와 거의 차이가 없어짐</li><li>memory cell을 추가해 새로운 input에 대해 과거의 정보에 대해 몇 퍼센트를 기억할지 결정하고 나머지 정보를 제거</li><li>일부가 제거된 cell state는 새로운 input에 대한 hidden state와 더해지고,<br>이것이 다시 hidden state와 곱해져 다음 hidden state를 생성</li></ul><h1 id=seq2seq><a href=https://youtu.be/4DzKM0vgG1Y>Seq2Seq</a>
<a class=anchor href=#seq2seq>#</a></h1><ul><li>LSTM을 활용한 효율적인 기계 번역 아키텍쳐</li><li>전통적인 RNN 기반의 기계 번역은 입력과 출력의 크기가 같다고 가정 (입력 토큰과 출력 토큰이 대응)</li><li>위 문제를 해결하기 위해 인코더에서 고정된 크기의 context vector를 추출,<br>디코더가 문맥 벡터로부터 번역 결과를 추론 (인코더와 디코더는 서로 다른 파라미터를 가짐)</li><li>시작 시점에 대한 토큰 &lt;sos>와 종료 시점에 대한 토큰 &lt;eos>를 추가하여,<br>종료 토큰을 생성할때까지 반복문을 반복</li><li>입력 문장의 순서를 거꾸로 했을 때 더 높은 정확도를 보임 (앞쪽에 위치한 단어끼리 연관성이 높음)</li><li>Attention을 적용한 Seq2Seq의 경우 모든 hidden states를 디코더로 전달하여,<br>필요한 hidden state를 선택</li></ul><h1 id=elmo><a href=https://youtu.be/zV8kIUwH32M>ELMo</a>
<a class=anchor href=#elmo>#</a></h1><ul><li>ELMo에서의 embedding vector는 bi-directional LSTM에서 가져옴</li><li>forward 및 backward의 각 단계별 hidden state를 concatenate하고,<br>각각의 벡터에 대한 가중합을 통해 embedding을 생성</li><li>특정 task에 대한 가중합 embedding과 task에 대한 scale vector를 곱해 계산</li><li>forward LM의 경우 k번째 토큰의 j번째 hidden state를 사용하며,<br>마지막 hidden state가 t+1 단어를 예측하는데 사용</li><li>backward LM의 경우 역방향으로 t-1 단어를 예측</li><li>bi-directional LSTM은 forward와 backward에서의 정확도를 함께 최대화</li><li>2L+1개의 표현 R에 가중합을 취해 하나의 벡터로 만드는데, 태스크에 맞는 j번째 layer에 가중칠르 두고 전반적으로 스케일을 취함</li><li>가중합을 취하는 방식에 대해선 task별로 각각 취하는 것이 가장 좋고,<br>1/(L+1)로 통합하는 것이 다음, 마지막 hidden state의 가중치를 사용하는 것이 다음으로 좋음</li><li>ELMo의 조합은 downstream task 모델의 input과 output 양쪽에 붙이는게 가장 좋고,<br>input에 붙이는 것, output에 붙이는 것 순으로 좋음</li></ul><h1 id=gpt><a href=https://youtu.be/o_Wl29aW5XM>GPT</a>
<a class=anchor href=#gpt>#</a></h1><ul><li>unlabeled text 데이터가 labeled text 데이터보다 훨씬 많기 때문에,<br>사전학습을 우선 수행하고 labeled 데이터에 대해 fine-tuning을 수행하면 더욱 도움이 될 것</li><li>transformer의 decoder block을 사용하며,<br>encoder-decoder 간의 masked multi-head attention 없이 단순히 쌓아올림</li><li>ELMo는 bi-directional LSTM을 사용하는 반면, GPT는 마스크된 forward를 사용해 다음 단어를 예측</li><li>일반적인 LM은 전 단계까지의 시퀀스에 대해 i번째 토큰의 likelihood를 최대화하는 것이 목적</li><li>토큰 임베딩과 포지션 임베딩을 더해 첫번째 hidden state를 만들고,<br>l-1 번째 hidden state를 decoder block을 통과시켜 l번째 hidden state 생성</li><li>GPT는 입력 단어에 대해 정방향으로 예측을 수행하기 때문에 BERT처럼 masking할 필요가 없음</li><li>BERT와 달리 각각의 토큰이 생성되면, 그 다음 토큰을 생성하기 위해 해당 토큰이 사용되는 auto-regressive 방식을 가짐</li></ul><h1 id=gpt-3><a href=https://youtu.be/xNdp3_Zrr8Q>GPT-3</a>
<a class=anchor href=#gpt-3>#</a></h1><ul><li>transformer 모델에서 log loss는 scale이 커질수록 개선되고, context도 마찬가지로 scale이 커질수록 정보가 향상됨</li><li>example이 많을수록 성능이 향상되는데 scale이 클수록 차이가 도드라짐</li><li>기존엔 각각의 example에 대한 결과를 보면서 gradient를 update하면서 fine-tuning 하는데,<br>GPT-3는 zero-shot일 경우 task description과 prompt를 주고,<br>one-shot일 경우 하나의 example, few-shot일 경우 여러 개의 example을 줌 (fine-tuning을 다시 하지 않음)</li><li>fine-tuning은 새로운 데이터셋이 필요하면서, 일반화 성능이 떨어지고 과적합 문제가 발생,<br>few-shot learning은 task-specific 데이터를 사용하지 않지만 SOTA보다는 성능이 떨어짐</li><li>기존 transformer는 이전 토큰에 대해 모두 attention이 걸리는데,<br>sparse transformer를 사용해 개선</li><li>학습 데이터와 테스트 데이터 간 overlap이 존재하는데, 비용이 많이 들어 해결하지 못함</li><li>문서 레벨에서 특정 표현을 반복하는 경우, 매우 긴 문장의 경우 등 특정 task에 대해 성능이 떨어짐</li><li>bidirectional이 아닌 auto-regressive한 구조적 단점</li></ul></article><div class=book-mobile-nav><button class=book-nav-btn3 onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=book-nav-btn3 onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
<button class=book-nav-btn3 onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class=post-tags><a href=/tags/til/ class=tag>#TIL</a>
<a href=/tags/nlp/ class=tag>#NLP</a>
<a href=/tags/paper/ class=tag>#Paper</a></div><div class=post-navigation><a href class="post-nav-link post-nav-prev post-nav-disabled"><span class=post-nav-direction><i class="fa-solid fa-backward"></i> PREV</span>
<span class=post-nav-title>이전 게시글이 없습니다</span>
</a><a href=/post/study/2022/2022-07/2022-07-21/ class="post-nav-link post-nav-next"><span class=post-nav-direction>NEXT <i class="fa-solid fa-forward"></i></span>
<span class=post-nav-title>2022-07-21 Log</span></a></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script><div class=book-comments><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://minyeamer.github.io/post/study/2022/2022-08/2022-08-26/",this.page.identifier="https://minyeamer.github.io/post/study/2022/2022-08/2022-08-26/"};(function(){var e=document,t=e.createElement("script");t.src="https://minyeamer.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})();function reloadDisqus(){window.DISQUS&&DISQUS.reset({reload:!0,config:function(){this.page.url="https://minyeamer.github.io/post/study/2022/2022-08/2022-08-26/",this.page.identifier="https://minyeamer.github.io/post/study/2022/2022-08/2022-08-26/"}})}</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div></main></body></html>