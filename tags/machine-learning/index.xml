<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Machine Learning on Minystory</title>
    <link>https://minyeamer.github.io/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on Minystory</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 13 Apr 2022 22:13:00 +0900</lastBuildDate><atom:link href="https://minyeamer.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[AI SCHOOL 5기] 머신 러닝 실습 - Pipeline</title>
      <link>https://minyeamer.github.io/blog/aischool-06-09-pipeline/</link>
      <pubDate>Wed, 13 Apr 2022 22:13:00 +0900</pubDate>
      
      <guid>https://minyeamer.github.io/blog/aischool-06-09-pipeline/</guid>
      <description>Feature Transformer Import Libraries 1 2 3 from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline ColumnTransformer 1 2 3 4 5 6 7 8 9 10 numeric_features = [&amp;#39;CRIM&amp;#39;, &amp;#39;ZN&amp;#39;, &amp;#39;INDUS&amp;#39;, &amp;#39;NOX&amp;#39;, &amp;#39;RM&amp;#39;, &amp;#39;AGE&amp;#39;, &amp;#39;DIS&amp;#39;, &amp;#39;TAX&amp;#39;, &amp;#39;PTRATIO&amp;#39;, &amp;#39;B&amp;#39;, &amp;#39;LSTAT&amp;#39;] numeric_transformer = StandardScaler() categorical_features = [&amp;#39;CHAS&amp;#39;, &amp;#39;RAD&amp;#39;] categorical_transformer = OneHotEncoder(categories=&amp;#39;auto&amp;#39;) preprocessor = ColumnTransformer( transformers=[ (&amp;#39;num&amp;#39;, numeric_transformer, numeric_features), (&amp;#39;cat&amp;#39;, categorical_transformer, categorical_features)]) OneHotEncoder()의 handle_unknown 설정
error: 숫자로 변환된 분류형 범주에 새로운 문자열 데이터가 들어올 경우 에러를 발생시킴 ignore: 카테고리에 해당되는 번호가 없으면 자동으로 0으로 바꿈 Preprocessing-Only 1 preprocessor_pipe = Pipeline(steps=[(&amp;#39;preprocessor&amp;#39;, preprocessor)]) steps: 전처리 도구를 순서대로 적용 (모델도 입력 가능) Model Fitting 1 2 3 4 preprocessor_pipe.</description>
    </item>
    
    <item>
      <title>[AI SCHOOL 5기] 머신 러닝 실습 - Model Stacking</title>
      <link>https://minyeamer.github.io/blog/aischool-06-08-model-stacking/</link>
      <pubDate>Wed, 13 Apr 2022 22:10:00 +0900</pubDate>
      
      <guid>https://minyeamer.github.io/blog/aischool-06-08-model-stacking/</guid>
      <description>Model Stacking 서로 다른 모델들을 모으고 Ensemble 기법을 사용해 개선된 모델을 만드는 것 기존 모델들로부터 예측 결과를 도출하는 1st Stage와
이를 기반으로 추가적인 판단을 진행하는 2nd Stage로 나뉨 1st Stage train_X를 가지고 1번 모델을 Training Training을 거친 1번 모델에 train_X를 넣었을 때 결과(예측값)을 저장 다른 모델에도 동일한 작업을 했을 때 나온 1열의 예측값들을 묶어 S_train을 생성 (기존 Ensemble은 S_train을 행별로 투표해서 분류함) 2nd Stage 새로운 모델 생성 (1st Stage에서 사용한 것과 다른 모델 사용 가능) S_train_X, train_Y를 가지고 새로운 모델을 Training Test Model test_X를 1st Stage 모델에 넣고 결과로 나온 예측값들의 묶음 S_test를 생성 (2nd Stage 모델의 학습 데이터는 원본 데이터와 다르기 때문에 test_X를 바로 넣으면 안됨) S_train_X, train_Y를 2nd Stage 모델에 넣었을 때 결과를 가지고 Accuracy 계산 Functional API Import Library 1 from vecstack import stacking 1st Level Models 1 2 3 4 models = [ ExtraTreesClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3), RandomForestClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3), XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.</description>
    </item>
    
    <item>
      <title>[AI SCHOOL 5기] 머신 러닝 실습 - PCA</title>
      <link>https://minyeamer.github.io/blog/aischool-06-07-pca/</link>
      <pubDate>Wed, 13 Apr 2022 22:03:00 +0900</pubDate>
      
      <guid>https://minyeamer.github.io/blog/aischool-06-07-pca/</guid>
      <description>Principal Component Analysis 차원 축소를 통해 최소 차원의 정보로 원래 차원의 정보를 모사하는 알고리즘 데이터의 열의 수가 많아 학습 속도가 느려질 때 열의 수를 줄이기 위해 사용 Dimension Reduction: 고차원 벡터에서 일부 차원의 값을 모두 0으로 만들어 차원을 줄임 원래의 고차원 벡터의 특성을 최대한 살리기 위해 가장 분산이 높은 방향으로 회전 변환 진행 전체 데이터를 기반으로 분산이 가장 큰 축을 찾아 PC 1으로 만들고,
PC 1에 직교하는 축 중에서 분산이 가장 큰 축을 PC 2로 만드는 과정 반복 정보의 누락이 있기 때문에 경우에 따라 모델의 성능 하락 발생 Feature Selection: 기존에 존재하는 열 중에 n개를 선택 Feature Extraction: 기존에 있는 열들을 바탕으로 새로운 열들을 만들어냄 (차원 축소) Learning Process Import Libraries 1 2 from sklearn import decomposition from sklearn import datasets Load Model 1 2 3 iris = datasets.</description>
    </item>
    
    <item>
      <title>[AI SCHOOL 5기] 머신 러닝 실습 - K-Means</title>
      <link>https://minyeamer.github.io/blog/aischool-06-06-k-means/</link>
      <pubDate>Wed, 13 Apr 2022 21:50:00 +0900</pubDate>
      
      <guid>https://minyeamer.github.io/blog/aischool-06-06-k-means/</guid>
      <description>1. K-Means Algorithm K는 전체 데이터를 몇 개의 그룹으로 묶어낼 것인지 결정하는 상수 어떤 K 값이 적절한 것인지 파악하는 것이 중요 각각의 데이터마다 중심값까지의 거리를 계속 물어보기 때문에 계산량이 많음 클러스터링 성능을 향상시키기 위해 GPU Accelerated t-SNE for CUDA 활용 Clustering Process K개의 임의의 중심값을 선택 각 데이터마다 중심값까지의 거리를 계산하여 가까운 중심값의 클러스터에 할당 각 클러스터에 속한 데이터들의 평균값으로 각 중심값을 이동 데이터에 대한 클러스터 할당이 변하지 않을 때까지 2와 3을 반복 2.</description>
    </item>
    
    <item>
      <title>[AI SCHOOL 5기] 머신 러닝 실습 - Kernelized SVM</title>
      <link>https://minyeamer.github.io/blog/aischool-06-05-kernelized-svm/</link>
      <pubDate>Wed, 13 Apr 2022 21:19:00 +0900</pubDate>
      
      <guid>https://minyeamer.github.io/blog/aischool-06-05-kernelized-svm/</guid>
      <description>Support Vector Machine 패턴 인식을 위한 지도 학습 모델 데이터를 분류하는 Margin을 최대화하는 결정 경계(Decision Boundary)를 찾는 기법 결정 경계와 가장 가까운 데이터를 가로지르는 선을 기준으로 Plus &amp;amp; Minus Plane 설정 Support Vector: 결정 경계와 가장 가까운 데이터의 좌표 Margin: b11(plus-plane)과 b12(minus-plane) 사이의 거리, 2/w 기존의 Hard Margin SVM은 소수의 Noise로 인해 결정 경계를 찾지 못할 수 있음 Plus &amp;amp; Minus Plane에 약간의 여유 변수를 두어 에러를 무시하는 Soft Margin SVM로 발전 arg min $$arg\ min\lbrace\frac{1}{2}{||w||}^2+C\Sigma^n_{i=1}\xi_i\rbrace$$ $$\text{단, }y_i({w}\cdot{x_i}-b)\ge{1-\xi_i},\quad{\xi_i\ge{0}},\quad{\text{for all }1\le{i}\le{n}}$$</description>
    </item>
    
    <item>
      <title>[AI SCHOOL 5기] 머신 러닝 실습 - KNN</title>
      <link>https://minyeamer.github.io/blog/aischool-06-04-knn/</link>
      <pubDate>Wed, 13 Apr 2022 20:57:00 +0900</pubDate>
      
      <guid>https://minyeamer.github.io/blog/aischool-06-04-knn/</guid>
      <description>K-Nearest Neightbor Algorithm 기존의 가까운 이웃 데이터를 살펴 새로운 데이터를 분류하는 알고리즘 K=3일 경우, 가장 가까운 나머지 3개 중 2개가 Red면 Red로 판단 K 값이 작아질수록 아주 작은 영향에로 판단이 바뀌는 Overfitting 발생 K 값이 커질수록 멀리보고 결정이 느려져 Overfitting 감소 Learning Process Load Data 1 iris = datasets.load_iris() # 붓꽃 데이터 (150행, 4열) Select Feature 1 2 x = iris.data[:, :2] # [꽃받침 길이, 꽃받침 넓이] y = iris.target Create Model 1 model = neighbors.</description>
    </item>
    
    <item>
      <title>[AI SCHOOL 5기] 머신 러닝 실습 - Gradient Boosting</title>
      <link>https://minyeamer.github.io/blog/aischool-06-03-gradient-boosting/</link>
      <pubDate>Wed, 13 Apr 2022 20:51:00 +0900</pubDate>
      
      <guid>https://minyeamer.github.io/blog/aischool-06-03-gradient-boosting/</guid>
      <description>XG Boost Extreme Gradient Boosting 대용량 분산 처리를 위한 Gradient Boosting 라이브러리 Decision Tree(의사결정나무) 에 Boosting 기법을 적용한 알고리즘 AdaBoost는 학습 성능은 좋으나, 모델의 학습 시간이 오래 걸리는 단점 병렬 처리 기법을 적용하여 Gradient Boost보다 학습 속도를 끌어올림 Hyper-Parameter가 너무 많기 때문에 권장 세팅 사용 @ http://j.mp/2PukeTS Decision Tree 이해하기 쉽고 해석도 용이함 입력 데이터의 작은 변동에도 Tree의 구성이 크게 달라짐 과적합이 쉽게 발생 (중간에 멈추지 않으면 Leaf 노드에 하나의 데이터만 남게 됨) 의사결정나무의 문제를 해결하기 위해 Boosting 기법 활용 ex) 테니스를 쳤던 과거 데이터를 보고 날씨 정보를 이용해 의사결정 AdaBoost Adaptive Boosting 데이터를 바탕으로 여러 weak learner 들을 반복적으로 생성 앞선 learner가 잘못 예측한 데이터에 가중치를 부여하고 학습 최종적으로 만들어진 strong learner를 이용하여 실제 예측 진행 에러를 최소화하는 weight를 매기기 위해 경사 하강법 사용 ex) Regression: 평균/가중평균, Classification: 투표 XG Boost References NGBoost Explained (Comparison to LightGBM and XGBoost) Gradient Boosting Interactive Playground Gradient Boosting explained Comparison for hyperparams of XGBoost &amp;amp; LightGBM XGBoost Parameters XG Boost 하이퍼 파라미터 상세 설명 Complete Guide to Parameter Tuning in XGBoost (with python codes) Microsoft EBM (Explainable Boosting Machine) 정형데이터를 위한 인공신경망 모델, TabNet Ensemble 주어진 데이터를 이용하여 여러 개의 서로 다른 예측 모형을 생성한 후,</description>
    </item>
    
    <item>
      <title>[AI SCHOOL 5기] 머신 러닝 실습 - 로지스틱 회귀</title>
      <link>https://minyeamer.github.io/blog/aischool-06-02-logistic-regression/</link>
      <pubDate>Wed, 13 Apr 2022 17:03:00 +0900</pubDate>
      
      <guid>https://minyeamer.github.io/blog/aischool-06-02-logistic-regression/</guid>
      <description>Logistic Regression 이진 분류(0 또는 1) 문제를 해결하기 위한 모델 다항 로지스틱 회귀(k-class), 서수 로지스틱 회귀(k-class &amp;amp; ordinal)도 존재 Sigmoid Function을 이용하여 입력값이 양성 클래스에 속할 확률을 계산 로지스틱 회귀를 MSE 식에 넣으면 지수 함정의 특징 때문에 함정이 많은 그래프가 나옴 분류를 위한 Cost Function인 Cross-Entropy 활용 성능 지표로는 Cross-Entropy 외에 Accuracy 등을 같이 사용 ex) 스팸 메일 분류, 질병 양성/음성 분류 등 양성/음성 분류 모델 선형 모델은 새로운 데이터가 들어오면 양성/음성 판단 기준이 크게 바뀜 모델을 지수 함수인 Sigmoid Function으로 변경 Sigmoid Function θ 값에 따라 기울기나 x축의 위치가 바뀌는 지수 함수 y축을 이동하는 선형 함수와 다르게 x축을 이동 y가 0.</description>
    </item>
    
    <item>
      <title>[AI SCHOOL 5기] 머신 러닝</title>
      <link>https://minyeamer.github.io/blog/aischool-06-00-machine-learning/</link>
      <pubDate>Wed, 13 Apr 2022 16:31:00 +0900</pubDate>
      
      <guid>https://minyeamer.github.io/blog/aischool-06-00-machine-learning/</guid>
      <description>인공지능 Intelligent Agents를 만드는 것 주변 환경들을 인식하고 원하는 행동을 취하여 목표를 성취하는 것 Artificial Narrow Intelligence 제한된 기능만 수행할 수 있는 인공지능 weak AI Artificial General Intelligence 사람만큼 다양한 분야에서 기능을 수행할 수 있는 인공지능 strong AI Artificial Super Intelligence 모든 분야에서 사람보다 뛰어난 인공지능 모델 데이터를 가장 잘 설명할 수 있는 함수 (y = ax + b) 모델에서 θ는 Parameter(가중치, Weight) 의미 모델에서 h(x)는 Hypotheses(가설) 의미 모델에서 b는 Bias(편향, 보정치) 의미 머신러닝 어떠한 과제를 해결하는 과정에서 특정한 평가 기준을 바탕으로 학습의 경험을 쌓는 프로그램 머신러닝 분류 Supervised 입력값에 대한 정답을 예측하기 위해 학습 데이터와 정답이 같이 존재 회귀(Regression): 결과가 실수 영역 전체에서 나타남 분류(Classification): 결과가 특정 분류에 해당하는 불연속값으로 나타남 ex) 주식 가격 예측, 이미지 인식 등 Unsupervised 입력값 속에 숨어있는 규칙성을 찾기 위해 학습 정답이 없는 데이터를 주고 비슷한 집단을 분류 ex) 고객군 분류, 장바구니 분석(Association Rule) 등 Reinforcement Trial &amp;amp; Error를 통한 학습 최종적으로 얻게 될 기대 보상을 최대화하기 위한 행동 선택 정책 학습 각 상태에 대해 결정한 행동을 통해 환경으로부터 받는 보상을 학습 ex) 로봇 제어, 공정 최적화 등 Automated ML 어떤 모델(함수, 알고리즘)을 써야할지를 컴퓨터가 알아서 정하게 함 인공신경망 레이어의 범위, 후보 등을 정해놓고 그 안에서 가장 좋은 조합을 찾음 ex) AutoML Tables (행의 수가 1000건이 넘어야하는 제약) 학습 데이터를 가장 잘 설명하는 방법을 찾는 과정 데이터에 맞는 모델을 찾는 과정 (= Model Fitting) 실제 정답과 예측 결과 사이의 오차(Loss, Cost, Error)를 줄여나가는 최적화 과정 학습 과정 초기 모델에 데이터를 입력 결과를 평가 (예측/분류의 정확도 등) 결과를 개선하기 위해 모델을 수정 (모델 내부 Parameter 수정 등) Model&amp;rsquo;s Capacity 2번 모델은 3번 모델보다 오차가 크지만 새로운 데이터가 생겼을 때 비슷하게 예측 가능 3번 모델은 오차가 가장 적지만 새로운 데이터가 생겼을 때 오차가 매우 커질 수 있음 3번 모델과 같은 Overfitting(과적합)이 발생하기 전에 학습을 멈춤 Cross Validation 새로운 데이터들에 대해서도 좋은 결과를 내게 하기 위해 데이터를 3개 그룹으로 나눠 학습 60%의 Training Data로 모델을 학습 20%의 Validation Data로 모델을 최적화/선택 20%의 Test Data로 모델을 평가 데이터를 분리하는 비율은 모델에 따라 달라짐 K-Fold Cross Validation 후보 모델 간 비교 및 선택을 위한 알고리즘 Training Data를 K 등분하고 그 중 하나를 Validation Data로 설정 K 값은 자체적으로 결정하며 보통 10-Fold 사용 (시간이 없으면 5-Fold) 머신러닝에서 K는 주로 사용자가 결정하는 상수 Stratified: 층화 표집 방법, 데이터의 분류 별 비율이 다르면 K-Fold 조각 안에서 비율을 유지시킴 10-Fold 학습 과정 데이터를 80%의 Training Data와 20%의 Test Data로 나누고 Training Data를 10등분</description>
    </item>
    
    <item>
      <title>[AI SCHOOL 5기] 머신 러닝 실습 - 선형 회귀</title>
      <link>https://minyeamer.github.io/blog/aischool-06-01-linear-regression/</link>
      <pubDate>Wed, 13 Apr 2022 16:31:00 +0900</pubDate>
      
      <guid>https://minyeamer.github.io/blog/aischool-06-01-linear-regression/</guid>
      <description>Linear Regression 종속 변수 y와 독립 변수 x 사이의 선형 상관 관계를 모델링하는 회귀분석 기법 정답이 있는 데이터의 추세를 잘 설명하는 선형 함수를 찾아 x에 대한 y를 예측 Linear Combination (선형 결합): 더하기와 곱하기로만 이루어진 식 단순 회귀분석: 1개의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때 다중 회귀분석: 2개 이상의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때 선형 회귀는 가장 적합한 θ들의 집합을 찾는 것이 목표 Cost Function 예측 값과 실제 값의 차이를 기반으로 모델의 성능(정확도)을 판단하기 위한 함수 Objective (MIN or MAX) 함수 안에 Cost Function이 존재 선형 회귀에서는 Mean Squre(d) Error Function (평균 제곱 오차 함수) 활용 MSE(Cost)가 최소가 되는 θ(a &amp;amp; b)를 찾아야하며,</description>
    </item>
    
  </channel>
</rss>
