[{"id":0,"href":"/blog/hugo-blog-3/","title":"Hugo 블로그 만들기 (3) - 태그, 카테고리 페이지 추가","section":"Posts","content":" 대상 독자\n마크다운으로 작성할 수 있는 나만의 블로그를 만들고 싶은 분들 블로그를 기능적으로 또는 시각적으로 커스터마이징 하고 싶은 분들 Hugo에서 지원하는 taxonomies 기능을 활용한 사례를 찾고 있는 분들 블로그에 2단계 카테고리를 구현하고 싶은 분들 HTML/CSS를 이용한 단일 웹 페이지 구성에 익숙하신 분들 주요 내용\nHugo의 taxonomies 기능과 구성 요소 이해 (Taxonomies) 게시글 목록과 페이지네이션 템플릿 구현 (게시글 목록 템플릿 추가 / 페이지네이션 템플릿 추가) 태그 페이지 추가 및 terms 템플릿 작성 (태그 페이지 추가) 2단계 카테고리 구조 설계 및 부모-자식 카테고리 템플릿 구현 (카테고리 페이지 추가) 메뉴 영역에 카테고리 트리 통합 및 게시글 카운팅 (메뉴에 카테고리 추가) 앞선 게시글에서 Book 테마의 메인 레이아웃인 메뉴, 목차, 헤더를 개선해보았습니다.\n앞에서의 과정을 거쳤다면 프로필 사진, 소셜 링크, 스크롤 이동 버튼 등이 블로그에 추가되었을 것입니다.\n이번 게시글에서는 Hugo의 taxonomies 기능을 활용하여 카테고리와 태그를 커스터마이징하는 방법을 알아보겠습니다.\nTaxonomies # Hugo는 taxonomies 라고 하는 콘텐츠 그룹화 기능을 제공합니다. 해당 기능은 게시글과 같은 콘텐츠를 사용자가 직접 정의하여 분류하기 위해 사용할 수 있는데, 대표적으로 카테고리와 태그가 있습니다.\nTaxonomies Hugo includes support for user-defined taxonomies. gohugo.io Taxonomy 구성 요소 # Hugo에서 Taxonomy는 다음 세 가지 요소로 구성됩니다.\nTaxonomy: 콘텐츠를 분류하는 데 사용할 수 있는 카테고리 체계 (예: tags, categories) Term: Taxonomy 내의 개별 항목 (예: Hugo, Blog, Frontend) Value: Term에 할당된 콘텐츠 (예: 특정 태그가 지정된 게시글) 예를 들어, 이 블로그의 현재 게시글은 다음과 같은 taxonomies를 가지고 있습니다.\nCopy yaml categories: [\u0026#34;Frontend\u0026#34;, \u0026#34;Blog\u0026#34;] tags: [\u0026#34;Hugo\u0026#34;, \u0026#34;태그\u0026#34;, \u0026#34;카테고리\u0026#34;] 여기서 categories와 tags는 Taxonomy이고, Frontend, Blog, Hugo 등은 Term입니다.\n그리고 이 게시글 자체가 이러한 Term들의 Value가 됩니다.\nTaxonomy 할당 방법 # Hugo는 기본적으로 categories 와 tags 두 가지 taxonomy를 제공합니다.\n게시글의 front matter 에 태그나 카테고리를 추가하면, Hugo는 자동으로 다음과 같은 페이지들을 생성합니다.\n(front matter에 대해서는 따로 설명하지 않았는데, 게시글의 최상단에 YAML, TOML 등 형식으로 입력하는 메타데이터 입니다.)\n모든 태그/카테고리를 나열하는 목록 페이지 (예: /categories/, /tags/) 각 태그/카테고리별 게시글 목록 페이지 (예: /categories/blog/, /tags/hugo/) 실제로 Hugo 정적 웹페이지를 만들면 아래와 같은 경로들이 생성되는 것을 확인할 수 있습니다.\nCopy bash public/ ├── asciinema/ ├── categories/ ├── docs/ ├── icons/ ├── katex/ ├── posts/ ├── showcases/ └── tags/ 여기서 categories/ 와 tags/ 경로가 taxonomy.html 및 terms.html 템플릿에 의해 만들어진 결과입니다.\n(Book 테마에서는 두 경로에 대한 레이아웃을 만들지 않았으므로 해당 페이지의 내용은 비어있습니다.)\nHugo에서 이러한 기능을 제공해준다면 그대로 사용해도 되지 않을까 싶지만, 안타깝게도 Hugo는 카테고리와 태그를 1차원적으로밖에 인식하지 못합니다. 태그는 단순히 1차원 배열로 봐도 괜찮지만, 카테고리는 최소 2단계 이상의 깊이가 있는 구조를 지향하므로 직접 커스터마이징할 필요가 있습니다.\n게시글 목록 템플릿 추가 # 카테고리와 태그 페이지는 두 종류로 나눠집니다. 하나는 전체 카테고리 및 태그 목록을 보는 페이지(이하 taxonomy 페이지)이고, 다른 하나는 단일 카테고리 또는 태그에 속하는 게시글 목록을 보여주는 페이지(이하 terms 페이지)입니다.\n이 중 게시글 목록을 보여주는 taxonomy 페이지에서 공통적인 기능과 외관을 가진 부분이 있는데, 해당 부분을 별도의 템플릿으로 구성하고자 합니다.\n게시글 목록 구상하기 # 일반적인 블로그 플랫폼에서 게시글 목록은 세로로 나열됩니다. 아래 이미지는 티스토리에서 볼 수 있는 게시글 목록의 한 부분인데, 게시글 제목, 작성일, 요약과 같은 메타데이터가 좌측에 보여지고 커버 이미지가 우측에 표시됩니다.\n티스토리 블로그에서는 게시글 목록에서 태그를 보여주지 않지만, 이번에 만들 taxonomy 페이지에서는 각 게시글 항목마다 태그를 보여주고 태그를 클릭하면 terms 페이지로 이동하도록 구성할 예정입니다. (카테고리도 마찬가지 입니다.)\n게시글 항목 템플릿 추가 # Book 테마는 layouts/_partials/docs 경로에 partial 템플릿이 위치해 있습니다.\n게시글 목록에서 단일 게시글 항목은 partial 템플릿으로 분류된다 생각하여 해당 경로에 post-item.html 이라는 템플릿을 추가했습니다. 커버 이미지의 위치를 조정하는 등의 CSS 스타일은 assets/_custom.scss 에 적용해줍니다. (CSS 스타일이 상대적으로 길어 주석으로 설명을 대체합니다.)\nHTML Copy html \u0026lt;!-- layouts/_partials/docs/post-item.html --\u0026gt; \u0026lt;article class=\u0026#34;post-item\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;post-meta\u0026#34;\u0026gt; \u0026lt;!-- 게시글 작성일 --\u0026gt; \u0026lt;time datetime=\u0026#34;{{ .Date.Format \u0026#34;2006-01-02\u0026#34; }}\u0026#34;\u0026gt; {{ .Date.Format \u0026#34;2006년 01월 02일\u0026#34; }} \u0026lt;/time\u0026gt; \u0026lt;!-- 카테고리 목록이 추가될 위치 --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;post-content-area{{ if .Params.cover }} has-cover{{ end }}\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;post-text-area\u0026#34;\u0026gt; \u0026lt;!-- 게시글 제목 --\u0026gt; \u0026lt;h2 class=\u0026#34;post-title\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt; \u0026lt;/h2\u0026gt; \u0026lt;!-- 게시글 요약 --\u0026gt; {{ $summary := cond .Description .Description (cond .Summary .Summary \u0026#34;\u0026#34;) }} {{ if $summary }} \u0026lt;div class=\u0026#34;post-summary\u0026#34;\u0026gt; {{ $summary | plainify | truncate 148 }} \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;post-summary-mobile\u0026#34;\u0026gt; {{ $summary | plainify | truncate 72 }} \u0026lt;/div\u0026gt; {{ end }} \u0026lt;!-- 태그 목록이 추가될 위치 --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;!-- 커버 이미지 --\u0026gt; {{ if .Params.cover }} \u0026lt;div class=\u0026#34;post-cover\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;{{ .Params.cover | absURL }}\u0026#34; alt=\u0026#34;Cover Image\u0026#34; class=\u0026#34;post-cover-img\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; {{ end }} \u0026lt;/div\u0026gt; \u0026lt;/article\u0026gt; CSS Copy scss // assets/_custom.scss .post-item { // `post-item` 영역을 명확하게 구분하기 위해 테두리를 추가 border: 1px solid var(--gray-200); border-radius: 8px; padding: $padding-24; background: var(--body-background); transition: box-shadow 0.3s ease, border-color 0.3s ease; \u0026amp;:hover { // `post-item` 영역에 마우스를 올리면 테두리를 링크 색상(파란색)으로 변경 box-shadow: 0 4px 12px var(--box-shadow); border-color: var(--color-link); } } .post-meta { // `post-meta` 영역 내 항목(작성일 및 카테고리)을 정렬하고 연한 글씨색 적용 display: flex; justify-content: space-between; align-items: center; color: var(--gray-500); flex-wrap: wrap; gap: 0.5rem; font-size: 0.9rem; margin-bottom: 0.9rem; time { font-weight: 500; } } .post-content-area { // `post-content-area` 영역 내 항목(제목, 요약 텍스트 및 커버 이미지)을 정렬 display: flex; gap: 1.5rem; align-items: flex-start; \u0026amp;.has-cover .post-text-area { // 게시글이 커버 이미지를 가질 경우 텍스트 영역의 세로 길이를 이미지 높이만큼 고정 // (태그 목록을 영역 하단에 고정시키기 위한 목적) min-height: $preview-height; // 170px; } } .post-text-area { // 제목 텍스트, 요약 텍스트, 태그 목록 요소들을 정렬 flex: 1; min-width: 0; display: flex; flex-direction: column; h2.post-title { margin-top: 0; margin-bottom: $padding-8; line-height: 1.4; } } .post-title { // 제목 텍스트의 스타일을 적용 font-size: $font-size-24; line-height: 1.3; a { color: var(--body-font-color); text-decoration: none; transition: color 0.3s ease; \u0026amp;:hover { color: var(--color-link); } } } .markdown .post-title { // 마크다운 H2 헤더에 기본으로 적용되는 마진을 무시 (여백 줄이기) margin-top: $padding-20; } .post-summary, .post-summary-mobile { // 요약 텍스트의 스타일을 적용 color: var(--gray-800); line-height: 1.6; margin-bottom: 1rem; overflow: hidden; word-wrap: break-word; flex: 1; } .post-summary-mobile { // 기본적으로는 모바일 전용 요약 텍스트를 숨기기 display: none; } .post-cover { // 커버 이미지의 가로, 세로 길이를 고정 width: $preview-width; // 200px; height: $preview-height; // 170px; border-radius: 8px; overflow: hidden; .post-cover-img { // 커버 이미지가 고정된 길이를 넘으면 가운데만 잘라서 표시하기 width: 100%; height: 100%; object-fit: cover; object-position: center; } } @media (max-width: $body-max-width) { .post-title { font-size: $font-size-20; // 모바일 사이즈에서 제목 텍스트 크기를 줄이기 } .post-summary { display: none; // 모바일 사이즈에서 기본 요약 텍스트를 숨기기 } .post-summary-mobile { display: block; // 모바일 사이즈에서 모바일 전용 요약 텍스트를 표시하기 } .post-content-area.has-cover { // 모바일 사이즈에서 커버 이미지 크기를 줄이고 정사각형 비율로 변경 .post-cover { width: $preview-mobile-width; // 160px; height: $preview-mobile-height; // 160px; } .post-text-area { min-height: $preview-mobile-height; // 160px; } } } 하나의 게시글 항목은 게시글 작성일, 제목, 요약, 커버 이미지로만 구성되어 있습니다.\n아직 게시글 항목을 볼 수 있는 페이지를 만들지 않아서 해당 템플릿을 직접 확인하기 어려울 것이기에 실제로 템플릿을 렌더링한 결과를 보여드립니다. (커버 이미지는 해당 게시글의 커버 이미지를 첨부했습니다.)\n간단하게 설명하자면, 게시글 요약은 게시글이 가진 Description 또는 Summary 를 가져와 일정 글자 수까지 잘라서 표시합니다. Description 은 front matter에서 작성하는 게시글에 대한 설명문이고, Summary 는 마찬가지로 front matter에서 지정할 수도 있지만 따로 지정하지 않으면 게시글의 본문 텍스트를 일정 글자 수까지 제공합니다.\n게시글 요약에 표시할 글자 수의 경우 여러가지 숫자를 넣어봤을 때 148자가 적절했고, 모바일의 경우 그보다 더 적은 72자 정도가 적절했습니다. 이어지는 CSS 설정에서 볼 수 있는데, 화면의 너비에 따라 일정 크기를 초과하면 post-summary 요소만 표시하고 일정 크기 이하로 줄어들면 post-summary-mobile 요소만 표시하는 원리입니다.\n커버 이미지를 불러오는 부분은 Hugo에서 이미지가 문자열로 전달되는 경우와 맵 형태로 전달되는 경우를 고려하여 작성되었습니다. 특별히 맵 형태의 이미지를 의식한 것은 아니고, Book 테마의 다른 템플릿에서 이런식으로 커버 이미지를 불러와서 그대로 가져다 썼습니다. front matter에서 커버 이미지에 대한 주소를 작성하면 img.post-cover-img 요소의 src 속성으로 해당 이미지 주소가 전달됩니다.\n이후에 카테고리와 태그 페이지를 추가하면서 게시글 항목의 우측 상단인 커버 이미지 위에 카테고리 목록을, 좌측 하단인 요약 텍스트 아래에 태그 목록을 추가할 예정입니다.\n페이지네이션 템플릿 추가 # 또 하나로 고려해야 할 것은 페이지네이션 입니다. 앞으로 게시글들이 계속 추가될건데, 그 많은 게시글들을 하나의 페이지에서 모두 보여주게 되면 보기도 불편할 것이며 장기적으로 게시글들을 불러오는 속도를 감당하지 못할 것입니다. 페이지네이션은 이런 문제를 개선하기 위해 게시글들을 여러 페이지로 나눠서 일정 개수만 표시하는 방식입니다.\n페이지네이션 구상하기 # 마찬가지로 티스토리 블로그를 보면 아래 이미지와 같이 페이지네이션 기능을 제공합니다.\n일반적인 게시판 페이지에서는 10개 단위로 페이지를 표시하는데 티스토리 블로그는 현재 페이지에서 앞뒤로 3개의 페이지만 보여줍니다. 이번에 만들 taxonomy 페이지에서는 일반적인 게시판처럼 10개 단위로 페이지를 표시할 것입니다.\n페이지네이션 템플릿 추가 # Book 테마에서는 기본적으로 pagination.html 템플릿을 제공하고 해당 템플릿은 Hugo의 Paginator라는 기능을 사용합니다.\nCopy html \u0026lt;!-- layouts/_partials/docs/pagination.html --\u0026gt; {{- if .Paginator -}} \u0026lt;!-- ... --\u0026gt; {{- end -}} 하지만, Paginator 기능에 대한 공식문서를 보면 다음과 같은 제약사항을 안내하고 있습니다.\nAlthough simple to invoke, with the Paginator method you can neither filter nor sort the page collection. It acts upon the page collection received in context. The Paginate method is more flexible, and strongly recommended. 페이지 컬렉션, 즉 게시글 목록을 필터하거나 정렬할 수 없다는건데 카테고리 또는 태그 관련 게시글을 페이지 단위로 나열할 때는 전체 게시글 중 관련된 일부 게시글만 필터할 필요가 있어서 Paginate라는 다른 기능을 사용할 것입니다.\n다음과 같이 paginate.html 템플릿을 변경해줍니다. (CSS 스타일도 assets/_custom.scss 에 추가해줍니다.)\nHTML Copy html \u0026lt;!-- layouts/_partials/docs/pagination.html --\u0026gt; {{ $paginate := . }} {{ if gt $paginate.TotalPages 1 }} \u0026lt;div id=\u0026#34;pagination-anchor\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;nav class=\u0026#34;pagination\u0026#34;\u0026gt; {{ $currentPage := $paginate.PageNumber }} {{ $totalPages := $paginate.TotalPages }} \u0026lt;!-- 10개 게시글 단위로 페이지를 그룹화 --\u0026gt; {{ $groupNumber := div (sub $currentPage 1) 10 }} {{ $groupStart := add (mul $groupNumber 10) 1 }} {{ $groupEnd := add $groupStart 9 }} {{ if gt $groupEnd $totalPages }} {{ $groupEnd = $totalPages }} {{ end }} \u0026lt;!-- 이전 및 다음 페이지 그룹을 계산 --\u0026gt; {{ $prevGroupPage := sub $groupStart 10 }} {{ $nextGroupPage := add $groupEnd 1 }} \u0026lt;!-- 이전 페이지 그룹으로 이동하는 링크 --\u0026gt; {{ if gt $groupStart 1 }} \u0026lt;a href=\u0026#34;{{ $paginate.First.URL }}{{ if gt $prevGroupPage 1 }}page/{{ $prevGroupPage }}/{{ end }}#pagination-anchor\u0026#34; class=\u0026#34;pagination-nav pagination-link\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa-solid fa-backward\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; 이전 \u0026lt;/a\u0026gt; {{ else }} \u0026lt;span class=\u0026#34;pagination-nav disabled\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa-solid fa-backward\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; 이전 \u0026lt;/span\u0026gt; {{ end }} \u0026lt;!-- 현재 페이지 그룹의 페이지로 이동하는 링크 --\u0026gt; \u0026lt;div class=\u0026#34;pagination-pages\u0026#34;\u0026gt; {{ range $i := seq $groupStart $groupEnd }} {{ if eq $i $currentPage }} \u0026lt;span class=\u0026#34;pagination-page current\u0026#34; id=\u0026#34;current-page\u0026#34;\u0026gt;{{ $i }}\u0026lt;/span\u0026gt; {{ else }} \u0026lt;a href=\u0026#34;{{ $paginate.First.URL }}{{ if gt $i 1 }}page/{{ $i }}/{{ end }}#pagination-anchor\u0026#34; class=\u0026#34;pagination-page pagination-link\u0026#34;\u0026gt; {{ $i }} \u0026lt;/a\u0026gt; {{ end }} {{ end }} \u0026lt;/div\u0026gt; \u0026lt;!-- 다음 페이지 그룹으로 이동하는 링크 --\u0026gt; {{ if le $nextGroupPage $totalPages }} \u0026lt;a href=\u0026#34;{{ $paginate.First.URL }}page/{{ $nextGroupPage }}/#pagination-anchor\u0026#34; class=\u0026#34;pagination-nav pagination-link\u0026#34;\u0026gt; 다음 \u0026lt;i class=\u0026#34;fa-solid fa-forward\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/a\u0026gt; {{ else }} \u0026lt;span class=\u0026#34;pagination-nav disabled\u0026#34;\u0026gt; 다음 \u0026lt;i class=\u0026#34;fa-solid fa-forward\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/span\u0026gt; {{ end }} \u0026lt;/nav\u0026gt; {{ end }} CSS Copy scss // assets/_custom.scss .pagination { // 이전 버튼, 페이지 링크, 다음 버튼을 정렬 display: flex; justify-content: center; align-items: center; gap: $padding-4; padding: 2rem 0; margin-top: 2rem; border-top: 1px solid var(--gray-200); max-width: 100%; .pagination-nav, .pagination-page { // 각각의 페이지 링크에 대한 스타일을 적용 // (페이지 번호를 모서리가 둥근 사각형 테두리로 감싸기) padding: $padding-8; border: 1px solid var(--gray-200); border-radius: 4px; flex-grow: 0; flex-shrink: 0; text-align: center; white-space: nowrap; \u0026amp;:hover { background: var(--gray-200); border-color: var(--color-link); } } } .markdown { // 마크다운 a 태그에 기본으로 적용되는 색상을 무시 a[href].pagination-link { color: var(--body-font-color); text-decoration: none; } a[href]:hover.pagination-link { color: var(--body-font-color); text-decoration: none; } a[href]:visited.pagination-link { color: var(--body-font-color); text-decoration: none; } } .pagination-nav { width: 4rem; \u0026amp;.disabled { // 이전, 다음 페이지 그룹이 없으면 버튼을 비활성화 border: 1px solid var(--gray-200); color: var(--gray-500); cursor: not-allowed; } i { // 이전, 다음 버튼의 아이콘 크기 설정 font-size: $font-size-12; } } .pagination-pages { // 페이지 링크들을 정렬 display: flex; gap: $padding-4; .pagination-page { min-width: 2.5rem; \u0026amp;.current { // 현재 페이지에 대한 링크를 강조 background: var(--color-page-link); border-color: var(--color-link); cursor: default; } } } #pagination-anchor { // 페이지 이동 후 스크롤을 유지하기 위한 앵커 요소 숨기기 position: relative; top: -100px; visibility: hidden; } @media (max-width: $body-max-width) { // 모바일 사이즈에서는 이전, 다음 버튼을 페이지 링크 아래로 배치 .pagination { flex-wrap: wrap; gap: $padding-8; } .pagination-pages { flex-wrap: wrap; justify-content: center; width: 100%; } .pagination-nav { order: 2; } } 페이지네이션 템플릿을 렌더링하면 아래 이미지처럼 보입니다.\n페이지네이션 템플릿은 이전 버튼, 페이지 링크 10개, 다음 버튼이 순서대로 나열되어 있습니다.\n페이지를 이동하는건 현재 경로 뒤에 /page/{페이지} 를 붙이면 됩니다.\n(Paginate 기능을 사용했다면 렌더링 시에 각 페이지별 정적 페이지가 만들어집니다.)\n페이지 링크는 이를 이용해 특정 페이지에 대한 경로로 이동할 수 있는 링크를 제공합니다. 또한, 다른 페이지로 이동한 후에도 스크롤 위치를 유지하도록 페이지네이션 영역에 대한 앵커 링크 #pagination-anchor 를 추가했습니다.\n이전 버튼과 다음 버튼은 직전, 직후 페이지로 이동하는게 아니라 이전, 다음 페이지 그룹으로 이동하는 링크를 제공합니다. 페이지 그룹은 10개 페이지 단위로 구성되어 있으며, 해당 버튼은 페이지 그룹의 첫 번째 페이지로 연결됩니다.\nTaxonomy 템플릿 경로 # 필요한 partial 템플릿들을 만들었으니 본격적으로 taxonomy 및 terms 템플릿을 만들겠습니다.\n그전에, 어디에 템플릿을 만들어야 하는지 알아보겠습니다.\nTaxonomy Templates Taxonomy templating includes taxonomy list pages, taxonomy terms pages, and using taxonomies in your single page … gohugobrasil.netlify.app 위 Hugo 공식문서를 참고해보면 다음과 같은 우선순위로 taxonomy 템플릿을 탐색한다는 것을 알 수 있습니다.\nlayouts/taxonomy/\u0026lt;SINGULAR\u0026gt;.html layouts/_default/taxonomy.html layouts/_default/list.html 동일하게, terms 템플릿은 다음과 같은 우선순위로 탐색합니다.\nlayouts/taxonomy/\u0026lt;SINGULAR\u0026gt;.terms.html layouts/_default/terms.html Book 테마에서 terms 템플릿은 제공되지 않지만 taxonomy 템플릿은 layouts/_partials/docs/taxonomy.html 경로에서 제공됩니다. 해당 taxonomy 템플릿은 태그와 카테고리에 공통적으로 적용됩니다.\n하지만, 태그와 카테고리에 대한 페이지 구성에 차이가 있어서 전용 템플릿을 만들어야 합니다. 이 경우에는 layouts/tags/ 및 layouts/categories/ 경로를 활용할 수 있습니다.\n태그 페이지 추가 # 태그 페이지는 taxonomy 템플릿을 먼저 만들고 terms 템플릿을 만들겠습니다.\n태그 전용 템플릿을 만들기 위해 layouts/tags/ 경로 아래에 두 개의 템플릿 파일을 추가하면 됩니다.\ntaxonomy 템플릿 추가 # 먼저, layouts/tags/list.html 파일을 추가하여 특정 태그를 포함하는 게시글 목록을 보여주는 템플릿을 만들어보겠습니다.\nHTML Copy html \u0026lt;!-- layouts/tags/list.html --\u0026gt; {{ define \u0026#34;main\u0026#34; }} \u0026lt;article class=\u0026#34;markdown book-article\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;list-header\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;i class=\u0026#34;fa-solid fa-tag\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; {{ .Title }}\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;전체 글 \u0026lt;em class=\u0026#34;list-count\u0026#34;\u0026gt;{{ len .Pages }}\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; {{ $paginator := .Paginate .Pages 10 }} \u0026lt;div class=\u0026#34;post-list\u0026#34;\u0026gt; {{ range $paginator.Pages }} {{ partial \u0026#34;docs/post-item.html\u0026#34; . }} {{ end }} \u0026lt;/div\u0026gt; {{ partial \u0026#34;docs/pagination.html\u0026#34; $paginator }} \u0026lt;/article\u0026gt; {{ end }} CSS Copy scss // assets/_custom.scss .list-header { // 헤더 영역의 요소들을 가운데 정렬 text-align: center; padding-bottom: $padding-16; margin-bottom: $padding-48; border-bottom: 1px solid var(--body-font-color); h1 { font-size: $font-size-40; margin-bottom: $padding-8; i { font-size: $font-size-40; } } p { font-size: $font-size-20; } } .list-count { // 관련 게시글 수를 파란색으로 강조 표현 font-style: normal; font-weight: bold; color: var(--color-link); } .post-list { // 게시글 목록 간의 간격을 조절하고 footer와의 여백도 추가 display: flex; flex-direction: column; gap: $padding-24; margin-bottom: $padding-48; } 앞에서 게시글 목록과 페이지네이션 템플릿을 미리 만들어뒀기 때문에 taxonomy 템플릿은 단순합니다.\n템플릿에서 사용되는 Pages 기능은 모든 Page 객체를 목록으로 반환하는데, taxonomy 레이아웃에서는 해당 taxonomy, 즉 태그를 포함하는 Page 객체들만 반환합니다.\n상단의 list-header 영역에서는 단순히 Pages 를 호출하고 그 개수를 세어 전체 몇 개의 게시글이 있는지 알려줍니다.\n헤더 아래 본문인 post-list 영역에서는 각각의 Page 객체에 대한 메타데이터를 목록으로 나타내는데, 한번에 10개 페이지씩 나눠서 보여주기 위해 Pages 를 Paginate로 감쌉니다.\n반복문을 통해 현재 페이지에 할당된 모든 Page 객체를 layouts/_partials/docs/post-item.html 템플릿을 사용해 보여주고, 본문 아래에는 layouts/_partials/docs/pagination.html 템플릿을 사용해 페이지 이동 링크를 나타냅니다.\n보기 좋게 CSS 스타일도 추가했습니다. list-header 및 post-list 클래스는 카테고리의 taxonomy 템플릿에서도 사용되므로 taxonomy 유형의 템플릿에서 공통으로 적용되는 스타일입니다.\n렌더링하고 Example Site의 태그 중 Development 에 대한 taxonomy 페이지를 접속하면 아래와 같이 관련 게시글 목록을 볼 수 있습니다. 관련 게시글이 2개 밖에 없어 페이지네이션은 안보이는데 관련 게시글을 10개 이상 추가하면 나타납니다.\nterms 템플릿 추가 # 모든 태그 목록을 보여주는 페이지가 있다면 블로그에 처음 들어오는 사람들이 블로그의 정체성을 이해하기 쉬울 것입니다.\nlayouts/tags/terms.html 파일을 추가하여 모든 태그 목록을 보여주는 템플릿을 만들어보겠습니다.\nHTML Copy html \u0026lt;!-- layouts/tags/terms.html --\u0026gt; {{ define \u0026#34;main\u0026#34; }} \u0026lt;article class=\u0026#34;markdown book-article\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;list-header\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;i class=\u0026#34;fa-solid fa-tags\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; Tags\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;전체 태그 \u0026lt;em class=\u0026#34;list-count\u0026#34;\u0026gt;{{ len .Data.Terms }}\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;tag-chips\u0026#34;\u0026gt; {{ range .Data.Terms.ByCount }} \u0026lt;div class=\u0026#34;tag-item\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{ .Page.RelPermalink }}\u0026#34; class=\u0026#34;tag-link\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;tag-name\u0026#34;\u0026gt;{{ .Page.Title }}\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;tag-count\u0026#34;\u0026gt;({{ .Count }})\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; {{ end }} \u0026lt;/div\u0026gt; \u0026lt;/article\u0026gt; {{ end }} CSS Copy scss // assets/_custom.scss .tag-chips { display: flex; flex-wrap: wrap; gap: $padding-8; .tag-link { display: flex; align-items: center; gap: $padding-4; padding: $padding-8 $padding-12; .tag-name { font-weight: 500; } .tag-count { font-size: $font-size-12; opacity: 0.7; } } } terms 템플릿은 단순히 태그 목록을 나열하는 것뿐이라서 내용은 단순합니다.\n.Data.Terms 방식으로 taxonomies를 가져오는 것은 Taxonomies 문서에 안내되어 있습니다. 이렇게 가져온 taxonomies, 즉 태그 목록에서 각 태그마다 관련된 게시글의 수를 세어서 태그 (개수) 형식으로 보여줄 것입니다.\n하지만, 이렇게하면 태그가 하나에 한줄씩 세로로 나열되어 보기에 좋지 않습니다. 단순한 텍스트인 각각의 태그를 칩 형태로 바꾸고 가로로 나열하는 CSS 스타일도 적용했습니다.\n{baseURL}/tags 경로를 통해 terms 페이지에 접근할 수 있는데, 좌측 메뉴에 해당 페이지에 대한 바로가기 버튼도 추가하면 누구나 찾아갈 수 있습니다. 다음과 같이 소셜 링크 영역에서 안쓰는 링크 하나를 태그 페이지로 연결시켰습니다.\nCopy html \u0026lt;!-- layouts/_partials/docs/brand.html --\u0026gt; \u0026lt;div class=\u0026#34;sidebar-profile\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;sidebar-social\u0026#34;\u0026gt; \u0026lt;!-- ... --\u0026gt; \u0026lt;a href=\u0026#34;{{ \u0026#34;/tags/\u0026#34; | relURL }}\u0026#34; title=\u0026#34;Tags\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa-solid fa-tags\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;!-- ... --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; 카테고리 페이지 추가 # 카테고리는 단순히 키워드들을 1차원적으로 나열한 태그와 다르게 2단계로 구성할 것입니다. 따라서, 저는 카테고리를 일반적인 taxonomies처럼 다루지 않고 첫 번째 원소가 부모 카테고리, 두 번째 원소가 자식 카테고리인 길이가 2인 배열의 형태로 정의했습니다.\n이 게시글은 다음과 같이 카테고리를 지정하고 있습니다.\nCopy yaml categories: [\u0026#34;Frontend\u0026#34;, \u0026#34;Blog\u0026#34;] 여기서 첫 번째 원소인 \u0026ldquo;Frontend\u0026quot;가 부모 카테고리가 되고, 두 번째 원소인 \u0026ldquo;Blog\u0026quot;는 자식 카테고리가 됩니다.\n카테고리 파싱 템플릿 추가 # 이어서 만들 taxonomy 템플릿 뿐 아니라 카테고리를 활용하는 모든 템플릿에서 부모/자식 카테고리를 추출해 사용하게 될건데, 해당 코드가 짧은 편이 아니라서 별도의 템플릿으로 만들려고 합니다.\nlayouts/_partials/ 경로 아래에 임의의 템플릿 파일을 추가하면 되지만, 비슷한 유형의 템플릿을 3개 정도 만들 것이므로 categories 라는 폴더로 묶어주겠습니다.\n첫 번째 템플릿은 .Params.categories 를 파라미터로 받아서 첫 번째 카테고리 값을 꺼내는 value-first 템플릿입니다. 카테고리는 기본적으로 배열 형태지만, 빈 배열 또는 단일 문자열이 지정될 경우도 고려하여 몇 가지 예외처리를 했습니다.\nCopy html \u0026lt;!-- layouts/_partials/categories/value-first.html --\u0026gt; {{ $categories := . }} {{ if reflect.IsSlice $categories }} {{- cond (gt (len $categories) 0) (index $categories 0 | string) \u0026#34;\u0026#34; -}} {{ else if eq (printf \u0026#34;%T\u0026#34; $categories) \u0026#34;string\u0026#34; }} {{- $categories -}} {{ else }} \u0026#34;\u0026#34; {{ end }} 두 번째 템플릿은 .Params.categories 를 파라미터로 받아서 두 번째 카테고리 값을 꺼내는 value-second 템플릿입니다. 카테고리가 단일 문자열인 경우 부모 카테고리만 있다고 판단하기 때문에, 카테고리의 길이가 2 이상인 배열로 제공될 조건만 고려하면 됩니다.\nCopy html \u0026lt;!-- layouts/_partials/categories/value-second.html --\u0026gt; {{ $categories := . }} {{- cond (and (reflect.IsSlice $categories) (gt (len $categories) 1)) (index $categories 1 | string) \u0026#34;\u0026#34; -}} 세 번째 템플릿은 .Params.categories 를 파라미터로 받아서 / 로 구분된 전체 카테고리 문자열을 반환하는 value 템플릿입니다. 현재는 2단계 카테고리까지만 지원하기 때문에 배열에 길이 제한을 걸면 디테일을 살릴 수 있지만, 이것이 딱히 오류가 발생하는 것도 아니고 향후 카테고리 단계를 더 늘릴 수도 있기 때문에 단순하게 만들었습니다.\nCopy html \u0026lt;!-- layouts/_partials/categories/value.html --\u0026gt; {{ $categories := . }} {{ if reflect.IsSlice $categories }} {{- delimit $categories \u0026#34;/\u0026#34; -}} {{ else if eq (printf \u0026#34;%T\u0026#34; $categories) \u0026#34;string\u0026#34; }} {{- $categories -}} {{ else }} \u0026#34;\u0026#34; {{ end }} 부모 카테고리 템플릿 추가 # 태그에서는 하나의 taxonomy 템플릿을 만들었지만, 카테고리는 부모와 자식 카테고리의 구성이 달라서 2개의 템플릿으로 분리할 것입니다. 태그에서 layouts/tags/ 경로 아래에 템플릿을 추가한 것처럼 카테고리도 layouts/categories/ 경로 아래에 템플릿을 추가하겠습니다.\nHTML Copy html \u0026lt;!-- layouts/categories/parent.html --\u0026gt; {{ define \u0026#34;main\u0026#34; }} \u0026lt;article class=\u0026#34;markdown book-article\u0026#34;\u0026gt; \u0026lt;!-- 제목을 부모 카테고리 명칭으로 인식 --\u0026gt; {{ $currentCategory := partial \u0026#34;categories/value-first\u0026#34; (slice .Title) }} {{ $pages := slice }} {{ $subCategories := slice }} \u0026lt;!-- 모든 \u0026#34;posts\u0026#34; 경로의 게시글들을 순회하면서 --\u0026gt; {{ range where .Site.RegularPages \u0026#34;Section\u0026#34; \u0026#34;posts\u0026#34; }} {{ $pageCategory := partial \u0026#34;categories/value-first\u0026#34; .Params.categories }} \u0026lt;!-- 게시글의 첫 번째 카테고리가 현재 부모 카테고리와 같으면 `$pages` 배열에 추가 --\u0026gt; {{ if eq $currentCategory $pageCategory }} {{ $pages = $pages | append . }} {{ $subCategory := partial \u0026#34;categories/value-second\u0026#34; .Params.categories }} \u0026lt;!-- 게시글의 두 번째 카테고리가 하위 카테고리 목록에 없으면 `$subCategories` 배열에 추가 --\u0026gt; {{ if not (in $subCategories $subCategory) }} {{ $subCategories = $subCategories | append $subCategory }} {{ end }} {{ end }} {{ end }} \u0026lt;div class=\u0026#34;list-header\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;i class=\u0026#34;fa-solid fa-folder\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; {{ .Title }}\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;전체 글 \u0026lt;em class=\u0026#34;list-count\u0026#34;\u0026gt;{{ len $pages }}\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;!-- 모든 하위 카테고리들에 대한 바로가기 링크를 나열 --\u0026gt; {{ if $subCategories }} \u0026lt;div class=\u0026#34;categories-section\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;하위 카테고리\u0026lt;/h2\u0026gt; \u0026lt;div class=\u0026#34;category-chips\u0026#34;\u0026gt; {{ range $subCategory := $subCategories }} \u0026lt;div class=\u0026#34;category-item\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{ \u0026#34;/categories/\u0026#34; | relURL }}{{ $currentCategory | urlize }}/{{ $subCategory | urlize }}\u0026#34; class=\u0026#34;category-link\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;category-name\u0026#34;\u0026gt;\u0026lt;i class=\u0026#34;fa-solid fa-file\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; {{ $subCategory }}\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; {{ end }} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {{ end }} {{ if $pages }} \u0026lt;div class=\u0026#34;post-list\u0026#34;\u0026gt; {{ $paginate := .Paginate $pages 10 }} \u0026lt;div class=\u0026#34;post-list\u0026#34;\u0026gt; {{ range $paginate.Pages }} {{ partial \u0026#34;docs/post-item.html\u0026#34; . }} {{ end }} \u0026lt;/div\u0026gt; {{ partial \u0026#34;docs/pagination.html\u0026#34; $paginate }} \u0026lt;/div\u0026gt; {{ else }} \u0026lt;div class=\u0026#34;post-list empty-list\u0026#34;\u0026gt; \u0026lt;p\u0026gt;이 카테고리에 게시글이 없습니다.\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; {{ end }} \u0026lt;/article\u0026gt; {{ end }} CSS Copy scss // assets/_custom.scss .categories-section { margin-bottom: $padding-40; } .category-chips { display: flex; flex-wrap: wrap; gap: $padding-8; .category-link { display: flex; align-items: center; gap: $padding-4; padding: $padding-8 $padding-12; .category-name { font-weight: 500; .fa-file, .fa-folder, .fa-folder-open { font-size: $font-size-base; } } } } .empty-list p { text-align: center; } 템플릿의 상단에서 부모 카테고리와 연관된 게시글들을 걸러내는 부분을 제외하면 태그의 taxonomy 템플릿과 유사합니다.\n해당 템플릿도 taxonomy 템플릿이기 때문에 Pages 를 직접 호출할 수도 있지만, Hugo에서 taxonomy엔 순서가 없기 때문에 부모 카테고리가 카테고리 배열에서 첫 번째 값인 조건을 만족시키기 위해 직접 Page 배열을 만들었습니다.\n태그의 taxonomy 템플릿과의 가장 큰 차이점은 헤더 아래에 하위 카테고리들에 대한 바로가기 링크를 표시한다는 것입니다. 부모-자식 카테고리 간에 이동을 용이하게 하기 위해 이와 같은 바로가기 링크를 추가했고, 이것이 폴더-파일 관계와 유사하다 생각해 그러한 아이콘으로 직관성을 더했습니다.\n렌더링하기 전에 한가지 유의할 점은, parent 라는 임의의 명칭을 사용하는 해당 템플릿은 자동으로 카테고리 페이지들을 만들어내지 않습니다. 게시글이 있는 content/ 폴더에서 모든 카테고리에 대한 인덱스 페이지를 만들어줘야 합니다.\nExample Site에서도 기본적으로 [\u0026quot;Development\u0026quot;, \u0026quot;golang\u0026quot;] 카테고리가 지정되어 있습니다. 부모 카테고리인 Development 에 대한 인덱스 페이지를 아래처럼 생성했습니다.\nCopy yaml # content/categories/Development/_index.md --- title: \u0026#34;Development\u0026#34; type: \u0026#34;categories\u0026#34; layout: \u0026#34;parent\u0026#34; --- 제목은 카테고리 명칭과 동일해야 하고, layout 를 parent 로 지정하면 부모 카테고리 페이지로 렌더링됩니다. 그 결과는 아래 이미지처럼 보입니다. (jekyll 카테고리는 임의로 추가했습니다.)\n자식 카테고리 템플릿 추가 # 자식 카테고리 템플릿은 동일하게 layouts/categories/ 경로 아래에 추가합니다. CSS 스타일은 부모 카테고리 템플릿과 동일한 설정을 공유합니다.\nCopy html \u0026lt;!-- layouts/categories/child.html --\u0026gt; {{ define \u0026#34;main\u0026#34; }} \u0026lt;article class=\u0026#34;markdown book-article\u0026#34;\u0026gt; \u0026lt;!-- front matter에서 `parent` 파라미터를 부모 카테고리 명칭으로, 제목을 자식 카테고리 명칭으로 인식 --\u0026gt; {{ $currentCategories := partial \u0026#34;categories/value\u0026#34; (slice .Params.parent .Title) }} {{ $pages := slice }} \u0026lt;!-- 모든 \u0026#34;posts\u0026#34; 경로의 게시글들을 순회하면서 --\u0026gt; {{ range where .Site.RegularPages \u0026#34;Section\u0026#34; \u0026#34;posts\u0026#34; }} {{ $pageCategories := partial \u0026#34;categories/value\u0026#34; .Params.categories }} \u0026lt;!-- 게시글의 전체 카테고리가 현재 카테고리와 같으면 `$pages` 배열에 추가 --\u0026gt; {{ if eq $currentCategories $pageCategories }} {{ $pages = $pages | append . }} {{ end }} {{ end }} \u0026lt;div class=\u0026#34;list-header\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;\u0026lt;i class=\u0026#34;fa-solid fa-file\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; {{ .Title }}\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;전체 글 \u0026lt;em class=\u0026#34;list-count\u0026#34;\u0026gt;{{ len $pages }}\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;!-- 부모 카테고리를 가리키는 `parent` 파라미터로 바로가기 링크를 생성 --\u0026gt; {{ if .Params.parent }} \u0026lt;div class=\u0026#34;categories-section\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;상위 카테고리\u0026lt;/h2\u0026gt; \u0026lt;div class=\u0026#34;category-chips\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;category-item\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{ \u0026#34;/categories/\u0026#34; | relURL }}{{ .Params.parent | urlize }}/\u0026#34; class=\u0026#34;category-link\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;category-name\u0026#34;\u0026gt;\u0026lt;i class=\u0026#34;fa-solid fa-folder-open\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; {{ .Params.parent }}\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {{ end }} {{ if $pages }} \u0026lt;div class=\u0026#34;post-list\u0026#34;\u0026gt; {{ $paginate := .Paginate $pages 10 }} \u0026lt;div class=\u0026#34;post-list\u0026#34;\u0026gt; {{ range $paginate.Pages }} {{ partial \u0026#34;docs/post-item.html\u0026#34; . }} {{ end }} \u0026lt;/div\u0026gt; {{ partial \u0026#34;docs/pagination.html\u0026#34; $paginate }} \u0026lt;/div\u0026gt; {{ else }} \u0026lt;div class=\u0026#34;post-list empty-list\u0026#34;\u0026gt; \u0026lt;p\u0026gt;이 카테고리에 게시글이 없습니다.\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; {{ end }} \u0026lt;/article\u0026gt; {{ end }} 자식 카테고리와 연관된 페이지들을 탐색하는 동작을 제외하면 부모 카테고리 템플릿과 동일합니다. 배열 형태의 categories 를 / 로 구분된 문자열로 만들어서 비교합니다.\n스타일은 부모 카테고리와 동일하므로 별도로 추가할건 없고, 자식 카테고리에 대한 인덱스 페이지들을 만들어줘야 합니다. 아래처럼 부모 카테고리의 인덱스 페이지와 동일한 경로에 자식 카테고리 폴더를 만들고 인덱스 페이지를 추가합니다.\nCopy bash content/categories/ └── Development/ ├── _index.md ├── golang/ │ └── _index.md └── jekyll/ └── _index.md 자식 카테고리의 인덱스 페이지 내용은 부모 카테고리처럼 front matter를 작성하는데, 부모 카테고리 명칭을 parent 파라미터로 추가합니다.\nCopy yaml --- title: \u0026#34;golang\u0026#34; type: \u0026#34;categories\u0026#34; layout: \u0026#34;child\u0026#34; parent: \u0026#34;Development\u0026#34; --- 렌더링하면 이렇게 보입니다.\n카테고리 목록 템플릿 추가 # 카테고리 목록을 보여주는 페이지는 결론부터 보여드리면 아래 이미지와 같습니다.\n트리처럼 카테고리를 단계적으로 보여주고 자식 카테고리 하위에는 관련 게시글을 최대 3개까지 표시합니다.\n과거 PaperMod 테마를 커스터마이징 시도할 때 다른 테마 사용자로부터 전달받은 소스코드를 코파일럿 Grok Code Fast 1 모델의 도움을 받아 가공했지만 코드 길이가 길어서 링크로 대체합니다.\nhugo-book-custom/layouts/categories/root.html at master · … Hugo documentation theme as simple as plain book. Contribute to minyeamer/hugo-book-custom development by creating an … GitHub hugo-book-custom/assets/_custom.scss at master · minyeamer/hugo-book-custom Hugo documentation theme as simple as plain book. Contribute to minyeamer/hugo-book-custom development by creating an … GitHub 첫 번째 링크는 layouts/categories/root.html 경로에 추가하는 파일로 terms 템플릿입니다.\n두 번째 링크는 assets/_custom.scss 파일인데, 중간에 /* Categories List */ 주석으로 시작하는 부분을 참고하면 됩니다.\n그리고, terms 템플릿으로 분류했지만 템플릿 명칭은 root 이므로 Hugo가 자동으로 인식하지 못합니다. content/categories/ 경로 아래에 인덱스 페이지를 추가해줘야 합니다.\nCopy yaml # content/categories/_index.md --- title: \u0026#34;Categories\u0026#34; type: \u0026#34;categories\u0026#34; layout: \u0026#34;root\u0026#34; --- {baseURL}/categories 경로를 통해 카테고리 terms 페이지에 접근할 수 있는데, 태그 terms 페이지처럼 소셜 링크 영역에서 안쓰는 링크 하나 대신에 카테고리 링크를 추가하겠습니다.\nCopy html \u0026lt;!-- layouts/_partials/docs/brand.html --\u0026gt; \u0026lt;div class=\u0026#34;sidebar-profile\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;sidebar-social\u0026#34;\u0026gt; \u0026lt;!-- ... --\u0026gt; \u0026lt;a href=\u0026#34;{{ \u0026#34;/categories/\u0026#34; | relURL }}\u0026#34; title=\u0026#34;Categories\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa-solid fa-folder\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;!-- ... --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; 메뉴에 카테고리 추가 # 카테고리 terms 페이지에서 전체 카테고리 목록을 파악할 수 있지만, 카테고리는 아무래도 어느 위치에서나 볼 수 있는 메뉴 영역에서 보여지는게 블로그 독자들의 이목을 끌기에 좋을 것입니다.\nBook 테마에서 메뉴를 표현하는 템플릿은 layouts/_partials/docs/menu.html 입니다. 여기서 카테고리 목록에 대한 템플릿을 호출할 것입니다.\n새로운 템플릿의 위치는 앞에서 카테고리 파싱 템플릿을 추가했던 layouts/_partials/categories/ 경로가 적절해보입니다. 여기에 menu.html 템플릿을 추가하겠습니다.\nHTML Copy html \u0026lt;!-- layouts/_partials/categories/menu.html --\u0026gt; {{ if .Site.Taxonomies.categories }} \u0026lt;div class=\u0026#34;book-categories\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;checkbox\u0026#34; class=\u0026#34;hidden toggle\u0026#34; id=\u0026#34;categories-control\u0026#34; checked /\u0026gt; \u0026lt;label for=\u0026#34;categories-control\u0026#34; class=\u0026#34;categories-toggle categories-link\u0026#34;\u0026gt; \u0026lt;!-- `전체` 카테고리 링크 -\u0026gt; 카테고리 terms 페이지로 연결 --\u0026gt; \u0026lt;a href=\u0026#34;/categories/\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa-solid fa-folder\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;span\u0026gt;{{ default \u0026#34;Categories\u0026#34; .Site.Params.BookMenu.categoriesLabel }}\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026#34;category-count\u0026#34;\u0026gt;({{ len (where .Site.RegularPages \u0026#34;Section\u0026#34; \u0026#34;posts\u0026#34;) }})\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;i class=\u0026#34;fa-solid fa-chevron-down categories-arrow\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;ul class=\u0026#34;categories-menu\u0026#34; id=\u0026#34;categories-menu\u0026#34;\u0026gt; \u0026lt;!-- 부모-자식 카테고리 트리 구성 --\u0026gt; {{ $categoryTree := dict }} {{ range (where .Site.RegularPages \u0026#34;Section\u0026#34; \u0026#34;posts\u0026#34;) }} {{ if .Params.categories }} {{ $parent := partial \u0026#34;categories/value-first\u0026#34; .Params.categories | string }} {{ if $parent }} \u0026lt;!-- 부모 카테고리 목록 구성 --\u0026gt; {{ if not (index $categoryTree $parent) }} {{ $categoryTree = $categoryTree | merge (dict $parent dict) }} {{ end }} {{ $child := partial \u0026#34;categories/value-second\u0026#34; .Params.categories | string }} {{ if $child }} \u0026lt;!-- 부모 카테고리에 대한 자식 카테고리 목록 구성 --\u0026gt; {{ $children := index $categoryTree $parent }} {{ if not (index $children $child) }} {{ $children = $children | merge (dict $child dict) }} {{ $categoryTree = $categoryTree | merge (dict $parent $children) }} {{ end }} {{ end }} {{ end }} {{ end }} {{ end }} {{ range $parent, $children := $categoryTree }} \u0026lt;!-- 부모 카테고리와 연관된 게시글 수를 카운팅 --\u0026gt; {{ $parentCount := 0 }} {{ range (where $.Site.RegularPages \u0026#34;Section\u0026#34; \u0026#34;posts\u0026#34;) }} {{ if .Params.categories }} {{ if eq $parent (partial \u0026#34;categories/value-first\u0026#34; .Params.categories | string) }} {{ $parentCount = add $parentCount 1 }} {{ end }} {{ end }} {{ end }} \u0026lt;li\u0026gt; {{ if $children }} \u0026lt;input type=\u0026#34;checkbox\u0026#34; class=\u0026#34;hidden toggle\u0026#34; id=\u0026#34;cat-{{ $parent | urlize }}\u0026#34; /\u0026gt; \u0026lt;label for=\u0026#34;cat-{{ $parent | urlize }}\u0026#34; class=\u0026#34;categories-toggle categories-link\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;/categories/{{ $parent | urlize }}/\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa-solid fa-folder\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; {{ $parent }} \u0026lt;span class=\u0026#34;category-count\u0026#34;\u0026gt;({{ $parentCount }})\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;i class=\u0026#34;fa-solid fa-chevron-down categories-arrow\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;ul\u0026gt; {{ range $child, $_ := $children }} \u0026lt;!-- 자식 카테고리와 연관된 게시글 수를 카운팅 --\u0026gt; {{ $childCount := 0 }} {{ range (where $.Site.RegularPages \u0026#34;Section\u0026#34; \u0026#34;posts\u0026#34;) }} {{ if .Params.categories }} {{ if eq (printf \u0026#34;%s/%s\u0026#34; $parent $child) (partial \u0026#34;categories/value\u0026#34; .Params.categories | string) }} {{ $childCount = add $childCount 1 }} {{ end }} {{ end }} {{ end }} \u0026lt;li class=\u0026#34;categories-link\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;/categories/{{ $parent | urlize }}/{{ $child | urlize }}/\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa-solid fa-file\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; {{ $child }} \u0026lt;span class=\u0026#34;category-count\u0026#34;\u0026gt;({{ $childCount }})\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; {{ else }} \u0026lt;a href=\u0026#34;/categories/{{ $parent | urlize }}/\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa-solid fa-file\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; {{ $parent }} \u0026lt;span class=\u0026#34;category-count\u0026#34;\u0026gt;({{ $parentCount }})\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; {{ end }} \u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; {{ end }} CSS Copy scss .book-categories { margin: $padding-16 0; padding-bottom: $padding-16; border-bottom: 1px solid var(--gray-500); position: relative; z-index: 1; i.fa-file, i.fa-folder, i.fa-chevron-down, i.fa-chevron-up { font-size: $font-size-14; transition: transform 0.2s ease; max-width: $font-size-18; } .category-count { font-size: $font-size-12; color: var(--gray-500); white-space: nowrap; text-align: left; margin: 0; } .categories-toggle { display: flex; align-items: center; padding: $padding-4 $padding-8; min-height: 2.381rem; font-weight: 500; cursor: pointer; } .categories-link:hover { color: var(--color-link); background-color: var(--link-background); } // `전체` 카테고리 펼치기/접기 #categories-control:checked + .categories-toggle .categories-arrow { transform: rotate(180deg); } #categories-control:checked ~ .categories-menu { max-height: 1000px; } // 부모-자식 카테고리 펼치기/접기 input[id^=\u0026#34;cat-\u0026#34;]:checked + .categories-toggle .categories-arrow { transform: rotate(180deg); } } 메뉴 카테고리의 핵심 기능은 카테고리를 펼치고 접을 수 있는 기능과 카테고리와 연관된 게시글의 수를 카운팅하여 표시하는 것입니다.\n첫 번째, 카테고리를 펼치고 접는 기능은 체크박스를 통해 이루어집니다. 기능상 체크박스지만 스타일을 적용하게 되면 화살표로 보이는데, 체크박스를 클릭하면 하위 카테고리가 펼쳐지게 됩니다.\n두 번째, 카테고리와 연관된 게시글의 수를 카운팅하는 방식은 처음에 categoryCount 라는 맵을 만들고 카테고리 트리를 만들 때 카운팅도 같이 하려고 했지만, Hugo에서 맵은 불변성을 가져서 아무리 카운팅해도 1 이상으로 올라가지 않았습니다. 그래서, 완성된 카테고리 트리를 순회하는 과정에서 매번 모든 게시글 목록을 가져와 현재 카테고리와 연관된 게시글의 수를 세는 방식을 채택했습니다.\n마지막으로, layouts/_partials/docs/menu.html 템플릿으로 돌아가서 검색창 아래에 방금 추가한 템플릿을 호출해주면 메뉴 영역에 카테고리 트리가 나타납니다.\nCopy html \u0026lt;!-- layouts/_partials/docs/menu.html --\u0026gt; \u0026lt;nav\u0026gt; {{ partial \u0026#34;docs/brand\u0026#34; . }} {{ partial \u0026#34;docs/search\u0026#34; . }} {{ partial \u0026#34;categories/menu\u0026#34; . }} \u0026lt;!-- ... --\u0026gt; 여기까지 적용했다면 아래 이미지와 같이 메뉴에 카테고리 트리가 보여집니다.\n게시글 항목 개선하기 # 태그와 카테고리 페이지를 추가했으니 처음에 만들다만 게시글 항목 템플릿을 완성시켜보겠습니다.\nHTML Copy html \u0026lt;!-- layouts/_partials/docs/post-item.html --\u0026gt; \u0026lt;article class=\u0026#34;post-item\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;post-meta\u0026#34;\u0026gt; \u0026lt;time datetime=\u0026#34;{{ .Date.Format \u0026#34;2006-01-02\u0026#34; }}\u0026#34;\u0026gt; {{ .Date.Format \u0026#34;2006년 01월 02일\u0026#34; }} \u0026lt;/time\u0026gt; \u0026lt;!-- 게시글이 가진 부모-자식 카테고리 속성을 표시 --\u0026gt; {{ if .Params.categories }} \u0026lt;div class=\u0026#34;post-categories\u0026#34;\u0026gt; {{ $parentCategory := partial \u0026#34;categories/value-first\u0026#34; .Params.categories }} {{- if $parentCategory -}} \u0026lt;a href=\u0026#34;{{ \u0026#34;/categories/\u0026#34; | relURL }}{{ $parentCategory | urlize }}/\u0026#34; class=\u0026#34;category\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa-solid fa-folder\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; {{ $parentCategory }} \u0026lt;/a\u0026gt; {{ $childCategory := partial \u0026#34;categories/value-second\u0026#34; .Params.categories }} {{- if $childCategory -}} \u0026lt;a href=\u0026#34;{{ \u0026#34;/categories/\u0026#34; | relURL }}{{ $parentCategory | urlize }}/{{ $childCategory | urlize }}/\u0026#34; class=\u0026#34;category\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa-solid fa-file\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; {{ $childCategory }} \u0026lt;/a\u0026gt; {{- end -}} {{- end -}} \u0026lt;/div\u0026gt; {{ end }} \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;post-content-area{{ if .Params.cover }} has-cover{{ end }}\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;post-text-area\u0026#34;\u0026gt; \u0026lt;h2 class=\u0026#34;post-title\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{ .RelPermalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt; \u0026lt;/h2\u0026gt; {{- $summary := cond .Description .Description (cond .Summary .Summary \u0026#34;\u0026#34;) -}} {{ if $summary }} \u0026lt;div class=\u0026#34;post-summary\u0026#34;\u0026gt; {{ $summary | plainify | truncate 148 }} \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;post-summary-mobile\u0026#34;\u0026gt; {{ $summary | plainify | truncate 72 }} \u0026lt;/div\u0026gt; {{ end }} \u0026lt;!-- 게시글이 가진 태그 속성을 표시 --\u0026gt; {{ if .Params.tags }} \u0026lt;div class=\u0026#34;post-tags\u0026#34;\u0026gt; {{ range first 5 .Params.tags }} \u0026lt;a href=\u0026#34;{{ \u0026#34;/tags/\u0026#34; | relURL }}{{ . | urlize }}/\u0026#34; class=\u0026#34;tag\u0026#34;\u0026gt;#{{ . }}\u0026lt;/a\u0026gt; {{ end }} \u0026lt;/div\u0026gt; {{ end }} \u0026lt;/div\u0026gt; {{ if .Params.cover }} \u0026lt;div class=\u0026#34;post-cover\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;{{ .Params.cover | absURL }}\u0026#34; alt=\u0026#34;Cover Image\u0026#34; class=\u0026#34;post-cover-img\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; {{ end }} \u0026lt;/div\u0026gt; \u0026lt;/article\u0026gt; CSS Copy scss .post-categories { display: flex; gap: 0.5rem; .category { background: var(--gray-200); color: var(--body-font-color); padding: 0.2rem 0.5rem; border-radius: 4px; font-size: 0.8rem; .fa-file, .fa-folder { font-size: 0.8rem; } } } .post-tags { display: flex; flex-wrap: nowrap; gap: $padding-8; overflow: hidden; white-space: nowrap; .tag { display: inline-block; flex: none; background: var(--gray-100); color: var(--body-font-color); padding: $padding-4 $padding-8; font-size: $font-size-14; border: 1px solid var(--gray-200); border-radius: 4px; } } .book-footer .post-tags { line-height: 1.6; } .markdown .category, .markdown .category-link, .markdown .tag, .markdown .tag-link, .book-footer .tag { \u0026amp;[href] { color: var(--body-font-color); background: var(--gray-200); border-radius: $border-radius; } \u0026amp;[href]:visited { color: var(--body-font-color); } \u0026amp;[href]:hover { background: var(--color-page-link); color: var(--body-font-color); text-decoration: none; } } 게시글 항목 템플릿 추가 시에 만들었던 post-item 템플릿 중간에 post-categories 및 post-tags 요소를 칩 형태로 추가했습니다.\n태그와 카테고리 페이지와 관련된 모든 구현 과정이 종료되었습니다.\n태그와 카테고리 페이지를 구성하는 과정은 각각의 컨텐츠로 나눠도 긴 내용이라서 2개의 글로 나눌지 고민했지만, taxonomies 나 게시글 목록 템플릿 등 서로 간에 공통으로 설명해야 할 부분이 많아서 하나의 글로 합치게 되었습니다.\n다음 게시글에서는 검색 페이지를 추가하고 기본 검색 기능을 개선하는 과정을 진행하겠습니다.\n"},{"id":1,"href":"/blog/hugo-blog-2/","title":"Hugo 블로그 만들기 (2) - 메인 레이아웃 개선","section":"Posts","content":" 대상 독자\n마크다운으로 작성할 수 있는 나만의 블로그를 만들고 싶은 분들 블로그를 기능적으로 또는 시각적으로 커스터마이징 하고 싶은 분들 Hugo 템플릿 구조에 대해 이해하고 싶으신 분들 HTML/CSS를 이용한 단일 웹 페이지 구성에 익숙하신 분들 주요 내용\nHugo 템플릿 구조와 Book 테마에서 활용되는 템플릿 (Book 테마 알아보기) Book 테마의 메인 레이아웃을 둘러보며 각 영역을 구별 (메인 레이아웃 개선하기) Book 테마에서 메뉴/목차/헤더 영역을 개선하는 과정 (메뉴 영역 개선하기 / 목차 영역 개선하기 / 헤더 영역 개선하기) 앞선 게시글에서 Hugo 테마에 대해 둘러보고 Hugo 프로젝트 구조를 설계하는 방법을 알아보았습니다.\nHugo 서버 실행하기 # Github Pages를 이용해 블로그를 github.io 주소로 배포해보았지만 테마를 개선하는 과정에서는 변경사항을 즉각적으로 확인해보기 위해 로컬에서 서버를 실행해 보는게 좋습니다.\nHugo 서버를 실행하려면 hugo server 명령어를 사용할 수 있습니다.\nCopy bash % hugo server Watching for changes in .../{archetypes,content,static,themes} ... Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) 기본적으로는 1313 포트에서 서버가 실행되며 --port \u0026lt;포트번호\u0026gt; 옵션을 추가하면 서버가 할당될 포트를 직접 지정할 수 있습니다.\n서버를 실행하면 터미널에서 출력되는 주소 http://localhost:1313/ 를 통해 블로그에 접속할 수 있습니다.\n이렇게 서버를 실행했을 때 장점은 테마를 수정할 때마다 자동으로 빌드되어 변경사항이 즉각적으로 반영된다는 점입니다. 만약 변경사항이 적용되지 않는다면 서버를 실행할 때 --disableFastRender 옵션을 추가하거나 브라우저에서 캐시를 삭제해 볼 수 있습니다.\nBook 테마 알아보기 # 앞선 게시글에서 Book 테마에 대해 둘러보면서 앞으로 해당 테마를 바탕으로 레이아웃을 개선해보겠다고 전달드렸습니다.\nBook 테마 가져오기 # Github에 올라온 hugo-book 저장소를 Fork하여 개인 소유의 저장소로 가져오면 앞으로의 변경사항을 관리하는데 편리합니다.\n저 또한 Book 테마를 개선하기 위해 hugo-book-custom이라는 별도의 저장소를 Fork해서 만들었습니다.\nFork한 저장소를 git clone 명령어로 로컬 경로로 가져와 코드를 확인해보겠습니다.\nCopy bash git clone https://github.com/minyeamer/hugo-book-custom Hugo 템플릿 구조 # Hugo v0.146.0 공식문서에서 안내하는 폴더 구조를 알아보겠습니다.\nNew template system in Hugo v0.146.0 Overview of the new template system in Hugo v0.146.0. gohugo.io layouts/ 경로 아래에 다음과 같은 경로로 레이아웃을 구성하도록 권장합니다.\n여기서 가장 기본이 되는 템플릿이 layouts/baseof.html 이고, 템플릿을 헤더, 목차 등 역할에 따라 작은 부분으로 나눠 layouts/_partials/ 경로에 배치시킵니다. layouts/baseof.html 에서는 이러한 부분 템플릿을 동적으로 가져옵니다.\nCopy bash layouts/ ├── baseof.html ├── baseof.term.html ├── home.html ├── page.html ├── section.html ├── taxonomy.html ├── term.html ├── term.mylayout.en.rss.xml ├── _markup/ │ ├── render-codeblock-go.term.mylayout.no.rss.xml │ └── render-link.html ├── _partials/ │ └── mypartial.html ├── _shortcodes/ │ ├── myshortcode.html │ └── myshortcode.section.mylayout.en.rss.xml ├── docs/ │ ├── baseof.html │ ├── _shortcodes/ │ │ └── myshortcode.html │ └── api/ │ ├── mylayout.html │ ├── page.html │ └── _markup/ │ └── render-link.html └── tags/ ├── taxonomy.html ├── term.html └── blue └── list.html Book 테마 구조 # layouts/docs/ 경로를 활용하는 Hugo 공식문서와 다르게 Book 테마는 layouts/_partials/ 경로에 docs 폴더를 배치시켰습니다. 따라서, 대부분의 테마 수정 작업은 layouts/_partials/docs/ 경로에서 진행됩니다.\nCopy bash layouts/_partials/docs/ ├── ... ├── brand.html ├── ... ├── footer.html ├── header.html ├── ... ├── html-head.html ├── ... ├── menu.html ├── ... └── toc.html Book 테마의 layouts/_partials/docs/ 경로에는 다양한 템플릿이 있지만, 그 중에서 주로 보게될 것은 위 파일들 입니다.\nbrand.html : 메뉴에서 블로그 제목을 표시합니다. footer.html : 본문 하단에 이전, 다음 게시글 및 글 수정 링크를 표시합니다. header.html : 본문 상단에 메뉴 또는 목차 영역을 펼치고 접는 버튼을 표시합니다. html-head.html : 메타(meta) 태그 등 \u0026lt;head\u0026gt; 태그 내에 들어갈 요소들을 나열합니다. menu.html : 본문 좌측 메뉴를 표시합니다. 여기서 brand.html 을 호출합니다. toc.html : 본문 우측에 목차(Table of Contents)를 표시합니다. 하지만, 기능적으로 무언가를 추가할게 아니라면, 단순히 시각적으로 테마를 변경하고자 한다면 템플릿 파일을 직접 건들지는 않고 CSS 파일을 주로 수정합니다.\nBook 테마의 CSS 파일들은 assets/ 경로 아래에 위치합니다.\nCopy bash assets/ ├── _custom.scss ├── _defaults.scss ├── ... ├── _main.scss ├── _markdown.scss ├── ... ├── _shortcodes.scss └── ... assets/ 경로의 파일들 중에서 위 파일들을 주로 수정합니다.\n_custom.scss : 사용자 커스터마이징을 위한 스타일을 작성합니다. _defaults.scss : 폰트 크기, 색상 등이 변수로 정의되어 있습니다. _main.scss : 메뉴, 목차 등 메인 레이아웃에 대한 스타일이 작성되어 있습니다. _markdown.scss : 마크다운을 HTML로 렌더링한 결과에 대한 스타일이 작성되어 있습니다. _shortcodes.scss : 마크다운 작성 시 미리 정해진 짧은 코드를 호출하는 경우가 있는데 이에 대한 스타일이 작성되어 있습니다. 이 외에 JS 파일 등도 assets/ 경로에 위치합니다.\nExample Site # 앞으로 Book 테마를 수정하게 되는데 아무런 글도 없으면 스타일이 어떻게 적용되었는지 확인하기 어렵습니다.\n이미 작성한 게시글이 있다면 content/ 경로에 가져다 놓아도 좋지만, 그러한 게시글이 없을 경우엔 테마에서 제공하는 exampleSite/ 를 참고할 수 있습니다.\nBook 테마의 Example Site는 영어, 히브리어, 중국어로 작성된 각각의 폴더로 나누어져 있는데, 이번 프로젝트에서 다국어 텍스트를 고려하지는 않으므로 영어 문서로 구성된 content.en/ 폴더 내 파일들을 현재 프로젝트의 content/ 경로로 가져옵니다.\nCopy bash exampleSite/content.en/ ├── _index.md ├── docs │ ├── example │ │ ├── _index.md │ │ └── ... │ └── ... ├── posts │ ├── _index.md │ └── ....md └── showcases.md 그리고, exampleSite/ 경로 바로 아래에 있는 hugo.yaml 설정 파일도 현재 프로젝트로 가져옵니다.\n해당 파일은 Hugo 블로그를 구성하는데 필요한 파라미터 등의 설정 정보가 기록되어 있는데, TOML, YAML, JSON 등 다양한 형식을 지원합니다. Hugo 프로젝트를 시작할 때 hugo.toml 이라는 파일이 기본적으로 생성되었을 것인데, TOML 형식은 중첩된 구조를 표현하기에는 번거로워 YAML 형식을 사용합니다.\n설정 파일에 대해서는 필요한 순간에 설명드릴 예정이지만, 설정 파일 상단에서 baseUrl 및 theme 값은 지금 변경해두는게 좋습니다.\nCopy yaml baseURL: http://localhost:1313/ title: Hugo Book theme: Book baseUrl 은 기본값으로 http://localhost:1313/hugo-book/ 경로가 적용되어 있는데, 매번 테스트 페이지에 접근할 때마다 hugo-book/ 경로를 추가하는게 불편하여 제거했습니다.\ntheme 은 기본값으로 hugo-book 이 적용되어 있는데, themes/ 경로에 위치한 Book 테마의 폴더명을 입력해야 합니다.\n메인 레이아웃 개선하기 # Book 테마는 다음과 같은 형태를 가집니다.\n브라우저에서 \u0026lt;body\u0026gt; 태그 바로 아래에 있는 .container 요소를 클랙해보면 아래와 같이 메뉴, 본문, 목차 3개의 부분으로 나눠져 있는 것을 확인할 수 있습니다. HTML 소스코드에서 각각의 요소에 대한 클래스를 확인해보면 book-menu, book-page, book-toc 로 지정되어 있습니다.\n브라우저의 너비를 줄이다 보면 메뉴와 목차 영역이 사라지고 헤더 영역이 나타나게 됩니다. 헤더 영역은 book-header 클래스로 특정할 수 있습니다. 헤더 양옆의 버튼을 클릭하면 메뉴(왼쪽 버튼)와 목차(오른쪽 버튼)가 다시 나타납니다.\n이번 게시글에서는 메뉴, 목차, 헤더 영역을 순차적으로 개선해보겠습니다.\n메뉴 영역 개선하기 # Book 테마에서 메뉴 영역은 아래 이미지에서 선택된 부분입니다.\n좌측 사이드바에 해당하는 메뉴 영역은 book-menu 클래스가 적용된 요소로 감싸져 있으며, baseof.html 파일에서 다음과 같이 menu.html 템플릿 파일을 호출합니다.\nCopy html \u0026lt;!-- baseof.html --\u0026gt; {{ define \u0026#34;menu-container\u0026#34; }} \u0026lt;aside class=\u0026#34;book-menu\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;book-menu-content\u0026#34;\u0026gt; {{ template \u0026#34;menu\u0026#34; . }} \u0026lt;/div\u0026gt; \u0026lt;/aside\u0026gt; {{ end }} menu.html 템플릿 파일은 layouts/_partials/docs/ 경로에 있으며, 이름에서 알 수 있듯이 검색창을 나타내는 docs/search 템플릿이 두 번째로 호출되고, 그 가장 먼저 호출되는 docs/brand 템플릿이 이번 문단에서 개선하고자 하는 부분입니다.\nCopy html \u0026lt;!-- layouts/_partials/docs/menu.html --\u0026gt; \u0026lt;nav\u0026gt; {{ partial \u0026#34;docs/brand\u0026#34; . }} {{ partial \u0026#34;docs/search\u0026#34; . }} \u0026lt;!-- ... --\u0026gt; \u0026lt;/nav\u0026gt; 다시 위 이미지와 아래 개선하고 싶은 사항을 같이 보면서 어떤 점을 해결해야 하는지 생각해보겠습니다.\n좌측에는 메뉴 영역을 표시합니다. 프로필 사진을 표시합니다. 클릭하면 블로그 홈페이지와 연결됩니다. 소셜 링크 또는 기능성 버튼을 표시합니다. 1. 메뉴 영역 위치 고정하기 # 일단, 결론적으로 제가 희망하는 디자인은 아래 이미지와 같습니다. 티스토리의 hELLO 테마입니다.\n위 스타일을 구현하는데 있어 문제점은 메뉴 영역이 어중간하게 화면 좌측 중간에 위치한다는 점입니다.\nBook 테마에서는 메뉴 영역에 배경색이 없어서 자연스러워 보이지만, 저는 메뉴 영역과 본문 영역의 경계를 명확히 하기 위해 배경색을 넣어보면서 해당 문제점을 발견했습니다.\n이러한 스타일을 만들어낸 assets/_main.scss 파일의 .book-menu 부분을 확인해보니까 flex 속성이 적용되어 있었습니다. 메뉴, 본문, 목차 영역이 나란히 붙어있고 모니터의 너비가 길어질수록 좌우에 공백이 생기게 됩니다.\nCopy scss // assets/_main.scss .book-menu { flex: 0 0 $menu-width; font-size: var(--font-size-smaller); // ... } 메뉴 영역을 고정시키려면 position: fixed; 속성을 부여하고, 좌측 끝에 고정시키기 위해 top: 0; left: 0; 속성을 추가로 부여합니다. 나머지 width: $menu-width; flex-shrink: 0; 속성은 메뉴 영역의 너비를 고정시키는 스타일입니다.\n참고로, 배경색은 .book-menu 하위의 .book-menu-content 요소에 적용했습니다.\nCopy scss .book-menu { position: fixed; top: 0; left: 0; width: $menu-width; flex-shrink: 0; .book-menu-content { background: var(--gray-200); // ... } // ... } 아직 목차 영역의 차례는 아니지만, 목차 영역도 메뉴 영역과 동일하게 우측 끝에 고정시킬 필요가 있어 미리 적용하겠습니다.\n목차 영역에 해당하는 book-toc 클래스가 적용된 요소를 대상으로 top: 0; right: 0; 부분만 다르게 하여 .book-menu 와 동일한 속성을 적용했습니다.\nCopy scss .book-toc { position: fixed; top: 0; right: 0; width: $menu-width; flex-shrink: 0; // ... } 이렇게 적용했을 때, 본문 영역이 가운데 위치하지 않고 메뉴 영역에 치우친 쪽으로 정렬됩니다.\n가운데 정렬하기 위해 본문 영역에 해당하는 book-page 클래스가 적용된 요소를 대상으로 margin: 0 auto; 속성을 적용하면 본문 영역 양 옆에 자동으로 동일한 크기의 여백을 만들어 줍니다.\n초기 Book 테마에는 min-width 속성만 적용되어 있는데, 본문이 전체 영역을 차지해버려 여백이 만들어지지 않기 때문에 max-width 속성을 추가합니다.\n$body-max-width 변수는 기본적으로 없을건데, 저는 assets/_defaults.scss 파일에서 $body-max-width: 48rem; 라인을 추가해 본문의 최대 너비가 48rem이 되도록 적용했습니다. (앞으로의 진행 과정에서 기존에 존재하지 않는 변수를 정의하여 사용하는 경우가 있을텐데, $padding-24 등 직관적으로 이해할 수 있는 변수에 대한 설명은 생략하겠습니다.)\nCopy scss .book-page { max-width: $body-max-width; min-width: $body-min-width; margin: 0 auto; flex: 1 1 0; padding: $padding-16; } 위 스타일을 적용하면 아래 이미지와 같이 좌측 끝의 고정된 위치에 메뉴가 보여집니다.\n잘 보이기 위해 브라우저의 너비를 1500px 정도로 줄이고 캡쳐한 것인데, 여기서 브라우저의 너비를 1200px까지 줄여보니까 아래 이미지처럼 본문이 메뉴 및 목차 영역 뒤에 겹쳐서 숨어버리는 현상이 발생했습니다.\n메뉴, 본문, 목차 영역이 flexible하게 정렬된 기존의 스타일과 다르게 메뉴와 목차 영역을 position 속성으로 양끝에 고정시키면서 본문 영역과 독립적인 요소가 되었습니다. 이로 인해 본문 영역이 메뉴와 목차 영역을 인식하지 못하고 침범하게 된 것입니다.\n본문 메뉴가 메뉴와 목차 영역에 겹치지 않기 위해 다양한 해결 방법들이 있겠지만, 저는 Book 테마에서 이미 만들어놓은 반응형 디자인을 사용했습니다.\nCopy scss @media screen and (max-width: $mobile-breakpoint) { .book-menu { visibility: hidden; margin-inline-start: -$menu-width; z-index: 1; } .book-toc { display: none; } .book-header { display: block; } // ... } @media screen and (max-width: $mobile-breakpoint) 는 화면의 너비가 $mobile-breakpoint 보다 작아지는 경우를 기점으로 발생하는 조건문 입니다.\nassets/_defaults.scss 파일에서 $mobile-breakpoint 는 $menu-width + $body-min-width * 1.2 + $toc-width 정도의 크기를 가집니다. 계산하면 56rem 정도 되는데, 화면의 너비가 이보다 작아지면 메뉴와 목차 영역을 숨기고 헤더를 표시하게 됩니다. (헤더에 대한 설명은 헤더를 개선할 때 할 예정이지만, 헤더에서 메뉴와 목차를 펼치고 접을 수 있습니다.)\n변경된 스타일에서 본문이 메뉴 및 목차와 겹치게 되는 지점은 메뉴와 목차 영역의 너비에 $body-max-width 길이를 더한 크기입니다. 즉, $mobile-breakpoint 에서 $body-min-width 를 $body-max-width 로 바꿔주기만 하면 됩니다. 이렇게 변경하고 다시 계산하면 93.6rem이 되어 본문이 메뉴 및 목차와 겹치지 않게 됩니다.\nCopy scss $mobile-breakpoint: $menu-width + $body-max-width * 1.2 + $toc-width !default; 이렇게 해결된 줄 알았지만, 32인치 모니터 및 모바일 기기를 사용하는 입장에서 문제가 없었던 것이고 13인치 노트북에서 블로그에 접속해보니까 모바일처럼 메뉴와 본문이 숨겨져 보였습니다. 그래서 이후에 단일 $mobile-breakpoint 를 $wide-breakpoint, $toc-breakpoint, $menu-breakpoint 3단계로 나누고 $wide-breakpoint 지점에서 본문의 너비를 한 번 줄여주어 11인치 너비까지는 메뉴와 목차가 전부 표시되도록 수정했습니다. 이 부분은 각자의 화면 크기에 맞춰 직접 진행해보시기 바랍니다.\n2. 프로필 사진을 표시하기 # 현재까지의 변경사항을 적용하면 블로그가 아래와 같이 보여집니다.\n메뉴 영역에서 블로그 제목만 있고 눈길을 끌만한 이미지가 없습니다. 이번에는 제목 위에 프로필 사진을 추가해보겠습니다.\n메뉴 영역 개선하기 문단의 첫 부분에서 설명했듯이 블로그 제목이 위치한 템플릿은 layouts/_partials/docs/brand.html 파일입니다. 파일의 내용은 다음과 같습니다.\nCopy html \u0026lt;!-- layouts/_partials/docs/brand.html --\u0026gt; \u0026lt;h2 class=\u0026#34;book-brand\u0026#34;\u0026gt; \u0026lt;a class=\u0026#34;flex align-center\u0026#34; href=\u0026#34;{{ cond (not .Site.Home.File) .Sites.Default.Home.RelPermalink .Site.Home.RelPermalink }}\u0026#34;\u0026gt; {{- with .Site.Params.BookLogo -}} \u0026lt;img src=\u0026#34;{{ . | relURL }}\u0026#34; alt=\u0026#34;{{ partial \u0026#34;docs/text/i18n\u0026#34; \u0026#34;Logo\u0026#34; }}\u0026#34; /\u0026gt; {{- end -}} \u0026lt;span\u0026gt;{{ .Site.Title }}\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/h2\u0026gt; 설정 파일에서 BookLogo 파라미터를 추가하면 제목 옆에 로고를 표시하는듯 하지만, 프로필 사진은 이보다 더 크게 제목 위에 나타낼 것이기 때문에 새로운 요소를 추가하겠습니다. (프로필 사진을 원형으로 표현하는 CSS 스타일도 assets/_custom.scss 에 적용합니다.)\nHTML Copy html \u0026lt;!-- layouts/_partials/docs/brand.html --\u0026gt; \u0026lt;div class=\u0026#34;sidebar-profile\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;profile-img-wrap\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{ .Site.BaseURL }}\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;{{ .Site.Params.BookMenu.profileImage }}\u0026#34; alt=\u0026#34;Profile\u0026#34; class=\u0026#34;profile-img\u0026#34; /\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;h2 class=\u0026#34;book-brand\u0026#34;\u0026gt; \u0026lt;!-- ... --\u0026gt; \u0026lt;/h2\u0026gt; CSS Copy scss // assets/_custom.scss .profile-img-wrap { border-radius: 50%; overflow: hidden; margin-bottom: 1rem; } .profile-img { width: 100%; height: 100%; object-fit: contain; border-radius: 50%; display: block; } 템플릿에서 {{ .Site.Params.BookMenu.profileImage }} 부분은 설정 파일에서 파라미터를 호출하는 부분입니다. 기본 설정 파일은 Hugo 프로젝트 루트 경로에 있는 hugo.toml 파일입니다. 해당 파일에서 profileImage 파라미터를 추가해야 하는데 YAML 형식에서는 이렇게 추가할 수 있습니다.\nCopy yaml # hugo.yaml params: BookMenu: profileImage: \u0026#34;\u0026lt;프로필-사진-주소\u0026gt;\u0026#34; 프로필 사진의 주소까지 설정한 결과는 아래 이미지와 같습니다.\n3. 소셜 링크를 표시하기 # 다음으로, 프로필 사진 아래에, 그리고 블로그 제목 위에 소셜 링크를 표시할 것입니다.\nbrand.html 템플릿에 소셜 링크를 추가하면 되는 것은 이미 알고 있으므로 부가적인 설명없이 바로 추가해보겠습니다.\nHTML Copy html \u0026lt;!-- layouts/_partials/docs/brand.html --\u0026gt; \u0026lt;div class=\u0026#34;sidebar-profile\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;profile-img-wrap\u0026#34;\u0026gt; \u0026lt;!-- ... --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;sidebar-social\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{ .Site.Params.BookMenu.githubLink }}\u0026#34; target=\u0026#34;_blank\u0026#34; title=\u0026#34;GitHub\u0026#34; {{ if not .Site.Params.BookMenu.githubLink }}class=\u0026#34;disabled\u0026#34;{{ end }}\u0026gt; \u0026lt;i class=\u0026#34;fa-brands fa-github\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;{{ .Site.Params.BookMenu.linkedinLink }}\u0026#34; target=\u0026#34;_blank\u0026#34; title=\u0026#34;LinkedIn\u0026#34; {{ if not .Site.Params.BookMenu.linkedinLink }}class=\u0026#34;disabled\u0026#34;{{ end }}\u0026gt; \u0026lt;i class=\u0026#34;fa-brands fa-linkedin\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;{{ .Site.Params.BookMenu.notionLink }}\u0026#34; target=\u0026#34;_blank\u0026#34; title=\u0026#34;Notion\u0026#34; {{ if not .Site.Params.BookMenu.notionLink }}class=\u0026#34;disabled\u0026#34;{{ end }}\u0026gt; \u0026lt;i class=\u0026#34;fa-brands fa-notion\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;{{ .Site.Params.BookMenu.twitterLink }}\u0026#34; target=\u0026#34;_blank\u0026#34; title=\u0026#34;Twitter\u0026#34; {{ if not .Site.Params.BookMenu.twitterLink }}class=\u0026#34;disabled\u0026#34;{{ end }}\u0026gt; \u0026lt;i class=\u0026#34;fa-brands fa-twitter\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;h2 class=\u0026#34;book-brand\u0026#34;\u0026gt; \u0026lt;!-- ... --\u0026gt; \u0026lt;/h2\u0026gt; CSS Copy scss // assets/_custom.scss .sidebar-social { display: flex; justify-content: center; align-items: center; gap: 0.5rem; font-size: 1.5rem; width: 100%; margin-bottom: 1rem; } .sidebar-social a, i { color: --color-social-link; font-size: 2rem; flex: 1 1 0; text-align: center; transition: color 0.2s; } .sidebar-social a.disabled { pointer-events: none; opacity: 0.5; cursor: default; } 총 4개의 소셜 링크 [ 깃허브, 링크드인, 노션, 트위터 ] 를 추가했습니다.\n\u0026lt;a\u0026gt; 태그에서 if 문을 사용하는 것을 볼 수 있는데, 이는 소셜 링크를 가리키는 BookMenu.githubLink 등의 파라미터가 설정 파일에 없을 경우 추가되는 구문입니다. 소셜 링크가 없으면 아래 CSS 설정에서 활용될 disabled 클래스가 적용되어 링크가 비활성화됩니다.\n그리고, \u0026lt;a\u0026gt; 태그의 내용을 보면 알 수 있듯이 소셜 플랫폼에 대한 로고 이미지가 아니라 \u0026lt;i\u0026gt; 태그 아이콘을 사용하고 있습니다. 이것은 Font Awesome에서 제공하는 스타일인데, 이를 사용하기 위해 외부 소스의 CSS 파일을 가져와야 합니다.\nFont Awesomefontawesome.com Book 테마 구조 문단에서 했던 것처럼 \u0026lt;head\u0026gt; 태그 내에 들어갈 요소는 layouts/_partials/docs/html-head.html 템플릿에 추가합니다.\nCopy html \u0026lt;!-- layouts/_partials/docs/html-head.html --\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.1/css/all.min.css\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; /\u0026gt; 로고 이미지를 사용했다면 소셜 로고 크기를 조절하기 위해 width 속성을 적용해야 하지만, 텍스트 유형인 \u0026lt;i\u0026gt; 태그를 사용하기 때문에 font-size 속성으로 크기를 조절해주어야 합니다. (--color-social-link 변수는 라이트 모드와 다크 모드에 따라 달라지는데, 아직 다크 모드를 추가하기 전이므로 #fff 색상을 적용해주세요. 변수가 정의되지 않아도 오류는 발생하지 않습니다.)\n또한, 소설 링크를 설정 파일에서 추가하지 않았을 경우에 추가되는 disabled 클래스에 링크를 비활성화하는 스타일을 적용했습니다.\n소셜 링크까지 추가한 결과는 아래 이미지와 같습니다.\n추후에 카테고리, 태그 등을 추가하기 위해 메뉴 영역을 수정할 일이 있지만, 이번 게시글에서는 메뉴 영역에 대해 여기까지 진행하겠습니다.\n목차 영역 개선하기 # Book 테마에서 목차 영역은 아래 이미지에서 선택된 부분입니다.\n위 이미지는 메뉴 영역을 개선하기 전인 Example Site 기준이고, 메뉴 영역을 위치 고정하기 문단에서 목차 영역도 우측 끝에 고정시켰습니다. (너무 끝에 붙어있으면 보기 안좋아 1.5rem 수준의 여백을 추가했습니다.)\n목차 영역은 book-toc 클래스가 적용된 요소로 감싸져 있으며, baseof.html 파일에서 다음과 같이 toc 템플릿을 호출합니다.\nCopy html \u0026lt;!-- baseof.html --\u0026gt; {{ define \u0026#34;toc-container\u0026#34; }} {{ if partial \u0026#34;docs/toc-show\u0026#34; . }} \u0026lt;aside class=\u0026#34;book-toc\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;book-toc-content\u0026#34;\u0026gt; {{ template \u0026#34;toc\u0026#34; . }} \u0026lt;/div\u0026gt; \u0026lt;/aside\u0026gt; {{ end }} {{ end }} toc.html 템플릿 파일은 layouts/_partials/docs/ 경로에 있는데, 내용은 별 거 없습니다. Hugo 공식문서 TableOfContents를 보면 {{ .TableOfContents }} 템플릿을 통해 목차를 만들 수 있다고 안내되어 있습니다. Book 테마에서도 Hugo에 내장된 목차 템플릿을 호출하여 목차를 생성합니다.\nCopy html \u0026lt;!-- layouts/_partials/docs/toc.html --\u0026gt; {{ partial \u0026#34;docs/inject/toc-before\u0026#34; . }} {{ .TableOfContents }} {{ partial \u0026#34;docs/inject/toc-after\u0026#34; . }} 참고로, 같은 경로에 있는 toc-show.html 파일은 baseof.html 에서 book-toc 를 생성할지 결정하는 역할을 수행하는데 내용을 보면 nav#TableOfContents 영역이 있는지 검사합니다.\nCopy html \u0026lt;!-- layouts/_partials/docs/toc-show.html --\u0026gt; {{ return default (not (eq .TableOfContents \u0026#34;\u0026lt;nav id=\\\u0026#34;TableOfContents\\\u0026#34;\u0026gt;\u0026lt;/nav\u0026gt;\u0026#34;)) (default .Site.Params.BookToC .Params.BookToC) }} 다시 Hugo 공식문서 TableOfContents를 보면 목차 영역이 아래 HTML 형태로 만들어지는 것을 알 수 있습니다. #TableOfContents 요소를 대상으로 CSS 스타일을 적용할 일이 있어서 알고 있으면 좋습니다.\nCopy text \u0026lt;nav id=\u0026#34;TableOfContents\u0026#34;\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;#section-1\u0026#34;\u0026gt;Section 1\u0026lt;/a\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;#section-11\u0026#34;\u0026gt;Section 1.1\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;#section-12\u0026#34;\u0026gt;Section 1.2\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;#section-2\u0026#34;\u0026gt;Section 2\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/nav\u0026gt; 목차 영역에서는 어떤 점을 해결하면 좋을지 생각해보겠습니다.\n목차와 본문의 사이에 목차의 길이만큼 구분선을 추가합니다. 스크롤이 위치한 목차를 하이라이트로 강조합니다. 우측 하단의 고정된 위치에 이동 버튼을 추가합니다. 맨 위로 이동, 맨 아래로 이동, 뒤로 가기 버튼을 세로로 나열합니다. 1. 목차 옆에 구분선 추가하기 # 단순히 목차 영역에 border 속성을 추가하면 되지만, 목차 영역을 어디까지 볼 것인지 고려해야 합니다.\nbook-toc 클래스가 적용된 요소는 목차를 포함한 우측 사이드바 전체 영역입니다. book-toc 클래스를 대상으로 border 속성을 추가하면 본문과 목차 사이에 끝없이 긴 라인이 만들어질 것입니다.\n제가 원하는 것은 목차 텍스트가 있는 영역에만 border 속성을 추가하는 것입니다.\nBook 테마의 toc.html 템플릿만 보았다면 어디에 border 속성을 추가해야 하는지 알 수 없지만, Hugo 공식문서 TableOfContents를 통해 #TableOfContents 요소가 목차 텍스트가 있는 영역이란 것을 확인했습니다.\n이제 assets/_main.scss 파일에 스타일을 추가하겠습니다.\nCopy scss // assets/_main.scss #TableOfContents { margin-top: 2rem; padding-left: 1rem; border-left: 1px solid var(--toc-font-color); a { color: var(--toc-font-color); } } 단순히 border 속성만 추가하면 구분선과 목차 텍스트가 딱 붙어버리기 때문에 적당한 여백을 추가했습니다.\n저는 구분선의 색상을 나타내는 --toc-font-color 변수에 검은색(black)을 지정했습니다. 추가로, 목차를 구성하는 \u0026lt;a\u0026gt; 태그가 링크와 동일한 파란색 글씨색을 가져서 보기 안좋아 구분선과 동일한 색상을 적용했는데, 이 부분은 취향에 맞게 수정해보시기 바랍니다.\n목차 영역을 돋보이기 위해 잠시 본문의 헤딩을 늘렸습니다.\n2. 목차 하이라이트 적용하기 # 독자에게 게시글에서 자신이 어떤 위치에 있는지 알려주는 것은 가독성을 크게 향상시킬 수 있다고 생각합니다.\n하지만, 이러한 기능을 구현하는 방법을 몰라 코파일럿의 Sonnet 4.5 모델에게 목차 하이라이트를 구현하는 방법을 물어보았습니다. 프롬프트는 특별히 길게 쓴건 아니라서 따로 저장해두진 않았는데, 에이전트 모드로 Book 테마 경로를 첨부하여 문맥을 제공하였고 아래와 같은 코드가 생성되었습니다.\nJavaScript Copy js // assets/toc-highlight.js document.addEventListener(\u0026#39;DOMContentLoaded\u0026#39;, function() { const headings = document.querySelectorAll(\u0026#39;h2[id], h3[id]\u0026#39;); function getVisibleToc() { const bookToc = document.querySelector(\u0026#39;.book-toc\u0026#39;); if (bookToc) { const style = window.getComputedStyle(bookToc); if (style.visibility === \u0026#39;visible\u0026#39;) { return bookToc.querySelector(\u0026#39;#TableOfContents\u0026#39;); } } return document.querySelector(\u0026#39;#TableOfContents\u0026#39;); } const observer = new IntersectionObserver(entries =\u0026gt; { entries.forEach(entry =\u0026gt; { const id = entry.target.getAttribute(\u0026#39;id\u0026#39;); const toc = getVisibleToc() const tocLinks = toc.querySelectorAll(\u0026#39;a\u0026#39;); const correspondingTocLink = toc.querySelector(`a[href=\u0026#34;#${id}\u0026#34;]`); if (correspondingTocLink) { if (entry.isIntersecting) { tocLinks.forEach(link =\u0026gt; link.classList.remove(\u0026#39;active\u0026#39;)); correspondingTocLink.classList.add(\u0026#39;active\u0026#39;); } } }); }, { rootMargin: \u0026#39;0px 0px -70% 0px\u0026#39; }); headings.forEach(heading =\u0026gt; { observer.observe(heading); }); }); CSS Copy scss // assets/_main.scss #TableOfContents { // ... a.active { color: var(--toc-active-color); font-weight: bold; } } 자바스크립트로 기능을 정의하는데 익숙하지는 않지만, 함수명 등으로 유추해봤을 때 getVisibleToc() 함수에서 #TableOfContents 요소를 가져와 toc 상수에 할당하는데 사용되는 것으로 보입니다. 그리고, toc 상수로부터 \u0026lt;a\u0026gt; 태그들을 꺼내서 순회하면서 \u0026lt;a\u0026gt; 태그가 가리키는 헤딩 요소가 화면에 들어오면 active 클래스를 추가한다고 해석할 수 있습니다.\nCSS 스타일을 통해 active 클래스인 요소에 대해서만 하이라이트를 적용했습니다. --toc-active-color 변수는 제가 입맛에 맞게 바꾼 색상인데 라이트 모드에서는 다른 목차 글씨 색상과 동일한 검은색(black)이며, font-weight: bold; 속성으로 글씨를 굵게하는 것으로 하이라이트를 표현했습니다.\n이렇게하면 목차 하이라이트가 적용되지만, assets/ 경로에 새로 추가한 자바스크립트 파일은 layouts/_partials/docs/html-head.html 템플릿에서 호출해주어야 합니다.\nCopy html \u0026lt;!-- layouts/_partials/docs/html-head.html --\u0026gt; {{- $tocHighlightJS := resources.Get \u0026#34;toc-highlight.js\u0026#34; | resources.ExecuteAsTemplate \u0026#34;toc-highlightjs\u0026#34; . | resources.Minify | resources.Fingerprint }} \u0026lt;script defer src=\u0026#34;{{ partial \u0026#34;docs/links/resource-precache\u0026#34; $tocHighlightJS }}\u0026#34; {{ template \u0026#34;integrity\u0026#34; $tocHighlightJS }}\u0026gt;\u0026lt;/script\u0026gt; 이렇게 적용하면 아래 이미지와 같이 현재 보고 있는 헤딩이 목차에서 하이라이트됩니다. 스크롤을 할만큼 본문이 길지 않지만, 이미지를 기준으로 \u0026ldquo;제목 2-3\u0026rdquo; 헤딩을 현재 보고 있는 것으로 인식하여 목차에서 \u0026ldquo;제목 2-3\u0026rdquo; 항목이 굵은 글씨로 강조되고 있습니다.\n3. 스크롤 이동 버튼 추가하기 # 제가 경험해 본 웹사이트 중에서 우측 하단에 스크롤 이동 버튼을 놓는 경우가 종종 있었습니다. PC에서는 굳이 이러한 버튼을 안써도 키보드 단축키로 이동하기 쉽지만, 모바일에서는 그 긴 스크롤을 일일이 내리기 불편해 있으면 매우 편한 기능이라고 생각합니다.\n그렇다면 이 버튼을 어느 템플릿에 넣으면 좋을지 생각해봐야 합니다.\n새로운 템플릿을 만들어서 toc.html 템플릿에서 호출할 수도 있지만, 저는 layouts/_partials/docs/inject/ 경로에 있는 toc-after.html 템플릿에 추가하는게 적절하다고 생각했습니다.\n이유는 toc.html 템플릿에서 이미 해당 템플릿을 호출하고 있고, toc-after.html 템플릿 자체는 비어있는 파일이라 새로운 템플릿 파일을 만드는 것과 별 차이가 없다고 판단했기 때문입니다.\nCopy html \u0026lt;!-- layouts/_partials/docs/toc.html --\u0026gt; {{ partial \u0026#34;docs/inject/toc-before\u0026#34; . }} {{ .TableOfContents }} {{ partial \u0026#34;docs/inject/toc-after\u0026#34; . }} toc-after.html 에 다음 3개의 버튼을 추가했습니다. onclick 이벤트에 대해서 위에서부터 스크롤을 맨 위로, 스크롤을 맨 아래로, 그리고 이전 페이지로 이동하는 3가지 기능을 하는 버튼입니다.\nHTML Copy html \u0026lt;!-- layouts/_partials/docs/inject/toc-after.html --\u0026gt; \u0026lt;div class=\u0026#34;book-nav\u0026#34;\u0026gt; \u0026lt;button class=\u0026#34;book-nav-btn3\u0026#34; onclick=\u0026#34;window.scrollTo({top: 0, behavior: \u0026#39;smooth\u0026#39;})\u0026#34; title=\u0026#34;Go to top\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa fa-chevron-up\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;button class=\u0026#34;book-nav-btn3\u0026#34; onclick=\u0026#34;window.scrollTo({top: document.body.scrollHeight, behavior: \u0026#39;smooth\u0026#39;})\u0026#34; title=\u0026#34;Go to bottom\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa fa-chevron-down\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;button class=\u0026#34;book-nav-btn3\u0026#34; onclick=\u0026#34;history.back()\u0026#34; title=\u0026#34;Go back\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa-solid fa-arrow-left\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; CSS Copy scss // assets/_main.scss .book-nav, .book-mobile-nav { position: fixed; bottom: 0; right: 0; margin-bottom: $padding-16; margin-right: $padding-16; display: flex; align-items: flex-end; flex-direction: column; .book-nav-btn3 { width: $font-size-40; height: $font-size-40; margin: $padding-4; border: 0px; border-radius: 50%; background: var(--gray-200); color: var(--body-font-color); cursor: pointer; i { font-size: $font-size-20; } } // ... } CSS 스타일에서 스크롤 이동 버튼을 표현하는데 핵심적인 속성은 border-radius: 50%; 및 background: var(--gray-200); 입니다. Font Awesome에서 가져온 단순한 화살표 아이콘을 동그란 버튼처럼 가공해 보기 좋아졌습니다. 그런데, 이렇게 적용했을 때 스크롤 이동 버튼이 우측 끝에 너무 딱 붙어있어서 book-toc 영역 전체에 margin-right: $padding-48; 속성을 추가했습니다.\n스크롤 이동 버튼까지 추가한 결과는 아래와 같습니다. (하단에 있는 버튼이 잘 보이기 위해 브라우저 높이를 450px 정도로 줄였습니다.)\n목차 영역은 여기서 완성입니다. 나중에 다크 모드를 적용할 때 전용 색상 스타일을 추가할 일이 있지만, 더 이상 목차 영역에 새로운 기능을 추가하는 경우는 없습니다.\n헤더 영역 개선하기 # 헤더 영역은 브라우저 너비가 일정 크기 이상 줄어들 때만 나타나는 모바일 전용 헤더입니다.\n헤더 영역은 book-header 클래스가 적용된 요소로 감싸져 있으며, baseof.html 파일에서 다음과 같이 header 템플릿을 호출합니다. book-header 의 바로 위를 보면 본문을 나타내는 book-page 요소의 하위에 헤더 영역이 있음을 알 수 있습니다.\nCopy html \u0026lt;!-- baseof.html --\u0026gt; \u0026lt;!-- ... --\u0026gt; \u0026lt;div class=\u0026#34;book-page\u0026#34;\u0026gt; \u0026lt;header class=\u0026#34;book-header\u0026#34;\u0026gt; {{ template \u0026#34;header\u0026#34; . }} \u0026lt;!-- Mobile layout header --\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;!-- ... --\u0026gt; header.html 템플릿 파일은 layouts/_partials/docs/ 경로에 있는데 내용은 직관적으로 읽힙니다. 메뉴 버튼을 의미하는 menu-control 요소, 제목을 가리키는 \u0026lt;h3\u0026gt; 태그, 그리고 목차 버튼을 의미하는 toc-control 요소가 있습니다.\nCopy html \u0026lt;!-- layouts/_partials/docs/header.html --\u0026gt; \u0026lt;div class=\u0026#34;flex align-center justify-between\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;menu-control\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;{{ partial \u0026#34;docs/icon\u0026#34; \u0026#34;menu\u0026#34; }}\u0026#34; class=\u0026#34;book-icon\u0026#34; alt=\u0026#34;{{ partial \u0026#34;docs/text/i18n\u0026#34; \u0026#34;Menu\u0026#34; }}\u0026#34; /\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;h3\u0026gt;{{ partial \u0026#34;docs/title\u0026#34; . }}\u0026lt;/h3\u0026gt; \u0026lt;label for=\u0026#34;toc-control\u0026#34;\u0026gt; {{ if partial \u0026#34;docs/toc-show\u0026#34; . }} \u0026lt;img src=\u0026#34;{{ partial \u0026#34;docs/icon\u0026#34; \u0026#34;toc\u0026#34; }}\u0026#34; class=\u0026#34;book-icon\u0026#34; alt=\u0026#34;{{ partial \u0026#34;docs/text/i18n\u0026#34; \u0026#34;Table of Contents\u0026#34; }}\u0026#34; /\u0026gt; {{ end }} \u0026lt;/label\u0026gt; \u0026lt;/div\u0026gt; 헤더 영역에서는 어떤 점을 해결하면 좋을지 생각해보겠습니다.\n헤더 영역의 가운데에는 블로그 제목을 표시하고 홈페이지로 이동하는 링크를 설정합니다. 헤더 영역은 반투명한 배경색을 가지고 스크롤 위치에 관계없이 고정됩니다. 헤더 영역은 특별히 수정할건 없습니다.\n먼저, 제목에 홈페이지로 이동하는 링크를 거는건 아래처럼 간단합니다.\nCopy html \u0026lt;h3\u0026gt; \u0026lt;a href=\u0026#34;{{ .Site.BaseURL }}\u0026#34; class=\u0026#34;site-title\u0026#34;\u0026gt;{{ .Site.Title }}\u0026lt;/a\u0026gt; \u0026lt;/h3\u0026gt; 다음으로, 헤더에 반투명한 배경색을 넣고 상단에 고정하는건 아래 스타일을 적용하면 됩니다.\nCopy scss // assets/_main.scss .book-header { display: block; position: sticky; opacity: 0.9; top: 0; background-color: var(--body-background); padding: 1rem; } 기존 Book 테마에서는 모바일 화면에서 스크롤하면 헤더가 사라져버려 메뉴 및 목차 버튼을 클릭하기 위해 매번 맨 위로 이동해야 하는 불편함이 있었습니다.\n그래서 헤더 영역을 상단에 고정시켜 봤는데, 문제가 이 버튼을 클릭하면 자동으로 맨 위로 올라가 버립니다. 결국 의도했던 동작을 수행하지 못해 아직까지 해결책을 찾고 있는데, 해결되었다면 해당 게시글에 업데이트 하겠습니다.\n스크롤을 내려보면 헤더가 반투명하게 보이면서 헤더 밑에 있는 본문을 읽을 수 있습니다.\n다음 게시글에서는 카테고리와 태그 기능을 추가하고, 관련 게시글을 목록을 보여주는 템플릿을 작성하는 과정을 진행하겠습니다. 카테고리와 태그 기능은 특히 내용이 길어서 다음 게시글로 분리하게 되었는데, 다음 게시글에서 이를 포함한 메뉴 영역을 완성할 수 있게 됩니다.\n"},{"id":2,"href":"/blog/hugo-blog-1/","title":"Hugo 블로그 만들기 (1) - 프로젝트 기획 및 구조 설계","section":"Posts","content":" 대상 독자\n마크다운으로 작성할 수 있는 나만의 블로그를 만들고 싶은 분들 블로그를 기능적으로 또는 시각적으로 커스터마이징 하고 싶은 분들 Github Pages 서비스를 활용한 웹 호스팅을 하고 싶은 분들 Git Submodules 기능을 활용한 프로젝트 구성 방식을 알고 싶은 분들 주요 내용\n과거 다른 블로그 플랫폼을 이용하면서 겪은 경험 및 장단점 (나만의 블로그를 만들게 된 계기) 나만의 블로그에 추가하고 싶은 기능 목록을 영역 별로 나열 (나만의 블로그 기획하기) 관심있는 Hugo 테마에 대한 소개 (기본 테마 선정하기) Hugo 설치부터 Submodule 활용까지 프로젝트 구조를 설계하고 배포 스크립트 작성 (블로그 프로젝트 구성하기) 나만의 블로그를 만들게 된 계기 # 블로그 플랫폼을 선택하는데 있어 편의 기능, 외관, 작성 방식 등을 고려할 수 있습니다.\nVelog, 티스토리 등의 블로그 플랫폼을 이용해 봤지만 원하는 요소들을 전부 추가하는데는 제약이 많았습니다.\nVelog 사용 경험 # Velog는 개인적으로 느끼기에 블로그 플랫폼들 중에서 따로 테마를 설정하지 않아도 기본 스타일이 보기 좋다는 장점이 있지만, 다크모드 ON/OFF 버튼 추가 또는 코드 블럭 스타일 변경 등의 커스터마이징이 불가능하고, 무엇보다 카테고리 없이 태그로만 게시글을 구별해야 한다는걸 가장 큰 제약으로 인식했습니다.\n티스토리 사용 경험 # 티스토리는 테마 수정은 자유로운 편이지만, 테마 편집이든 블로그 게시글 작성이든 티스토리 UI에서 편집 과정을 거쳐야 반영되는 점에서 즉각적인 피드백이 어렵다고 느꼈습니다. 그리고, 무엇보다 글 편집이나 블로그 관리 등의 설정 메뉴에는 다크모드가 적용이 안돼서 개인적으로 쓰기 싫었습니다.\nHugo 사용 경험 # 3년 전에 Hugo를 활용한 Github 블로그를 만든 적이 있었는데, 당시에는 테마를 원하는대로 수정할 수 있을 만큼의 전문 지식이 없어서 다른 블로그 플랫폼들과 마찬가지로 잠깐 이용하다 말았지만, AI 에이전트의 도움을 받으면 자연어 프롬프트로 원하는 기능들을 추가해볼 수 있지 않을까라는 막연한 생각에 다시 Hugo 블로그를 도전해보게 되었습니다.\nHugo 블로그의 단점이라고 한다면 웹 호스팅을 직접 해야한다는 점입니다. 과거에 블로그를 운영할 때는 Github Pages 기능을 활용해 무료로 웹 호스팅을 제공받았습니다. 물론, 이번 블로그도 동일하게 Github Pages 기능을 사용할 것입니다.\n또 하나의 단점으로는 이미지 등록하기 어렵다는 점이 있습니다. 과거에는 Github에 이미지를 올려서 참조했지만, 이미지 포함한 변경사항을 커밋/푸쉬한 후 Github 웹사이트에서 raw 이미지 링크를 찾아와야 해서 비효율적이었습니다.\n이번에 새로운 블로그를 준비하면서 Dropbox를 통해 이미지를 드래그해서 올리고 간편하게 링크를 복사할 수 있는 방법을 알게 되어서 이미지 호스팅은 해결되었습니다. 이미지 호스팅과 관련해서는 별도의 글을 작성할 예정입니다.\n나만의 블로그 기획하기 # 나만의 블로그인만큼 원하는 외형과 기능들을 전부 집어넣을 생각입니다.\n물론, 기획한 내용들은 이미 현재 블로그에 구현되어 있어서 시각적으로 참고해볼 수 있습니다.\n레이아웃 # 블로그의 전체적인 구성은 과거 티스토리 블로그를 이용할 때 사용했던 hELLO 테마를 참고합니다.\n좌측에는 메뉴 영역을 표시합니다. 우측에는 목차 영역을 표시합니다. 중앙에는 본문 영역을 표시합니다. 본문의 상단에는 글 제목을 표시합니다. 본문의 하단에는 댓글 영역을 표시합니다. 모바일 화면에 맞춘 반응형 디자인을 고려합니다. 기능은 각 영역 별로 구분합니다.\n메뉴 영역 # 좌측 메뉴 영역의 기능들을 위에서부터 순서대로 나열합니다.\n프로필 사진을 표시합니다. 클릭하면 블로그 홈페이지와 연결됩니다. 소셜 링크 또는 기능성 버튼을 표시합니다. 제목을 표시하고, 다음으로 검색창을 표시합니다. 검색어를 입력하고 엔터 또는 검색 아이콘을 클릭하면 검색 페이지로 이동합니다. 카테고리 목록을 표시합니다. 카테고리는 최대 2단계로 펼치고 접을 수 있습니다. 최신글 목록을 보여줍니다. 작성일 순으로 최대 5개까지 표시합니다. 목차 영역 # 우측 목차 영역은 상단과 하단으로 구분됩니다.\n우측 상단의 고정된 위치에 목차를 표시합니다. 목차와 본문의 사이에 목차의 길이만큼 구분선을 추가합니다. 스크롤이 위치한 목차를 하이라이트로 강조합니다. 목차를 클릭하면 해당 위치로 이동하는 앵커 링크를 설정합니다. 우측 하단의 고정된 위치에 이동 버튼을 추가합니다. 맨 위로 이동, 맨 아래로 이동, 뒤로 가기 버튼을 세로로 나열합니다. 본문 영역 # 중앙의 본문 영역은 마크다운 문서가 렌더링되는 부분이며, 상단과 하단에 기능이 추가됩니다.\n본문 상단에는 카테고리, 글 제목, 작성일시를 순서대로 표시합니다. 글 하단에는 태그 목록을 표시합니다. 카테고리 또는 태그를 클릭하면 관련 게시글 목록으로 이동합니다. 헤더 영역 # 헤더 영역은 반응형 디자인의 일부로, 기본적으로는 나타나지 않고 모바일 사이즈에서만 표시됩니다.\n헤더 영역에는 메뉴 또는 목차를 펼치고 접을 수 있는 버튼이 좌우 양끝에 표시됩니다. 브라우저 너비에 따라 목차, 메뉴 순서로 숨기고 해당 영역이 숨겨질 때 버튼이 활성화됩니다. 헤더 영역의 가운데에는 블로그 제목을 표시하고 홈페이지로 이동하는 링크를 설정합니다. 헤더 영역은 반투명한 배경색을 가지고 스크롤 위치에 관계없이 고정됩니다. 푸터 영역 # 푸터 영역은 게시글(post) 유형의 레이아웃을 사용하는 경우만 설정합니다.\n본문에 설정된 카테고리 내 게시글 중에서 작성일 순으로 이전, 다음 게시글로 이동하는 버튼을 추가합니다. 댓글 영역을 추가합니다. 댓글 기능을 직접 만들진 않고 외부 서비스를 이용합니다. 기타 기능 # Open Graph 설정 및 외부 링크 미리보기 기능을 추가합니다. 스크롤 위치에 맞춰 상단에 진행도를 표시합니다. 다크모드 ON/OFF 버튼을 추가합니다. 기본 시스템 설정을 인식하여 라이트/다크모드를 설정합니다. 코드 블럭을 맥 터미널처럼 보이게 꾸밉니다. 코드 블럭 우측 상단에 복사 버튼을 추가합니다. 코드 블럭 우측 상단에 언어를 표시합니다. 홈 페이지를 추가합니다. 등록일 순으로 게시글을 정렬하여 목록으로 표시합니다. 태그 전용 페이지를 추가합니다. 태그는 Hugo에서 기본적으로 지원합니다. 카테고리 전용 레이아웃을 추가합니다. 자식 카테고리를 부모 카테고리로 그룹화한 디렉터리 구조를 가집니다. 부모/자식 카테고리 페이지에서 각각 하위/상위 카테고리로 이동을 지원합니다. 전체 카테고리 목록을 나열하는 페이지를 추가합니다. GA4를 연결하고 구글, 네이버 검색엔진에 등록합니다. 처음부터 위 목록을 전부 생각했던 것은 아니고, 블로그 테마를 발전시켜 나가면서 점진적으로 추가한 기능들을 같이 정리한 것입니다.\n기본 테마 선정하기 # 빈 프로젝트부터 시작하지는 않고 잘 만들어진 테마의 레이아웃을 참고할 예정입니다.\nHugo 테마는 아래 경로에서 찾아볼 수 있습니다.\nHugo Themes The world’s fastest framework for building websites themes.gohugo.io PaperMod 테마 사용 경험 # 처음 Hugo 블로그를 만들 땐 PaperMod라는 테마를 사용했습니다.\nPaperMod A fast, clean, responsive Hugo theme themes.gohugo.io 다크모드 ON/OFF 버튼 및 소셜 링크 기능을 지원하고 디자인이 마음에 들어 초기 테마로 사용을 했지만 Velog에서 느꼈던 단점인, 카테고리를 지원하지 않는 문제로 인해 블로그를 쓰고 싶다는 생각이 갈수록 줄어들었습니다.\n그래도 당시에 카테고리 기능을 만들어 보려고 여기저기 알아보면서, Tree-style category list page #24 이슈를 작성한 분께 메일을 보내 HTML 소스 파일을 공유받기도 했습니다. 이때 공유받은 소스 코드는 당시에는 사용하지 못했지만, 보관해뒀다가 현재 블로그에 녹여서 사용하고 있습니다.\nBook 테마 선정 # 최근에 다시 테마를 찾아보면서 가장 마음에 들은 것은 Book 테마입니다.\nBook Hugo documentation theme as simple as plain book themes.gohugo.io Book 테마는 좌측의 book-menu, 우측의 book-toc, 그리고 중앙의 본문인 book-page 영역으로 나눠집니다.\nbook-menu 영역에는 상단에 검색창이 있고 그 아래에 카테고리 목록을 표시합니다. 검색창에 키워드를 입력하면 카테고리 목록을 밀어내고 검색창 바로 아래에 검색 결과를 표시합니다. 따로 검색 결과를 나타내는 페이지가 존재하지는 않습니다.\nbook-toc 영역은 일반적인 목차(Table of Contents) 영역이며, 스크롤 위치에 관계없이 고정된 위치에 있습니다. 게시글 내 특정 위치로 이동할 수 있는 앵커 링크를 지원합니다. 앵커 링크를 클릭하면 부드럽게 이동하는 애니메이션이 적용되어 있습니다.\nbook-page 영역은 마크다운으로 작성한 본문이 렌더링되는 영역입니다. 해당 영역의 아래에 있는 book-footer 영역에는 이전, 다음 게시글로 이동할 수 있는 링크가 표시됩니다.\n추가로 관심을 가져볼만한 기능은, 브라우저가 정해진 모바일 크기만큼 줄어들게 되면 book-menu 및 book-toc 영역을 숨기고 book-header 영역을 표시하는 반응형 디자인입니다. book-header 영역은 본문에 표시되지 않는 게시글 제목을 중앙에 보여주고 좌우에 book-,enu 및 book-toc 영역을 펼치고 접을 수 있는 버튼을 제공합니다.\n과거 티스토리 블로그를 이용할 때 사용했던 hELLO 테마와 구성이 비슷해 해당 테마를 보자마자 기본 레이아웃으로 사용하면 좋겠다고 생각했습니다. 마침 hELLO 테마에서 가져오려는 기능들도 많아서 해당 테마가 적절했습니다.\n블로그 프로젝트 구성하기 # 마음에 드는 테마를 선정했다면 본격적으로 Hugo 프로젝트를 구성하여 테마를 적용해볼 차례입니다.\n1. Hugo 설치 # Mac 사용자라면 Homebrew를 통해 간단하게 Hugo를 설치하여 사용할 수 있습니다.\nCopy bash brew install hugo 설치가 완료되면, 버전 정보를 출력해서 정상 설치 여부를 확인해 봅니다.\nCopy bash % hugo version hugo v0.150.0+extended+withdeploy darwin/arm64 BuildDate=2025-09-08T13:01:12Z VendorInfo=brew 2. Hugo 프로젝트 생성 # Hugo 프로젝트를 생성하기 위해서는 터미널에서 아래 명령어를 입력합니다.\nCopy bash hugo new site \u0026lt;프로젝트명\u0026gt; Hugo 프로젝트는 다음과 같은 구조를 가집니다.\nCopy bash . ├── archetypes/ │ └── default.md ├── content/ ├── data/ ├── layouts/ ├── public/ ├── static/ ├── themes/ └── hugo.toml 각 폴더는 다음과 같은 역할 또는 목적이 있습니다.\narchtypes/ : 게시글 템플릿이 위치한 폴더이며, hugo new \u0026lt;파일명\u0026gt; 명령어로 템플릿 내용을 가지는 게시글 파일을 생성할 수 있습니다. content/ : 게시글 목록이 위치한 폴더입니다. data/ : CSV, JSON, YAML 등의 데이터들을 관리하는 폴더입니다. layouts/ : 블로그에 적용되는 HTML 형식을 관리하는 폴더입니다. themes/ 폴더보다 우선순위를 가집니다. public/ : 빌드한 결과, 즉 정적 HTML 파일들이 생성되는 폴더입니다. static/ : 빌드할 때 포함시킬 이미지, JS, CSS 등의 파일들이 위치하는 폴더입니다. themes/ : 만들어진 테마를 블로그에 적용하기 위해 위치시키는 폴더입니다. hugo.toml : 블로그에 대한 설정을 위한 파일입니다. YAML 등 다른 형식도 지원합니다. 3. Github 저장소 생성 # 블로그를 개발하면서 진행 과정을 기록해두면 과거에 어떤 작업을 했는지, 그리고 실수로 돌이킬 수 없는 오류가 발생해 이전 시점으로 돌아가고 싶은 경우에 버전 관리를 해두면 좋습니다.\n로컬에서 Git을 통해 버전 관리를 하면서 커밋한 이력을 Github에 올릴 것입니다. 이를 위한 Github 저장소를 생성합니다.\n단순히 버전 관리 목적으로 Github을 이용한다면 저장소 명칭은 아무렇게나 해도 괜찮지만, 아래에서 설명할 Github Pages 서비스를 이용하려면 \u0026lt;사용자명\u0026gt;.github.io 명칭을 사용해야 합니다.\nGithub 저장소를 생성했다면 앞에서 생성한 Hugo 프로젝트와 연동합니다.\nCopy bash git init git add . git commit -m \u0026#34;feat: new site\u0026#34; git branch -M main git remote add origin https://github.com/\u0026lt;사용자명\u0026gt;/\u0026lt;사용자명\u0026gt;.github.io.git git push -u origin main Hugo 프로젝트를 생성하면서 발생한 변경사항을 커밋하고 Github 저장소의 main 를 원격 저장소로 등록한 후 푸쉬합니다.\n4. Github Pages 설정 # 블로그를 인터넷 상의 모두에게 공개하기 위해서는 일반적으로 홈서버 구축, 도메인 구매, DNS 설정 등 신경써야 할게 많아서 편리한 웹 호스팅 서비스를 이용합니다. 컴퓨팅 자원을 제공하는 웹 호스팅 서비스들은 대부분 유료로 제공되기 때문에 무료로 간단하게 이용할 수 있는 Github Pages 서비스를 사용하고자 합니다.\nGithub 저장소의 설정(Settings) 탭에 접근한 후, 사이드바의 Pages 메뉴에서 Github Pages를 설정할 수 있습니다.\nGithub Actions를 사용해 빌드 과정을 커스터마이징할 수 있지만, 해당 프로젝트에서는 단순하게 별도의 브랜치에 HTML 소스코드를 두고 해당 브랜치에서 변경사항이 발생할 때마다 빌드되도록 설정하겠습니다.\nSource에 Deploy from a branch 를 선택하고 Branch에 대상 브랜치를 지정하면 Github Pages 설정이 완료되지만, 브랜치 지정 시 고려할 사항이 있어 우선 다음 단계로 넘어갑니다.\n5. Submodule 구성 # Submodule은 Git에서 외부 프로젝트를 현재 프로젝트에서 포함시킬 수 있게 하는 기능입니다. Submodule이 무엇인지 설명하려면 너무 길어지기 때문에 자세하게 알고 싶다면 저장소 안에 저장소 - git submodule 영상 등을 참고해주시기 바랍니다.\nSubmodule은 외부 프로젝트를 연결시키는 역할을 하지만, 반대로 이용하면 현재 프로젝트의 일부를 독립적인 외부 프로젝트로 분리할 수 있습니다. 해당 프로젝트에서는 다음 2가지 사유로 Submodule을 이용합니다.\nSubmodule로 분리하려는 폴더는 public/ 입니다. 해당 폴더는 Hugo 빌드한 결과가 생성되는 경로인데, 이때마다 많은 파일들이 변경사항에 포함됩니다. 자동으로 생성되는 파일들을 main 브랜치의 변경사항에 포함하고 싶지 않아 분리하려고 합니다.\n앞에서 Github Pages를 설정할 때 public/ 경로를 지정하려고 했는데, 이 때 고려할 사항이 브랜치에서 변경사항이 발생할 때마다 빌드가 진행된다는 점입니다. 커밋을 푸쉬하면서 빌드하고 싶지 않은 경우가 있는데 main 브랜치를 Github Pages 대상으로 지정하게 되면 빌드 시점을 제어할 수 없어 독립적인 브랜치를 사용해야 합니다.\n따라서, public/ 폴더를 독립적인 브랜치의 루트 경로로 지정하고, 해당 브랜치를 Submodule로 분리하려고 합니다.\n우선, 브랜치를 생성합니다. 브랜치명은 자유롭게 지정할 수 있으며 해당 프로젝트는 source 라고 지정했습니다.\nCopy bash git branch source main git checkout source source 브랜치는 public/ 폴더를 루트 경로로 가집니다. 즉, 현재 public/ 폴더는 비어있으므로 모든 파일을 삭제하고 원격 저장소에 올립니다. 이때, 모든 파일을 삭제한다고 .git 폴더까지 삭제하면 안됩니다.\nCopy bash find . -maxdepth 1 -not -name \u0026#39;.*\u0026#39; -exec rm -rf {} \\; git add . git commit -m \u0026#34;update: init source\u0026#34; git push origin source 원격 저장소에 브랜치를 추가했으면 다시 main 브랜치로 되돌아가서 원격의 source 브랜치를 Submodule로 연결합니다.\nCopy bash git checkout main rm -rf public git submodule add -b source https://github.com/\u0026lt;사용자명\u0026gt;/\u0026lt;사용자명\u0026gt;.github.io public git commit -m \u0026#34;feat: add submodule for source\u0026#34; 그리고, 다시 Github Pages 설정으로 되돌아가서 source 브랜치를 지정하고 설정을 저장하면 됩니다.\n추가적으로, 게시글 목록이 위치하는 content/ 폴더도 독립적으로 관리하고 싶어서 Submodule로 분리했습니다.\nsource 브랜치와 동일한 과정으로 content 브랜치를 생성했습니다.\nCopy bash git branch content main git checkout content find . -maxdepth 1 -not -name \u0026#39;.*\u0026#39; -exec rm -rf {} \\; git add . git commit -m \u0026#34;update: init content\u0026#34; git push origin content git checkout main rm -rf content git submodule add -b content https://github.com/\u0026lt;사용자명\u0026gt;/\u0026lt;사용자명\u0026gt;.github.io content git commit -m \u0026#34;feat: add submodule for content\u0026#34; 6. 테마 적용하기 # 해당 단계에서도 Submodule을 활용하여 별도의 저장소에 있는 테마를 themes/ 폴더 아래에 연결시킬 것입니다.\n만들어진 테마를 그대로 사용할 것이라면 해당 테마의 Github 저장소를 Submodule로 연결합니다.\nHugo Book 원본 테마를 사용하고 싶다면 아래 명령어를 입력합니다.\nBook 은 Hugo 설정에서 지정한 hugo-book 에 대한 테마명이고, 다른 테마라면 명칭을 변경합니다.\nCopy bash git submodule add https://github.com/alex-shpak/hugo-book themes/Book 테마를 커스터마이징할 것이라면 원본 테마를 Fork한 저장소를 Submodule로 연결합니다.\n테마 커스터마이징을 위해 만든 hugo-book-custom 저장소를 연결하겟습니다.\nCopy bash git submodule add https://github.com/minyeamer/hugo-book-custom themes/Book 7. 배포 스크립트 작성하기 # Hugo 빌드는 hugo -t \u0026lt;테마명\u0026gt; 명령어를 통해 수행합니다. 그리고, public/ 경로에 생성된 HTML 소스코드를 source 브랜치에 푸쉬하여 Github Pages 배포를 진행합니다.\n이 과정을 쉘 스크립트로 표현하면 아래와 같습니다.\nCopy bash #!/bin/bash echo -e \u0026#34;\\033[0;32mDeploying updates to GitHub***\\033[0m\u0026#34; # 모든 서브모듈의 변경사항을 업데이트 git submodule update --remote # `hugo -t \u0026lt;테마명\u0026gt;` 명령어로 Hugo 정적 페이지 렌더링 # `--gc` 옵션은 `garbage collection` 을 의미하며 불필요한 페이지를 삭제 hugo -t Book --gc # `source` 브랜치로 이동 cd public git add . # 인자가 없을 경우 현재 시간을 커밋 메시지로 등록 msg=\u0026#34;rebuild: $(date +\u0026#34;%Y-%m-%dT%H:%M:%S%z\u0026#34;)\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; # 빌드 결과를 `source` 브랜치에 반영 git push origin source # `main` 브랜치로 이동 cd .. # 현재까지의 변경사항을 `main` 브랜치에 반영 git add . if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; git push origin main 해당 내용의 deploy.sh 스크립트 파일을 생성하고 chmod 755 deploy.sh 명령어로 실행 권한을 부여하여 복잡한 배포 과정을 쉘 스크립트 하나를 실행하는 것으로 대체합니다.\n스크립트를 실행하면 Hugo 프로젝트를 HTML 소스코드로 빌드한 후 원격 저장소에 푸쉬하여 Github Pages 배포하게 됩니다. Github Actions를 확인하면 다음과 같은 과정으로 배포가 진행되는 것을 확인할 수 있습니다.\n모든 과정이 성공하면 https://\u0026lt;사용자명\u0026gt;.github.io/ 주소로 배포된 블로그를 조회할 수 있습니다.\n기본 Hugo Book 테마를 사용할 경우 배포했을 때 좌측 메뉴에 블로그 제목과 검색창만 덩그러니 놓여있을 것입니다. 임시로 게시글을 생성하고 게시글 경로로 직접 이동해보면 아래와 같은 결과를 확인할 수 있습니다.\n다음 게시글에서는 본격적으로 테마를 커스터마이징하는 과정을 진행하겠습니다.\n"},{"id":3,"href":"/blog/openup-handson/","title":"[OSSCA] 2025 오픈소스 컨트리뷰션 아카데미 - PyTorch 문서 한글화 참여 후기","section":"Posts","content":" 오픈소스 컨트리뷰션 아카데미 소개\n오픈소스 컨트리뷰션 아카데미 체험형 프로그램은 오픈소스 프로젝트에 익숙하지 않은 예비 개발자를 위한 6주 간의 속성 컨트리뷰션 체험형 패키지 입니다. 제가 참여한 PyTorch 문서 한글화 프로젝트는 파이토치 한국어 튜토리얼 문서를 번역하여 PR을 올리고 멘토/멘티로부터 리뷰를 받는 활동을 했습니다. 아카데미 활동에 관심있다면 해당 링크로 이동하여 향후 일정을 확인할 수 있습니다. 25년 9월 17일부터 10월 31일까지 6주간 OSSCA에 멘티로 참여하면서 경험한 과정을 공유드리고자 합니다.\nOSSCA 참가 신청 # 참여하게 된 계기 # OSSCA라는 과정을 처음 알게 된 건 2025 파이콘의 OpenUp 부스를 접하게 된 것입니다.\n평소에 오픈소스에 기여하는 활동을 동경했고, Github에 개인 프로젝트를 올리다 보면 오픈소스 단체로부터 메일을 받아 디스코드에 참가하는 경우도 있었습니다. 하지만, 아쉽게도 Github이란 도구를 통해서 팀 프로젝트를 진행한 경험이 없었고, PR을 올리는 법도 몰라 감히 기여해보겠다는 생각을 갖지 못했습니다.\n그렇게 솔로 코딩만 해오던 저에게 OSSCA 과정은 오픈소스라는 영역에 첫 발을 내딛을 수 있는 기회가 될 것이라 생각했습니다. 더욱이, 체험형은 기간도 6주로 비교적 짧고 직장인도 참여할 수 있어서 부담없이 참가 신청하게 되었습니다.\n프로젝트 선택 # 이번 체험형 프로그램에는 PyTorch, MDN, Ubuntu, Yocto, Braillify 프로젝트 중 하나를 선택할 수 있었습니다. 내가 선택한다고 합격이 보장되는 건 아니지만, 그래도 개인적으로는 과거 딥러닝 모델을 학습하면서 활용한 PyTorch가 가장 익숙했고, 그 외엔 MDN과 Ubuntu가 아는 거라 눈에 띄었습니다.\n참가 신청할 때 아마 2지망까지 고를 수 있었던 걸로 기억하는데, 1지망으로 PyTorch를 고르고 다음으로 MDN을 골랐을 것입니다. 다행히 1지망인 PyTorch 프로젝트에 합격하여 이번 6주 간의 과정을 완주하게 되었습니다.\n디스코드 참여 # 아쉽게도 오프라인 발대식에는 개인 일정과 겹쳐서 참가하지 못했고,\n해당 일정 전에 OSSCA 디스코드에 초대받아 어떤걸 하게 될지 둘러보았습니다.\n프로젝트 시작 전에 특별히 볼 건 없었고 팀별 채널에서 대화를 나누면서 향후 일정을 전달받게 되었습니다.\n처음 미팅은 온라인으로 진행했고, 첫 번째 과제로 멘토님의 테스트 repo에서 샘플 번역할 것을 전달받았습니다.\n1주차 - PyTorch 문서 샘플 번역 # PyTorch 튜토리얼에는 advanced, beginner, intermediate, recipes 4가지 폴더가 있는데, 각 조별로 특정 폴더에 할당되어 개인 별로 폴더 내 문서 하나를 선택해 3줄 정도의 샘플 번역을 진행했습니다.\nPR 올리기 # 로컬에서 샘플 번역한 결과는 테스트 repo에 PR을 올리고 조원들 간에 PR 리뷰 댓글을 달았습니다.\n저는 advanced/generic_join.rst 문서를 선택했고 조금 의욕이 들어 35줄 정도를 번역해서 올렸습니다.\n클로드로 문서를 전체 번역한 후에 직접 한 줄씩 다듬는 방식으로 하다보니 이때 이미 번역 초안은 완성되어 있었습니다. 그래서, 마지막 주차에 문서 전체를 번역하는 과제에서 다시 해당 문서를 선택했습니다.\n샘플 번역을 진행한 후엔 로컬에서 전체 튜토리얼을 렌더링하여 단순히 번역 퀄리티 뿐 아니라, reStructuredText 문법이 잘못된게 없는지 시각적으로 확인했습니다.\n이슈사항 (1) - RST 문법 # 이 과정에서, 마크다운만 알다가 RST라는 형식을 작성하다 보니, 일부 문법의 차이에서 오는 오류를 경험했습니다. 대표적으로 인라인 코드블럭을 마크다운에서는 백틱(`) 하나로 감싸서 표현했는데, RST에서는 백틱 2개로 감쌌습니다. 그리고, 백틱과 한글이 겹치면 외부 링크가 텍스트로 노출되어 보기에 좋지 않았습니다.\n이슈사항 (2) - 한국어 문법 # 온라인 미팅에서 PyTorchKorea 번역 가이드 및 번역 모범 사례를 공유 받았는데, 이 중에서 특히 신경써야 했던게 콜론(:)을 마침표(.)로 변환하는 규칙이었습니다. 평소에 특수문자 사용에 신경을 쓰지 않았다 보니, 그리고 개인적으로도 영어로 된 공식 문서를 자주 읽어 콜론 사용이 익숙하다 보니 처음 번역할 때 이러한 표현을 놓치는 경우가 많았습니다.\n이슈사항 (3) - 코드 블럭 # 마지막으로 겪었던 문제는 로컬에서 렌더링할 때 인라인 코드블럭이 아래 이미지처럼 공백 단위로 분리되어 보였습니다.\n원문 튜토리얼과 샘플 번역한 문서 간에 인라인 코드블럭을 표현하는 문법에 차이가 없기 때문에 로컬에서 렌더링할 때만 발생하는 문제겠지만 그래도 거슬려서 해결해보기로 했습니다.\n다행히 해당 사례에 대한 해결법이 Stack Overflow에 올라왔었고, 이를 참고하여 python 코드 블럭으로 인식하는 Role을 추가했습니다. Role에 대한 설명은 Sphinx 공식 문서에서 확인할 수 있습니다.\nInline code highlighting in reStructuredText - Stack OverflowStack Overflow Copy python .. role:: python(code) :language: python - :python:`__init__(self, joinables: List[Joinable], enable: bool = True, throw_on_early_termination: bool = False)` 위와 같이 수정하고 다시 렌더링하니 아래 이미지처럼 보기 좋게 나타났습니다.\n하지만, 마지막 주차에서 번역할 때 원문과 형식을 맞추기 위해 해당 표현은 제외했습니다.\n2주차 - 기존 문서 오탈자 수정 # 2주차 과제는 (1) 기존 문서 번역을 개선하고 (2) 용어집에 새 용어를 추가하는 2개의 활동을 전달받았습니다.\n두 작업 간에 순서는 없지만, 중간에 추석 연휴가 끼어서 이번 과제를 3주차까지 진행했기 때문에 편의상 각각의 주차로 나눴습니다.\n문법 오류 수정 # 기존 문서 번역을 개선하는 과제는 advanced_source/rpc_ddp_tutorial.rst 문서를 대상으로 진행했습니다. 분산 모델 병렬 처리를 결합하여 간단한 모델 학습시키는 방법에 대해 설명하는 문서인데, 번역 퀄리티를 개선할만한 점은 찾지 못했지만 이슈사항 (1) - RST 문법에 해당하는 링크가 텍스트로 노출되는 이슈 사항을 해결했습니다.\n#1010 PR을 올려 현재는 Merged 되었습니다.\n3주차 - 용어집에 새 용어 추가 # 오탈자 수정과 동시에 진행한 용어집 추가 과제는 PyTorch 튜토리얼 번역 가이드에 적절한 단어를 등록하는 과제입니다.\n용어 선정 # OSSCA 프로그램 참가자들 간에 투표를 통해 추가될 용어를 선정하기 때문에 (즉, 충분한 투표를 받지 않으면 용어집에 등록될 수 없기 때문에) 아무 단어나 고르지 않고 PyTorch를 사용하는 모두가 알만한 단어를 찾으려 했습니다.\n제가 선정한 단어는 confusion matrix 입니다. 정확도, 정밀도 등 모델의 성능 평가 시 사용되는 성능 지표를 도출할 때 바탕이 되는 참 긍정(TP), 거짓 긍정(FP) 수치를 나타낸 이 행렬은 PyTorch로 딥러닝 모델 학습을 해본 사람이라면 누구나 알만한 개념이라 생각했습니다.\n이슈 올리기 # 물론, 제 생각만을 설득의 근거로 사용할 순 없기 때문에 아래 이미지와 같이 #978 이슈를 올리면서 실제 번역 사례를 제시했습니다.\nconfusion matrix 는 \u0026ldquo;오차 행렬\u0026rdquo; 또는 \u0026ldquo;혼동 행렬\u0026rdquo; 이라고 번역되는 사례가 있는데, 비교적 공신력 있는 문서에서는 \u0026ldquo;혼동 행렬\u0026rdquo; 이라고 번역되는 사례가 많아서 후자를 선택했습니다. PyTorch 튜토리얼에서 \u0026ldquo;오차 행렬\u0026rdquo; 이라고 번역된 사례가 있어 고민했지만, 개인이 번역한 문서보다는 위키나 논문에서 사용된 사례가 더 공신력 있다고 판단했습니다.\n용어 투표 # 제안된 용어는 디스코드 채널에서 민주적으로 투표하여 선정했고, 제가 제안한 confusion matrix 도 채택되었습니다.\n#1009 PR을 올려 현재 Merged 되었으며 PyTorch 튜토리얼 번역 가이드에서 아래 표와 같이 보여지고 있습니다.\n영문 한글 작성자 추가 설명 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; CUDA CUDA 박지은 번역안함 confusion matrix 혼동 행렬 김민엽 convolution 합성곱 김현길 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 4~6주차 - PyTorch 문서 통번역 # 앞선 과제가 끝나고 다음 2주 동안은 PyTorch 튜토리얼에서 아직 번역되지 않은 문서를 선택해 통번역을 진행하는 과제를 수행했습니다.\n물론, 제 선택은 1주차에 진행한 advanced/generic_join.rst 문서입니다.\n2주 동안 진행되는 과제였지만, 이미 1주차에 만들어둔 번역 초안이 있어서 1주 내에 끝냈습니다. 에디터에서 영어 원문과 클로드가 한글로 번역한 문장을 비교해 보면서 어색한 표현을 고치는 방식으로 진행했습니다. 이후 마지막 주차에는 멘토분들이 PR 리뷰를 달아주며 번역 중 놓친 부분들을 추가로 개선했습니다.\n고려사항 (1) - 고유명사 구분 # 이번에 번역한 문서는 불균등한 입력이 주어지는 상황에서 분산 학습을 하기 위해 Join Context Manager 를 사용하는 예시를 안내합니다. 여기서 Context Manager 는 컨텍스트 관리자 라고 번역할 수 있는데, Join 은 해당 문서에서 설명하는 컨텍스트 관리자를 나타내는 고유명사로 강조해야 한다고 판단해 (그리고 이에 대한 번역 사례도 없어서) 영문으로 표기했습니다.\n또한, 번역을 하면 할수록 문서 전반적으로 Join 을 언급하는 경우가 많고, 예제의 클래스명이나 메서드명으로도 사용되어 더더욱 영문 표기가 맞다고 생각했습니다.\n중간에 아래의 문장처럼 join 을 동사로 사용하는 경우도 있었는데 아래 한글 문장처럼 번역했습니다.\n``Join`` is a context manager \u0026hellip; The context manager allows the ranks that exhaust their inputs early (i.e. *join* early) to shadow the collective communications performed by those that have not yet joined.\n``Join`` 은 \u0026hellip; 컨텍스트 관리자입니다. 입력이 먼저 끝난 (즉, 먼저 *join* 된) 랭크는 아직 *join* 되지 않은 랭크가 수행하는 집합통신을 모방할 수 있게 됩니다.\n원문에서 join 동사를 이탤릭체로 강조하기도 했고, 위와 같은 맥락에서 한글로 번역해버리면 (위 예시에서 \u0026ldquo;*join* early\u0026rdquo; 를 \u0026ldquo;먼저 참가된\u0026rdquo; 으로 번역하면) 문서에서 설명하는 고유명사 Join 과의 관련성을 잃어버릴 것 같아 영문으로 표기했습니다. 대신, 명사처럼 인라인 코드블럭으로 표현하지 않고 원문과 동일한 이탤릭체로 나타냈습니다.\n고려사항 (2) - 링크 검증 # 문서를 번역할 때 다른 문서와 연결된 링크도 올바른지 확인했습니다.\n연결된 페이지가 잘못된 경우는 없었지만, 한글로 번역된 페이지가 있음에도 원문으로 연결되는 경우 [1] 와, 영문 페이지의 특정 문단으로 향하는 앵커를 한글로 번역된 페이지 대상으로 그대로 사용하여 앵커가 동작하지 않는 경우 [2] 가 있었습니다.\n두 가지 경우를 모두 충족하는 사례가 아래 문장입니다.\nIn `Getting Started with Distributed Data Parallel - Basic Use Case`, you saw the general skeleton for using `DistributedDataParallel` to perform data parallel training. This implicitly schedules all-reduces in each backward pass to synchronize gradients across ranks.\n분산 데이터 병렬 처리 시작하기 - 기본적인 사용법 \u0026lt;https://tutorials.pytorch.kr/intermediate/ddp_tutorial.html#id3\u0026gt;`_ 에서, `DistributedDataParallel`_ 을 사용한 데이터 병렬 학습의 기본 구조를 살펴보았습니다.\n문장 도입부에 Getting Started... 링크는 렌더링되면서 https://tutorials.pytorch.kr/intermediate/ddp_tutorial.html#basic-use-case 로 연결되는데, 원문의 Basic Use Case 목차가 기본적인 사용법 이라고 번역되면서 #basic-use-case 앵커가 #id3 로 변환되었습니다. 따라서, 링크에 대한 표시 텍스트를 한글로 번역하는 김에 앵커도 한글로 번역된 문서에 맞게 직접 지정했습니다.\nPR 올리기 및 리뷰 반영 # 파이썬 코드를 포함해 450줄 분량을 번역했는데 작업량이 많다보니 놓치는 부분도 있었습니다.\n#1009 PR을 올린 후에 멘티와 멘토들로부터 리뷰를 받아 추가로 반영했습니다.\n예를 들어, 아래 이미지처럼 문장 끝에 마침표를 빼먹어 추가해야 함을 요청 받은 경우도 있고, 용어집과 다른 단어를 사용한 경우나 원문의 형식(공백 등)을 유지하지 않은 경우를 지적받기도 했습니다.\n리뷰로 제안된 사항을 반영한 후 리뷰를 남긴 멘토님들에게 커밋 해시를 포함한 답변을 남겼습니다. 커밋 해시를 적으면 Github에서 커밋에 대한 변경사항을 보여주는 페이지로 링크를 달아줘서 편했습니다.\n요청주신 변경사항은 새로운 커밋(564104a)을 통해 반영되었습니다!\n활동을 돌아보며 # PyTorch 팀에서 튜토리얼 문서 번역을 통해 오픈소스에 기여하는 활동을 해보며, PR 등 오픈소스에 기여하는 방식을 익히고 문서 번역 시에 고려해야 할 사항들을 배웠습니다. 오프라인 모임에서 직장인, 대학생 분들과 사소한 개발 토크를 나누며 네트워킹도 해보고, 매주마다 주어진 목표를 달성하는 성취감을 느낄 수 있어서 매우 보람찼습니다.\n앞으로는 OSSCA와 마찬가지로 2025 파이콘에서 알게 된 Airflow 한국 사용자 모임에서 활동하며 PyTorch와 같은 오픈소스인 Airflow의 기능적 개선에 기여해보려 합니다. OSSCA 과정은 제가 오픈소스에 기여하기 위한 첫 경험을 제공해주었고 이 경험은 앞으로도 잊지 않을 것입니다.\n"},{"id":4,"href":"/blog/spark-study-8/","title":"Apache Spark - 고차함수(Higher-Order Functions)","section":"Posts","content":"User-Defined Functions # 스파크는 자신의 기능을 정의할 수 있는 유연성을 제공한다. 이를 사용자 정의 함수(User-Defined Function, UDF)라고 한다.\nUDF를 생성하는 이점은 스파크 SQL 안에서 이를 사용할 수 있다는 것이다.\nSpark SQL UDF 활용 # 다음은 스파크 SQL UDF를 만드는 예시로, 인수를 세제곱하는 함수 cubed() 를 생성한다.\nCopy python from pyspark.sql.types import LongType # 큐브 함수 생성 def cubed(s): return s * s * s # UDF로 등록 spark.udf.register(\u0026#34;cubed\u0026#34;, cubed, LongType()) 스파크 SQL을 사용하여 cubed() 함수를 실행할 수 있다.\nCopy python # 임시 뷰 생성 spark.range(1, 9).createOrReplaceTempView(\u0026#34;udf_test\u0026#34;) # 큐브 UDF를 사용하여 쿼리 spark.sql(\u0026#34;SELECT id, cubed(id) AS id_cubed FROM udf_test\u0026#34;).show() Copy bash +---+--------+ | id|id_cubed| +---+--------+ | 1| 1| | 2| 8| | 3| 27| | 4| 64| | 5| 125| | 6| 216| | 7| 343| | 8| 512| +---+--------+ 스파크 SQL 평가 순서 # 스파크 SQL은 하위 표현식의 평가 순서를 보장하지 않는다. 예를 들어, 다음 쿼리에서 s IS NOT NULL 절이 strlen(s) \u0026gt; 1 절 이전에 실행된다는 것을 보장할 수 없다.\nCopy python spark.sql(\u0026#34;SELECT s FROM test1 WHERE s IS NOT NULL AND strlen(s) \u0026gt; 1\u0026#34;) 따라서, 다음 두 가지 null 검사 방식을 수행하는 것이 좋다.\nUDF 안에서 null 을 인식하고 null 검사를 수행할 필요가 있다. SQL문에서 IF 또는 CASE WHEN 식을 사용하여 null 검사를 수행하고 조건에 맞으면 UDF를 호출한다. Pandas UDF # PySpark UDF는 JVM과 파이썬 사이의 데이터 이동을 필요로 해서 Scala UDF보다 성능이 느렸다.\n이 문제를 해결하기 위해 Pandas UDF가 스파크 2.3 버전부터 도입되었다. Pandas UDF는 Apache Arrow를 사용하여 Pandas UDF를 정의하거나 함수 자체를 래핑할 수 있다. Apache Arrow 형식에 포함된 데이터라면 더이상 JVM으로 데이터를 전달하기 위해 직렬화나 피클할 필요가 없다.\nPandas UDF는 pandas.Series, pandas.DataFrame 과 같은 파이썬 유형 힌트로 유추한다. 예시로, 앞에서 정의한 큐브 함수를 Pandas UDF로 만들면 아래와 같다.\nCopy python import pandas as pd from pyspark.sql.functions import col, pandas_udf from pyspark.sql.types import LongType # 큐브 함수 생성 def cubed(a: pd.Series) -\u0026gt; pd.Series: return a * a * a # 큐브 함수에 대한 Pandas UDF 생성 cubed_udf = pandas_udf(cubed, returnType=LongType()) Pandas UDF는 아래와 같이 실행할 수 있다.\nCopy python # 스파크 데이터프레임 생성 df = spark.range(1, 4) # Pandas UDF를 실행 df.select(\u0026#34;id\u0026#34;, cubed_udf(col(\u0026#34;id\u0026#34;))).show() Copy bash +---+---------+ | id|cubed(id)| +---+---------+ | 1| 1| | 2| 8| | 3| 27| +---+---------+ 스파크 UI에서 시각화된 pandas_udf 함수의 실행 단계에 대한 DAG을 조회할 수 있다. Stage 0에서 ArrowEvalPython 연산이 Pandas UDF를 평가하는 단계이다.\n고차 함수 # 복잡한 데이터 유형은 단순한 데이터 유형의 결합이기 때문에 다음과 같이 조작할 수 있다.\n중첩된 구조를 개별 행으로 분해하고 각각에 함수를 적용한 후 중첩된 구조를 다시 만드는 방법 사용자 정의 함수를 사용하는 방법 하지만 배열과 같은 중첩된 구조를 분해하고 다시 만든다고 가정할 때, 셔플 작업이 발생해 결과 배열의 순서가 원래 배열의 순서와 동일하지 않을 수 있다.\n사용자 정의 함수를 사용할 경우에는 정렬 문제는 해결할 수 있지만, 직렬화 및 역직렬화 과정을 거치면서 발생하는 비용이 크다는 문제가 있다.\n내장 함수 # 복잡한 데이터 유형에 대해 스파크 2.4 이상 버전에 포함된 내장 함수를 사용할 수 있다. 자세한 건 공식 문서를 참고해볼 수 있는데, 그 중에서 배열과 맵에 대해서 일부를 알아본다.\n배열과 관련된 함수는 공식 문서에서 array 문단부터 시작하는 함수들을 참고하면 된다. 대표적으로 array_distinct 함수는 배열 내 중복을 제거하고, array_sort 함수는 배열을 오름차순으로 정렬한다. array로 시작하지 않는 함수 중에서도 concat 함수는 복수 개의 배열을 받아 하나의 배열로 합쳐주고, flatten 함수는 2차원 이상 중첩된 배열을 단일 배열로 플랫화한다. sequence 함수로 시작과 끝에 대한 배열을 생성할 수 있고, slice 함수로 배열의 특정 부분만 잘라낼 수도 있다.\n맵과 관련된 함수는 공식 문서에서 map 문단부터 시작하는 함수들을 참고하면 된다. 대표적으로 map_concat 함수는 복수 개의 맵을 하나의 맵으로 합쳐주고, map_keys 함수로 맵에서 키 배열만 추출할 수도 있다. map으로 시작하지 않는 함수 중에서도 element_at 함수는 주어진 키에 대한 값을 반환하고, cardinality 함수는 맵의 크기를 반환한다.\ntransform() # 내장 함수 외에도 익명 람다 함수를 인수로 사용하는 고차 함수 transform() 이 있다.\n고차 함수를 실행해보기 위해 아래와 같이 샘플 데이터 tC 를 만들어본다.\nCopy python from pyspark.sql.types import * schema = StructType([StructField(\u0026#34;celsius\u0026#34;, ArrayType(IntegerType()))]) t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]] t_c = spark.createDataFrame(t_list, schema) t_c.createOrReplaceTempView(\u0026#34;tC\u0026#34;) t_c.show() Copy bash +--------------------+ | celsius| +--------------------+ |[35, 36, 32, 30, ...| |[31, 32, 34, 55, 56]| +--------------------+ Celsius 단위를 Fahrenheit 단위로 바꾸는 transform() 함수를 사용해, celsius 열로부터 fahrenheit 열을 계산했다. 출력 결과는 아래와 같다.\nCopy python spark.sql(\u0026#34;\u0026#34;\u0026#34; SELECT celsius, transform(celsius, t -\u0026gt; ((t * 9) div 5) + 32) AS fahrenheit FROM tc \u0026#34;\u0026#34;\u0026#34;).show() Copy bash +--------------------+--------------------+ | celsius| fahrenheit| +--------------------+--------------------+ |[35, 36, 32, 30, ...|[95, 96, 89, 86, ...| |[31, 32, 34, 55, 56]|[87, 89, 93, 131,...| +--------------------+--------------------+ filter() # filter() 함수는 입력한 배열의 요소 중 부울 함수가 참인 요소만으로 구성된 배열을 생성한다.\nCopy python spark.sql(\u0026#34;\u0026#34;\u0026#34; SELECT celsius, filter(celsius, t -\u0026gt; t \u0026gt; 38) AS high FROM tc \u0026#34;\u0026#34;\u0026#34;).show() Copy bash +--------------------+--------+ | celsius| high| +--------------------+--------+ |[35, 36, 32, 30, ...|[40, 42]| |[31, 32, 34, 55, 56]|[55, 56]| +--------------------+--------+ exists() # exists() 함수는 입력한 배열의 요소 중 불린 함수를 만족시키는 것이 존재할 때 참을 반환한다.\nCopy python spark.sql(\u0026#34;\u0026#34;\u0026#34; SELECT celsius, exists(celsius, t -\u0026gt; t = 38) AS threshold FROM tc \u0026#34;\u0026#34;\u0026#34;).show() Copy bash +--------------------+---------+ | celsius|threshold| +--------------------+---------+ |[35, 36, 32, 30, ...| true| |[31, 32, 34, 55, 56]| false| +--------------------+---------+ 스파크 SQL 작업 # 스파크 SQL의 기능 중 일부는 DataFrame의 다양한 기능에서 유래된다. 이용가능한 작업에는 집계 함수, 수집 함수, 날짜 함수, 수학 함수, 정렬 함수, 문자열 함수, 윈도우 함수 등 매우 광범위하다.\nUnion # Union은 동일한 스키마를 가진 두 개의 서로 다른 DataFrame을 하나로 합치는 작업이다.\nSQL문으로 다음과 같이 표현할 수 있다.\nCopy sql (SELECT * FROM first_half) UNION ALL (SELECT * FROM second_half); 파이썬으로는 다음과 같이 표현할 수 있다.\nCopy python result = first_half.union(second_half) Join # Join은 두 개 이상의 DataFrame을 특정 조건을 기준으로 결합하여 하나로 합치는 작업이다.\nSQL문으로 다음과 같이 표현할 수 있다.\nCopy sql SELECT p.id AS productId, p.storeId, s.name AS storeName, p.name AS productName FROM product AS p LEFT JOIN store AS s ON p.storeId = s.id; 파이썬으로는 다음과 같이 표현할 수 있다.\nCopy python from pyspark.sql.functions import col product.join( store, product.storeId == store.id, how = \u0026#34;left\u0026#34; ).select( col(\u0026#34;p.id\u0026#34;).alias(\u0026#34;productId\u0026#34;), \u0026#34;p.storeId\u0026#34;, col(\u0026#34;s.name\u0026#34;).alias(\u0026#34;storeName\u0026#34;), col(\u0026#34;p.name\u0026#34;).alias(\u0026#34;productName\u0026#34;) ).show() 윈도우 # 윈도우 함수를 사용하면 모든 입력 행에 대해 단일값을 반환하면서 행 그룹에 대해 작업할 수 있다.\n순위를 매기는 작업과 관련해서는 RANK, DENSE_RANK, PERCENT_RANK, NTILE, ROW_NUMBER 함수가 있고, 집계와 관련해서는 MAX, MIN, COUNT, SUM, AVG 등의 함수가 있다.\nSQL문으로 다음과 같이 표현할 수 있다.\nCopy sql SELECT name, dept, salary, RANK() OVER (PARTITION BY dept ORDER BY salary) AS rank FROM employees; 파이썬으로는 다음과 같이 표현할 수 있다.\nCopy python from pyspark.sql.functions import rank from pyspark.sql import Window window = rank().over(Window.partitionBy(\u0026#34;dept\u0026#34;).orderBy(\u0026#34;salary\u0026#34;)).alias(\u0026#34;rank\u0026#34;) employees.select(\u0026#34;name\u0026#34;, \u0026#34;dept\u0026#34;, \u0026#34;salary\u0026#34;, window).show() 수정 # DataFrame 자체는 변경할 수 없지만, 열을 가공하여 새로운 DataFrame을 만드는 것과 같은 작업을 통해 수정할 수 있다.\n파이썬으로 활용 가능한 다음과 같은 예시가 있다.\nCopy python from pyspark.sql.functions import expr # 열 추가 foo2 = foo.withColumn( \u0026#34;status\u0026#34;, expr(\u0026#34;CASE WHEN delay \u0026lt;= 10 THEN \u0026#39;On-time\u0026#39; ELSE \u0026#39;Delayed\u0026#39; END\u0026#34;)) # 열 삭제 foo3 = foo2.drop(\u0026#34;delay\u0026#34;) # 칼럼명 바꾸기 foo4 = foo3.withColumnRenamed(\u0026#34;status\u0026#34;, \u0026#34;flight_status\u0026#34;) 피벗 # 로우와 칼럼을 바꿔야 하는 경우가 있다. 이 경우에 pivot 함수를 지원한다.\n피벗을 실행해보기 위해 아래와 같이 샘플 데이터를 만들어본다.\nCopy python from pyspark.sql import Row df1 = spark.createDataFrame([ Row(course=\u0026#34;dotNET\u0026#34;, year=2012, earnings=10000), Row(course=\u0026#34;Java\u0026#34;, year=2012, earnings=20000), Row(course=\u0026#34;dotNET\u0026#34;, year=2012, earnings=5000), Row(course=\u0026#34;dotNET\u0026#34;, year=2013, earnings=48000), Row(course=\u0026#34;Java\u0026#34;, year=2013, earnings=30000),]) df1.show() Copy bash +------+----+--------+ |course|year|earnings| +------+----+--------+ |dotNET|2012| 10000| | Java|2012| 20000| |dotNET|2012| 5000| |dotNET|2013| 48000| | Java|2013| 30000| +------+----+--------+ 위 데이터에서 course 를 열로, year 를 행으로, earnings 를 값으로 sum 집계해 구성한 피벗 테이블을 아래와 같이 만들 수 있다.\nCopy python df1.groupBy(\u0026#34;year\u0026#34;).pivot( \u0026#34;course\u0026#34;, [\u0026#34;dotNET\u0026#34;, \u0026#34;Java\u0026#34;]).sum(\u0026#34;earnings\u0026#34;).sort(\u0026#34;year\u0026#34;).show() Copy bash +----+------+-----+ |year|dotNET| Java| +----+------+-----+ |2012| 15000|20000| |2013| 48000|30000| +----+------+-----+ References # https://spark.apache.org/docs/latest/sql-ref-functions-udf-scalar.html https://books.japila.pl/pyspark-internals/sql/ArrowEvalPython/#evalType https://spark.apache.org/docs/latest/api/sql/index.html https://docs.databricks.com/aws/en/semi-structured/higher-order-functions https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.rank.html https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.pivot.html "},{"id":5,"href":"/blog/spark-study-7/","title":"Apache Spark - JDBC 및 데이터베이스","section":"Posts","content":"Spark SQL CLI # 스파크 SQL 쿼리를 실행하는 쉬운 방법은 spark-sql CLI이다. 스파크 SQL CLI는 Hive 메타스토어와 서비스와 통신하는 대신 Thrift JDBC 서버와 통신할 수 없다.\nHive 설치 # 진행하기 전에 Hive가 설치되어 있지 않아서 설치해야 했다.\n설치 과정은 [Hive] virtual box linux [ubuntu 18.04]에 하이브 설치,다운로드 4.ubuntu 에 Hive(하이브) 다운로드 게시글을 참고했다.\n브라우저 또는 curl, wget 등 명령어를 통해 압축 파일을 내려받는다. Copy bash wget https://dlcdn.apache.org/hive/hive-4.0.1/apache-hive-4.0.1-bin.tar.gz 압축 해제 프로그램을 사용하거나, 터미널에서 아래 명령어를 입력하여 압축 해제한다. Copy bash tar zxvf apache-hive-4.0.1-bin.tar.gz Hive 경로에 접근하기 위해 ~/.zshrc 에 환경변수를 설정한다. Copy bash export HIVE_HOME=/Users/{username}/hive-4.0.1 export PATH=$PATH:$HIVE_HOME/bin 변경 사항을 적용하기 위해 터미널을 재시작하거나 아래 명령어를 실행한다. Copy bash source ~/.zshrc $HIVE_HOME/bin/hive-config.sh 파일에 HDFS 경로를 추가한다. Copy bash export HADOOP_HOME=/Users/{username}/hadoop-3.4.0 HDFS에 Hive 디렉터리를 생성한다. Copy bash $HADOOP_HOME/sbin/start-all.sh Copy bash hdfs dfs -mkdir /tmp hdfs dfs -chmod g+w /tmp Copy bash hdfs dfs -mkdir -p /user/hive/warehouse hdfs dfs -chmod g+w /user/hive/warehouse Copy bash % hdfs dfs -ls / drwxrwxr-x - user supergroup 0 2025-07-12 10:08 /tmp drwxr-xr-x - user supergroup 0 2025-07-12 10:08 /user $HIVE_HOME/conf/hive-site.xml 파일에 아래 속성을 맨 윗부분에 추가한다. 파일이 없을 경우 동일한 경로의 hive-default.xml.template 파일을 hive-site.xml 이름의 파일로 복사한다. Copy xml \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;system:java.io.tmpdir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/tmp/hive/java\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;system:user.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;${user.name}\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; Derby DB를 시작한다. 오류가 발생할 경우 참고한 게시글을 확인해볼 수 있다. Copy bash $HIVE_HOME/bin/schematool -initSchema -dbType derby Copy bash Initializing the schema to: 4.0.0 Metastore connection URL:\tjdbc:derby:;databaseName=metastore_db;create=true Metastore connection Driver :\torg.apache.derby.jdbc.EmbeddedDriver Metastore connection User:\tAPP Starting metastore schema initialization to 4.0.0 Initialization script hive-schema-4.0.0.derby.sql ... Initialization script completed Hive CLI를 시작해본다. Copy bash $HIVE_HOME/bin/hive Copy bash Beeline version 4.0.1 by Apache Hive beeline\u0026gt; Hive 메타스토어 서버를 실행한다. Copy bash hive --service metastore \u0026amp; hive-site.xml 편집하기 # Hive Tables 공식문서에 따르면, Spark SQL로 Hive에 저장된 데이터에 액세스하려면 hive-site.xml, core-site.xml, hdfs-site.xml 파일들을 $SPARK_HOME/conf/ 경로에 배치해야 한다.\n그런데 위 파일들을 복사한 후 spark-sql 을 실행하니까 WARN HiveConf 메시지가 460줄이나 발생했다.\nCopy bash 25/07/12 11:00:36 WARN HiveConf: HiveConf of name hive.repl.dump.metadata.only.for.external.table does not exist 25/07/12 11:00:36 WARN HiveConf: HiveConf of name hive.druid.rollup does not exist 25/07/12 11:00:36 WARN HiveConf: HiveConf of name hive.repl.retain.prev.dump.dir does not exist ... 단순히 Hive 경로에서 $SPARK_HOME/conf/ 경로로 hive-site.xml 파일을 복사했는데, Spark가 사용하지 않는 속성들이 들어있어서 이러한 메시지가 발생했다.\n실제 동작에는 영향을 주지 않지만 spark-sql 을 실행할 때마다 이런 메시지를 볼 수는 없어서 hive-site.xml 파일에서 문제되는 속성들을 전부 삭제했다.\n속성을 하나씩 삭제하기에는 너무 많아서 파이썬 코드를 사용해 hive-site.xml 파일을 수정했다. properties 변수에 문제되는 속성의 이름에 대한 문자열 리스트를 할당하고 코드를 실행한다.\nCopy python import xml.etree.ElementTree as ET import os SPARK_HOME = os.environ.get(\u0026#34;SPARK_HOME\u0026#34;) properties = [] # 제거하고 싶은 속성 이름 리스트 tree = ET.parse(f\u0026#34;{SPARK_HOME}/conf/hive-site.xml\u0026#34;) root = tree.getroot() targets = [] # 삭제 대상 property 수집 for prop in root.findall(\u0026#34;property\u0026#34;): name = prop.find(\u0026#34;name\u0026#34;) if (name is not None) and (name.text in properties): targets.append(prop) for prop in targets: root.remove(prop) # 속성 삭제 tree.write(f\u0026#34;{SPARK_HOME}/conf/hive-site.cleaned.xml\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;, xml_declaration=True) 생성된 hive-site.cleaned.xml 내용을 확인하고 hive-site.xml 로 바꿔준다.\nspark-sql # $SPARK_HOME/bin/spark-sql 스크립트를 실행해 스파크 SQL CLI를 시작한다.\nCopy bash $SPARK_HOME/bin/spark-sql 셸을 시작하면 스파크 SQL 쿼리를 대화 형식으로 수행할 수 있다. --help 옵션을 통해 아래와 같은 CLI 옵션을 확인할 수 있다.\nCopy text CLI options: -d,--define \u0026lt;key=value\u0026gt; Hive 쿼리에서 사용할 변수(key)와 값(value)을 지정 --database \u0026lt;databasename\u0026gt; 사용할 데이터베이스 지정 -e \u0026lt;quoted-query-string\u0026gt; 명령어 입력창에서 직접 SQL 쿼리를 실행할 때 사용 -f \u0026lt;filename\u0026gt; SQL 쿼리가 작성된 파일을 실행할 때 사용 -H,--help 도움말 제공 --hiveconf \u0026lt;property=value\u0026gt; Hive 설정값을 지정할 때 사용 --hivevar \u0026lt;key=value\u0026gt; Hive 쿼리에서 사용할 변수(key)와 값(value)을 지정 -i \u0026lt;filename\u0026gt; CLI 실행 시 먼저 실행될 쿼리 파일 제공 -S,--silent 대화형 셸에서 결과만 출력하고 기타 정보는 무시 -v,--verbose SQL 쿼리문을 콘솔에 출력 스파크 SQL 테이블을 생성하려면 다음 쿼리를 실행한다.\nCopy bash spark-sql (default)\u0026gt; CREATE TABLE people (name STRING, age INT); Time taken: 0.685 seconds 테이블이 생성되었는지 확인한다.\nCopy bash spark-sql (default)\u0026gt; SHOW TABLES; people Time taken: 0.239 seconds, Fetched 1 row(s) 테이블을 생성하고 테이블에 데이터를 삽입한다.\nCopy bash spark-sql (default)\u0026gt; INSERT INTO people VALUES (\u0026#34;Michael\u0026#34;, NULL); Time taken: 1.728 seconds spark-sql (default)\u0026gt; INSERT INTO people VALUES (\u0026#34;Andy\u0026#34;, 30); Time taken: 0.601 seconds spark-sql (default)\u0026gt; INSERT INTO people VALUES (\u0026#34;Samantha\u0026#34;, 19); Time taken: 0.149 seconds 테이블에서 20세 미만의 사람들이 몇 명인지 확인해본다.\nCopy bash spark-sql (default)\u0026gt; SELECT * FROM people WHERE age \u0026lt; 20; Samantha\t19 Time taken: 0.285 seconds, Fetched 1 row(s) 비라인 작업 # 비라인은 SQLLine CLI를 기반으로 하는 JDBC 클라이언트다. 동일한 유틸리티를 사용해 스파크 쓰리프트 서버에 대해 스파크 SQL 쿼리를 실행할 수 있다.\n스파크 쓰리프트 JDBC/ODBC 서버를 시작하려면 $SPARK_HOME/sbin/start-thriftserver.sh 스크립트를 실행한다.\nCopy bash $SPARK_HOME/sbin/start-thriftserver.sh 비라인을 사용하여 쓰리프트 JDBC/ODBC 서버를 테스트한다.\nCopy bash $SPARK_HOME/bin/beeline 비라인을 구성하여 로컬 쓰리프트 서버에 연결한다. 사용자 이름은 로그인 계정을 입력하고 비밀번호는 비어 있다.\nCopy bash beeline\u0026gt; !connect jdbc:hive2://localhost:10000 Connecting to jdbc:hive2://localhost:10000 Enter username for jdbc:hive2://localhost:10000: user Enter password for jdbc:hive2://localhost:10000: Connected to: Spark SQL (version 4.0.0) Driver: Hive JDBC (version 2.3.10) Transaction isolation: TRANSACTION_REPEATABLE_READ 0: jdbc:hive2://localhost:10000\u0026gt; 비라인에서 스파크 SQL 쿼리를 실행할 수 있다.\nCopy bash 0: jdbc:hive2://localhost:10000\u0026gt; SHOW TABLES; +------------+------------+--------------+ | namespace | tableName | isTemporary | +------------+------------+--------------+ | default | people | false | +------------+------------+--------------+ 1 row selected (0.297 seconds) Copy bash 0: jdbc:hive2://localhost:10000\u0026gt; SELECT * FROM people; +-----------+-------+ | name | age | +-----------+-------+ | Samantha | 19 | | Andy | 30 | | Michael | NULL | +-----------+-------+ 3 rows selected (1.44 seconds) 쓰리프트 서버를 중지할 때는 stop-thriftserver.sh 스크립트를 실행한다.\nCopy bash $SPARK_HOME/sbin/stop-thriftserver.sh 외부 데이터 소스 # JDBC # 스파크 SQL에는 JDBC를 사용하여 다른 데이터베이스에서 데이터를 읽을 수 있는 데이터 소스 API가 포함되어 있다. 스파크 SQL의 이점을 활용하여 쿼리 결과를 DataFrame으로 반환받을 수 있다.\nJDBC 데이터 소스에 연결하려면 JDBC 드라이버를 지정해야 한다. spark-shell 을 실행할 때 클래스 경로를 지정할 수 있다. 클래스 경로에 특정 데이터베이스용 JDBC 드라이버를 포함해야 한다.\nCopy bash $SPARK_HOME/bin/spark-shell --driver-class-path $database.jar --jars $database.jar 데이터 소스 옵션 # 사용자는 데이터 소스 옵션에서 JDBC 연결 속성을 지정할 수 있다. 다음과 같은 일반적인 연결 속성을 제공한다.\nuser, password : 데이터 소스에 로그인하기 위한 계정 정보 url : JDBC 연결 URL, jdbc:subprotocol:subname 와 같은 형식 dbtable : 읽거나 쓸 JDBC 테이블, query 옵션과 동시에 사용할 수는 없다. query : 스파크로 데이터를 읽어오는 데 사용되는 쿼리, dbtable 옵션과 동시에 사용할 수는 없다. driver : 지정한 URL에 연결하는 데 사용할 JDBC 드라이버의 클래스 이름 스파크 SQL과 JDBC 외부 소스 간에 많은 양의 데이터를 전송할 때 데이터 소스를 분할할 필요가 있다. 대규모 작업에서 다음과 같은 속성을 사용할 수 있다.\nnumPartitions : 테이블 읽기 및 쓰기에서 병렬 처리를 위해 사용할 수 있는 최대 파티션 수, 또는 최대 동시 JDBC 연결 수 partitionColumn : 외부 소스를 읽을 때 파티션을 결정하기 위해 사용되는 칼럼 (숫자, 날짜, 또는 타임스탬프) lowerBound : 파티션 크기에 대한 파티션 열의 최솟값 upperBound : 파티션 크기에 대한 파티션 열의 최댓값 numPartitions 는 스파크 워커 수의 배수를 사용하는 것이 좋지만, 소스 시스템이 읽이 요청을 얼마나 잘 처리할 수 있는지 확인해야 한다.\npartitionColumn 은 데이터 스큐를 방지하기 위해 균일하게 분산될 수 있는 열을 선택해야 한다.\n예를 들어, {numPartitions : 10, lowerBound : 1000, upperBound : 10000} 을 선택했지만 대부분이 2000에서 3000 사이의 값을 요청하는 경우 다른 partitionColumn 을 사용하거나 새 항목을 생성하는 것이 좋다.\nPostgreSQL # PostgreSQL 실행 # PostgreSQL은 따로 설치하지 않고 Docker 컨테이너로 실행했다.\nCopy bash % docker run --name postgres13 -d -p 5432:5432 postgres:13 % docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6d3a827005a6 postgres:13 \u0026#34;docker-entrypoint.s…\u0026#34; 1 seconds ago Up 2 seconds 0.0.0.0:5432-\u0026gt;5432/tcp, [::]:5432-\u0026gt;5432/tcp postgres13 postgres13 컨테이너에 접속하면서 PostgreSQL 프롬프트에 진입한다.\nCopy bash % docker exec -it postgres13 psql -U postgres psql (13.21 (Debian 13.21-1.pgdg120+1)) Type \u0026#34;help\u0026#34; for help. postgres=# SparkSession에서 접속해보기 위해 임시로 사용자, 스키마, 테이블을 생성했다.\nCopy sql CREATE USER spark WITH PASSWORD \u0026#39;spark\u0026#39;; CREATE SCHEMA spark_schema AUTHORIZATION spark; CREATE TABLE spark_schema.users ( id SERIAL PRIMARY KEY, name VARCHAR(100) NOT NULL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); GRANT ALL PRIVILEGES ON SCHEMA spark_schema TO spark; GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA spark_schema TO spark; 3개 행만 추가해보고 내용을 확인해봤다.\nCopy sql INSERT INTO spark_schema.users (name) VALUES (\u0026#39;김민수\u0026#39;); INSERT INTO spark_schema.users (name) VALUES (\u0026#39;이민수\u0026#39;); INSERT INTO spark_schema.users (name) VALUES (\u0026#39;박민수\u0026#39;); Copy sql postgres=# SELECT * FROM spark_schema.users; id | name | created_at ----+--------+---------------------------- 1 | 김민수 | 2025-07-12 11:29:42.40485 2 | 이민수 | 2025-07-12 11:29:47.036362 3 | 박민수 | 2025-07-12 11:29:50.087099 (3 rows) PostgreSQL 드라이버 다운로드 # PostgreSQL 데이터베이스에 연결하려면 JDBC 드라이버 파일을 클래스 경로에 추가한다.\n이미지 링크로 연결된 위 웹사이트에서 Java 버전에 맞는 파일을 다운로드 받을 수 있는데 Java 8 이상인 경우 아래 URL을 통해 직접 다운로드 받을 수도 있다.\nCopy bash wget https://jdbc.postgresql.org/download/postgresql-42.7.7.jar PostgreSQL 데이터 읽기 # SparkSession을 생성할 때 앞단계에서 내려받은 JDBC 드라이버 파일의 경로를 spark.driver.extraClassPath 설정값으로 전달한다.\nCopy python from pyspark.sql import SparkSession import os SPARK_HOME = os.environ.get(\u0026#34;SPARK_HOME\u0026#34;) spark = (SparkSession .builder .config(\u0026#34;spark.driver.extraClassPath\u0026#34;, f\u0026#34;{SPARK_HOME}/jars/postgresql-42.7.7.jar\u0026#34;) \\ .appName(\u0026#34;PostgresExample\u0026#34;) .getOrCreate()) postgres 데이터베이스의 spark_schema.users 테이블의 데이터를 가져온다. 데이터를 출력해보면 앞에서 추가한 3개 행이 반환되는 것을 볼 수 있다.\nCopy python df = spark.read.format(\u0026#34;jdbc\u0026#34;) \\ .option(\u0026#34;url\u0026#34;, \u0026#34;jdbc:postgresql://localhost:5432/postgres\u0026#34;) \\ .option(\u0026#34;dbtable\u0026#34;, \u0026#34;spark_schema.users\u0026#34;) \\ .option(\u0026#34;user\u0026#34;, \u0026#34;spark\u0026#34;) \\ .option(\u0026#34;password\u0026#34;, \u0026#34;spark\u0026#34;) \\ .load() df.show() Copy sql +---+------+--------------------+ | id| name| created_at| +---+------+--------------------+ | 1|김민수|2025-07-12 11:29:...| | 2|이민수|2025-07-12 11:29:...| | 3|박민수|2025-07-12 11:29:...| +---+------+--------------------+ PostgreSQL 데이터 쓰기 # 반대로 DataFrame을 PostgreSQL에 새로운 테이블로 저장할 수도 있다.\nCopy python df.write.format(\u0026#34;jdbc\u0026#34;) \\ .option(\u0026#34;url\u0026#34;, \u0026#34;jdbc:postgresql://localhost:5432/postgres\u0026#34;) \\ .option(\u0026#34;dbtable\u0026#34;, \u0026#34;spark_schema.new_users\u0026#34;) \\ .option(\u0026#34;user\u0026#34;, \u0026#34;spark\u0026#34;) \\ .option(\u0026#34;password\u0026#34;, \u0026#34;spark\u0026#34;) \\ .save() PostgreSQL에서 새로운 테이블을 조회했을 때 동일한 데이터가 저장된 것을 볼 수 있다.\nCopy sql postgres=# SELECT * FROM spark_schema.new_users; id | name | created_at ----+--------+------------------------------- 1 | 김민수 | 2025-07-12 02:29:42.40485+00 2 | 이민수 | 2025-07-12 02:29:47.036362+00 3 | 박민수 | 2025-07-12 02:29:50.087099+00 (3 rows) 또한, 기존 테이블에 새로운 행으로 추가할 수도 있다.\nCopy python df.select(\u0026#34;name\u0026#34;).write.format(\u0026#34;jdbc\u0026#34;) \\ .option(\u0026#34;url\u0026#34;, \u0026#34;jdbc:postgresql://localhost:5432/postgres\u0026#34;) \\ .option(\u0026#34;dbtable\u0026#34;, \u0026#34;spark_schema.users\u0026#34;) \\ .option(\u0026#34;user\u0026#34;, \u0026#34;spark\u0026#34;) \\ .option(\u0026#34;password\u0026#34;, \u0026#34;spark\u0026#34;) \\ .mode(\u0026#34;append\u0026#34;) \\ .save() spark_schema.users 테이블의 id 열이 SERIAL 타입인데, 시퀀스에 대한 권한이 없어서 위 명령어를 실행하면 오류가 발생했다. 그래서 PostgreSQL에서 spark 사용자에게 권한을 부여했다.\nCopy sql GRANT USAGE, SELECT ON SEQUENCE spark_schema.users_id_seq TO spark; 스파크의 DataFrameWriter를 통해 spark_schema.users 테이블에 새로운 행을 추가하고 데이터를 조회하면 아래와 같이 3개의 행이 더 추가된 것을 볼 수 있다.\nCopy sql postgres=# SELECT * FROM spark_schema.users; id | name | created_at ----+--------+---------------------------- 1 | 김민수 | 2025-07-12 11:29:42.40485 2 | 이민수 | 2025-07-12 11:29:47.036362 3 | 박민수 | 2025-07-12 11:29:50.087099 4 | 김민수 | 2025-07-12 20:42:51.519473 5 | 이민수 | 2025-07-12 20:42:51.519473 6 | 박민수 | 2025-07-12 20:42:51.519473 (6 rows) 비슷한 시간에 데이터를 추가했는데 컨테이너는 UTC 시간대고 SparkSession은 KST 시간대에 있어서 created_at 이 9시간 차이가 나는 것 같다.\nMySQL # MySQL 실행 # 마찬가지로 MySQL 컨테이너를 실행한다.\nCopy bash % docker run --name mysql8 -e MYSQL_ROOT_PASSWORD=root -d -p 3306:3306 mysql:8 % docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 48d3c4f28fd6 mysql:8 \u0026#34;docker-entrypoint.s…\u0026#34; 11 seconds ago Up 10 seconds 0.0.0.0:3306-\u0026gt;3306/tcp, [::]:3306-\u0026gt;3306/tcp mysql8 6d3a827005a6 postgres:13 \u0026#34;docker-entrypoint.s…\u0026#34; About an hour ago Up About an hour 0.0.0.0:5432-\u0026gt;5432/tcp, [::]:5432-\u0026gt;5432/tcp postgres13 mysql8 컨테이너에 접속하면서 MySQL 프롬프트에 진입한다.\nCopy bash % docker exec -it mysql8 mysql -u root -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 9 Server version: 8.4.5 MySQL Community Server - GPL ... mysql\u0026gt; SparkSession에서 접속해보기 위해 임시로 사용자, 스키마, 테이블을 생성했다.\nCopy sql CREATE USER \u0026#39;spark\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;spark\u0026#39;; CREATE DATABASE spark_db; USE spark_db; CREATE TABLE users ( id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(100) NOT NULL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); GRANT ALL PRIVILEGES ON spark_db.* TO \u0026#39;spark\u0026#39;@\u0026#39;%\u0026#39;; FLUSH PRIVILEGES; 3개 행만 추가해보고 내용을 확인해봤다.\nCopy sql INSERT INTO users (name) VALUES (\u0026#39;kim\u0026#39;); INSERT INTO users (name) VALUES (\u0026#39;lee\u0026#39;); INSERT INTO users (name) VALUES (\u0026#39;park\u0026#39;); Copy sql mysql\u0026gt; SELECT * FROM users; +----+------+---------------------+ | id | name | created_at | +----+------+---------------------+ | 1 | kim | 2025-07-12 12:08:41 | | 2 | lee | 2025-07-12 12:08:45 | | 3 | park | 2025-07-12 12:08:48 | +----+------+---------------------+ 3 rows in set (0.01 sec) MySQL 드라이버 다운로드 # MySQL 데이터베이스에 연결하려면 JDBC 드라이버 파일을 클래스 경로에 추가한다.\n이미지 링크로 연결된 위 웹사이트 또는 아래와 같은 curl, wget 명령어 등으로 버전에 맞는 압축 파일을 다운로드 받을 수 있다.\nCopy bash wget https://downloads.mysql.com/archives/get/p/3/file/mysql-connector-j-8.4.0.tar.gz 압축 파일을 해제하면 JDBC 드라이버 파일을 확인할 수 있다.\nCopy bash % tar zxvf mysql-connector-j-8.4.0.tar.gz % ls -la mysql-connector-j-8.4.0 total 5888 drwxr-xr-x@ 10 user staff 320 Mar 13 2024 . drwx------@ 49 user staff 1568 Jul 12 21:12 .. -rw-r--r--@ 1 user staff 282811 Mar 13 2024 CHANGES -rw-r--r--@ 1 user staff 188 Mar 13 2024 INFO_BIN -rw-r--r--@ 1 user staff 134 Mar 13 2024 INFO_SRC -rw-r--r--@ 1 user staff 82896 Mar 13 2024 LICENSE -rw-r--r--@ 1 user staff 1624 Mar 13 2024 README -rw-r--r--@ 1 user staff 91633 Mar 13 2024 build.xml -rw-r--r--@ 1 user staff 2533399 Mar 13 2024 mysql-connector-j-8.4.0.jar drwxr-xr-x@ 8 user staff 256 Mar 13 2024 src MySQL 데이터 읽기 # SparkSession을 생성할 때 MySQL JDBC 드라이버 파일의 경로를 spark.driver.extraClassPath 설정값으로 전달한다.\nCopy python from pyspark.sql import SparkSession import os SPARK_HOME = os.environ.get(\u0026#34;SPARK_HOME\u0026#34;) spark = (SparkSession .builder .config(\u0026#34;spark.driver.extraClassPath\u0026#34;, f\u0026#34;{SPARK_HOME}/jars/mysql-connector-j-8.4.0.jar\u0026#34;) \\ .appName(\u0026#34;MySQLExample\u0026#34;) .getOrCreate()) spark_db 데이터베이스의 users 테이블의 데이터를 가져온다. 데이터를 출력해보면 앞에서 추가한 3개 행이 반환되는 것을 볼 수 있다.\nCopy python df = spark.read.format(\u0026#34;jdbc\u0026#34;) \\ .option(\u0026#34;url\u0026#34;, \u0026#34;jdbc:mysql://localhost:3306/spark_db\u0026#34;) \\ .option(\u0026#34;dbtable\u0026#34;, \u0026#34;users\u0026#34;) \\ .option(\u0026#34;user\u0026#34;, \u0026#34;spark\u0026#34;) \\ .option(\u0026#34;password\u0026#34;, \u0026#34;spark\u0026#34;) \\ .load() df.show() Copy sql +---+----+-------------------+ | id|name| created_at| +---+----+-------------------+ | 1| kim|2025-07-12 12:08:41| | 2| lee|2025-07-12 12:08:45| | 3|park|2025-07-12 12:08:48| +---+----+-------------------+ MySQL 데이터 쓰기 # 반대로 DataFrame을 MySQL에 새로운 테이블로 저장할 수도 있다.\nCopy python df.write.format(\u0026#34;jdbc\u0026#34;) \\ .option(\u0026#34;url\u0026#34;, \u0026#34;jdbc:mysql://localhost:3306/spark_db\u0026#34;) \\ .option(\u0026#34;dbtable\u0026#34;, \u0026#34;new_users\u0026#34;) \\ .option(\u0026#34;user\u0026#34;, \u0026#34;spark\u0026#34;) \\ .option(\u0026#34;password\u0026#34;, \u0026#34;spark\u0026#34;) \\ .save() MySQL에서 새로운 테이블을 조회했을 때 동일한 데이터가 저장된 것을 볼 수 있다.\nCopy sql mysql\u0026gt; SELECT * FROM new_users; +------+------+---------------------+ | id | name | created_at | +------+------+---------------------+ | 1 | kim | 2025-07-12 12:08:41 | | 2 | lee | 2025-07-12 12:08:45 | | 3 | park | 2025-07-12 12:08:48 | +------+------+---------------------+ 3 rows in set (0.00 sec) 또한, 기존 테이블에 새로운 행으로 추가할 수도 있다.\nCopy python df.select(\u0026#34;name\u0026#34;).write.format(\u0026#34;jdbc\u0026#34;) \\ .option(\u0026#34;url\u0026#34;, \u0026#34;jdbc:mysql://localhost:3306/spark_db\u0026#34;) \\ .option(\u0026#34;dbtable\u0026#34;, \u0026#34;users\u0026#34;) \\ .option(\u0026#34;user\u0026#34;, \u0026#34;spark\u0026#34;) \\ .option(\u0026#34;password\u0026#34;, \u0026#34;spark\u0026#34;) \\ .mode(\u0026#34;append\u0026#34;) \\ .save() 스파크의 DataFrameWriter를 통해 users 테이블에 새로운 행을 추가하고 데이터를 조회하면 아래와 같이 3개의 행이 더 추가된 것을 볼 수 있다.\nCopy sql mysql\u0026gt; SELECT * FROM users; +----+------+---------------------+ | id | name | created_at | +----+------+---------------------+ | 1 | kim | 2025-07-12 12:08:41 | | 2 | lee | 2025-07-12 12:08:45 | | 3 | park | 2025-07-12 12:08:48 | | 4 | kim | 2025-07-12 12:24:29 | | 5 | lee | 2025-07-12 12:24:29 | | 6 | park | 2025-07-12 12:24:29 | +----+------+---------------------+ 6 rows in set (0.00 sec) References # https://spark.apache.org/docs/latest/sql-distributed-sql-engine-spark-sql-cli.html https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html https://spidyweb.tistory.com/215 https://dlcdn.apache.org/hive/hive-4.0.1/ https://kevin717.tistory.com/50 https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html https://jdbc.postgresql.org/download/ https://downloads.mysql.com/archives/c-j/ "},{"id":6,"href":"/blog/spark-study-6/","title":"Apache Spark - 데이터 소스","section":"Posts","content":"Data Source API # DataFrameReader # DataFrameReader는 데이터 소스에서 DataFrame으로 데이터를 읽는 방식이다. 아래와 같이 권장되는 사용 패턴이 있다.\nCopy python DataFrameReader .format(args) # 데이터 소스 형식 .option(\u0026#34;key\u0026#34;, \u0026#34;value\u0026#34;) # 키/값 쌍으로 연결되는 옵션 .schema(args) # DDL 문자열 또는 StructType .load() # 데이터 소스의 경로 데이터 소스 형식에는 인수로 \u0026ldquo;parquet\u0026rdquo;, \u0026ldquo;csv\u0026rdquo;, \u0026ldquo;txt\u0026rdquo;, \u0026ldquo;json\u0026rdquo;, \u0026ldquo;jdbc\u0026rdquo;, \u0026ldquo;orc\u0026rdquo;, \u0026ldquo;avro\u0026rdquo; 등이 전달된다. 기본값은 \u0026ldquo;parquet\u0026rdquo; 또는 spark.sql.sources.default 에 지정된 항목이 설정된다.\nJSON이나 CSV 형식은 option() 함수에서 스키마를 유추하는 inferSchema 옵션을 적용할 수 있지만, 스키마를 제공하면 로드 속도가 빨라진다.\nSparkSession 인스턴스를 통해서 DataFrame에 액세스할 경우 read() 또는 readStream() 을 사용할 수 있다. read() 는 정적 데이터 소스에서 DataFrame을 읽어 오며, readStream() 은 스트리밍 소스에서 인스턴스를 반환한다.\nDataFrameWriter # DataFrameWriter는 데이터 소스에 데이터를 저장하거나 쓰는 작업을 수행한다. 권장되는 사용 형식은 다음과 같다.\nCopy python DataFrameWriter .format(args) # 데이터 소스 형식 .option(args) # 키/값 쌍으로 연결되는 옵션 .bucketBy(args) # 버킷 개수 및 버킷 기준 칼럼 이름 .partitionBy(args) # 데이터 소스의 경로 .save(path) # 저장할 테이블 DataFrame에서 인스턴스에 액세스할 경우 write() 또는 writeStream() 을 사용할 수 있다.\nData Sources # Parquet # 스파크의 기본 데이터 소스인 Parquet는 다양한 I/O 최적화를 제공하는 오픈소스 칼럼 기반 파일 형식이다. 압축을 통해 저장 공간을 절약하고 데이터 칼럼에 대한 빠른 액세스를 허용한다.\nParquet 파일은 데이터 파일, 메타데이터, 여러 압축 파일 및 일부 상태 파일이 포함된 디렉터리 구조가 저장된다. 메타데이터에는 파일 형식의 버전, 스키마, 경로 등의 칼럼 데이터가 포함된다.\ndatabricks/LearningSparkV2의 databricks-datasets/learning-spark-v2/flights/summary-data/parquet 경로에서 2010-summary.parquet/ 디렉터리를 가져온다.\nParquet 파일의 디렉터리에는 다음과 같은 파일 집합이 포함된다.\nCopy bash % ls -la data/flights/summary-data/parquet/2010-summary.parquet/ -rwxr-xr-x@ ... 0 ... _SUCCESS -rwxr-xr-x@ ... 3921 ... part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet Parquet 파일을 DataFrame으로 읽으려면 형식과 경로를 지정하기만 하면 된다. spark.sql.sources.default 설정을 하지 않았다면 .format(\u0026quot;parquet\u0026quot;) 함수는 생략해도 된다.\nCopy python file = \u0026#34;data/flights/summary-data/parquet/2010-summary.parquet\u0026#34; df = spark.read.format(\u0026#34;parquet\u0026#34;).load(file) df.show(5) Copy bash +-----------------+-------------------+-----+ |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| +-----------------+-------------------+-----+ | United States| Romania| 1| | United States| Ireland| 264| | United States| India| 69| | Egypt| United States| 24| |Equatorial Guinea| United States| 1| +-----------------+-------------------+-----+ Parquet 파일을 Spark SQL 테이블로 읽으려면 아래와 같은 스파크 SQL을 사용할 수 있다.\nCopy sql CREATE OR REPLACE TEMPORARY VIEW delay_flights USING parquet OPTIONS ( path \u0026#34;data/flights/summary-data/parquet/2010-summary.parquet\u0026#34;) 메타데이터가 궁금해서 parquet-tools 라이브러리를 설치하고, inspect 명령어로 part-XXXX 압축 파일을 조회했다. 아래와 같이 출력되었는데, 3개의 칼럼 DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count 에 대한 데이터 타입 등의 정보가 상세히 적혀 있다. 스파크는 해당 데이터 타입을 읽기 때문에, 위 DataFrame에 printSchema() 출력한 결과는 아래 칼럼별 데이터 타입과 같다.\nCopy bash % parquet-tools inspect data/flights/summary-data/parquet/2010-summary.parquet/part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet ############ file meta data ############ created_by: parquet-mr (build 32c46643845ea8a705c35d4ec8fc654cc8ff816d) num_columns: 3 num_rows: 255 num_row_groups: 1 format_version: 1.0 serialized_size: 658 ############ Columns ############ DEST_COUNTRY_NAME ORIGIN_COUNTRY_NAME count ############ Column(DEST_COUNTRY_NAME) ############ name: DEST_COUNTRY_NAME path: DEST_COUNTRY_NAME max_definition_level: 1 max_repetition_level: 0 physical_type: BYTE_ARRAY logical_type: String converted_type (legacy): UTF8 compression: GZIP (space_saved: 37%) ############ Column(ORIGIN_COUNTRY_NAME) ############ name: ORIGIN_COUNTRY_NAME path: ORIGIN_COUNTRY_NAME max_definition_level: 1 max_repetition_level: 0 physical_type: BYTE_ARRAY logical_type: String converted_type (legacy): UTF8 compression: GZIP (space_saved: 39%) ############ Column(count) ############ name: count path: count max_definition_level: 1 max_repetition_level: 0 physical_type: INT64 logical_type: None converted_type (legacy): NONE compression: GZIP (space_saved: 53%) DataFrame을 DataFrameWriter를 사용해 Parquet 파일로 저장할 때는 아래와 같은 함수를 사용할 수 있다.\nCopy python (df.write.format(\u0026#34;parquet\u0026#34;) .mode(\u0026#34;overwrite\u0026#34;) .option(\u0026#34;compression\u0026#34;, \u0026#34;snappy\u0026#34;) .save(\u0026#34;/tmp/data/parquet/df_parquet\u0026#34;)) 압축 방식으로 snappy 를 사용하여 snappy 압축 파일이 생성된다. DataFrame을 직접 저장하면 아래와 같은 파일이 생성된다.\nCopy bash % ls -la /tmp/data/parquet/df_parquet/ -rw-r--r--@ ... 8 ... ._SUCCESS.crc -rw-r--r--@ ... 52 ... .part-00000-9828b287-9956-40cb-9c33-d59bea52e5be-c000.snappy.parquet.crc -rw-r--r--@ ... 0 ... _SUCCESS -rw-r--r--@ ... 5442 ... part-00000-9828b287-9956-40cb-9c33-d59bea52e5be-c000.snappy.parquet JSON # JSON 데이터 형식은 XML에 비해 읽기 쉽고, 구문을 분석하기 쉬운 형식이다. 단일 라인 모드와 다중 라인 모드를 모두 지원한다. 단일 라인 모드에서는 각 라인이 단일 JSON 개체를 나타내지만, 다중 라인 모드에서는 전체 라인 객체가 단일 JSON 개체를 구성한다. option() 함수에서 multiLine 옵션에 ture 또는 false 를 설정할 수 있다.\n단일 라인 모드의 JSON 데이터는 아래와 같이 구성된다.\nCopy bash % head -n 5 data/flights/summary-data/json/2010-summary.json {\u0026#34;ORIGIN_COUNTRY_NAME\u0026#34;:\u0026#34;Romania\u0026#34;,\u0026#34;DEST_COUNTRY_NAME\u0026#34;:\u0026#34;United States\u0026#34;,\u0026#34;count\u0026#34;:1} {\u0026#34;ORIGIN_COUNTRY_NAME\u0026#34;:\u0026#34;Ireland\u0026#34;,\u0026#34;DEST_COUNTRY_NAME\u0026#34;:\u0026#34;United States\u0026#34;,\u0026#34;count\u0026#34;:264} {\u0026#34;ORIGIN_COUNTRY_NAME\u0026#34;:\u0026#34;India\u0026#34;,\u0026#34;DEST_COUNTRY_NAME\u0026#34;:\u0026#34;United States\u0026#34;,\u0026#34;count\u0026#34;:69} {\u0026#34;ORIGIN_COUNTRY_NAME\u0026#34;:\u0026#34;United States\u0026#34;,\u0026#34;DEST_COUNTRY_NAME\u0026#34;:\u0026#34;Egypt\u0026#34;,\u0026#34;count\u0026#34;:24} {\u0026#34;ORIGIN_COUNTRY_NAME\u0026#34;:\u0026#34;United States\u0026#34;,\u0026#34;DEST_COUNTRY_NAME\u0026#34;:\u0026#34;Equatorial Guinea\u0026#34;,\u0026#34;count\u0026#34;:1} JSON 파일을 DataFrame으로 읽으려면 아래처럼 .format(\u0026quot;json\u0026quot;) 을 지정한다.\nCopy python file = \u0026#34;data/flights/summary-data/json/*\u0026#34; df = spark.read.format(\u0026#34;json\u0026#34;).load(file) df.show(5) Copy bash +-----------------+-------------------+-----+ |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| +-----------------+-------------------+-----+ | United States| Romania| 15| | United States| Croatia| 1| | United States| Ireland| 344| | Egypt| United States| 15| | United States| India| 62| +-----------------+-------------------+-----+ JSON 데이터 소스에 대해 DataFrameReader 및 DataFrameWriter 에 대한 일반적인 옵션은 아래와 같다.\ncompression : 압축 코덱을 쓰기 시에 사용할 수 있다. bzip2, gzip, snappy 등이 값으로 전달된다. dateFormat : Java의 DateTimeFormatter에서 제공하는 모든 형식을 사용할 수 있다. (yyyy-MM-dd 등) multiLine : true 를 지정하면 다중 라인 모드를 사용한다. 기본값은 false 이다. allowUnquotedFileNames : 따옴표로 묶이지 않은 JSON 필드 이름을 허용한다. 기본값은 false 이다. CSV # 쉼표로 각 데이터 또는 필드를 구분하며, 쉼표로 구분된 각 줄은 레코드를 나타낸다. 쉼표가 데이터의 일부인 경우, 값을 쌍따옴표로 감싸주거나, 다른 구분 기호를 사용하여 필드를 분리할 수 있다.\n일반적인 CSV 데이터는 아래와 같이 구성된다.\nCopy bash % head -n 5 data/flights/summary-data/csv/2010-summary.csv DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count United States,Romania,1 United States,Ireland,264 United States,India,69 Egypt,United States,24 CSV 파일을 DataFrame으로 읽으려면 아래처럼 .format(\u0026quot;csv\u0026quot;) 을 지정한다. 위 파일과 같이 헤더가 있는 경우 header 옵션에 true 를 설정한다. nullValue 옵션을 사용해 null 데이터를 특정 값으로 교체할 수 있다.\nCopy python file = \u0026#34;data/flights/summary-data/csv/*\u0026#34; df = spark.read.format(\u0026#34;csv\u0026#34;).option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;).load(file) df.show(5) Copy bash +-----------------+-------------------+-----+ |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| +-----------------+-------------------+-----+ | United States| Romania| 1| | United States| Ireland| 264| | United States| India| 69| | Egypt| United States| 24| |Equatorial Guinea| United States| 1| +-----------------+-------------------+-----+ CSV 데이터 소스에 대해 DataFrameReader 및 DataFrameWriter 에 대한 일반적인 옵션은 아래와 같다.\ncompression : 압축 코덱을 쓰기 시에 사용할 수 있다. bzip2, gzip, snappy 등이 값으로 전달된다. dateFormat : Java의 DateTimeFormatter에서 제공하는 모든 형식을 사용할 수 있다. (yyyy-MM-dd 등) multiLine : true 를 지정하면 다중 라인 모드를 사용한다. 기본값은 false 이다. interSchema : true 를 지정하면 스파크가 칼럼 데이터 유형을 결정한다. 기본값은 false 이다. sep : 칼럼을 구분하기 위한 문자이며, 기본 구분 기호는 쉼표(,)다. escape : 따옴표를 이스케이프할 때 사용하는 문자이며, 기본값은 / 다. header : 첫 번째 줄이 칼럼명을 나타내는 헤더일 경우 true 를 지정하고, 기본값은 false 이다. Avro # 스파크 2.4에 내장된 데이터 소스인 Avro 형식은 특히 아파치 카프카에서 메시지를 직렬화 및 역직렬화할 때 사용된다. JSON에 대한 직접 매핑, 속도와 효율성 등 많은 이점을 제공한다.\n스파크 공식 문서의 Apache Avro Data Source Guide에 따르면, spark-avro 모듈은 외부 모듈로 spark-submit 또는 spark-shell 에 포함되어 있지 않다. 따라서, 아래와 같이 --packages 를 사용하여 종속성을 추가할 수 있다.\nCopy bash ./bin/spark-shell --packages org.apache.spark:spark-avro_2.13:4.0.0 ... SparkSession 인스턴스를 사용할 경우, spark.jars.packages 설정에 spark-avro_2.13 종속성을 추가한다.\nCopy python from pyspark.sql import SparkSession spark = (SparkSession .builder .appName(\u0026#34;SparkAvroExampleApp\u0026#34;) .config(\u0026#34;spark.jars.packages\u0026#34;, \u0026#34;org.apache.spark:spark-avro_2.13:4.0.0\u0026#34;) .getOrCreate()) Avro 파일을 DataFrame으로 읽으려면 아래처럼 .format(\u0026quot;avro\u0026quot;) 을 지정한다.\nCopy python file = \u0026#34;data/flights/summary-data/avro/*\u0026#34; df = spark.read.format(\u0026#34;avro\u0026#34;).load(file) df.show(5) Copy bash +-----------------+-------------------+-----+ |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| +-----------------+-------------------+-----+ | United States| Romania| 1| | United States| Ireland| 264| | United States| India| 69| | Egypt| United States| 24| |Equatorial Guinea| United States| 1| +-----------------+-------------------+-----+ Avro 데이터 소스에 대해 DataFrameReader 및 DataFrameWriter 에 대한 일반적인 옵션은 아래와 같다.\navroSchema : JSON 형식으로 제공할 수 있는 Avro 스키마이다. Avro 데이터나 카탈리스트 데이터와 일치하지 않으면 읽기/쓰기 작업이 실패한다. recordName : Avro 사양에 필요한 쓰기 결과의 최상위 레코드명이다. recrodNamespace : 쓰기 결과에 네임스페이스를 기록한다. ignoreExtension : 확장자가 .avro 인지 여부에 관계없이 모든 파일을 읽어들인다. compression : 쓰기에 사용할 압축 코덱을 지정할 수 있다. Image # 스파크 2.4에서 머신러닝 프레임워크를 지원하기 위해 새로운 데이터 소스인 이미지 파일을 도입했다.\ndatabricks/LearningSparkV2의 databricks-datasets/learning-spark-v2 경로에서 cctvVideos/ 디렉터리를 가져온다. 해당 디렉터리의 구조는 다음과 같다.\nCopy text cctvVideos/ ├── README.md └── train_images/ ├── label=0/ │ ├── Browse2frame0000.jpg │ ├── Browse2frame0001.jpg │ ├── Browse2frame0002.jpg │ ├── ... | └── Walk2frame0042.jpg └── label=1/ ├── Fight_Chaseframe0012.jpg ├── Fight_Chaseframe0013.jpg ├── Fight_Chaseframe0014.jpg ├── ... └── Rest_WiggleOnFloorframe0050.jpg 이미지 파일은 아래와 같이 DataFrameReader 함수로 읽을 수 있다. 이미지 파일을 읽을 때 numpy 라이브러리가 필요하다.\nCopy python from pyspark.ml import image image_dir = \u0026#34;data/cctvVideos/train_images\u0026#34; images_df = spark.read.format(\u0026#34;image\u0026#34;).load(image_dir) images_df.printSchema() Copy bash root |-- image: struct (nullable = true) | |-- origin: string (nullable = true) | |-- height: integer (nullable = true) | |-- width: integer (nullable = true) | |-- nChannels: integer (nullable = true) | |-- mode: integer (nullable = true) | |-- data: binary (nullable = true) |-- label: integer (nullable = true) 이미지의 높이 및 너비와 같은 정보는 아래와 같이 조회할 수 있다.\nCopy python images_df.select(\u0026#34;image.height\u0026#34;, \u0026#34;image.width\u0026#34;, \u0026#34;image.nChannels\u0026#34;, \u0026#34;image.mode\u0026#34;, \u0026#34;label\u0026#34;).show(5) Copy bash +------+-----+---------+----+-----+ |height|width|nChannels|mode|label| +------+-----+---------+----+-----+ | 288| 384| 3| 16| 0| | 288| 384| 3| 16| 1| | 288| 384| 3| 16| 0| | 288| 384| 3| 16| 0| | 288| 384| 3| 16| 0| +------+-----+---------+----+-----+ Binary File # 이진 파일을 읽으려면 데이터 소스 형식을 binaryFile 로 지정해야 한다. DataFrameReader는 이진 파일을 원본 내용과 메타데이터를 포함하는 단일 DataFrame 행으로 변환한다.\npathGlobFilter 를 사용하면 지정된 전역 패턴과 일치하는 경로로 파일을 로드할 수 있다. 아래 코드는 디렉터리에서 모든 .jpg 파일을 읽는다.\nCopy python path = \u0026#34;data/cctvVideos/train_images\u0026#34; binary_files_df = spark.read.format(\u0026#34;binaryFile\u0026#34;).option(\u0026#34;pathGlobFilter\u0026#34;, \u0026#34;*.jpg\u0026#34;).load(path) binary_files_df.show(5) Copy bash +--------------------+-------------------+------+--------------------+-----+ | path| modificationTime|length| content|label| +--------------------+-------------------+------+--------------------+-----+ |file:/Users/cuz/D...|2025-01-28 13:30:40| 55037|[FF D8 FF E0 00 1...| 0| |file:/Users/cuz/D...|2025-01-28 13:30:40| 54634|[FF D8 FF E0 00 1...| 1| |file:/Users/cuz/D...|2025-01-28 13:30:40| 54624|[FF D8 FF E0 00 1...| 0| |file:/Users/cuz/D...|2025-01-28 13:30:40| 54505|[FF D8 FF E0 00 1...| 0| |file:/Users/cuz/D...|2025-01-28 13:30:40| 54475|[FF D8 FF E0 00 1...| 0| +--------------------+-------------------+------+--------------------+-----+ References # https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ https://github.com/databricks/LearningSparkV2/tree/master/databricks-datasets/learning-spark-v2/flights/summary-data https://spark.apache.org/docs/latest/api/python/tutorial/sql/python_data_source.html https://spark.apache.org/docs/latest/sql-data-sources.html https://spark.apache.org/docs/latest/sql-data-sources-avro.html "},{"id":7,"href":"/blog/install-ubuntu-server/","title":"Mac에 Ubuntu Server 24.04 설치하기 (UTM)","section":"Posts","content":"리눅스 개발 환경이 필요해졌는데 단순 테스트를 위해 새로운 PC를 사거나 기존에 사용하는 Mac에 직접 설치하는건 비용이나 리스크가 있어 간단하게 가상환경을 사용하려 합니다.\nMac에서 무료로 이용할 수 있는 가상화 소프트웨어인 UTM을 사용해 우분투 서버를 설치해보려 합니다.\n개발 환경 # macOS 15.5 Sequoia (Apple M4)\nMemory : 16GB\nCPU : 10코어\n1. Ubuntu 이미지 다운로드 # Ubuntu Server 24.04.2 LTS 이미지를 다운로드 받습니다. (이미지 클릭 시 다운로드 경로로 이동)\n주의할 점은, 기본 다운로드 경로인 https://ubuntu.com/download/server로 접속하면 x86 아키텍처와 호환되는 amd64 이미지로 연결되기 때문에, 반드시 다운로드 받는 파일이 arm64 이미지인지 확인해야 합니다.\n또는 터미널에서 내려받을 수도 있습니다.\nCopy bash curl -O -L https://cdimage.ubuntu.com/releases/24.04/release/ubuntu-24.04.2-live-server-arm64.iso Copy bash % ls -la ubuntu-24.04.2-live-server-arm64.iso -rw-r--r--@ 1 user group 2922393600 Jul 6 11:29 ubuntu-24.04.2-live-server-arm64.iso 2. UTM 설치하기 # UTM 최신 버전(작성일 기준 4.6.5)을 설치합니다. (이미지 클릭 시 다운로드 경로로 이동)\n앱스토어에서도 설치할 수 있는데 $9.99를 지불해야 합니다.\nUTM을 실행하면 다음과 같은 화면이 나타납니다.\n3. 가상머신 생성하기 # 새 가상머신 만들기를 선택합니다.\nStart 화면에서 Virtualize를 선택합니다.\n운영체제는 Linux를 선택합니다.\n이미지 파일 추가 # Boot ISO Image에 앞에서 다운로드 받았던 Ubuntu 이미지 파일을 추가합니다.\n하드웨어 설정 # 메모리와 CPU 크기는 목적에 맞게 설정합니다. 간단한 실습용 쿠버네티스 마스터 노드로 사용할 예정이라 메모리 4GB, CPU 2코어로 설정했습니다.\n저장공간도 목적에 맞게 설정합니다.\n가상머신 이름 설정 # 저장공간 설정 후에 나오는 공유폴더 설정은 무시합니다. 마지막으로 요약 화면이 나오는데 가상머신 이름을 설정합니다.\n저장을 누르면 가상머신이 생성된 것을 확인할 수 있습니다.\n네트워크 설정 # 추가로, Ubuntu를 설치하기 전에 가상머신에서 네트워크 설정을 적용했습니다. 여러 개의 가상머신과 Mac 간의 통신을 원활히 하기 위해 브릿지 모드를 선택했습니다. 이러한 경우가 아니라면 기본 설정인 Shared Network(NAT) 모드를 사용해도 됩니다.\n4. Ubuntu 설치하기 # 앞에서 생성한 가상머신을 실행합니다. \u0026ldquo;Try or Install Ubuntu Server\u0026rdquo; 를 선택합니다.\n언어 및 설치 유형 # 언어 및 키보드 레이아웃은 기본값인 \u0026ldquo;English\u0026rdquo; 를 선택합니다.\n설치 유형은 기본값인 \u0026ldquo;Ubuntu Server\u0026rdquo; 를 선택합니다.\n네트워크 설정 # 네트워크 설정에선 기본적으로 DHCP를 통한 동적 IP 주소가 적용되어 있습니다.\n클러스터 구성을 위해 여러 가상머신 간 고정된 IP 주소를 가지고 통신할 필요가 있기 때문에 정적으로 IP 주소를 지정합니다.\n가상머신을 실행하기 전에 네트워크 설정에서 브릿지 모드로 변경했기 때문에 맥의 네트워크와 동일한 대역을 사용할 수 있습니다. NAT 모드로 가상머신을 실행 중이라면 DHCP를 통해 배정된 IP 주소를 바탕으로 대역을 추정해 IP 주소를 지정해야 합니다.\n정적 IP 주소를 할당했다면 다음과 같이 static 으로 표시됩니다.\n프록시 및 미러 서버 설정 # 프록시 서버는 기본값으로 무시합니다.\n미러 서버는 소프트웨어 패키지를 다운로드 받는 공식 서버의 복제본입니다. 보통 패키지를 다운로드 받을 때 미러 서버를 통해 받습니다. 기본값으로는 \u0026ldquo;kr.ports.ubuntu.com/ubuntu-ports\u0026rdquo; 로 지정되어 있는데, 속도가 더 빠른 카카오 미러 서버 \u0026ldquo;mirror.kakao.com\u0026rdquo; 로 변경했습니다.\n저장공간 설정 # 저장공간도 기본 설정인 \u0026ldquo;Use an entire disk\u0026rdquo; 를 적용합니다. 목적에 따라 파티션을 분리할 수도 있지만, 현재는 파티션을 나눌 필요가 없습니다.\n설치를 진행하게 되면 디스크 포맷을 통해 저장된 데이터가 삭제될 수 있다고 경고하는데 그대로 진행합니다.\n프로필 설정 # 사용자 이름, 서버 이름 등을 설정합니다.\nYour name : 이름 정보 (서버 운영과 무관) Your server\u0026rsquo;s name : 서버 호스트명 Pick a username : 로그인 사용자 이름 Choose a password : 로그인 사용자 비밀번호 Confirm your password : 로그인 사용자 비밀번호 확인 기타 설정 및 설치 # Ubuntu Pro 업그레이드 여부를 묻는데 사용하지 않으므로 넘어갑니다.\nOpenSSH 서버 설치를 묻는데 SSH 서버를 사용하기 위해 체크합니다.\n설치 패키지 선택창이 나오는데 필요한건 직접 설치할 것이기 때문에 다음으로 넘어갑니다.\n설치가 진행되고, 설치가 완료되면 \u0026ldquo;Reboot now\u0026rdquo; 선택지가 생깁니다. 재부팅을 수행합니다.\n5. Ubuntu 접속 # 최초 설치 후 재부팅하면 더이상 진행되지 않고 커서만 깜빡이는데, 일단 종료하고 UTM 화면으로 돌아갑니다. 가상머신에서 부팅용 이미지 파일을 초기화한 후 다시 실행합니다.\n가상머신을 실행하면 로그인 화면이 나타납니다. 프로필 설정에 지정한 사용자 이름과 비밀번호를 순차적으로 입력합니다.\n정상적으로 로그인되었다면 아래와 같이 명령어를 입력할 수 있는 프롬프트가 나타납니다.\n참고 자료 # https://ubuntu.com/download/server/arm https://mac.getutm.app/ https://gymdev.tistory.com/75 https://moneymentors.tistory.com/entry/우분투Ubuntu-22042-LTS-서버-설치-방법 https://mirror.kakao.com/ "},{"id":8,"href":"/blog/spark-study-5/","title":"Apache Spark - 스파크 SQL","section":"Posts","content":"Spark SQL # 스파크 SQL은 다음과 같은 특징을 갖는다.\n정형화 API가 엔진으로 제공한다. 다양한 정형 데이터(Parquet 등)를 읽거나 쓸 수 있다. 외부 BI 툴(태블로 등)의 데이터 소스나 RDBMS(MySQL 등)의 데이터를 쿼리할 수 있다. 정형 데이터에 대해 SQL 쿼리를 실행할 수 있는 대화형 쉘을 제공한다. Spark SQL 사용법 # Copy python spark.sql(\u0026#34;SELECT * FROM table\u0026#34;) SparkSession 객체에 sql() 함수를 사용한다. 쿼리 결과로는 DataFrame 객체가 반환된다.\nSpark SQL 활용 (Python) # databricks/LearningSparkV2의 databricks-datasets/learning-spark-v2/flights 경로에서 미국 항공편 운항 지연 데이터세트 departuredelays.csv 를 가져온다. 해당 데이터를 활용해 아래와 같이 임시뷰를 생성한다.\nCopy python from pyspark.sql import SparkSession spark = (SparkSession .builder .appName(\u0026#34;SparkSQLExampleApp\u0026#34;) .getOrCreate()) # 데이터 경로 csv_file = \u0026#34;data/flights/departuredelays.csv\u0026#34; # 스키마를 추론하여 데이터를 읽기 df = (spark.read.format(\u0026#34;csv\u0026#34;) .option(\u0026#34;inferSchema\u0026#34;, \u0026#34;true\u0026#34;) .option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;) .load(csv_file)) # 데이터로부터 임시뷰를 생성 df.createOrReplaceTempView(\u0026#34;delay_flights\u0026#34;) 스파크 SQL을 사용해 임시뷰에 대해 SQL 쿼리를 실행할 수 있다. 스파크 SQL은 ANSI:2003과 호환되는 SQl 인터페이스를 제공한다.\nCopy python spark.sql(\u0026#34;\u0026#34;\u0026#34; SELECT distance, origin, destination FROM delay_flights WHERE distance \u0026gt; 1000 ORDER BY distance DESC;\u0026#34;\u0026#34;\u0026#34;).show(10) Copy bash +--------+------+-----------+ |distance|origin|destination| +--------+------+-----------+ | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| +--------+------+-----------+ only showing top 10 rows 위의 쿼리는 아래 파이썬 예제와 같이 동등한 DataFrame API로 표현할 수 있다.\nCopy python from pyspark.sql.functions import col, desc (df.select(\u0026#34;distance\u0026#34;, \u0026#34;origin\u0026#34;, \u0026#34;destination\u0026#34;) .where(col(\u0026#34;distance\u0026#34;) \u0026gt; 1000) .orderBy(desc(\u0026#34;distance\u0026#34;))).show(10) SQL 쿼리로 단순 SELECT 조회뿐 아니라 현재 생성된 임시뷰 목록을 조회할 수도 있다.\nCopy python spark.sql(\u0026#34;SHOW TABLES;\u0026#34;).show() Copy bash +---------+-------------+-----------+ |namespace| tableName|isTemporary| +---------+-------------+-----------+ | |delay_flights| true| +---------+-------------+-----------+ 앞에서 스키마를 추론하여 DataFrame을 읽었는데, SQL 쿼리로 어떤 스키마로 인식되었는지 확인해 보았다. DataFrame 객체의 스키마 df.schema 의 출력 결과와 동일하다.\nCopy python spark.sql(\u0026#34;DESC delay_flights;\u0026#34;).show() Copy bash +-----------+---------+-------+ | col_name|data_type|comment| +-----------+---------+-------+ | date| int| NULL| | delay| int| NULL| | distance| int| NULL| | origin| string| NULL| |destination| string| NULL| +-----------+---------+-------+ Table \u0026amp; View # 스파크는 테이블을 위한 별도 메타스토어를 생성하지 않고 기본적으로 /user/hive/warehouse 경로에 있는 아파치 하이브 메타스토어를 사용해 테이블에 대한 모든 메타데이터를 유지한다.\n스파크는 관리형과 비관리형이라는 두 가지 유형의 테이블로 만들 수 있다.\n관리형 테이블은 스파크가 메타데이터와 파일 저장소의 데이터를 모두 관리한다. 따라서, DROP TABLE 과 같은 SQL 명령에 대해 메타데이터와 실제 데이터를 모두 삭제한다.\n반면에 비관리형 테이블은 스파크가 메타데이터만 관리하고 외부 데이터 소스에서 데이터를 직접 관리한다. 그래서, DROP TABLE 명령에도 실제 데이터는 그대로 두고 메타데이터만 삭제한다.\n테이블 생성하기 # 스파크는 기본적으로 default 데이터베이스 안에 테이블을 생성한다. SparkSession을 열고 현재 접속한 데이터베이스를 조회하면 알 수 있다.\nCopy python spark.sql(\u0026#34;SELECT current_database();\u0026#34;).show() Copy bash +----------------+ |current_schema()| +----------------+ | default| +----------------+ 우선, SQL 명령어를 실행하여 새로운 데이터베이스 learn_spark_db 를 생성할 수 있다. 생성한 데이터베이스를 사용하고 다시 현재 접속한 데이터베이스를 확인해 보았다.\nCopy python spark.sql(\u0026#34;CREATE DATABASE learn_spark_db;\u0026#34;) spark.sql(\u0026#34;USE learn_spark_db;\u0026#34;) spark.sql(\u0026#34;SELECT current_database();\u0026#34;).show() Copy bash +----------------+ |current_schema()| +----------------+ | learn_spark_db| +----------------+ 관리형 테이블 생성하기 # CREATE 문을 사용하여 현재 데이터베이스 안에 관리형 테이블을 생성할 수 있다.\nCopy python table = \u0026#34;managed_delay_flights\u0026#34; schema = \u0026#34;date STRING, delay INT, distaince INT, origin STRING, destination STRING\u0026#34; spark.sql(\u0026#34;CREATE TABLE {} ({});\u0026#34;.format(table, schema)) 위 SQL 쿼리는 마찬가지로 아래처럼 DataFrame API로 표현할 수도 있다. 이미 테이블을 만들었을 경우, 아래 파이썬 예제를 그대로 실행하면 TABLE_OR_VIEW_ALREADY_EXISTS 에러가 발생하므로 mode=\u0026quot;overwrite\u0026quot; 옵션을 넣어주어 기존 테이블을 덮어쓴다.\nCopy python csv_file = \u0026#34;data/flights/departuredelays.csv\u0026#34; flights_df = spark.read.csv(csv_file, schema=schema) flights_df.write.saveAsTable(table, mode=\u0026#34;overwrite\u0026#34;) 테이블을 생성하게 되면 현재 위치 아래의 spark-warehouse/{{DB명}}.db/{{테이블명}} 경로에 .parquet 파일들이 생성된다. 스파크 공식문서 중 Hive Table을 참고하면, 기본 디렉토리인 spark-warehouse 는 SparkSession을 실행할 때 spark.sql.warehouse.dir 설정을 통해 변경할 수 있다.\nCopy python warehouse_location = abspath(\u0026#39;spark-warehouse\u0026#39;) spark = SparkSession \\ .builder \\ .appName(\u0026#34;Python Spark SQL Hive integration example\u0026#34;) \\ .config(\u0026#34;spark.sql.warehouse.dir\u0026#34;, warehouse_location) \\ .enableHiveSupport() \\ .getOrCreate() 정적 설정이라 세션 실행 중에는 변경할 수 없어서 세션을 종료하고 다시 실행했다. saved 경로로 변경하고 다시 관리형 테이블을 생성해보니 해당 위치에 Parquet 파일들이 만들어졌다.\nCopy python from pyspark.sql import SparkSession from pathlib import Path warehouse_location = Path(\u0026#34;saved\u0026#34;) warehouse_location.mkdir(exist_ok=True) spark = (SparkSession .builder .appName(\u0026#34;SparkSQLExampleApp\u0026#34;) .config(\u0026#34;spark.sql.warehouse.dir\u0026#34;, str(warehouse_location.absolute())) .getOrCreate()) 비관리형 테이블 생성하기 # 기존 CREATE 문을 사용하는 SQL 쿼리에서 뒤에 USING 키워드로 시작하는 CSV 파일 경로를 붙여주어 외부 데이터 소스로부터 비관리형 테이블을 생성할 수 있다.\nCopy python import os table = \u0026#34;unmanaged_delay_flights\u0026#34; schema = \u0026#34;date STRING, delay INT, distaince INT, origin STRING, destination STRING\u0026#34; csv_file = os.path.join(os.getcwd(), \u0026#34;data/flights/departuredelays.csv\u0026#34;) spark.sql(\u0026#34;CREATE TABLE {} ({}) USING csv OPTIONS (PATH \u0026#39;{}\u0026#39;);\u0026#34;.format(table, schema, csv_file)) CSV 파일의 상대경로로 SQL 쿼리를 실행해보니까 FileStreamSink: Assume no metadata directory. 경고 메시지가 발생했는데 절대경로로 바꿔주니까 해결되었다.\nSQL 쿼리를 DataFrame API로 표현하면 아래와 같다. 관리형 테이블을 만드는 구문이랑 거의 비슷한데, path 옵션으로 /tmp 경로를 지정하는데 차이가 있다.\nCopy python (flights_df .write .option(\u0026#34;path\u0026#34;, \u0026#34;/tmp/data/delay_flights\u0026#34;) .saveAsTable(table, mode=\u0026#34;overwrite\u0026#34;)) 뷰 생성하기 # 기존 테이블을 토대로 뷰를 만들 수 있다. 전역(모든 SparkSession) 또는 세션 범위에서 생성할 수 있고, Spark Application이 종료되면 뷰는 사라진다.\n전역 임시 뷰는 SQL 쿼리에서는 GLOBAL TEMP VIEW 키워드를 추가하고, 파이썬에서는 createOrReplaceGlobalTempView() 함수를 호출하여 생성할 수 있다.\nCopy python table = \u0026#34;us_origin_airport_SFO\u0026#34; spark.sql(\u0026#34;\u0026#34;\u0026#34; CREATE OR REPLACE GLOBAL TEMP VIEW {} AS SELECT date, delay, origin, destination FROM delay_flights WHERE origin = \u0026#39;SFO\u0026#39;; \u0026#34;\u0026#34;\u0026#34;.format(table)) Copy python from pyspark.sql.functions import col table = \u0026#34;us_origin_airport_SFO\u0026#34; (df.select(\u0026#34;date\u0026#34;, \u0026#34;delay\u0026#34;, \u0026#34;origin\u0026#34;, \u0026#34;destination\u0026#34;) .where(col(\u0026#34;origin\u0026#34;) == \u0026#34;SFO\u0026#34;) .createOrReplaceGlobalTempView(table)) 전역 임시 뷰는 global_temp 라는 전역 임시 데이터베이스에 생성되며, global_temp.\u0026lt;view_name\u0026gt; 처럼 명시하여 뷰 테이블에 접근할 수 있다. 일반 임시 뷰는 현재 데이터베이스에 생성되므로 global_temp 접두사를 붙일 필요가 없다.\nCopy python spark.sql(\u0026#34;SELECT * FROM global_temp.{};\u0026#34;.format(table)).show(5) 메타데이터 보기 # 스파크는 관리형 및 비관리형 테이블에 대한 메타데이터를 관리한다. 메타데이터는 스파크 SQL의 상위 추상화 모듈인 카탈로그에 저장된다. 카탈로그에서 아래와 같이 데이터베이스, 테이블, 칼럼 목록을 조회할 수 있다.\nCopy python \u0026gt;\u0026gt;\u0026gt; spark.catalog.listDatabases() [Database(name=\u0026#39;default\u0026#39;, catalog=\u0026#39;spark_catalog\u0026#39;, description=\u0026#39;default database\u0026#39;, locationUri=\u0026#39;file:/Users/cuz/Documents/Github/study/spark/saved\u0026#39;)] Copy python \u0026gt;\u0026gt;\u0026gt; spark.catalog.listTables() [Table(name=\u0026#39;delay_flights\u0026#39;, catalog=None, namespace=[], description=None, tableType=\u0026#39;TEMPORARY\u0026#39;, isTemporary=True)] Copy python \u0026gt;\u0026gt;\u0026gt; spark.catalog.listColumns(\u0026#34;delay_flights\u0026#34;) [Column(name=\u0026#39;date\u0026#39;, description=None, dataType=\u0026#39;int\u0026#39;, nullable=True, isPartition=False, isBucket=False, isCluster=False), Column(name=\u0026#39;delay\u0026#39;, description=None, dataType=\u0026#39;int\u0026#39;, nullable=True, isPartition=False, isBucket=False, isCluster=False), Column(name=\u0026#39;distance\u0026#39;, description=None, dataType=\u0026#39;int\u0026#39;, nullable=True, isPartition=False, isBucket=False, isCluster=False), Column(name=\u0026#39;origin\u0026#39;, description=None, dataType=\u0026#39;string\u0026#39;, nullable=True, isPartition=False, isBucket=False, isCluster=False), Column(name=\u0026#39;destination\u0026#39;, description=None, dataType=\u0026#39;string\u0026#39;, nullable=True, isPartition=False, isBucket=False, isCluster=False)] SQL 테이블 캐싱하기 # 스파크 공식문서 중 CACHE TABLE을 참고하면, CACHE TABLE 쿼리를 사용하여 임시 뷰를 캐싱할 수 있다. CACHE LAZY TABLE 과 같이 LAZY 파라미터를 추가하면 테이블이 사용되는 시점까지 캐싱을 미룰 수 있다.\nCopy python spark.sql(\u0026#34;CACHE TABLE delay_flights;\u0026#34;) 테이블 캐시가 활성화 상태인지 보려면 카탈로그가 가진 isCached 함수를 참고할 수 있다. 캐시가 활성화되었다면 True, 아니면 False 를 반환한다.\nCopy python \u0026gt;\u0026gt;\u0026gt; spark.catalog.isCached(\u0026#34;delay_flights\u0026#34;) True Copy python \u0026gt;\u0026gt;\u0026gt; spark.catalog.isCached(\u0026#34;global_temp.us_origin_airport_sfo\u0026#34;) False 테이블 캐시를 삭제하고 싶다면 UNCACHE TABLE 쿼리를 사용한다.\nCopy python \u0026gt;\u0026gt;\u0026gt; spark.sql(\u0026#34;UNCACHE TABLE delay_flights;\u0026#34;) \u0026gt;\u0026gt;\u0026gt; spark.catalog.isCached(\u0026#34;delay_flights\u0026#34;) False 테이블을 DataFrame으로 변환하기 # SQL 쿼리로 테이블 전체를 읽을 수도 있지만, table() 함수를 사용할 수도 있다.\nCopy python spark.sql(\u0026#34;SELECT * FROM delay_flights;\u0026#34;) Copy python spark.table(\u0026#34;delay_flights\u0026#34;) sql() 과 table() 모두 DataFrame 객체를 반환한다.\nReferences # https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ https://github.com/databricks/LearningSparkV2/tree/master/databricks-datasets/learning-spark-v2/flights https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html https://spark.apache.org/docs/latest/sql-ref-syntax-aux-cache-cache-table.html https://spark.apache.org/docs/latest/sql-ref-syntax-aux-cache-uncache-table.html "},{"id":9,"href":"/blog/programmers-sql-4-5/","title":"프로그래머스 SQL Lv.4, 5 문제풀이","section":"Posts","content":"이직 준비를 하면서 오랜만에 코딩테스트를 보게되었는데, SQL 코딩테스트는 어떻게 나오나 궁금해서 프로그래머스 Lv.4, 5 수준의 문제를 모두 풀어보았습니다.\n모든 문제 # 표에서 ID를 클릭하면 프로그래머스 문제 풀이로 이동합니다. 문제는 최신순으로 정렬되어 있는데 풀이 순서는 표에서 역순으로, 즉 오래된순으로 나열합니다.\n문제를 순서대로 풀어보면 유사한 데이터를 사용하는 경우가 있어 비슷한건 큰 제목으로 묶었습니다. 테이블 구조는 생략하고 문제 요약 \u0026gt; 문제 해석 + 풀이 \u0026gt; SQL문 순서로 구성합니다.\nID 제목 유형 난이도 정답률 301651 멸종위기의 대장균 찾기 SELECT Lv. 5 21% 301650 특정 세대의 대장균 찾기 SELECT Lv. 4 61% 284528 연간 평가점수에 해당하는 평가 등급 및 성과금 조회하기 GROUP BY Lv. 4 72% 276036 언어별 개발자 분류하기 GROUP BY Lv. 4 41% 276035 FrontEnd 개발자 찾기 JOIN Lv. 4 51% 157339 특정 기간동안 대여 가능한 자동차들의 대여비용 구하기 JOIN Lv. 4 49% 151141 자동차 대여 기록 별 대여 금액 구하기 String, Date Lv. 4 51% 144856 저자 별 카테고리 별 매출액 집계하기 GROUP BY Lv. 4 76% 133027 주문량이 많은 아이스크림들 조회하기 JOIN Lv. 4 74% 132204 취소되지 않은 진료 예약 조회하기 String, Date Lv. 4 79% 131537 오프라인/온라인 판매 데이터 통합하기 SELECT Lv. 4 67% 131534 상품을 구매한 회원 비율 구하기 JOIN Lv. 5 46% 131532 년, 월, 성별 별 상품 구매 회원 수 구하기 GROUP BY Lv. 4 75% 131124 그룹별 조건에 맞는 식당 목록 출력하기 JOIN Lv. 4 71% 131118 서울에 위치한 식당 목록 출력하기 SELECT Lv. 4 75% 131117 5월 식품들의 총매출 조회하기 JOIN Lv. 4 84% 131116 식품분류별 가장 비싼 식품의 정보 조회하기 GROUP BY Lv. 4 85% 62284 우유와 요거트가 담긴 장바구니 GROUP BY Lv. 4 74% 59413 입양 시각 구하기(2) GROUP BY Lv. 4 61% 59045 보호소에서 중성화한 동물 JOIN Lv. 4 85% 동물 입양 테이블 # 보호소에서 중성화한 동물 # ANIMAL_INS 테이블은 동물 보호소에 들어온 동물의 정보를 담은 테이블입니다. ANIMAL_OUTS 테이블은 동물 보호소에서 입양 보낸 동물의 정보를 담은 테이블입니다.\n보호소에서 중성화 수술을 거친 동물 정보를 알아보려 합니다. 보호소에 들어올 당시에는 중성화되지 않았지만, 보호소를 나갈 당시에는 중성화된 동물의 아이디와 생물 종, 이름을 조회하는 아이디 순으로 조회하는 SQL 문을 작성해주세요.\n보호소에 들어올 당시에 대한 테이블 ANIMAL_INS 에서 특정 조건만 선택하고, 보호소를 나갈 당시에 대한 테이블 ANIMAL_OUTS 에서 특정 조건만 선택해서, 두 테이블을 INNER JOIN 하여 두 조건을 만족하는 경우만 선택하는 문제입니다.\nANIMAL_INS 테이블에서는 중성화되지 않은 조건인, 성별 및 중성화 여부(SEX_UPON_INTAKE)에 \u0026ldquo;Intact\u0026rdquo; 단어가 포함된 경우를 선택하면 됩니다. 중성화 여부를 나타내는 단어는 앞에 있기 때문에 LIKE 'Intact%' 조건식을 사용했습니다.\nANIMAL_OUTS 테이블에서는 중성화된 조건인, 성별 및 중성화 여부(SEX_UPON_INTAKE)에 \u0026ldquo;Spayed\u0026rdquo; 또는 \u0026ldquo;Neutered\u0026rdquo; 단어가 포함된 경우를 선택하면 됩니다. 너무 길게 쓰는건 좋아하지 않아서 정규표현식을 활용하여 REGEXP '^Spayed|Neutered' 로 표현했습니다.\nCopy sql SELECT INS.ANIMAL_ID, INS.ANIMAL_TYPE, INS.NAME FROM ANIMAL_INS AS INS INNER JOIN ANIMAL_OUTS AS OUTS ON INS.ANIMAL_ID = OUTS.ANIMAL_ID WHERE INS.SEX_UPON_INTAKE LIKE \u0026#39;Intact%\u0026#39; AND OUTS.SEX_UPON_OUTCOME REGEXP \u0026#39;^Spayed|Neutered\u0026#39; ORDER BY ANIMAL_ID; 입양 시각 구하기(2) # ANIMAL_OUTS 테이블은 동물 보호소에서 입양 보낸 동물의 정보를 담은 테이블입니다.\n보호소에서는 몇 시에 입양이 가장 활발하게 일어나는지 알아보려 합니다. 0시부터 23시까지, 각 시간대별로 입양이 몇 건이나 발생했는지 조회하는 SQL문을 작성해주세요. 이때 결과는 시간대 순으로 정렬해야 합니다.\n입양일(DATETIME)에서 시간만 추출하고, 추출한 값을 기준으로 COUNT 집계하는 문제입니다.\n주의할 점은, 0시부터 23시까지 모든 행이 존재해야 합니다. ANIMAL_OUTS 테이블에 모든 시간대가 있지 않기 때문에 0시부터 23시까지 값을 가지는 테이블을 만들어야 합니다.\n잘 안써본 구문이라 찾아봤는데 RECURSIVE 라는 재귀적 쿼리를 활용하면 간단하게 해결될 것 같았습니다. 재귀적 쿼리를 사용한 HOURS 임시 테이블을 생성하고, 여기에 ANIMAL_OUTS 테이블을 COUNT 집계한 COUNTS 임시 테이블을 LEFT JOIN 으로 붙여서 모든 시간대를 표시했습니다.\nCopy sql WITH RECURSIVE HOURS AS ( SELECT 0 AS HOUR UNION ALL SELECT HOUR + 1 FROM HOURS WHERE HOUR \u0026lt; 23 ), COUNTS AS ( SELECT HOUR(DATETIME) AS HOUR, COUNT(ANIMAL_ID) AS COUNT FROM ANIMAL_OUTS GROUP BY HOUR ) SELECT HR.HOUR, COALESCE(CNT.COUNT, 0) AS COUNT FROM HOURS AS HR LEFT JOIN COUNTS AS CNT ON HR.HOUR = CNT.HOUR ORDER BY HOUR; 식품 테이블 # 우유와 요거트가 담긴 장바구니 # CART_PRODUCTS 테이블은 장바구니에 담긴 상품 정보를 담은 테이블입니다.\n데이터 분석 팀에서는 우유(Milk)와 요거트(Yogurt)를 동시에 구입한 장바구니가 있는지 알아보려 합니다. 우유와 요거트를 동시에 구입한 장바구니의 아이디를 조회하는 SQL 문을 작성해주세요. 이때 결과는 장바구니의 아이디 순으로 나와야 합니다.\n장바구니의 아이디(CART_ID) 그룹으로 집계하면서, 상품 종류(NAME) 에 \u0026ldquo;Milk\u0026quot;가 있는 경우와 \u0026ldquo;Yogurt\u0026quot;가 있는 경우를 모두 만족하는 아이디만 선택하는 문제입니다.\n자주 쓰는 Python에서는 Boolean 타입의 True / False 를 각각 정수 1과 0으로 인식할 수 있는데, SQL도 그렇지 않을까 싶어서 SUM 집계해봤습니다. 두 가지 조건에 대해 SUM 집계 결과가 1 이상인 경우만 선택해서 아이디만 조회했습니다.\nCopy sql WITH COUNTS AS ( SELECT CART_ID, SUM(NAME = \u0026#34;Milk\u0026#34;) AS MILK_COUNT, SUM(NAME = \u0026#34;Yogurt\u0026#34;) AS YOG_COUNT FROM CART_PRODUCTS GROUP BY CART_ID ) SELECT CART_ID FROM COUNTS WHERE MILK_COUNT \u0026gt; 0 AND YOG_COUNT \u0026gt; 0 ORDER BY CART_ID; 식품분류별 가장 비싼 식품의 정보 조회하기 # FOOD_PRODUCT 테이블은 식품의 정보를 담은 테이블입니다.\nFOOD_PRODUCT 테이블에서 식품분류별로 가격이 제일 비싼 식품의 분류, 가격, 이름을 조회하는 SQL문을 작성해주세요. 이때 식품분류가 \u0026lsquo;과자\u0026rsquo;, \u0026lsquo;국\u0026rsquo;, \u0026lsquo;김치\u0026rsquo;, \u0026lsquo;식용유\u0026rsquo;인 경우만 출력시켜 주시고 결과는 식품 가격을 기준으로 내림차순 정렬해주세요.\n식품분류(CATEGORY) 그룹에서 가격(PRICE)이 가장 큰 항목만 선택하는 문제입니다.\n가격(PRICE)이 MAX 집계 결과와 동일한 항목만 선택하는 경우도 있지만, 더 간단하게 WINDOW 함수를 사용했습니다. 식품분류(CATEGORY) 파티션에 대해 가격(PRICE)을 내림차순으로 정렬한 순번(ROW_NUMBER)이 1인 경우만 선택하면 동일한 결과를 조회할 수 있습니다.\nCopy sql WITH HIGHEST AS ( SELECT PRODUCT_ID, ROW_NUMBER() OVER (PARTITION BY CATEGORY ORDER BY PRICE DESC) AS SEQ FROM FOOD_PRODUCT WHERE CATEGORY IN (\u0026#34;과자\u0026#34;,\u0026#34;국\u0026#34;,\u0026#34;김치\u0026#34;,\u0026#34;식용유\u0026#34;) ) SELECT PRD.CATEGORY, PRD.PRICE AS MAX_PRICE, PRD.PRODUCT_NAME FROM FOOD_PRODUCT AS PRD INNER JOIN (SELECT * FROM HIGHEST WHERE SEQ = 1) AS HGT ON PRD.PRODUCT_ID = HGT.PRODUCT_ID ORDER BY MAX_PRICE DESC; 5월 식품들의 총매출 조회하기 # FOOD_PRODUCT 테이블은 식품의 정보를 담은 테이블입니다. FOOD_ORDER 테이블은 식품의 주문 정보를 담은 테이블입니다.\nFOOD_PRODUCT 와 FOOD_ORDER 테이블에서 생산일자가 2022년 5월인 식품들의 식품 ID, 식품 이름, 총매출을 조회하는 SQL문을 작성해주세요. 이때 결과는 총매출을 기준으로 내림차순 정렬해주시고 총매출이 같다면 식품 ID를 기준으로 오름차순 정렬해주세요.\n식품 ID(PRODUCT_ID) 그룹에 대해 총매출(SUM(AMOUNT * PRICE))을 계산하는 문제입니다.\nJOIN 을 먼저해서 매출(AMOUNT * PRICE)을 계산하고 SUM 집계하는 경우도 가능한데, 개인적으로는 GROUP BY 시에 식품 이름(PRODUCT_NAME)과 같은 긴 문자열을 포함하는 것을 선호하지 않습니다.\n입고일(PRODUCE_DATE)에 따라 가격(PRICE)이 바뀌는 것도 아니기 때문에 JOIN 과 GROUP BY 의 순서를 신경쓸 필요도 없습니다. 물론, FOOD_PRODUCT 테이블에서 식품 ID(PRODUCT_ID)에 중복이 없음을 전제로 합니다.\n따라서, 식품 ID(PRODUCT_ID)에 대해서 먼저 GROUP BY 하여 총주문량(SUM(AMOUNT))을 계산하고, 가격(PRICE)을 나중에 곱해 총매출(SUM(AMOUNT) * PRICE)을 만들었습니다.\nCopy sql WITH AMOUNTS AS ( SELECT PRODUCT_ID, SUM(AMOUNT) AS TOTAL_AMOUNT FROM FOOD_ORDER WHERE PRODUCE_DATE BETWEEN \u0026#39;2022-05-01\u0026#39; AND \u0026#39;2022-05-31\u0026#39; GROUP BY PRODUCT_ID ) SELECT AMT.PRODUCT_ID, PRD.PRODUCT_NAME, (AMT.TOTAL_AMOUNT * PRD.PRICE) AS TOTAL_SALES FROM AMOUNTS AS AMT INNER JOIN FOOD_PRODUCT AS PRD ON AMT.PRODUCT_ID = PRD.PRODUCT_ID ORDER BY TOTAL_SALES DESC, PRODUCT_ID ASC; 식당 테이블 # 서울에 위치한 식당 목록 출력하기 # REST_INFO 테이블은 식당의 정보를 담은 테이블입니다. REST_REVIEW 테이블은 식당의 리뷰 정보를 담은 테이블입니다.\nREST_INFO 와 REST_REVIEW 테이블에서 서울에 위치한 식당들의 식당 ID, 식당 이름, 음식 종류, 즐겨찾기수, 주소, 리뷰 평균 점수를 조회하는 SQL문을 작성해주세요. 이때 리뷰 평균점수는 소수점 세 번째 자리에서 반올림 해주시고 결과는 평균점수를 기준으로 내림차순 정렬해주시고, 평균점수가 같다면 즐겨찾기수를 기준으로 내림차순 정렬해주세요.\nREST_REVIEW 테이블에서 식당 ID(REST_ID) 그룹에 대해 리뷰 평균 점수(AVG(REVIEW_SCORE))를 계산하고, REST_INFO 테이블에 계산 결과를 결합하는 문제입니다.\nREST_INFO 테이블을 조회하면 서울에 있는 식당은 전부 주소(ADDRESS)가 \u0026ldquo;서울\u0026quot;로 시작합니다. 정렬 기준만 맞춰서 조회하면 정답을 도출할 수 있습니다.\nCopy sql WITH REVIEWS AS ( SELECT REST_ID, ROUND(AVG(REVIEW_SCORE),2) AS SCORE FROM REST_REVIEW GROUP BY REST_ID ) SELECT INFO.REST_ID, INFO.REST_NAME, INFO.FOOD_TYPE, INFO.FAVORITES, INFO.ADDRESS, RVW.SCORE FROM REST_INFO AS INFO INNER JOIN REVIEWS AS RVW ON INFO.REST_ID = RVW.REST_ID WHERE INFO.ADDRESS LIKE \u0026#34;서울%\u0026#34; ORDER BY SCORE DESC, FAVORITES DESC; 그룹별 조건에 맞는 식당 목록 출력하기 # MEMBER_PROFILE 테이블은 고객의 정보를 담은 테이블입니다. REST_REVIEW 테이블은 식당의 리뷰 정보를 담은 테이블입니다.\nMEMBER_PROFILE 와 REST_REVIEW 테이블에서 리뷰를 가장 많이 작성한 회원의 리뷰들을 조회하는 SQL문을 작성해주세요. 회원 이름, 리뷰 텍스트, 리뷰 작성일이 출력되도록 작성해주시고, 결과는 리뷰 작성일을 기준으로 오름차순, 리뷰 작성일이 같다면 리뷰 텍스트를 기준으로 오름차순 정렬해주세요.\nREST_REVIEW 테이블에서 리뷰 수(COUNT(REVIEW_ID))가 가장 큰 회원 ID(MEMBER_ID)에 대한 리뷰 목록을 조회하는 문제입니다.\n회원 ID(MEMBER_ID) 그룹별 리뷰 수(COUNT(REVIEW_ID))를 계산하고, 내림차순 정렬하여 첫 번째 항목만 조회하여 리뷰를 가장 많이 작성한 회원을 구합니다.\n해당 회원 ID(MEMBER_ID)에 대한 REST_REVIEW 항목을 모두 조회하면서, MEMBER_PROFILE 테이블을 결합해 회원 이름(MEMBER_NAME)을 같이 표시합니다. DATE 타입의 리뷰 작성일(REVIEW_DATE)은 그대로 출력하면 안되고 %Y-%m-%d 날짜 포맷팅을 해야합니다.\nCopy sql WITH BEST_MEMBER AS ( SELECT MEMBER_ID, COUNT(REVIEW_ID) AS REVIEW_COUNT FROM REST_REVIEW GROUP BY MEMBER_ID ORDER BY REVIEW_COUNT DESC LIMIT 1 ) SELECT MEM.MEMBER_NAME, RVW.REVIEW_TEXT, DATE_FORMAT(RVW.REVIEW_DATE, \u0026#39;%Y-%m-%d\u0026#39;) AS REVIEW_DATE FROM REST_REVIEW AS RVW INNER JOIN BEST_MEMBER AS BST ON RVW.MEMBER_ID = BST.MEMBER_ID INNER JOIN MEMBER_PROFILE AS MEM ON RVW.MEMBER_ID = MEM.MEMBER_ID ORDER BY REVIEW_DATE; 쇼핑몰 테이블 # 년, 월, 성별 별 상품 구매 회원 수 구하기 # USER_INFO 테이블은 의류 쇼핑몰에 가입한 회원 정보를 담은 테이블입니다. ONLINE_SALE 테이블은 온라인 상품 판매 정보를 담은 테이블입니다.\nUSER_INFO 테이블과 ONLINE_SALE 테이블에서 년, 월, 성별 별로 상품을 구매한 회원수를 집계하는 SQL문을 작성해주세요. 결과는 년, 월, 성별을 기준으로 오름차순 정렬해주세요. 이때, 성별 정보가 없는 경우 결과에서 제외해주세요.\n두 테이블을 INNER JOIN 하면서 성별(GENDER)이 있는(IS NOT NULL) 경우만 선택합니다.\n판매일(SALES_DATE)에서 각각 연도(YEAR), 월(MONTH)을 추출하고, 성별(GENDER)과 함께 세 가지 기준에 대해 GROUP BY 합니다. 그룹 내에서 중복 없는 회원 ID(DISTINCT USER_ID)를 COUNT 집계해 회원수를 계산합니다.\nCopy sql SELECT EXTRACT(YEAR FROM SALES.SALES_DATE) AS YEAR, EXTRACT(MONTH FROM SALES.SALES_DATE) AS MONTH, USR.GENDER, COUNT(DISTINCT SALES.USER_ID) AS USERS FROM ONLINE_SALE AS SALES INNER JOIN USER_INFO AS USR ON SALES.USER_ID = USR.USER_ID WHERE USR.GENDER IS NOT NULL GROUP BY YEAR, MONTH, GENDER ORDER BY YEAR, MONTH, GENDER; 상품을 구매한 회원 비율 구하기 # USER_INFO 테이블은 의류 쇼핑몰에 가입한 회원 정보를 담은 테이블입니다. ONLINE_SALE 테이블은 온라인 상품 판매 정보를 담은 테이블입니다.\nUSER_INFO 테이블과 ONLINE_SALE 테이블에서 2021년에 가입한 전체 회원들 중 상품을 구매한 회원수와 상품을 구매한 회원의 비율(=2021년에 가입한 회원 중 상품을 구매한 회원수 / 2021년에 가입한 전체 회원 수)을 년, 월 별로 출력하는 SQL문을 작성해주세요. 상품을 구매한 회원의 비율은 소수점 두번째자리에서 반올림하고, 전체 결과는 년을 기준으로 오름차순 정렬해주시고 년이 같다면 월을 기준으로 오름차순 정렬해주세요.\n2021년에 가입한 회원수와, 그중에서 상품을 구매한 회원수를 집계해 그 비율을 계산하는 문제입니다.\n우선, 대상을 2021년에 가입한 회원으로 좁히기 위해 USER_INFO 에서 가입일(JOINED)의 연도가 2021인 항목만 선택한 TARGET_USERS 임시 테이블 결과를 ONLINE_SALE 에 INNER JOIN 으로 연결합니다.\n가입일(JOINED)에서 연도(YEAR) 와 월(MONTH)을 추출하고, 두 가지 기준에 대해 GROUP BY 합니다. 그룹 내에서 중복 없는 회원 ID(DISTINCT USER_ID)를 COUNT 집계해 회원수를 계산하는데, 이때 판매량(SALES_AMOUNT)이 1 이상인, 즉 상품을 구매한 회원 ID만 고려합니다. 조건에 맞지 않는 회원 ID는 NULL 로 바꿔서 COUNT 집계에서 제외했습니다.\n이렇게 계산한 PURCHASED_USERS 값을 분자로, 회원 ID(USER_ID)를 COUNT 집계한 값을 분모로 하여 상품을 구매한 회원의 비율(PURCHASED_RATIO)을 계산합니다.\nCopy sql WITH TARGET_USERS AS ( SELECT DISTINCT USER_ID FROM USER_INFO WHERE EXTRACT(YEAR FROM JOINED) = 2021 ), PURCHASED AS ( SELECT EXTRACT(YEAR FROM SALES.SALES_DATE) AS YEAR, EXTRACT(MONTH FROM SALES.SALES_DATE) AS MONTH, COUNT(DISTINCT IF(SALES.SALES_AMOUNT \u0026gt; 0, USERS.USER_ID, NULL)) AS PURCHASED_USERS FROM ONLINE_SALE AS SALES INNER JOIN TARGET_USERS AS USERS ON SALES.USER_ID = USERS.USER_ID GROUP BY YEAR, MONTH ) SELECT YEAR, MONTH, PURCHASED_USERS, ROUND(PURCHASED_USERS / (SELECT COUNT(USER_ID) FROM TARGET_USERS), 1) AS PUCHASED_RATIO FROM PURCHASED ORDER BY YEAR, MONTH; 오프라인/온라인 판매 데이터 통합하기 # ONLINE_SALE 테이블은 온라인 상품 판매 정보를 담은 테이블입니다. ONLINE_SALE 테이블은 온라인 상품 판매 정보를 담은 테이블입니다.\nONLINE_SALE 테이블과 OFFLINE_SALE 테이블에서 2022년 3월의 오프라인/온라인 상품 판매 데이터의 판매 날짜, 상품ID, 유저ID, 판매량을 출력하는 SQL문을 작성해주세요. OFFLINE_SALE 테이블의 판매 데이터의 USER_ID 값은 NULL 로 표시해주세요. 결과는 판매일을 기준으로 오름차순 정렬해주시고 판매일이 같다면 상품 ID를 기준으로 오름차순, 상품ID까지 같다면 유저 ID를 기준으로 오름차순 정렬해주세요.\n두 테이블을 결과를 세로로 결합하고 정렬하는 문제입니다.\nONLINE_SALE 테이블은 판매일(SALES_DATE), 상품 ID(PRODUCT_ID), 회원 ID(USER_ID) 그룹별로 판매량(SALES_AMOUNT)을 SUM 집계했습니다. GROUP BY 하라는 지문이 없어 안해도 되었던 것 같은데 개인적으로 판매 데이터를 보면 버릇처럼 GROUP BY 하게 됩니다.\nOFFLINE_SALE 테이블은 회원 ID(USER_ID)가 없지만, UNION ALL 연산을 위해 ONLINE_SALE 테이블과 구성을 맞춰줄 목적으로 NULL 값을 가지는 USER_ID 열을 추가합니다.\nDATE 타입의 판매일(REVIEW_DATE)은 그대로 출력하면 안되고 %Y-%m-%d 날짜 포맷팅을 해야합니다.\nCopy sql SELECT DATE_FORMAT(TOTAL.SALES_DATE, \u0026#39;%Y-%m-%d\u0026#39;) AS SALES_DATE, TOTAL.PRODUCT_ID, TOTAL.USER_ID, TOTAL.SALES_AMOUNT FROM ( SELECT SALES_DATE, PRODUCT_ID, USER_ID, SUM(SALES_AMOUNT) AS SALES_AMOUNT FROM ONLINE_SALE WHERE SALES_DATE BETWEEN \u0026#39;2022-03-01\u0026#39; AND \u0026#39;2022-03-31\u0026#39; GROUP BY SALES_DATE, PRODUCT_ID, USER_ID UNION ALL SELECT SALES_DATE, PRODUCT_ID, NULL AS USER_ID, SUM(SALES_AMOUNT) AS SALES_AMOUNT FROM OFFLINE_SALE WHERE SALES_DATE BETWEEN \u0026#39;2022-03-01\u0026#39; AND \u0026#39;2022-03-31\u0026#39; GROUP BY SALES_DATE, PRODUCT_ID ) AS TOTAL ORDER BY SALES_DATE, PRODUCT_ID, USER_ID; 진료 테이블 # 취소되지 않은 진료 예약 조회하기 # PATIENT 테이블은 환자 정보를 담은 테이블입니다. DOCTOR 테이블은 의사 정보를 담은 테이블입니다. APPOINTMENT 테이블은 진료 예약목록을 담은 테이블입니다.\nPATIENT, DOCTOR 그리고 APPOINTMENT 테이블에서 2022년 4월 13일 취소되지 않은 흉부외과(CS) 진료 예약 내역을 조회하는 SQL문을 작성해주세요. 진료예약번호, 환자이름, 환자번호, 진료과코드, 의사이름, 진료예약일시 항목이 출력되도록 작성해주세요. 결과는 진료예약일시를 기준으로 오름차순 정렬해주세요.\n세 개의 테이블에서 조건에 맞는 항목들만 선택해 결합하는 문제입니다.\nAPPOINTMENT 테이블에서는 진료 예약일시(APNT_YMD)가 \u0026lsquo;2022-04-13\u0026rsquo; 과 동일하고 예약취소여부(APNT_CNCL_YN)가 \u0026lsquo;N\u0026rsquo;에 해당하는 경우만 선택합니다.\nDOCTOR 테이블에서는 진료과코드(MCDP_CD)가 \u0026lsquo;CS\u0026rsquo;에 해당하는 경우만 선택합니다.\nPATIENT 테이블은 환자이름(PT_NAME)을 가져오기 위한 목적이며 따로 조건은 없습니다.\n세 개의 테이블을 INNER JOIN 하여 조건을 결합하고 필요한 항목들을 출력합니다.\nCopy sql SELECT APP.APNT_NO, PAT.PT_NAME, APP.PT_NO, APP.MCDP_CD, DOC.DR_NAME, APP.APNT_YMD FROM APPOINTMENT AS APP INNER JOIN DOCTOR AS DOC ON APP.MDDR_ID = DOC.DR_ID INNER JOIN PATIENT AS PAT ON APP.PT_NO = PAT.PT_NO WHERE DATE_FORMAT(APP.APNT_YMD, \u0026#39;%Y-%m-%d\u0026#39;) = \u0026#39;2022-04-13\u0026#39; AND APP.APNT_CNCL_YN = \u0026#39;N\u0026#39; AND DOC.MCDP_CD = \u0026#39;CS\u0026#39; ORDER BY APNT_YMD 판매 테이블 # 주문량이 많은 아이스크림들 조회하기 # FIRST_HALF 테이블은 아이스크림 가게의 상반기 주문 정보를 담은 테이블입니다. JULY 테이블은 7월의 아이스크림 주문 정보를 담은 테이블입니다.\n7월 아이스크림 총 주문량과 상반기의 아이스크림 총 주문량을 더한 값이 큰 순서대로 상위 3개의 맛을 조회하는 SQL 문을 작성해주세요.\n아이스크림 총주문량(TOTAL_ORDER)을 내림차순 정렬하여 상위 3개 항목만 조회하는 문제입니다.\n구성이 동일한 FIRST_HALF 테이블과 JULY 테이블을 UNION ALL 결합하고 아이스크림 맛(FLAVOR) 그룹별로 아이스크림 총주문량(TOTAL_ORDER)을 SUM 집계합니다.\n이때, 집계한 값을 출력할 필요는 없기 때문에 굳이 SELECT 하지는 않고 ORDER BY 구문에서 직접적으로 사용합니다. 집계한 값을 내림차순 정렬한 후 LIMIT 3 으로 상위 3개 항목만 조회합니다.\nCopy sql SELECT FLAVOR FROM ( SELECT * FROM FIRST_HALF UNION ALL SELECT * FROM JULY ) AS TOTAL GROUP BY FLAVOR ORDER BY SUM(TOTAL_ORDER) DESC LIMIT 3 저자 별 카테고리 별 매출액 집계하기 # BOOK 테이블은 각 도서의 정보를 담은 테이블입니다. AUTHOR 테이블은 도서의 저자의 정보를 담은 테이블입니다. BOOK_SALES 테이블은 각 도서의 날짜 별 판매량 정보를 담은 테이블입니다.\n2022년 1월의 도서 판매 데이터를 기준으로 저자 별, 카테고리 별 매출액(TOTAL_SALES = 판매량 * 판매가) 을 구하여, 저자 ID(AUTHOR_ID), 저자명(AUTHOR_NAME), 카테고리(CATEGORY), 매출액(SALES) 리스트를 출력하는 SQL문을 작성해주세요. 결과는 저자 ID를 오름차순으로, 저자 ID가 같다면 카테고리를 내림차순 정렬해주세요.\n세 개의 테이블을 결합하고 제시된 그룹별 매출액을 계산하는 문제입니다.\nBOOK_SALES 테이블에서 BOOK_ID, AUTHOR_ID 를 기준으로 각각 BOOK, AUTHOR 테이블과 INNER JOIN 합니다. BOOK_SALES 테이블의 판매량(SALES)과 BOOK 테이블의 판매가(PRICE)를 곱한 매출액을 SUM 집계하여 총매출액(TOTAL_SALES)을 계산합니다.\nCopy sql SELECT BK.AUTHOR_ID, AU.AUTHOR_NAME, BK.CATEGORY, SUM(SALES.SALES * BK.PRICE) AS TOTAL_SALES FROM BOOK_SALES AS SALES INNER JOIN BOOK AS BK ON SALES.BOOK_ID = BK.BOOK_ID INNER JOIN AUTHOR AS AU ON BK.AUTHOR_ID = AU.AUTHOR_ID WHERE SALES.SALES_DATE BETWEEN \u0026#39;2022-01-01\u0026#39; AND \u0026#39;2022-01-31\u0026#39; GROUP BY AUTHOR_ID, AUTHOR_NAME, CATEGORY ORDER BY AUTHOR_ID, CATEGORY DESC; 자동차 대여 테이블 # 자동차 대여 기록 별 대여 금액 구하기 # CAR_RENTAL_COMPANY_CAR 테이블은 대여 중인 자동차들의 정보를 담은 테이블입니다. CAR_RENTAL_COMPANY_RENTAL_HISTORY 테이블은 자동차 대여 기록 정보를 담은 테이블입니다. CAR_RENTAL_COMPANY_DISCOUNT_PLAN 테이블은 자동차 종류 별 대여 기간 종류 별 할인 정책 정보를 담은 테이블 입니다.\nCAR_RENTAL_COMPANY_CAR 테이블과 CAR_RENTAL_COMPANY_RENTAL_HISTORY 테이블과 CAR_RENTAL_COMPANY_DISCOUNT_PLAN 테이블에서 자동차 종류가 \u0026lsquo;트럭\u0026rsquo;인 자동차의 대여 기록에 대해서 대여 기록 별로 대여 금액(컬럼명: FEE)을 구하여 대여 기록 ID와 대여 금액 리스트를 출력하는 SQL문을 작성해주세요. 결과는 대여 금액을 기준으로 내림차순 정렬하고, 대여 금액이 같은 경우 대여 기록 ID를 기준으로 내림차순 정렬해주세요.\n세 개의 테이블을 결합하는데, 우선 대여 시작일(START_DATE)과 대여 종료일(END_DATE) 기간에 대한 날짜수를 기준으로 할인율 적용 기준을 분류합니다. 할인율 적용 기준을 통해 할인율(DISCOUNT_RATE) 수치를 도출하여 대여 금액(FEE)에 할인 적용을 하는 문제입니다.\n일수(DAYS)를 계산하는 방법으로 DATEDIFF 함수를 사용합니다. 시작일과 종료일도 기간에 포함되기 때문에 함수의 결과에 +1 을 더합니다. 일수(DAYS)에 대해 CASE 조건문을 통해 대여 기간 종류(DURATION_TYPE)에 해당하는 4가지 분류로 구분합니다. 대여 기간 종류(DURATION_TYPE)는 그대로 LEFT JOIN 의 기준으로써, CAR_RENTAL_COMPANY_DISCOUNT_PLAN 테이블로부터 할인율(DISCOUNT_RATE)을 도출하는데 사용됩니다.\n대여 금액(FEE)은 일일 대여 요금(DAILY_FEE)에 앞에서 구한 일수(DAYS)와 할인율(DISCOUNT_RATE)을 곱한 결과입니다. 이때, 정수 타입인 할인율(DISCOUNT_RATE)은 그대로 사용하지 않고, 1.0 - (할인율 / 100) 계산식에 넣어 배율로 변환합니다.\nCopy sql WITH HISTORY AS ( SELECT *, (DATEDIFF(END_DATE, START_DATE) + 1) AS DAYS FROM CAR_RENTAL_COMPANY_RENTAL_HISTORY ) SELECT HIST.HISTORY_ID, ROUND(CAR.DAILY_FEE * HIST.DAYS * (1.0 - (IFNULL(DIS.DISCOUNT_RATE, 0) / 100))) AS FEE FROM HISTORY AS HIST INNER JOIN CAR_RENTAL_COMPANY_CAR AS CAR ON HIST.CAR_ID = CAR.CAR_ID LEFT JOIN CAR_RENTAL_COMPANY_DISCOUNT_PLAN AS DIS ON DIS.CAR_TYPE = CAR.CAR_TYPE AND DIS.DURATION_TYPE = ( CASE WHEN HIST.DAYS \u0026gt;= 90 THEN \u0026#39;90일 이상\u0026#39; WHEN HIST.DAYS \u0026gt;= 30 THEN \u0026#39;30일 이상\u0026#39; WHEN HIST.DAYS \u0026gt;= 7 THEN \u0026#39;7일 이상\u0026#39; ELSE \u0026#39;7일 미만\u0026#39; END) WHERE CAR.CAR_TYPE = \u0026#39;트럭\u0026#39; ORDER BY FEE DESC, HISTORY_ID DESC 특정 기간동안 대여 가능한 자동차들의 대여비용 구하기 # CAR_RENTAL_COMPANY_CAR 테이블은 대여 중인 자동차들의 정보를 담은 테이블입니다. CAR_RENTAL_COMPANY_RENTAL_HISTORY 테이블은 자동차 대여 기록 정보를 담은 테이블입니다. CAR_RENTAL_COMPANY_DISCOUNT_PLAN 테이블은 자동차 종류 별 대여 기간 종류 별 할인 정책 정보를 담은 테이블 입니다.\nCAR_RENTAL_COMPANY_CAR 테이블과 CAR_RENTAL_COMPANY_RENTAL_HISTORY 테이블과 CAR_RENTAL_COMPANY_DISCOUNT_PLAN 테이블에서 자동차 종류가 \u0026lsquo;세단\u0026rsquo; 또는 \u0026lsquo;SUV\u0026rsquo; 인 자동차 중 2022년 11월 1일부터 2022년 11월 30일까지 대여 가능하고 30일간의 대여 금액이 50만원 이상 200만원 미만인 자동차에 대해서 자동차 ID, 자동차 종류, 대여 금액(컬럼명: FEE) 리스트를 출력하는 SQL문을 작성해주세요. 결과는 대여 금액을 기준으로 내림차순 정렬하고, 대여 금액이 같은 경우 자동차 종류를 기준으로 오름차순 정렬, 자동차 종류까지 같은 경우 자동차 ID를 기준으로 내림차순 정렬해주세요.\n앞선 문제와 테이블 및 대여 금액(FEE) 계산 방법이 동일한데, 세부적인 조건이 늘어났습니다.\n첫 번째 조건인 자동차 종류를 선택하는 것은 CAR_RENTAL_COMPANY_CAR 테이블에서 WHERE 절을 통해 자동차 종류(CAR_TYPE)가 \u0026lsquo;세단\u0026rsquo; 또는 \u0026lsquo;SUV\u0026rsquo;인 것만 선택하면 됩니다.\n두 번째 조건인 2022년 11월 1일부터 2022년 11월 30일까지 대여 가능한 경우는 CAR_RENTAL_COMPANY_RENTAL_HISTORY 테이블에서 대여 시작일(START_DATE)과 대여 종료일(END_DATE)이 해당 기간에 걸치는 경우를 파악하고 대상 자동차 ID(CAR_ID)를 WHERE 절의 AND 조건으로 추가해 제외합니다.\n세 번째 조건은 대여 금액(FEE)을 계산한 후, WHERE 절을 통해 50만원 이상 200만원 미만인 경우를 선택합니다. FEE \u0026gt;= (50 * 10000) AND FEE \u0026lt; (200 * 10000) 조건을 적용할 수도 있지만, 상대적으로 길이가 짧은 BETWEEN 절로 표현했습니다. 대여 금액(FEE)은 소수점까지 떨어지지는 않기 때문에 -1 하여 미만(\u0026lt;)과 동일하게 표현할 수 있습니다.\nCopy sql WITH HISTORY AS ( SELECT *, (DATEDIFF(END_DATE, START_DATE) + 1) AS DAYS FROM CAR_RENTAL_COMPANY_RENTAL_HISTORY ), DISCOUNT_PLAN AS ( SELECT CAR_TYPE, DURATION_TYPE, (1.0 - (DISCOUNT_RATE / 100)) AS DISCOUNT_RATE FROM CAR_RENTAL_COMPANY_DISCOUNT_PLAN ), CAR_IN_USE AS ( SELECT DISTINCT CAR_ID FROM HISTORY WHERE START_DATE \u0026lt;= \u0026#39;2022-11-30\u0026#39; AND END_DATE \u0026gt;= \u0026#39;2022-11-01\u0026#39; ), CAR AS ( SELECT CAR.CAR_ID, CAR.CAR_TYPE, ROUND(CAR.DAILY_FEE * 30 * IFNULL(DIS.DISCOUNT_RATE, 1.0)) AS FEE FROM CAR_RENTAL_COMPANY_CAR AS CAR LEFT JOIN DISCOUNT_PLAN AS DIS ON CAR.CAR_TYPE = DIS.CAR_TYPE AND DIS.DURATION_TYPE = \u0026#39;30일 이상\u0026#39; WHERE CAR.CAR_TYPE IN (\u0026#39;세단\u0026#39;,\u0026#39;SUV\u0026#39;) AND CAR.CAR_ID NOT IN (SELECT CAR_ID FROM CAR_IN_USE) ) SELECT CAR_ID, CAR_TYPE, FEE FROM CAR WHERE FEE BETWEEN (50 * 10000) AND (200 * 10000 - 1) ORDER BY FEE DESC, CAR_TYPE ASC, CAR_ID DESC 개발자 스킬 테이블 # FrontEnd 개발자 찾기 # SKILLCODES 테이블은 개발자들이 사용하는 프로그래밍 언어에 대한 정보를 담은 테이블입니다. DEVELOPERS 테이블은 개발자들의 프로그래밍 스킬 정보를 담은 테이블입니다.\nDEVELOPERS 테이블에서 Front End 스킬을 가진 개발자의 정보를 조회하려 합니다. 조건에 맞는 개발자의 ID, 이메일, 이름, 성을 조회하는 SQL 문을 작성해 주세요. 결과는 ID를 기준으로 오름차순 정렬해 주세요.\n두 개의 테이블에서 정수 타입인 스킬 코드 열을 비트 연산하여 공통된 경우를 JOIN 하는 문제입니다.\nSKILLCODES 테이블에서 스킬의 범주(CATEGORY)가 \u0026lsquo;Front End\u0026rsquo;에 해당하는 항목만 선택합니다. 해당 테이블의 스킬의 코드(CODE)는 2진수로 표현했을 때 각 비트마다 하나의 스킬에 대응됩니다.\nDEVELOPERS 테이블에도 마찬가지로 개발자에 대한 스킬 코드(SKILL_CODE)가 있습니다. 두 개 테이블의 스킬 코드를 \u0026amp; 연산자를 활용해 비트 연산하면 공통된 비트만 얻을 수 있습니다. 이것이 SKILLCODES 테이블의 스킬의 코드(CODE)와 같다면 해당 스킬을 보유하고 있다고 판단할 수 있습니다.\n\u0026lsquo;Front End\u0026rsquo; 스킬을 가진 개발자의 ID(ID)만 따로 추출하고, DEVELOPERS 테이블에서 해당 ID 에 해당하는 항목들만 선택하여 조회합니다.\nCopy sql WITH TARGET AS ( SELECT DISTINCT DEV.ID FROM DEVELOPERS AS DEV INNER JOIN SKILLCODES AS SKL ON (DEV.SKILL_CODE \u0026amp; SKL.CODE) = SKL.CODE WHERE SKL.CATEGORY = \u0026#39;Front End\u0026#39; ) SELECT ID, EMAIL, FIRST_NAME, LAST_NAME FROM DEVELOPERS WHERE ID IN (SELECT ID FROM TARGET) ORDER BY ID; 언어별 개발자 분류하기 # SKILLCODES 테이블은 개발자들이 사용하는 프로그래밍 언어에 대한 정보를 담은 테이블입니다. DEVELOPERS 테이블은 개발자들의 프로그래밍 스킬 정보를 담은 테이블입니다.\nDEVELOPERS 테이블에서 GRADE별 개발자의 정보를 조회하려 합니다. GRADE는 다음과 같이 정해집니다.\nA : Front End 스킬과 Python 스킬을 함께 가지고 있는 개발자 B : C# 스킬을 가진 개발자 C : 그 외의 Front End 개발자 GRADE가 존재하는 개발자의 GRADE, ID, EMAIL을 조회하는 SQL 문을 작성해 주세요. 결과는 GRADE와 ID를 기준으로 오름차순 정렬해 주세요.\n앞선 문제와, 테이블 및 스킬 코드를 비트 연산하여 개발자가 스킬을 가졌는지 판단하는 방법이 동일한데, 세부적인 조건이 늘어났습니다.\nCASE 조건문을 통해 \u0026lsquo;A\u0026rsquo;, \u0026lsquo;B\u0026rsquo;, \u0026lsquo;C\u0026rsquo; 세 가지 등급으로 개발자를 나누는데, CASE 안에서 직접적으로 조건을 표현하면 길어질 수도 있습니다. 또한, \u0026lsquo;Front End\u0026rsquo; 스킬을 보유한 경우에 대해서는 \u0026lsquo;A\u0026rsquo;, \u0026lsquo;C\u0026rsquo; 두 가지 등급에서 참조하기 때문에 중복 선언도 피하고 싶습니다.\n따라서, 원하는 스킬을 보유한 경우를 HAS_SKILLS 라는 임시 테이블로 구성하고, 해당 테이블을 DEVELOPERS 테이블에 INNER JOIN 하여 미리 계산한 조건에 대한 Boolean 값을 CASE 안에서 가져다 사용합니다. 조건에 해당되지 않는 나머지 개발자에 대한 등급은 NULL 로 분류하고, 등급이 NULL 인 경우만 제외하여 조회합니다.\nCopy sql WITH HAS_SKILLS AS ( SELECT DEV.ID, (SUM(SKL.CATEGORY = \u0026#39;Front End\u0026#39;) \u0026gt; 0) AS HAS_FRONT, (SUM(SKL.NAME = \u0026#39;Python\u0026#39;) \u0026gt; 0) AS HAS_PYTHON, (SUM(SKL.NAME = \u0026#39;C#\u0026#39;) \u0026gt; 0) AS HAS_CSHARP FROM DEVELOPERS AS DEV LEFT JOIN SKILLCODES AS SKL ON (DEV.SKILL_CODE \u0026amp; SKL.CODE) = SKL.CODE GROUP BY ID ), WITH_GRADE AS ( SELECT (CASE WHEN SKL.HAS_FRONT AND SKL.HAS_PYTHON THEN \u0026#39;A\u0026#39; WHEN SKL.HAS_CSHARP THEN \u0026#39;B\u0026#39; WHEN SKL.HAS_FRONT THEN \u0026#39;C\u0026#39; ELSE NULL END) AS GRADE, DEV.ID, DEV.EMAIL FROM DEVELOPERS AS DEV INNER JOIN HAS_SKILLS AS SKL ON DEV.ID = SKL.ID ) SELECT * FROM WITH_GRADE WHERE GRADE IS NOT NULL ORDER BY GRADE, ID; 인사 테이블 # 연간 평가점수에 해당하는 평가 등급 및 성과금 조회하기 # HR_DEPARTMENT 테이블은 회사의 부서 정보를 담은 테이블입니다. HR_EMPLOYEES 테이블은 회사의 사원 정보를 담은 테이블입니다. HR_GRADE 테이블은 2022년 사원의 평가 정보를 담은 테이블입니다.\nHR_DEPARTMENT, HR_EMPLOYEES, HR_GRADE 테이블을 이용해 사원별 성과금 정보를 조회하려합니다. 평가 점수별 등급과 등급에 따른 성과금 정보가 아래와 같을 때, 사번, 성명, 평가 등급, 성과금을 조회하는 SQL문을 작성해주세요.\n평가등급의 컬럼명은 GRADE 로, 성과금의 컬럼명은 BONUS 로 해주세요. 결과는 사번 기준으로 오름차순 정렬해주세요.\nHR_GRADE 테이블의 평가 점수(SCORE)를 기준으로 평가등급(GRADE)과 성과금(BONUS)을 도출하는 문제입니다.\n사원 평가는 반기마다 발생하는데 2022년 내에 두 차례 발생했기 때문에, 사번(EMP_NO) 그룹별로 평가 점수(SCORE)를 AVG 집계합니다. 평균 평가 점수(AVG(SCORE))를 CASE 조건문에 넣어 평가등급(GRADE)과 성과금(BONUS)을 분류합니다.\n분류한 성과금은 연봉에 대한 배율이기 때문에, 최종 답안을 낼 때는 HR_EMPLOYEES 테이블의 연봉(SAL)에 배율을 곱해서 실제 성과금(BONUS)을 계산합니다.\nCopy sql WITH SCORE AS ( SELECT EMP_NO, AVG(SCORE) AS SCORE FROM HR_GRADE GROUP BY EMP_NO ), GRADE AS ( SELECT EMP_NO, (CASE WHEN SCORE \u0026gt;= 96 THEN \u0026#39;S\u0026#39; WHEN SCORE \u0026gt;= 90 THEN \u0026#39;A\u0026#39; WHEN SCORE \u0026gt;= 80 THEN \u0026#39;B\u0026#39; ELSE \u0026#39;C\u0026#39; END) AS GRADE, (CASE WHEN SCORE \u0026gt;= 96 THEN 0.2 WHEN SCORE \u0026gt;= 90 THEN 0.15 WHEN SCORE \u0026gt;= 80 THEN 0.1 ELSE 0.0 END) AS BONUS FROM SCORE ) SELECT EMP.EMP_NO, EMP.EMP_NAME, GRD.GRADE, ROUND(EMP.SAL * GRD.BONUS) AS BONUS FROM HR_EMPLOYEES AS EMP INNER JOIN GRADE AS GRD ON EMP.EMP_NO = GRD.EMP_NO ORDER BY EMP_NO 대장균 개체 테이블 # 특정 세대의 대장균 찾기 # ECOLI_DATA 테이블은 실험실에서 배양한 대장균들의 정보를 담은 테이블입니다.\n3세대의 대장균의 ID(ID) 를 출력하는 SQL 문을 작성해주세요. 이때 결과는 대장균의 ID 에 대해 오름차순 정렬해주세요.\n세대별로 JOIN 연산을 중첩해서 사용하여 3세대 대장균의 ID(ID)를 도출하는 문제입니다.\n1세대 대장균(GEN1)은 부모 개체의 ID(PARENT_ID)가 NULL 인 경우입니다.\n2세대 대장균(GEN2)은 1세대(GEN1) 대장균 개체의 ID(ID)가 부모 개체의 ID(PARENT_ID)인 경우입니다. 두 항목을 INNER JOIN 하여 공통된 경우만 2세대로 판단합니다.\n3세대 대장균은 2세대 대장균(GEN2)을 구한 것과 동일한 방식으로 INNER JOIN 연산합니다. 해당하는 3세대 대장균의 ID(ID)를 출력합니다.\nCopy sql WITH GEN1 AS ( SELECT DISTINCT ID FROM ECOLI_DATA WHERE PARENT_ID IS NULL ), GEN2 AS ( SELECT DISTINCT ECOLI.ID FROM ECOLI_DATA AS ECOLI INNER JOIN GEN1 AS PARENT ON ECOLI.PARENT_ID = PARENT.ID ) SELECT DISTINCT ECOLI.ID FROM ECOLI_DATA AS ECOLI INNER JOIN GEN2 AS PARENT ON ECOLI.PARENT_ID = PARENT.ID ORDER BY ID; 멸종위기의 대장균 찾기 # ECOLI_DATA 테이블은 실험실에서 배양한 대장균들의 정보를 담은 테이블입니다.\n각 세대별 자식이 없는 개체의 수(COUNT)와 세대(GENERATION)를 출력하는 SQL문을 작성해주세요. 이때 결과는 세대에 대해 오름차순 정렬해주세요. 단, 모든 세대에는 자식이 없는 개체가 적어도 1개체는 존재합니다.\n재귀 쿼리를 사용해 이전 세대의 대장균 ID(ID)와 ECOLI_DATA 테이블의 부모 개체 ID(PARENT_ID)가 동일한 경우를 반복해서 탐색합니다. 다음 세대가 없을 때까지 조회한 모든 결과를 결합하고, 대장균의 ID(ID)가 어떠한 부모 개체의 ID(PARENT_ID)에도 포함되지 않는 경우만 선택하여, 세대(GENERATION)별 개체의 수(COUNT)를 집계하는 문제입니다.\n앞선 문제처럼 n세대까지 JOIN 연산을 중첩하려다가 도저히 아닌 것 같아서 찾아봤는데, 재귀 쿼리를 사용하여 해결할 수 있는 문제였습니다. 평소에 재귀 쿼리를 사용해보지 않아 GEN_DATA 를 도출하는 과정은 검색 결과를 참고했습니다.\nGEN_DATA 에서 자식이 없는 개체만 선택하고 GENERATION 그룹별로 COUNT 집계한 결과를 조회합니다.\nCopy sql WITH RECURSIVE GEN_DATA AS( SELECT ID, 1 AS GENERATION FROM ECOLI_DATA WHERE PARENT_ID IS NULL UNION ALL SELECT ECOLI.ID, PARENT.GENERATION+1 AS GENERATION FROM ECOLI_DATA AS ECOLI INNER JOIN GEN_DATA AS PARENT ON ECOLI.PARENT_ID = PARENT.ID ) SELECT COUNT(PARENT.GENERATION) AS COUNT, PARENT.GENERATION FROM GEN_DATA AS PARENT WHERE PARENT.ID NOT IN ( SELECT DISTINCT PARENT_ID FROM ECOLI_DATA WHERE PARENT_ID IS NOT NULL) GROUP BY GENERATION ORDER BY GENERATION; "},{"id":10,"href":"/blog/spark-study-4/","title":"Apache Spark - Structured API","section":"Posts","content":"Spark Structure # 정형화 API에 대해 알아보기에 앞서, 정형적 모델 이전의 RDD 프로그래밍 API 모델을 확인해본다.\nRDD # RDD는 Spark 1.x 버전에 있던 저수준의 DSL을 의미하고, 스파크에서 가장 기본적인 추상적인 부분이다. RDD에는 세 가지의 핵심으로 특성이 있다.\n의존성\n어떤 입력을 필요로 하고 RDD가 어떻게 만들어지는지 Spark에게 가르쳐 주는 의존성이 필요하다.\n파티션\nExecutor들에 작업을 분산해 파티션별로 병렬 연산할 수 있는 능력을 부여한다. 지역성 정보를 사용하여 각 Executor가 가까이 있는 Executor에게 우선적으로 작업을 보낸다.\n연산 함수\n파티션에 저장되는 데이터를 Iterator[T] 형태로 만들어준다. 하지만, Iterator[T] 데이터 타입이 파이썬 RDD에서 기본 객체로만 인식이 가능해 불투명했다. Spark가 함수에서 연산이나 표현식을 검사하지 못해 객체를 바이트 뭉치로 직렬화해 쓰는 것밖에 못했다. 이로 인해 연산 순서를 재정렬해 효과적인 질의 계획으로 바꾸기가 어려웠다.\nSpark DSL # Spark 2.x는 RDD의 한계를 극복하기 위해 고수준의 DSL을 도입했다. Spark DSL은 다음과 같은 네 가지 특징이 있다.\n도메인 특화 언어\nSpark DSL은 분산 데이터 처리와 분석에 최적화된 명령어와 함수를 제공하여, 대규모 데이터셋에 대한 복잡한 연산을 간결하게 표현할 수 있다.\n다중 언어 지원\nScala 언어 뿐 아니라, Java, Python, R 등 다양한 언어에서 Spark DSL의 기능을 사용할 수 있게 지원한다.\n함수형 프로그래밍 지원\n람다 함수, 고차 함수 등 함수형 프로그래밍 기법을 활용하여 Transformation 및 Action을 간결하게 구현할 수 있다.\nSQL 통합\nSpark SQ DSL을 통해 SQL 쿼리와 유사한 구문으로 DataFrame 및 Dataset을 조작할 수 있다.\n고수준 DSL을 통한 Spark 구조를 갖추면서 더 나은 성능과 공간 효율성 등 많은 이득을 얻을 수 있었다. DataFrame API나 Dataset API를 다루면서 표현성, 단순성, 구성 용이성, 통일성 등의 장점도 가지게 되었다.\n이름별로 모든 나이들을 모아서 그룹화하고, 나이의 평균을 구하는 예제를 저수준의 RDD API로 구현한다고 하면 다음과 같을 수 있다.\nCopy python dataRDD = sc.parallelize([ (\u0026#34;Brooke\u0026#34;, 20), (\u0026#34;Denny\u0026#34;, 31), (\u0026#34;Jules\u0026#34;, 30), (\u0026#34;TD\u0026#34;, 35), (\u0026#34;Brooke\u0026#34;, 25)]) agesRDD = (dataRDD .map(lambda x: (x[0], (x[1], 1))) .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) .map(lambda x: x[0], x[1][0]/x[1][1])) 해당 코드를 Spark에게 쿼리를 계산하는 과정을 직접적으로 지시하여 의도가 전달되지 않는다. 동일한 질의를 Python의 고수준 DSL 연산자들과 DataFrame API를 사용하면 다음과 같다.\nCopy python from pyspark.sql import SparkSession from pyspark.sql.functions import avg # SparkSession 객체 생성 data_df = spark.createDataFrame( [(\u0026#34;Brooke\u0026#34;, 20), (\u0026#34;Denny\u0026#34;, 31), (\u0026#34;Jules\u0026#34;, 30), (\u0026#34;TD\u0026#34;, 35), (\u0026#34;Brooke\u0026#34;, 25)]) avg_df = data_df.groupBy(\u0026#34;name\u0026#34;).agg(avg(\u0026#34;age\u0026#34;)) avg_df.show() 고수준 DSL은 표현력이 높고 저수준 DSL보다 간단하다. Spark는 groupBy, avg 등의 연산자들을 통해 사용자의 의도를 이해하고 효과적인 실행을 위해 연산자들을 최적화하거나 적절하게 재배열할 수 있다.\n단순히 간단하기만 할 뿐 아니라 고수준 DSL은 언어 간에 일관성을 갖고 있다. 예를 들어 이름별로 나이의 평균을 집계하는 코드는 아래와 같다. 겉보기에도 똑같고 실제로 하는 일도 동일하다.\nCopy python # 파이썬 예제 avg_df = data_df.groupBy(\u0026#34;name\u0026#34;).agg(avg(\u0026#34;age\u0026#34;)) Copy kotlin // 스칼라 예제 val avgDf = dataDf.groupBy(\u0026#34;name\u0026#34;).agg(avg(\u0026#34;age\u0026#34;)) DataFrame API # pandas의 DataFrame에 영향을 받은 Spark DataFrame은 칼럼과 스키마를 가진 분산된 테이블처럼 동작하며, 각 칼럼은 정수, 문자열, 배열, 날짜 등 특정한 데이터 타입을 가질 수 있다.\n기본 데이터 타입 # 데이터 타입은 Spark Application에서 선언하거나, 스키마에서 정의할 수 있다. 먼저, Scala와 Python의 기본적인 데이터 타입은 아래와 같다.\n데이터 타입 스칼라에서 할당되는 값 파이썬에서 할당되는 값 ByteType Byte int ShortType Short int IntegerType Integer int LongType Long int FloatType Float float DoubleType Double float StringType String str BooleanType Boolean bool DecimalType java.math.BigDecimal decimal.Decimal 정형화 타입과 복합 타입 # 복합 데이터 분석을 위해서는 기본적인 데이터 타입을 사용하지 않는다. 대상 데이터는 맵, 배열, 구조체 등 자체적 구조를 갖고 있기 때문에, 이를 다루기 위한 타입을 지원한다.\n데이터 타입 스칼라에서 할당되는 값 파이썬에서 할당되는 값 BinaryType Array[Byte] bytearray TimestampType java.sqlTimestamp datetime.datetime DateType java.sql.Date datetime.date ArrayType scala.collection.Seq list, tuple, array 등 MapType scala.collection.Map dict StructType org.apache.spark.sql.Row list 또는 tuple StructField 해당 필드와 맞는 값의 타입 해당 필드와 맞는 값의 타입 Schema # 스키마는 DataFrame의 칼럼명과 데이터 타입을 정의한 것이다. 보통 외부 데이터 소스에서 구조화된 데이터를 읽어 들일 때 사용한다. 미리 스키마를 정의할 경우 두 가지 장점이 있다.\nSpark가 스키마를 추측하기 위해 파일을 읽어들이는 과정을 방지한다. 파일이 큰 경우, 비용과 시간을 절약할 수 있다. 데이터가 스키마와 맞지 않는 경우, 조기에 발견할 수 있다. 스키마 정의 # 스키마를 정의하는 방법은 두 가지가 있다.\n프로그래밍 스타일로 정의하는 것 Copy kotlin // 스칼라 예제 import org.apache.spark.sql.types._ val schema = StructType(Array( StructField(\u0026#34;author\u0026#34;, StringType, false), StructField(\u0026#34;title\u0026#34;, StringType, false), StructField(\u0026#34;pages\u0026#34;, IntegerType, false))) Copy python # 파이썬 예제 from pyspark.sql.types import * schema = StructType([ StructField(\u0026#34;author\u0026#34;, StringType(), False), StructField(\u0026#34;title\u0026#34;, StringType(), False), StructField(\u0026#34;pages\u0026#34;, IntegerType(), False)]) DDL(Data Definition Language)을 사용하는 것 Copy python schema = \u0026#34;author STRING, title, STRING, pages INT\u0026#34; 스키마 활용 (Python) # databricks/LearningSparkV2/chapter3 에서 스키마 활용 예제를 가져온다.\nCopy python # src/example_schema.py from pyspark.sql.types import * from pyspark.sql import SparkSession from pyspark.sql.functions import * # 프로그래밍 스타일로 스키마를 정의한다. schema = StructType([ StructField(\u0026#34;Id\u0026#34;, IntegerType(), False), StructField(\u0026#34;First\u0026#34;, StringType(), False), StructField(\u0026#34;Last\u0026#34;, StringType(), False), StructField(\u0026#34;Url\u0026#34;, StringType(), False), StructField(\u0026#34;Published\u0026#34;, StringType(), False), StructField(\u0026#34;Hits\u0026#34;, IntegerType(), False), StructField(\u0026#34;Campaigns\u0026#34;, ArrayType(StringType()), False)]) # DDL을 사용해서 스키마를 정의할 수도 있다. # schema = \u0026#34;\u0026#39;Id\u0026#39; INT, \u0026#39;First\u0026#39;, STRING, \u0026#39;Last\u0026#39; STRING, \u0026#39;Url\u0026#39; STRING, \u0026#34; \\ # \u0026#34;\u0026#39;Published\u0026#39; STRING, \u0026#39;Hits\u0026#39; INT, \u0026#39;Campaigns\u0026#39; ARRAY\u0026lt;STRING\u0026gt;\u0026#34; # 예제 데이터를 생성한다. data = [ [1, \u0026#34;Jules\u0026#34;, \u0026#34;Damji\u0026#34;, \u0026#34;https://tinyurl.1\u0026#34;, \u0026#34;1/4/2016\u0026#34;, 4535, [\u0026#34;twitter\u0026#34;, \u0026#34;LinkedIn\u0026#34;]], [2, \u0026#34;Brooke\u0026#34;,\u0026#34;Wenig\u0026#34;,\u0026#34;https://tinyurl.2\u0026#34;, \u0026#34;5/5/2018\u0026#34;, 8908, [\u0026#34;twitter\u0026#34;, \u0026#34;LinkedIn\u0026#34;]], [3, \u0026#34;Denny\u0026#34;, \u0026#34;Lee\u0026#34;, \u0026#34;https://tinyurl.3\u0026#34;,\u0026#34;6/7/2019\u0026#34;,7659, [\u0026#34;web\u0026#34;, \u0026#34;twitter\u0026#34;, \u0026#34;FB\u0026#34;, \u0026#34;LinkedIn\u0026#34;]], [4, \u0026#34;Tathagata\u0026#34;, \u0026#34;Das\u0026#34;,\u0026#34;https://tinyurl.4\u0026#34;, \u0026#34;5/12/2018\u0026#34;, 10568, [\u0026#34;twitter\u0026#34;, \u0026#34;FB\u0026#34;]], [5, \u0026#34;Matei\u0026#34;,\u0026#34;Zaharia\u0026#34;, \u0026#34;https://tinyurl.5\u0026#34;, \u0026#34;5/14/2014\u0026#34;, 40578, [\u0026#34;web\u0026#34;, \u0026#34;twitter\u0026#34;, \u0026#34;FB\u0026#34;, \u0026#34;LinkedIn\u0026#34;]], [6, \u0026#34;Reynold\u0026#34;, \u0026#34;Xin\u0026#34;, \u0026#34;https://tinyurl.6\u0026#34;, \u0026#34;3/2/2015\u0026#34;, 25568, [\u0026#34;twitter\u0026#34;, \u0026#34;LinkedIn\u0026#34;]]] if __name__ == \u0026#34;__main__\u0026#34;: spark = (SparkSession .builder .appName(\u0026#34;Example-3_6\u0026#34;) .getOrCreate()) # 위에서 정의한 스키마로 DataFrame을 생성하고 상위 행을 출력한다. blogs_df = spark.createDataFrame(data, schema) blogs_df.show() # DataFrame 처리에 사용된 스키마를 출력한다. print(blogs_df.printSchema()) spark.stop() 예제 데이터에 대해 프로그래밍 스타일과 DDL을 사용하는, 두 가지 스타일로 스키마를 정의할 수 있다. DataFrame 생성 시 스키마를 전달하고, printSchema() 를 실행하여 어떤 스키마가 적용되었는지 출력해 볼 수 있다.\nspark-submit 에 예제 파일을 전달하면 아래와 같은 결과를 확인할 수 있다.\nCopy bash (spark) % spark-submit src/example_schema.py +---+---------+-------+-----------------+---------+-----+--------------------+ | Id| First| Last| Url|Published| Hits| Campaigns| +---+---------+-------+-----------------+---------+-----+--------------------+ | 1| Jules| Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]| | 2| Brooke| Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]| | 3| Denny| Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...| | 4|Tathagata| Das|https://tinyurl.4|5/12/2018|10568| [twitter, FB]| | 5| Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...| | 6| Reynold| Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]| +---+---------+-------+-----------------+---------+-----+--------------------+ root |-- Id: integer (nullable = false) |-- First: string (nullable = false) |-- Last: string (nullable = false) |-- Url: string (nullable = false) |-- Published: string (nullable = false) |-- Hits: integer (nullable = false) |-- Campaigns: array (nullable = false) | |-- element: string (containsNull = true) DataFrame에 할당된 스키마를 다른 곳에서 사용하고 싶다면, blogs_df.schema 와 같이 호출하여 스키마 객체를 반환할 수 있다. 스키마 객체는 스키마를 정의할 때 사용했던 것과 동일한 pyspark.sql.types.StructType 타입이다.\nScala를 사용하는 경우에도 Python과 동일하게 정의한 스키마를 JSON 파일을 읽는데 적용한다면 아래와 같이 표현할 수 있다.\nCopy kotlin // 스칼라 예제 val blogsDF = spark.read.schema(schema).json(jsonFile) Column # 칼럼은 pandas의 DataFrame과 유사하게 어떤 특정한 타입의 필드를 나타내는 개념이다. RDBMS를 다루는 것처럼 관계형 표현이나 계산식 형태의 표현식으로 칼럼 단위의 값들에 연산을 수행할 수 있다.\n칼럼명에 대해 expr(\u0026quot;columnName * 5\u0026quot;) 같은 단순한 표현식으로 연산을 수행할 수 있다. 파이썬에서 expr() 은 pyspark.sql.functions 패키지에서 가져올 수 있다.\n표현식 활용 (Python) # 스키마 활용 예제에서 만든 blogs_df 객체를 사용한다.\nCopy python # src/example_schema.py from pyspark.sql.types import * from pyspark.sql import SparkSession from pyspark.sql.functions import * if __name__ == \u0026#34;__main__\u0026#34;: # SparkSession 및 blogs_df 객체 생성 # 표현식을 사용해 값을 계산하고 결과를 출력한다. 모두 동일한 결과를 보여준다. blogs_df.select(expr(\u0026#34;Hits\u0026#34;) * 2).show(2) blogs_df.select(col(\u0026#34;Hits\u0026#34;) * 2).show(2) blogs_df.select(expr(\u0026#34;Hits * 2\u0026#34;)).show(2) # 블로그 우수 방문자를 계산하고 결과를 출력한다. blogs_df.withColumn(\u0026#34;Big Hitters\u0026#34;, (expr(\u0026#34;Hits \u0026gt; 10000\u0026#34;))).show() spark-submit 에 예제 파일을 전달하면 아래와 같은 결과를 확인할 수 있다.\nCopy bash (spark) % spark-submit src/example_schema.py +----------+ |(Hits * 2)| +----------+ | 9070| | 17816| +----------+ only showing top 2 rows +---+---------+-------+-----------------+---------+-----+--------------------+-----------+ | Id| First| Last| Url|Published| Hits| Campaigns|Big Hitters| +---+---------+-------+-----------------+---------+-----+--------------------+-----------+ | 1| Jules| Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]| false| | 2| Brooke| Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]| false| | 3| Denny| Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...| false| | 4|Tathagata| Das|https://tinyurl.4|5/12/2018|10568| [twitter, FB]| true| | 5| Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...| true| | 6| Reynold| Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]| true| +---+---------+-------+-----------------+---------+-----+--------------------+-----------+ 첫 번째 표현식으로 계산한 결과는 모두 동일하여 하나만 출력했다. expr() 또는 col() 표현식으로 칼럼 연산을 수행할 수 있다.\nwithColumn() 을 호출하면 새로운 칼럼을 추가할 수 있다. 기존의 \u0026ldquo;Hits\u0026rdquo; 칼럼에 표현식을 사용해 블로그 우수 방문자를 분류하고, \u0026ldquo;Big Hitters\u0026rdquo; 라는 새로운 칼럼을 붙여서 출력했다.\nScala에서는 col() 대신에 칼럼명 앞에 $ 를 붙여서 Column 타입으로 변환할 수도 있다.\nCopy kotlin // \u0026#34;Id\u0026#34; 칼럼값에 따라 역순으로 정렬한다. blogsDF.sort(col(.desc).show() blogsDF.sort($\u0026#34;Id\u0026#34;.desc).show() Row # Spark에서 하나의 행은 하나 이상의 칼럼을 갖고 있는 Row 객체로 표현된다. Row 객체에 속하는 칼럼들은 동일한 타입일 수도 있고 다른 타입일 수도 있다. Row는 순서가 있는 필드 집합 객체이므로 0부터 시작하는 인덱스로 접근한다.\nCopy python # 파이썬 예제 from pyspark.sql import Row blog_row = Row(6, \u0026#34;Reynold\u0026#34;, \u0026#34;Xin\u0026#34;, \u0026#34;https://tinyurl.6\u0026#34;, 255568, \u0026#34;3/2/2015\u0026#34;, [\u0026#34;twitter\u0026#34;, \u0026#34;LinedIn\u0026#34;]) # 인덱스로 개별 값에 접근한다. blog_row[1] \u0026#39;Reynold\u0026#39; Row 객체들을 DataFrame으로 만들 수 있다.\nCopy python # 파이썬 예제 rows = [Row(\u0026#34;Matei Zaharia\u0026#34;, \u0026#34;CA\u0026#34;), Row(\u0026#34;Reynold Xin\u0026#34;, \u0026#34;CA\u0026#34;)] authors_df = spark.createDataFrame(rows, [\u0026#34;Authors\u0026#34;, \u0026#34;State\u0026#34;]) authors_df.show() DataFrame 작업 # 읽기/쓰기 # 데이터 소스에서 DataFrame으로 로드하기 위해 DataFrameReader 를 사용할 수 있다. JSON, CSV, Parquet, 텍스트, Avro, ORC 같은 다양한 포맷의 데이터 소스를 지원한다. 반대로 특정 포맷으로 DataFrame을 내보낼 때는 DataFrameWriter 를 사용할 수 있다.\nPython과 Scala에서 spark.read.csv() 함수로 CSV 파일을 읽을 수 있다.\nCopy python # 파이썬 예제 sf_fire_file = \u0026#34;data/sf-fire-calls.csv\u0026#34; fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema) Copy kotlin // 스칼라 예제 val sfFireFile = \u0026#34;data/sf-fire-calls.csv\u0026#34; val fireDF = spark.read.schema(fireSchema).option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;).csv(sfFireFile) DataFrame을 외부 데이터 소스에 내보내려면 DataFrame 객체가 가진 write() 메서드를 사용할 수 있다. 기본 포맷으로 인기있는 포맷은 칼럼 지향적인 Parquet 포맷이다. Parquet에는 스키마가 메타데이터에 들어있어 수동으로 스키마를 적용할 필요가 없다.\nCopy python # 파이썬 예제 fire_df.write.format(\u0026#34;parquet\u0026#34;).save(parquet_path) Copy kotlin // 스칼라 예제 fireDF.write.format(\u0026#34;parquet\u0026#34;).save(parquetPath) 프로젝션/필터 # 프로젝션은 필터를 이용해 특정 관계 상태와 매치되는 행들만 반환하는 방법이다. 프로젝션은 select(), 필터는 filter() 또는 where() 메서드로 표현된다.\nCopy python # 파이썬 예제 few_fire_df = (fire_df .select(\u0026#34;IncidentNumber\u0026#34;, \u0026#34;AvailableDtTm\u0026#34;, \u0026#34;CallType\u0026#34;) .where(col(\u0026#34;CallType\u0026#34;) != \u0026#34;Medical Incident\u0026#34;)) few_fire_df.show(5, truncate=False) 칼럼 변경 # 칼럼의 이름을 변경하거나 추가 또는 삭제하는 경우가 있다. 컬럼명을 변경할 때는 withColumnRenamed() 함수를 사용할 수 있다. 아래 예제는 \u0026ldquo;Delay\u0026rdquo; 칼럼의 명칭을 \u0026ldquo;ResponseDelayedinMins\u0026rdquo; 라고 변경한다.\nCopy python # 파이썬 예제 new_fire_df = fire_df.withColumnRenamed(\u0026#34;Delay\u0026#34;, \u0026#34;ResponseDelayedinMins\u0026#34;) (new_fire_df .select(\u0026#34;ResponseDelayedinMins\u0026#34;) .where(col(\u0026#34;ResponseDelayedinMins\u0026#34;) \u0026gt; 5) .show(5, False)) 기존 칼럼을 가공해 새로운 칼럼을 만들 때는 withColumn() 메서드를 사용할 수 있다. 이때, spark.sql.functions 패키지에 있는 to_timestamp() 또는 to_date() 같은 함수들을 같이 사용할 수 있다. 가공된 칼럼을 추가한 후 필요하지 않은 칼럼을 제거하려면 drop() 메서드를 사용할 수 있다.\nCopy python # 파이썬 예제 fire_ts_df = (new_fire_df .withColumn(\u0026#34;IncidentDate\u0026#34;, to_timestamp(col(\u0026#34;CallDate\u0026#34;), \u0026#34;MM/dd/yyyy\u0026#34;)).drop(\u0026#34;CallDate\u0026#34;) .withColumn(\u0026#34;OnWatchDate\u0026#34;, to_timestamp(col(\u0026#34;WatchDate\u0026#34;), \u0026#34;MM/dd/yyyy\u0026#34;)).drop(\u0026#34;WatchDate\u0026#34;) .withColumn(\u0026#34;AvailableDtTs\u0026#34;, to_timestamp(col(\u0026#34;AvailableDtTm\u0026#34;), \u0026#34;MM/dd/yyyy hh:mm:ss\u0026#34;)).drop(\u0026#34;AvailableDtTm\u0026#34;)) (fire_ts_df .select(\u0026#34;IncidentDate\u0026#34;, \u0026#34;OnWatchDate\u0026#34;, \u0026#34;AvailableDtTs\u0026#34;) .show(5, False)) 집계 연산 # groupBy(), orderBy(), count() 와 같은 Transformation 또는 Action을 사용하여 칼럼명을 가지고 집계할 수 있다. 아래 예제는 \u0026ldquo;CallType\u0026rdquo; 칼럼을 기준으로 행 개수를 세는 연산을 표현한다. 내림차순으로 정렬하여 가장 일반적인 신고 타입(CallType)을 확인할 수 있다.\nCopy python # 파이썬 예제 (fire_ts_df .select(\u0026#34;CallType\u0026#34;) .where(col(\u0026#34;CallType\u0026#34;).isNotNull()) .groupBy(\u0026#34;CallType\u0026#34;) .count() .orderBy(\u0026#34;count\u0026#34;, ascending=False) .show(n=10, truncate=False)) 집계 함수로는 min(), max(), sum(), avg() 등의 통계 함수들을 지원한다.\nCopy python # 파이썬 예제 import pyspark.sql.functions as F (fire_ts_df .select(F.sum(\u0026#34;NumAlarms\u0026#34;), F.avg(\u0026#34;ResponseDelayedinMins\u0026#34;), F.min(\u0026#34;ResponseDelayedinMins\u0026#34;), F.max(\u0026#34;ResponseDelayedinMins\u0026#34;)) .show()) Dataset API # Dataset는 정적 타입 API와 동적 타입 API의 두 가지 특성을 모두 가진다.\nDataset # DataFrame은 Dataset[Row] 로 표현할 수 있다. Row는 서로 다른 타입의 값을 저장할 수 있는 JVM 객체다. 반면에 Dataset는 엄격하게 타입이 정해진 JVM 객체의 집합으로, Java의 클래스와 유사하다.\nDataset는 Java와 Scala에서만 사용할 수 있고, Python과 R에서는 DataFrame만 사용할 수 있다. 이것은 Python과 R이 컴파일 시 타입의 안전을 보장하는 언어가 아니기 때문이다. 반대로 Java는 컴파일 시점에 타입 안정성을 제공하기 때문에 Dataset만 사용할 수 있다. Scala는 DataFrame을 Dataset[Row] 로 표현하며, Dataset[T] 도 같이 사용할 수 있다.\nCase Class # DataFrame에서 스키마로 데이터 타입을 정의한느 것처럼, Scala에서 Dataset를 만들 때 스키마를 지정하기 위해 케이스 클래스를 사용할 수 있다. Java에서는 JavaBean 클래스를 쓸 수 있다.\n예제로, IoT 디바이스에서 JSON 파일을 읽어 들일 때 케이스 클래스를 아래와 같이 정의한다.\nCopy kotlin // 스칼라 예제 case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) val ds = spark.read.json(\u0026#34;iot_devices.json\u0026#34;).as[DeviceIoTData] Dataset는 DataFrame과 같은 연산이 가능하다. 예제로, filter() 에 함수를 인자로 전달하는 질의는 아래와 같다.\nCopy kotlin // 스칼라 예제 val filterTempDS = ds.filter(d =\u0026gt; d.temp \u0026gt; 30 \u0026amp;\u0026amp; d.humidity \u0026gt; 70) DataFrame vs Dataset # DataFrame과 Dataset을 사용 중 오류가 발생하는 시점을 정리하면 아래 표와 같다. Dataset가 DataFrame과 다른점은 컴파일 시점에 엄격한 타입 체크를 한다는 것이다. 반대로, SQL과 유사한 질의를 쓰는 관계형 변환을 필요로 한다면 DataFrame을 사용한다.\nSQL DataFrame Dataset 문법 오류 실행 시점 컴파일 시점 컴파일 시점 분석 오류 실행 시점 실행 시점 컴파일 시점 Spark SQL # Spark SQL은 고수준 정형화 기능들이 구축되도록 하는 방대한 엔진으로 진화해 왔다. Spark SQL 엔진은 다음과 같은 일을 한다.\n스파크 컴포넌트들을 통합하고 DataFrame/Dataset 관련 작업을 단순화할 수 있도록 추상화를 한다. 정형화된 파일 포맷(JSON, CSV 등)을 읽고 쓰며 데이터를 임시 테이블로 변환한다. 빠른 데이터 탐색을 위한 대화형 Spark SQL 쉘을 제공한다. JDBC/ODBC 커넥터를 통해 외부의 도구들과 연결할 수 있는 중간 역할을 한다. JVM을 위한 최적화된 코드를 생성한다. Catalyst Optimizer # Spark SQL 엔진의 핵심은 Catalyst Optimizer다. Catalyst Optimizer는 두 가지 목적으로 설계되었다.\nSpark SQL에 최적화 기법을 쉽게 추가한다. 개발자가 최적화 프로그램을 확장할 수 있도록 한다. 예시로, 데이터 소스별 규칙을 추가하거나 새로운 데이터 유형을 지원하는 것 등이 있다. Catalyst Optimizer는 연산 쿼리를 받아 실행 계획으로 변환한다. 그 과정은 아래 그림과 같이 4단계의 과정을 거친다.\n분석\n제공된 코드가 유효하고 오류가 없는지 확인한다. 칼럼, 데이터 타입, 함수, 테이블, 데이터베이스 이름 목록을 갖고 있는 Catalog 객체를 참조한다. 분석 단계를 성공적으로 통과하면 Spark에서 이해하고 해결할 수 있는 요소만이 포함되어 있다는 의미를 가진다.\n논리적 최적화\n표준적인 규칙을 기반으로 최적화 접근 방식을 적용하여 효율성을 향상시킨다. 최적화를 위한 여러 계획들을 수립하는데, 예를 들면 조건절 하부 배치, 칼럼 걸러내기, 부울 표현식 단순화 등이 포함된다. 논리 계획은 물리 계획 수립의 입력 데이터가 된다.\n물리 계획 수립\n논리 계획을 바탕으로 대응되는 물리적 연산자를 사용해 최적화된 물리 계획을 생성한다. CPU, 메모리, I/O 활용을 포함한 컴퓨팅 리소스 비용을 기반으로 실행 전략을 평가한다. 리소스 가용성을 기반으로 가장 비용이 적게 드는 계획을 선택한다.\n코드 생성\n물리 계획을 Java 바이트 코드로 변환한다. 최신 컴파일러 기술을 활용해 최적화된 바이트 코드를 생성한다. Spark가 JIT(Just-In-Time) 컴파일러처럼 작동하여 런타임 성능을 최적화하고 실행 속도를 크게 향상시킨다.\nReferences # https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ https://itwiki.kr/w/아파치_스파크_DSL https://github.com/databricks/LearningSparkV2 https://www.databricks.com/spark/getting-started-with-apache-spark/datasets https://www.databricks.com/glossary/catalyst-optimizer https://blog.det.life/apache-spark-sql-engine-and-query-planning-37cafb2b98f6 "},{"id":11,"href":"/blog/spark-study-3/","title":"Apache Spark - 스파크 애플리케이션 및 RDD","section":"Posts","content":"Spark Application # Spark Application은 Driver Process 하나와 일련의 일련의 Executors로 구성된다.\nDriver Process # Driver Process는 main() 함수를 실행하고 클러스터 내 노드에서 세 가지 작업을 담당한다.\nSpark Application 관련 정보를 유지한다. 사용자의 프로그램이나 입력에 대응한다. Executor 작업을 분석, 배포, 예약한다. Executor # Executor는 Driver가 할당한 작업을 실제로 실행하는 역할을 하는데, 두 가지 작업을 담당한다.\nDriver가 할당한 Task를 실행한다. Task의 상태와 결과를 Driver 노드에 보고한다. Cluster Manager # 실물 시스템을 제어하고 Spark Application에 리소스를 할당하는 작업은 Cluster Manager가 맡는다. Spark Application의 실행 과정에서 Cluster Manager는 Application이 실행되는 물리적인 머신을 관리한다. Spark Application은 클러스터에서 독립적인 프로세스로 실행되며, SparkContext 객체에 의해 조정된다.\nSparkContext는 여러 유형의 Cluster Manager(Standalone, YARN, Kubernetes)에 연결될 수 있으며, Application 간에 리소스를 할당한다. Spark가 연결되어 클러스터의 노드에서 Executor가 확보되면, SparkContext에 전달된 Application 코드가 Executor에게 전달된다.\nJob # Spark Driver는 Spark Application을 하나 이상의 Spark Job으로 변환한다. 각 Job은 DAG로 변환되며, DAG 그래프에서 각각의 노드는 하나 이상의 Spark Stage에 해당한다.\nStage # 어떤 작업이 연속적으로 또는 병렬적으로 수행되는지에 맞춰 Stage에 해당하는 DAG 노드가 생성된다. Spark 연산은 하나의 Stage 안에서 실행되지 않고 여러 Stage로 나뉘어 실행된다.\nTask # 각 Stage는 최소 실행 단위이며 Executor들 위에서 실행되는 Spark Task들로 이루어진다. 각 Task는 개별적인 CPU 코어에 할당되어 개별적인 파티션을 갖고 작업하기 때문에, 철저하게 병렬 처리가 이루어진다.\nRDD(Resilient Distributed Data) # RDD는 탄력적인 분산 데이터셋이란 의미로, 분산 데이터를 병렬로 처리하고 장애가 발생할 경우 스스로 복구될 수 있는 내성을 가지고 있다. RDD는 Spark에서 정의한 분산 데이터 모델로, 여러 서버에 나누어 저장되어 각 서버에서 저장된 데이터를 동시에 병렬로 처리할 수 있다.\nRDD 특징 # RDD는 5가지 특징을 가지고 있다.\nDistributed Collection\n데이터는 클러스터에 흩어져 있지만 하나의 파일인 것처럼 사용이 가능한다. 즉, 여러 군데의 데이터를 하나의 객체로 사용할 수 있다.\nResilient \u0026amp; Immutable\n데이터는 탄력적이고 불변하는 성질이 있다. RDD의 변환 과정은 DAG로 그릴 수 있기 때문에 문제가 생길 경우 쉽게 이전의 RDD로 돌아갈 수 있다. 연산 중 문제가 생겨도 다시 복원해서 연산하면 되기 때문에 탄력적인 성질을 가진다고 볼 수 있다. 또한, 여러 노드 중 하나의 노드에서 장애가 발생해도 복원이 가능하기 때문에 불변하다는 성질을 가진다고도 볼 수 있다.\nType-Safe\nRDD는 컴파일 시 타입을 판별할 수 있다. Integer RDD, String RDD, Double RDD 등으로 미리 판단할 수 있기 때문에 문제를 일찍 발견할 수 있다.\nStructured \u0026amp; Unstructured Data\n정형 데이터인 테이블, RDB, DataFrame과 비정형 데이터인 텍스트, 로그, 자연어 등을 모두 담을 수 있다.\nLazy Evaluation\n분산 데이터의 Spark 연산은 Transformation과 Action으로 구분된다. Action을 할 때까지 Transformation을 실행하지 않는다. Action을 하게 되면 Transformation을 실행하는 게으른 연산 방식을 가진다.\nTransformation # Transformation은 불변성의 특징을 가진 원본 데이터를 수정하지 않고 하나의 Spark DataFrame을 새로운 DataFrame으로 변형(Transform)한다. select() 나 filter() 같은 연산은 원본 DataFrame을 수정하지 않는다.\nTransformation은 즉시 계산되지 않고 Lineage라 불리는 형태로 기록된다. 기록된 Lineage는 더 효율적으로 연산할 수 있도록 Transformation들끼리 재배열하거나 합치도록 최적화된다. Lazy Evaluation은 Action이 실행되는 시점이나 데이터에 실제 접근하는 시점까지 실제 실행을 미루는 전략이다.\nLazy Evaluation이 일련의 Transformation들을 최적화한다면, Lineage는 데이터 불변성 및 장애에 대한 내구성을 제공한다. Lineage에는 Transformation들이 기록되어 있고 실행 전까지 DataFrame이 변하지 않기 때문에, 단순히 기록된 Lineage를 재실행하는 것만으로 원래 상태를 다시 만들어낼 수 있다.\nTransformation은 Narrow Transformation과 Wide Transformation으로 구분된다.\nNarrow Transformation # Narrow Transformation은 하나의 입력 파티션을 연산하여 하나의 출력 파티션을 내놓는 경우다. 입력 파티션에 대한 연산이 독립적으로 이루어지며, 연산의 결과인 출력 파티션은 입력 파티션의 데이터에만 의존한다. 즉, 다른 파티션의 데이터를 참조할 필요가 없다는 것을 의미한다. filter() 와 contains() 등의 연산이 여기에 해당된다.\nNarrow Transformation은 실행 비용이 상대적으로 낮고, 성능이 좋아 빠른 처리가 가능하다.\nWide Transformation # Wide Transformation은 입력 데이터의 여러 파티션 간에 데이터가 재분배되어야 하는 경우다. 다른 파티션으로부터 데이터를 읽어들여서 Shuffle(데이터 재분배)하는 과정이 필요하며, groupBy() 나 orderBy() 등의 연산이 여기에 해당된다.\nWide Transformation은 네트워크를 통한 대량의 데이터 이동을 발생시켜 실행 시간이 오래 걸리고 리소스 사용량이 많다.\nAction # Action은 RDD로 결과 값을 계산하고, 연산 결과를 반환하거나 외부 스토리지(HDFS 등)에 저장한다. count() 나 show() 함수는 연산 결과를 반환하거나 출력하고, saveAsTextFile() 과 같은 함수로 연산 결과를 스토리지에 저장할 수 있다.\nAction을 호출할 때마다 RDD가 처음부터 계산되는데, 반복적인 연산에 의한 비효율성을 피하기 위해 cache() 와 persist() 를 사용해 데이터를 메모리에 보관할 수 있다.\nWeb UI # 스파크는 클러스터 상태와 리소스 사용을 모니터링하기 위해 Web UI를 제공한다. 기본적으로 4040 포트를 사용하는데 다음과 같은 내용을 볼 수 있다.\n스케줄러의 Stage와 Task 목록 RDD 크기와 메모리 사용의 요약 환경 정보 실행 중인 Executor 정보 모든 스파크 SQL 쿼리 아래는 AWS 문서에서 제공하는 화면이다. Web UI를 통해 Job, Stage, Task들이 어떻게 구성되는지 DAG 형태로 시각화해서 볼 수 있다. Stage 안에서 각각의 Task는 파란 박스로 표시되는데, 아래 예시에서 Stage 2는 2개의 Task로 구성되어 있음을 알 수 있다. Task가 여러 개라면 모두 병렬로 실행된다.\nspark-submit # databricks/LearningSparkV2/chapter2 에서 각 주별로 학생들이 어떤 색깔의 M\u0026amp;M을 좋아하는지 알려주는 스파크 프로그램을 작성한 예제 mnmcount.py 를 가져온다. 동일한 위치의 data/ 경로에서 M\u0026amp;M 데이터셋 mnm_dataset.csv 을 확인할 수 있다.\nM\u0026amp;M 개수 집계 (Python) # Copy python # src/mnmcount.py from pyspark.sql import SparkSession import sys if __name__ == \u0026#34;__main__\u0026#34;: if len(sys.argv) != 2: print(\u0026#34;Usage: mnmcount \u0026lt;file\u0026gt;\u0026#34;, file=sys.stderr) sys.exit(-1) # SparkSession 객체를 만든다. spark = (SparkSession .builder .appName(\u0026#34;PythonMnMCount\u0026#34;) .getOrCreate()) # 인자에서 M\u0026amp;M 데이터가 들어있는 파일 이름을 얻는다. mnm_file = sys.argv[1] # 데이터가 CSV 형식이며 헤더가 있음을 알리고 스키마를 추론하도록 한다. mnm_df = (spark.read.format(\u0026#34;csv\u0026#34;) .option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;) .option(\u0026#34;inferSchema\u0026#34;, \u0026#34;true\u0026#34;) .load(mnm_file)) mnm_df.show(n=5, truncate=False) # State, Color, Count 필드를 선택하고 State, Color를 기준으로 Count를 sum 집계한다. # select, groupBy, sum, orderBy 메서드를 연결하여 연속적으로 호출한다. count_mnm_df = (mnm_df.select(\u0026#34;State\u0026#34;, \u0026#34;Color\u0026#34;, \u0026#34;Count\u0026#34;) .groupBy(\u0026#34;State\u0026#34;, \u0026#34;Color\u0026#34;) .sum(\u0026#34;Count\u0026#34;) .orderBy(\u0026#34;sum(Count)\u0026#34;, ascending=False)) # 상위 60개 결과를 보여주고, 모든 행 개수를 count 집계해 출력한다. count_mnm_df.show(n=60, truncate=False) print(\u0026#34;Total Rows = %d\u0026#34; % (count_mnm_df.count())) # 위 집계 과정에서 중간에 where 메서드를 추가해 캘리포니아(CA) 주에 대해서만 집계한다. ca_count_mnm_df = (mnm_df.select(\u0026#34;*\u0026#34;) .where(mnm_df.State == \u0026#39;CA\u0026#39;) .groupBy(\u0026#34;State\u0026#34;, \u0026#34;Color\u0026#34;) .sum(\u0026#34;Count\u0026#34;) .orderBy(\u0026#34;sum(Count)\u0026#34;, ascending=False)) # 상위 10개 결과를 보여준다. ca_count_mnm_df.show(n=10, truncate=False) # SparkSession을 멈춘다. spark.stop() Application 실행 # spark-submit 스크립트에 파이썬 코드를 첫 번째 인자로, CSV 파일을 두 번째 인자로 전달한다.\n실행 과정에서 불필요한 INFO 로그를 무시하고 싶다면, $SPARK_HOME/conf/ 경로에서 log4j2.properties.template 파일의 이름을 log4j2.properties 로 변경하고 파일 내용에서 rootLogger.level = info 부분의 값을 warn 으로 변경하면 된다.\nCopy bash (spark) % $SPARK_HOME/bin/spark-submit src/mnmcount.py data/mnm_dataset.csv +-----+------+-----+ |State|Color |Count| +-----+------+-----+ |TX |Red |20 | |NV |Blue |66 | |CO |Blue |79 | |OR |Blue |71 | |WA |Yellow|93 | +-----+------+-----+ only showing top 5 rows +-----+------+----------+ |State|Color |sum(Count)| +-----+------+----------+ |CA |Yellow|100956 | |WA |Green |96486 | |CA |Brown |95762 | |TX |Green |95753 | |TX |Red |95404 | |CO |Yellow|95038 | |NM |Red |94699 | |OR |Orange|94514 | |WY |Green |94339 | |NV |Orange|93929 | |TX |Yellow|93819 | |CO |Green |93724 | |CO |Brown |93692 | |CA |Green |93505 | |NM |Brown |93447 | |CO |Blue |93412 | |WA |Red |93332 | |WA |Brown |93082 | |WA |Yellow|92920 | |NM |Yellow|92747 | |NV |Brown |92478 | |TX |Orange|92315 | |AZ |Brown |92287 | |AZ |Green |91882 | |WY |Red |91768 | |AZ |Orange|91684 | |CA |Red |91527 | |WA |Orange|91521 | |NV |Yellow|91390 | |UT |Orange|91341 | |NV |Green |91331 | |NM |Orange|91251 | |NM |Green |91160 | |WY |Blue |91002 | |UT |Red |90995 | |CO |Orange|90971 | |AZ |Yellow|90946 | |TX |Brown |90736 | |OR |Blue |90526 | |CA |Orange|90311 | |OR |Red |90286 | |NM |Blue |90150 | |AZ |Red |90042 | |NV |Blue |90003 | |UT |Blue |89977 | |AZ |Blue |89971 | |WA |Blue |89886 | |OR |Green |89578 | |CO |Red |89465 | |NV |Red |89346 | |UT |Yellow|89264 | |OR |Brown |89136 | |CA |Blue |89123 | |UT |Brown |88973 | |TX |Blue |88466 | |UT |Green |88392 | |OR |Yellow|88129 | |WY |Orange|87956 | |WY |Yellow|87800 | |WY |Brown |86110 | +-----+------+----------+ Total Rows = 60 +-----+------+----------+ |State|Color |sum(Count)| +-----+------+----------+ |CA |Yellow|100956 | |CA |Brown |95762 | |CA |Green |93505 | |CA |Red |91527 | |CA |Orange|90311 | |CA |Blue |89123 | +-----+------+----------+ 처음에는 mnm_dataset.csv 의 상위 5개 행을 보여주고, 이어서 각 주별, 색깔별 합계를 출력한다. 그리고, 캘리포니아(CA)에 대한 결과만 별도로 출력한다.\nReferences # https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ https://www.databricks.com/en/glossary/what-are-spark-applications https://spark.apache.org/docs/latest/cluster-overview.html https://velog.io/@dbgpwl34/Spark-스파크-애플리케이션의-아키텍처-스파크-애플리케이션의-생애-주기 https://spark.apache.org/docs/latest/rdd-programming-guide.html https://6mini.github.io/data%20engineering/2021/12/12/rdd/ https://mengu.tistory.com/27 https://sunrise-min.tistory.com/entry/Apache-Spark-RDD https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-ui.html https://github.com/databricks/LearningSparkV2 "},{"id":12,"href":"/blog/spark-study-2/","title":"Apache Spark - 설치하고 PySpark 실행하기","section":"Posts","content":"Spark Installation # Apple Silicon 환경에서 스파크 설치를 진행합니다.\n각 섹션의 이미지를 클릭하면 설치 페이지 또는 관련 문서로 이동합니다.\nSpark 설치 # 아파치 스파크 다운로드 페이지로 가서 최신 버전 4.0.0 및 \u0026ldquo;Pre-built for Apache Hadoop\u0026rdquo; 옵션을 선택하면 해당 버전의 다운로드 링크 spark-4.0.0-bin-hadoop3.tgz 가 나타난다. 해당 링크로 이동하면 아래와 같이 Hadoop 관련 바이너리 파일이 포함된 압축 파일의 설치 경로를 확인할 수 있다.\n브라우저 또는 curl, wget 등 명령어를 통해 압축 파일을 내려받을 수 있다. Copy bash wget https://dlcdn.apache.org/spark/spark-4.0.0/spark-4.0.0-bin-hadoop3.tgz 압축 해제 프로그램을 사용하거나, 터미널에서 아래 명령어를 입력하여 압축 해제한다. Copy bash tar zxvf spark-4.0.0-bin-hadoop3.tgz Spark 경로에 접근하기 위해 환경변수를 설정한다. Copy bash vi ~/.zshrc vi 편집기로 .zshrc 파일에 Spark 경로를 등록한다. SPARK_HOME 은 압축 해제한 Spark 경로를 입력한다. Copy bash export SPARK_HOME=/Users/{username}/spark-4.0.0 export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin 변경 사항을 적용하기 위해 터미널을 재시작하거나 아래 명령어를 실행한다. Copy text source ~/.zshrc 주의할 점은 Spark를 실행하기 전에 Java와 Hadoop이 설치되어 있어야 한다. 보통은 Java 또는 Hadoop 버전에 맞춰서 Spark를 설치하지만, 어떤 것도 설치되어 있지 않기 때문에 스파크 버전에 맞춰서 Java와 Hadoop을 설치한다.\nHadoop은 다운로드할 때 지정한 것과 같은 3.4 버전을 설치하고, Java는 Spark 4.0.0에서 요구하는 최소 버전인 OpenJDK 17 버전을 설치한다. 이미 설치되어 있다면 Spark 실행 섹션으로 넘어간다.\nSpark runs on Java 17/21, Scala 2.13, Python 3.9+, and R 3.5+ (Deprecated)\nJava 설치 # Homebrew가 설치되었다는 전제 하에 OpenJDK 17 버전을 설치한다. Copy bash brew install openjdk@17 설치가 완료되면, 시스템에서 JDK를 찾을 수 있도록 심볼릭 링크로 연결한다. Copy bash sudo ln -sfn /opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk-17.jdk 환경변수에 OpenJDK 11의 bin 디렉터리를 추가한다. vi 편집기 등으로 직접 수정할 수도 있다. Copy bash echo \u0026#39;export PATH=/opt/homebrew/opt/openjdk@17/bin:$PATH\u0026#39; \u0026gt;\u0026gt; ~/.zshrc 변경 사항을 적용하기 위해 터미널을 재시작하거나 아래 명령어를 실행한다. Copy bash source ~/.zshrc OpenJDK 11 버전이 정상적으로 설치되었는지 확인하기 위해 아래 명령어를 입력한다. Copy bash % java -version openjdk version \u0026#34;17.0.15\u0026#34; 2025-04-15 OpenJDK Runtime Environment Homebrew (build 17.0.15+0) OpenJDK 64-Bit Server VM Homebrew (build 17.0.15+0, mixed mode, sharing) Hadoop 설치 # Homebrew로 설치할 수도 있지만, Hadoop 3.4.0 버전을 맞추기 위해 압축 파일을 직접 내려받는다. 다운로드 버튼을 클릭하거나, curl, wget 등 명령어로 내려받을 수 있다. (ARM 아키텍처에서 설치할 때는 파일명에 aarch64 가 포함되어야 한다) Copy bash wget https://archive.apache.org/dist/hadoop/common/hadoop-3.4.0/hadoop-3.4.0-aarch64.tar.gz 압축 해제 프로그램을 사용하거나, 터미널에서 아래 명령어를 입력하여 압축 해제한다. Copy bash tar zxvf hadoop-3.4.0-aarch64.tar.gz -C ~/ Hadoop 명령어에 접근하기 위해 환경변수를 설정한다. Copy bash vi ~/.zshrc vi 편집기로 .zshrc 파일에 Hadoop 경로를 등록한다. HADOOP_HOME 은 압축 해제한 Hadoop 경로를 입력한다. Copy bash export HADOOP_HOME=/Users/{username}/hadoop-3.4.0 export PATH=$PATH:$HADOOP_HOME/bin 변경 사항을 적용하기 위해 터미널을 재시작하거나 아래 명령어를 실행한다. Copy text source ~/.zshrc Hadoop 환경 설정 파일을 수정한다. 파일들은 $HADOOP_HOME/etc/hadoop/ 경로에 있다. hadoop-env.sh\nCopy bash export JAVA_HOME=/Library/Java/JavaVirtualMachines/openjdk-17.jdk/Contents/Home core-site.xml\nCopy xml \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.defaultFS\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://localhost:9000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; hdfs-site.xml\nCopy xml \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;1\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; mapred-site.xml\nCopy xml \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.framework.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;yarn\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; yarn-site.xml\nCopy xml \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; HDFS은 자체적으로 SSH를 사용한다. SSH 키를 생성하고 본인 계정에 인증한다. Copy bash ssh-keygen -t rsa cat ~/.ssh/id_rsa.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys ssh localhost HDFS 실행 # 네임노드를 포맷하고 HDFS을 구성하는 모든 데몬을 실행한다.\nCopy bash hdfs namenode -format $HADOOP_HOME/sbin/start-dfs.sh 먼저, 네임노드를 포맷하면 아래와 같은 로그가 발생한다. 호스트명으로 localhost 대신 다른 명칭을 사용하는데, 이로 인해 오류가 발생했다.\nCopy bash % sudo hdfs namenode -format Password: 2025-06-28 18:33:03,038 INFO namenode.NameNode: STARTUP_MSG: /************************************************************ STARTUP_MSG: Starting NameNode STARTUP_MSG: host = minyeamer/127.0.0.1 STARTUP_MSG: args = [-format] STARTUP_MSG: version = 3.4.0 STARTUP_MSG: classpath = /Users... 정상적으로 HDFS이 실행된다면 아래와 같은 메시지를 조회할 수 있다.\nCopy bash % $HADOOP_HOME/sbin/start-dfs.sh Starting namenodes on [minyeamer] Starting datanodes minyeamer: datanode is running as process 21771. Stop it first and ensure /tmp/hadoop-cuz-datanode.pid file is empty before retry. Starting secondary namenodes [minyeamer] minyeamer: secondarynamenode is running as process 21906. Stop it first and ensure /tmp/hadoop-cuz-secondarynamenode.pid file is empty before retry. 2025-06-28 18:51:34,493 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable jps 명령어를 입력하면 실행중인 노드를 확인할 수 있다.\nCopy bash % jps 21906 SecondaryNameNode 23016 Jps 22216 NameNode 21771 DataNode HDFS을 종료하고 싶다면 start-dfs.sh 와 동일한 경로에서 stop-all.sh 스크립트를 실행하면 된다.\nCopy bash % $HADOOP_HOME/sbin/stop-all.sh WARNING: Stopping all Apache Hadoop daemons as cuz in 10 seconds. WARNING: Use CTRL-C to abort. Stopping namenodes on [minyeamer] Stopping datanodes Stopping secondary namenodes [minyeamer] Stopping nodemanagers Stopping resourcemanager HDFS 실행 중 오류 처리 # localhost 가 아닌 minyeamer 라는 호스트명을 사용하는데, start-dfs.sh 실행 시 아래와 같은 에러 메시지가 발생했다. localhost 명칭을 사용한다면 해당 과정은 무시하고 Spark 실행 섹션으로 넘어간다.\nCopy bash minyeamer: ssh: Could not resolve hostname minyeamer: nodename nor servname provided, or not known 첫 번째 에러는 SSH 연결 시 호스트명을 인식할 수 없다는 문제로, /etc/hosts 에 minyeamer 호스트명과 127.0.0.1 IP 주소가 매칭되지 않아서 발생한 문제다. 아래와 같이 추가할 수 있다.\nCopy text 127.0.0.1 localhost minyeamer 255.255.255.255 broadcasthost ::1 localhost minyeamer 해당 호스트명으로 SSH 접속을 시도하면 정상적으로 접속할 수 있다.\nCopy bash ssh minyeamer 또한, Hadoop 설정 파일도 일부 수정해주어야 한다. $HADOOP_HOME/etc/hadoop/ 경로의 workers 파일에는 localhost 한줄만 적혀있을 건데, minyeamer 호스트명으로 변경한다. 또한, 동일한 경로의 core-site.xml 파일의 hdfs://localhost:9000 부분도 변경해야 한다.\n그리고 나서 다시 start-dfs.sh 를 실행했는데 다른 에러가 발생했다.\nCopy bash minyeamer: ERROR: Cannot set priority of namenode process 19072 이것만으로는 오류를 파악하기 어려워서 $HADOOP_HOME/logs/ 경로 아래 hadoop-*-namenode-*.log 형식의 로그 파일을 확인했다.\n로그 파일에서 에러가 발생한 부분에 아래와 같은 에러 메시지를 확인할 수 있었다.\nCopy bash 2025-06-28 18:44:47,867 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode. java.net.BindException: Problem binding to [minyeamer:9000] java.net.BindException: Address already in use; For more details see: http://wiki.apache.org/hadoop/BindException 9000번 포트가 사용되고 있다는 건데, 확인해보니 localhost:cslistener 라는 이름의 프로세스가 실행 중에 있었다.\nCopy bash % lsof -i :9000 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME python3.1 1943 ... 아마 localhost 호스트명으로 HDFS을 실행시켰을 때 프로세스가 중지되지 않고 남아있는 것 같아 강제로 중지했다.\nCopy bash kill -9 1943 다시 네임노드를 포맷하고 HDFS을 실행하니 앞에서 보았던 정상적인 메시지를 확인할 수 있었다.\nCopy bash hdfs namenode -format $HADOOP_HOME/sbin/start-dfs.sh Spark 실행 # spark-shell 을 실행하면 정상적으로 동작하는 것을 확인할 수 있다.\nCopy bash (main) cuz@minyeamer ~ % spark-shell WARNING: Using incubator modules: jdk.incubator.vector Using Spark\u0026#39;s default log4j profile: org/apache/spark/log4j2-defaults.properties 25/06/28 19:45:07 WARN Utils: Your hostname, minyeamer, resolves to a loopback address: 127.0.0.1; using 192.168.x.x instead (on interface en0) 25/06/28 19:45:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address Using Spark\u0026#39;s default log4j profile: org/apache/spark/log4j2-defaults.properties Setting default log level to \u0026#34;WARN\u0026#34;. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ \u0026#39;_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 4.0.0 /_/ Using Scala version 2.13.16 (OpenJDK 64-Bit Server VM, Java 17.0.15) Type in expressions to have them evaluated. Type :help for more information. Spark context Web UI available at http://localhost:4040 Spark context available as \u0026#39;sc\u0026#39; (master = local[*], app id = local-1751107510852). Spark session available as \u0026#39;spark\u0026#39;. scala\u0026gt; 메시지에서 알려주는대로 http://localhost:4040 경로에 접근하니까 아래와 같은 웹 UI 화면을 조회할 수 있었다.\nSpark 디렉터리 구조 # Spark 경로 아래에는 다음과 같은 디렉터리 또는 파일이 존재한다.\nCopy bash % ls ~/spark-4.0.0 bin/\tconf/\tdata/\texamples/\thive-jackson/\tjars/\tkubernetes/\tlicenses/ python/\tR/\tsbin/\tyarn/\tLICENSE\tNOTICE\tREADME.md\tRELEASE READMD.md 스파크 셸을 어떻게 사용하는지에 대한 안내 및 스파크 문서의 링크와 설정 가이드 등이 기록되어 있다.\nbin/ spark-shell 을 포함한 대부분의 스크립트가 위치한다.\nsbin/ 다양한 배포 모드에서 클러스터의 스파크 컴포넌트들을 시작하고 중지하기 위한 관리 목적이다.\nkubernetes/ 쿠버네티스 클러스터에서 쓰는 스파크를 위해, 도커 이미지 제작을 위한 Dockerfile들을 담고 있다.\ndata/ MLlib, 정형화 프로그래밍, GraphX 등에서 입력으로 사용되는 .txt 파일이 있다.\nexamples/ Java, Python, R, Scala에 대한 예제들을 제공한다.\nPySpark Installation # PyPi 설치 # Copy bash pip install pyspark 명령어를 입력해 PyPi 저장소에서 PySpark 라이브러리를 설치할 수 있다. 다운로드한 파일과 동일하게 25년 5월 23일 릴리즈된 4.0.0 버전을 설치한다. 다른 버전을 설치하고 싶다면 pip install pyspark=3.0.0 과 같이 입력할 수 있다.\nSQL, ML, MLlib 등 추가적인 라이브러리를 같이 설치하려면 pip install pyspark[sql,ml,mllib] 와 같이 입력할 수 있다.\n별도의 가상환경에서 PySpark를 설치하는 것을 추천한다. 4.0.0 버전 기준으로 Python 3.9 버전부터 지원한다. 개인적으로는 Python 3.10 버전을 사용한다.\nCopy bash (spark) % pip install pyspark Collecting pyspark Downloading pyspark-4.0.0.tar.gz (434.1 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 434.1/434.1 MB 3.9 MB/s eta 0:00:00 ... Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9.9 pyspark-4.0.0 pyspark 실행 # spark-shell 은 Scala 쉘을 실행한다. Scala가 아닌 Python을 사용하고 싶다면 pyspark 쉘을 실행할 수 있다.\nCopy bash (spark) % pyspark Python 3.10.18 | packaged by conda-forge | (main, Jun 4 2025, 14:46:00) [Clang 18.1.8 ] on darwin Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. WARNING: Using incubator modules: jdk.incubator.vector Using Spark\u0026#39;s default log4j profile: org/apache/spark/log4j2-defaults.properties 25/06/28 21:35:50 WARN Utils: Your hostname, minyeamer, resolves to a loopback address: 127.0.0.1; using 192.168.x.x instead (on interface en0) 25/06/28 21:35:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address Using Spark\u0026#39;s default log4j profile: org/apache/spark/log4j2-defaults.properties Setting default log level to \u0026#34;WARN\u0026#34;. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 25/06/28 21:35:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ \u0026#39;_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 4.0.0 /_/ Using Python version 3.10.18 (main, Jun 4 2025 14:46:00) Spark context Web UI available at http://localhost:4040 Spark context available as \u0026#39;sc\u0026#39; (master = local[*], app id = local-1751114150819). SparkSession available as \u0026#39;spark\u0026#39;. \u0026gt;\u0026gt;\u0026gt; Python 쉘에서는 대화형으로 Python API를 사용할 수 있다. 현재 경로에 있는 README.md 파일을 첫 번째 10줄만 읽어보았다.\nCopy python \u0026gt;\u0026gt;\u0026gt; strings = spark.read.text(\u0026#34;README.md\u0026#34;) \u0026gt;\u0026gt;\u0026gt; strings.show(10, truncate=False) +--------------------------------------------------------------------------------------------------+ |value | +--------------------------------------------------------------------------------------------------+ |# Apache Spark | | | |Spark is a unified analytics engine for large-scale data processing. It provides | |high-level APIs in Scala, Java, Python, and R (Deprecated), and an optimized engine that | |supports general computation graphs for data analysis. It also supports a | |rich set of higher-level tools including Spark SQL for SQL and DataFrames, | |pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing,| |and Structured Streaming for stream processing. | | | |- Official version: \u0026lt;https://spark.apache.org/\u0026gt; | +--------------------------------------------------------------------------------------------------+ only showing top 10 rows \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; strings.count() 125 쉘을 나가고 싶다면 Ctrl+D 를 눌러 나갈 수도 있고, quit() 함수를 실행해 종료할 수도 있다.\nCopy python \u0026gt;\u0026gt;\u0026gt; quit() "},{"id":13,"href":"/blog/spark-study-1/","title":"Apache Spark - 스파크의 기본 개념과 아키텍처","section":"Posts","content":"Study Overview # 러닝 스파크 2nd 개정판 과정을 따릅니다.\n목적 # 대용량 데이터 처리를 위한 아파치 스파크를 이론적으로 학습 책에서 대상으로 하는 스파크 3.x 버전과 25년 5월 출시된 Spark 4.0 버전을 비교 각 챕터에서 배운 것으로 실습할만한 것이 있다면 추가로 시도하기 실습은 PySpark API를 사용하며, 최신화된 PySpark 4.0.0 문서를 참조 챕터 # 아파치 스파크 소개: 통합 분석 엔진 아파치 스파크 다운로드 및 시작 아파치 스파크의 정형화 API 스파크 SQL과 데이터 프레임: 내장 데이터 소스 소개 스파크 SQL과 데이터 프레임: 외부 데이터 소스와 소통하기 스파크 SQL과 데이터세트 스파크 애플리케이션의 최적화 및 튜닝 정형화 스트리밍 아파치 스파크를 통한 안정적인 데이터 레이크 구축 MLlib을 사용한 머신러닝 아파치 스파크로 머신러닝 파이프라인 관리, 배포 및 확장 에필로그: 아파치 스파크 3.0 Spark Overview # 스파크의 시작 # RDBMS 같은 전통적인 저장 시스템으로는 구글이 방대한 규모의 인터넷 문서를 다룰 수 없어 구글 파일 시스템(GFS), 맵리듀스(MapReduce), 빅테이블(BigTable) 등을 만들어 냈다. GFS는 클러스터 환경에서 분산 파일시스템을 제공하고, 빅테이블은 GFS를 기반으로 대규모 데이터 저장 수단을 제공한다. 맵리듀스는 함수형 프로그래밍 기반으로 대규모 데이터 분산 처리를 구현했다. 클러스터의 워커 노드들이 분산된 데이터에 연산을 하고(Map), 그 결과를 하나로 합쳐(Reduce) 최종 결과를 생성해낸다. 이러한 접근 방식은 네트워크 트래픽을 크게 감소시키면서 로컬 디스크에 대한 I/O를 극대화한다.\nGFS는 하둡 파일 시스템과 맵리듀스 구현에 영향을 주었다. HDFS의 맵리듀스에는 몇 가지 단점이 있었다. 첫째, 운영이 복잡해 관리가 쉽지 않았다. 둘째, 배치 처리를 위한 맵리듀스 API의 기본 설정 코드가 너무 많이 필요했다. 셋째, 맵리듀스 태스크가 필요해질 때마다 중간 과정의 데이터를 로컬 디스크에 써야 했다. 반복적인 I/O 작업에 의해 거대한 맵리듀스 작업에 며칠이 걸리기도 했다.\nUC 버클리 연구원들은 동적이고 반복적인 작업에서 비효율적인 맵리듀스를 개선하여 단순하고 빠르고 쉬운 스파크를 만들기로 했다. 구체적으로는 더 높은 장애 내구성을 갖고, 병렬성을 높이면서, 맵리듀스 연산을 위한 중간 결과를 메모리에 저장하고, 간편한 API를 다양한 언어로 제공하고자 했다.\n아파치 스파크란? # 아파치 스파크는 데이터 센터나 클라우드에서 대규모 분산 데이터 처리를 위한 통합형 엔진이다. 중간 연산을 메모리에 저장하고 머신러닝, SQL, 스트리밍 처리, 그래프 처리 등을 간편하게 API로 지원한다.\n스파크의 설계 철학에는 속도, 사용 편리성, 모듈성, 확장성이 있다.\n속도 스파크는 하드웨어 산업의 발전으로 메모리 성능 향상에 많은 이득을 얻었는데, 모든 중간 결과를 메모리에 저장해 I/O 작업을 제한하고 속도를 향상시켰다. 또한, 질의 연산을 DAG로 구성해 효율적인 연산 그래프를 만들고 병렬 수행을 지원한다.\n사용 편리성 데이터프레임이나 데이터세트 같이 고수준으로 추상화된 자료 구조를 사용해 단순성을 실현시켰다. 다양한 언어로 연산을 지원하여 사용자들이 편한 언어로 빅데이터를 처리할 수 있다.\n모듈성 문서화가 잘된 API를 제공하며, 스파크 SQL이나 정형화 스트리밍 등의 핵심 컴포넌트를 하나의 엔진 안에서 연동된 상태로 사용할 수 있다.\n확장성 스파크는 저장보다는 빠른 병렬 연산 엔진에 초점을 맞춰, 수많은 데이터 소스에서 데이터를 읽어 들여 메모리에서 처리하는 것이 가능하다. 서드파티 패키지 목록에는 다양한 외부 데이터 소스가 포함되어 있다.\n아파치 컴포넌트 # 다양한 워크로드를 위해 스파크 SQL, 스파크 MLlib, 스파크 정형화 스트리밍, GraphX를 제공한다. 자바, R, 스칼라, SQL, 파이썬 중 어느 것으로 스파크 코드를 작성해도 바이트 코드로 변환되어 워커 노드의 JVM에서 실행된다.\n스파크 SQL RDBMS 테이블이나 CSV와 같은 구조화된 데이터 파일 포맷에서 데이터를 읽어 들여 영구적이거나 임시적인 테이블을 생성한다. SQL 계통의 질의를 써서 데이터를 데이터프레임으로 읽어 들일 수 있다.\n스파크 MLlib 범용 머신러닝 알고리즘들이 들어 있다. 특성을 추출 및 가공하고 학습/검증 파이프라인을 구축하는 기능을 지원하며, 경사 하강법 최적화를 포함한 저수준 ML 기능을 포함한다.\n스파크 정형화 스트리밍 실시간으로 연결하고 반응하기 위한 데이터 모델은 스트림을 연속적으로 증가하는 테이블이자, 끝에 새로운 레코드가 추가되는 형태이다. 단순히 정형화 테이블로 보고 쿼리를 날리면 된다. 정형화 스트리밍 모델의 하부에는 스파크 SQL 엔진이 장애 복구와 지연 데이터의 모든 측면을 관리한다.\nGraphX 그래프를 조작하고 그래프 병렬 연산을 수행하기 위한 라이브러리다. 분석, 연결 탐색 등 표준적인 그래프 알고리즘과 커뮤니티 사용자들이 기여한 알고리즘을 포함한다.\nSpark Architecture # Spark Driver # SparkSession 객체를 초기화하는 책임을 가진 Spark Application의 일부이다. Spark Driver는 여러 가지 역할을 한다.\nCluster Manager와 통신하며 Spark Executor들을 위해 필요한 자원을 요청한다. 모든 스파크 작업을 DAG 연산 형태로 변환해 스케줄링한다. 각 실행 단위를 태스크로 나누어 Spark Executor들에게 분배한다. 자원이 한번 할당되면 그 다음부터는 Driver가 Executor와 직접 통신한다. SparkSession # 스파크 2.0에서 모든 스파크 연산과 데이터에 대한 통합 연결 채널이 만들어졌다.\nSparkContext, SQLContext, HiveContext, SparkConf, StreamingContext 등이 합쳐졌다. 일원화된 연결 채널을 통해 JVM 실행 파라미터들을 만들고 데이터프레임이나 데이터세트를 정의한다. 데이터 소스에서 데이터를 읽고 메타데이터에 접근해 스파크 SQL 질의를 실행할 수 있다. SparkSession은 모든 스파크 기능을 한 군데에서 접근할 수 있는 시작점을 제공한다.\npyspark.sql.SparkSession 문서를 참조해 SparkSession 생성\nCopy python spark = ( SparkSession.builder .master(\u0026#34;local\u0026#34;) # 원격 접속의 경우 .remote(\u0026#34;sc://localhost\u0026#34;) .appName(\u0026#34;LearnSpark\u0026#34;) .config(\u0026#34;spark.sql.shuffle.partitions\u0026#34;, 6) .getOrCreate() ) Cluster Manager # Spark Application이 실행되는 클러스터에서 자원을 관리 및 할당하는 책임을 가진다. Standalone, Hadoop YARN, Apache Mesos, Kubernetes 네 종류의 Cluster Manager를 지원한다.\nSpark Executor # 클러스터의 각 워커 노드에서 동작하며, Driver와 통신하며 Task를 실행하는 역할을 한다. 대부분의 배포 모드에서 노드당 하나의 Executor만 실행한다.\n배포 모드 # 스파크가 여러 환경에서 돌아갈 수 있도록 다양한 배포 모드를 지원한다. 추상화되어 있어 Cluster Manager는 실행 환경에 대한 정보가 필요없고, YARN이나 Kubernetes 같은 인기 있는 환경에 배포가 가능하다.\nMode Spark Driver Spark Executor Cluster Manager Local 단일 서버 같은 머신에서 단일 JVM 위에서 실행 Driver와 동일한 JVM 위에서 동작 동일한 호스트에서 실행 Standalone Cluster의 아무 노드에서나 실행 Cluster의 각 노드가 자체적인 Executor를 실행 Cluster의 아무 호스트에나 할당 YARN(Client) Cluster 외부의 Client에서 동작 YARM의 노드 매니저의 컨테이너 YARN의 리소스 매니저가 노드 매니저에 컨테이너 할당 YARN(Cluster) YARN 애플리케이션 마스터에서 동작 YARN(Client)와 동일 YARN(Client)와 동일 Kubernetes Kubernetes Pod에서 동작 각 워커가 자신의 Pod 내에서 실행 Kubernetes 마스터 분산 데이터 # 물리적인 데이터는 HDFS나 클라우드 저장소에 존재한다. 데이터는 파티션으로 물리적인 수준에서 분산되고, 스파크는 파티션을 추상화하여 메모리의 데이터프레임 객체를 바라본다.\n파티셔닝은 효과적인 병렬 처리를 가능하게 해준다. 데이터를 조각내 청크나 파티션 단위로 분산해 Spark Executor가 네트워크 사용을 최소화하고 가까이 있는 데이터만 처리한다.\n스파크 활용사례 # 데이터 사이언스 # 데이터 사이언티스트들은 데이터를 정제하고 패턴을 발견하기 위해 데이터를 살펴본다. 대부분은 SQL에 능하고, NumPy나 pandas 같은 라이브러리를 편하게 사용한다. 모델 구축을 위해 분류, 회귀, 클러스터링 알고리즘을 어떻게 사용할지도 알아야 한다.\n스파크는 MLlib은 모델 파이프라인을 구축할 수 있는 일반적인 머신러닝 알고리즘들을 제공한다. 또한, 스파크 SQL로 일회성 데이터 탐색을 가능하게 해준다.\n데이터 엔지니어링 # 클러스터링 모델은 독립적으로 존재하지 않고 아파치 카프카 같은 스트리밍 엔진과 연계해 동작한다. 데이터 파이프라인은 다양한 소스에서 오는 원본 데이터를 최종 단계로까지 변형해주며, 그런 데이터는 NoSQL이나 RDBMS 등에 저장된다.\n스파크의 정형화 스트리밍 API를 써서 실시간 또는 정적인 데이터 소스에 대한 ETL 파이프라인을 구축할 수 있게 해준다. 또한, 스파크가 연산을 쉽게 병렬화 해주어 고수준 언어에만 집중해 ETL을 수행할 수 있게 지원한다.\n스파크 사용 사례 # 클러스터 전체에 걸쳐 분산된 대규모 데이터세트의 병렬 처리 데이터 탐색이나 시각화를 위한 일회성이나 대화형 질의 수행 MLlib을 이용해 머신러닝 모델을 구축, 훈련, 평가 "},{"id":14,"href":"/blog/airflow-study-7/","title":"Apache Airflow - Connection, Hook","section":"Posts","content":"Docker Compose 이해 # 목적 : 1개 이상의 도커 컨테이너 생성 시 컨테이너들의 설정을 관리할 수 있도록 해주는 기능 방법 : docker-compose.yaml 파일에 컨테이너들의 설정을 입력 사용 : .yaml 파일이 있는 위치에서 docker compose up 명령어를 입력하여 실행 yaml 파일은 들여쓰기 문법을 사용하며 Airflow의 Docker Compose는 아래와 같이 구분 Copy yaml x-airflow-common: # 각 서비스에 공통 적용될 항목들 services: # 컨테이너로 실행할 서비스를 정의 volumns: # 컨테이너에 연결할 볼륨을 정의 networks: # 컨테이너에 연결할 네트워크를 정의 x-airflow-common\n공통으로 사용할 항목을 \u0026amp; 를 붙여서 지정 \u0026amp; 아래의 모든 영역은 공통 항목으로 묶여 한번에 가져올 수 있음 Copy yaml x-airflow-common: \u0026amp;airflow-common image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:3.0.1} environment: \u0026amp;airflow-common-env AIRFLOW__CORE__EXECUTOR: CeleryExecutor services\n컨테이너로 올릴 서비스 목록을 지정 공통 항목을 \u0026lt;\u0026lt;: *airflow-common 과 같은 형식으로 불러오기 환경 변수가 있을 경우 environment 아래에 입력 ports 는 호스트에서 컨테이너에 접속하기 위해 맵핑할 포트를 명시 expose 는 ports 와 다르게 내부 컨테이너 간에 연결할 때 사용할 포트를 명시 depends_on 은 컨테이너의 실행 순서를 정의 airflow-apiserver 는 airflow-init 에 대한 종속적 관계 Copy yaml services: airflow-apiserver: \u0026lt;\u0026lt;: *airflow-common command: api-server ports: - \u0026#34;8080:8080\u0026#34; healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;--fail\u0026#34;, \u0026#34;http://localhost:8080/api/v2/version\u0026#34;] interval: 30s timeout: 10s retries: 5 start_period: 30s restart: always depends_on: \u0026lt;\u0026lt;: *airflow-common-depends-on airflow-init: condition: service_completed_successfully volumes\n컨테이너 내 데이터를 유지하기 위해 외부와 연결하기 위한 볼륨 정보 볼륨 리스트를 보려면 docker volume ls 명령어 사용 Copy yaml volumes: postgres-db-volume: 서비스를 정의할 때 생성한 볼륨과 컨테이너의 내부 경로를 연결 가능 Copy yaml services: postgres: volumes: - postgres-db-volume:/var/lib/postgresql/data networks\n컨테이너를 격리된 네트워크로 그룹화하기 위한 네트워크 정보 컨테이너는 유동 IP를 가지게 되어 재가동할 때마다 IP 주소가 변경될 수 있는데, 고정 IP를 할당하기 위해 networks 활용 가능 네트워크 리스트를 보려면 docker network ls 명령어 사용 Copy yaml networks: network_custom: driver: bridge ipam: driver: default config: - subnet: 172.28.0.0/16 gateway: 172.28.0.1 Airflow에서 기본적으로 사용하는 airflow_default 네트워크 대역이 172.18.0.0/16 서브넷 범위인데, 아래에서 새 컨테이너를 만들기 위해 겹치지 않는 대역의 network_custom 을 정의 Copy bash % docker network inspect {network_id} [ { \u0026#34;Name\u0026#34;: \u0026#34;airflow_default\u0026#34;, ... \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: null, \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;172.18.0.0/16\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;172.18.0.1\u0026#34; } ] }, ... } ] Postgres 컨테이너 생성 # Docker Compose # Airflow에서 기본으로 사용하는 Postgres 컨테이너 외에 새로운 Postgres 컨테이너 하나를 추가로 생성 DB 사용자에 대한 정보는 환경 변수 environment 로 입력 5432 포트로 접속할 수 있게 포트 맵핑 적용 기존의 postgres 컨테이너도 5431 포트로 접속할 수 있게 마찬가지로 포트 맵핑 적용 postgres-custom-db-volume 볼륨을 추가하고 컨테이너 내부 경로와 연결 위에서 정의한 network_custom 네트워크에 연결하면서 IP 주소는 172.28.0.3 으로 고정 다른 컨테이너에도 network_custom 및 겹치지 않는 고정 IP 주소를 할당 Copy yaml services: postgres_custom: image: postgres:13 environment: POSTGRES_USER: minyeamer POSTGRES_PASSWORD: minyeamer POSGRES_DB: minyeamer TZ: Asia/Seoul volumes: - postgres-custom-db-volume:/var/lib/postgresql/data ports: - 5432:5432 networks: network_custom: ipv4_address: 172.28.0.3 postgres: image: postgres:13 ... ports: - 5431:5432 networks: network_custom: ipv4_address: 172.28.0.4 ... volumes: postgres-db-volume: postgres-custom-db-volume: networks: network_custom: ... 실행 후 컨테이너 목록을 조회하면 airflow-postgres_custom-1 명칭의 컨테이너가 같이 올라온 것을 확인 Copy bash % docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 72541e89bff4 apache/airflow:3.0.1 \u0026#34;/usr/bin/dumb-init …\u0026#34; 57 seconds ago Up 20 seconds (healthy) 8080/tcp airflow-airflow-worker-1 1735bc475bff apache/airflow:3.0.1 \u0026#34;/usr/bin/dumb-init …\u0026#34; 57 seconds ago Up 36 seconds (healthy) 0.0.0.0:8080-\u0026gt;8080/tcp airflow-airflow-apiserver-1 d2b680f273e3 apache/airflow:3.0.1 \u0026#34;/usr/bin/dumb-init …\u0026#34; 57 seconds ago Up 36 seconds (healthy) 8080/tcp airflow-airflow-scheduler-1 f23c6d30d22c apache/airflow:3.0.1 \u0026#34;/usr/bin/dumb-init …\u0026#34; 57 seconds ago Up 36 seconds (healthy) 8080/tcp airflow-airflow-dag-processor-1 09992f09ac5e apache/airflow:3.0.1 \u0026#34;/usr/bin/dumb-init …\u0026#34; 57 seconds ago Up 36 seconds (healthy) 8080/tcp airflow-airflow-triggerer-1 5c1b13f33229 postgres:13 \u0026#34;docker-entrypoint.s…\u0026#34; 58 seconds ago Up 56 seconds 0.0.0.0:5432-\u0026gt;5432/tcp airflow-postgres_custom-1 e23d4eb919fc postgres:13 \u0026#34;docker-entrypoint.s…\u0026#34; 58 seconds ago Up 56 seconds (healthy) 0.0.0.0:5431-\u0026gt;5432/tcp airflow-postgres-1 86fc82a16953 redis:7.2-bookworm \u0026#34;docker-entrypoint.s…\u0026#34; 58 seconds ago Up 56 seconds (healthy) 6379/tcp airflow-redis-1 DBeaver 접속 # Beaver Community 버전을 다운로드 및 설치한 후 실행 좌측 상단의 New Database Connection 을 클릭해 PostgreSQL 연결을 생성 Port는 Docker Compose에서 맵핑한 5432 사용 Database, Username, Password 또한 Docker Compose에서 지정한 값을 사용 정상적으로 연결되었다면 아래와 같이 Database 명칭을 확인 가능 Postgres 테이블 생성 # PythonOperator 를 사용해 새로 생성한 PostgreSQL 컨테이너에 임의의 값을 INSERT 하는 작업을 구현하기 전에, 아래와 같이 public.dag_run 테이블을 생성 Copy sql CREATE TABLE public.dag_run ( dag_id varchar(100) NULL, task_id varchar(100) NULL, run_id varchar(100) NULL, msg text NULL ); 테이블이 정상적으로 만들어졌다면 새로고침 후 아래와 같이 테이블 내 컬럼 내역을 확인 가능 PythonOperator # PostgreSQL에 연결해 현재 실행 정보를 INSERT INTO 로 추가하는 함수 insert_into_postgres() 를 실행 DB 연결 시 conn 세션 객체 생성 후 .close() 로 종료하는 구문을 closing 으로 대체 DB 세션에서 쿼리를 실행하는 cursor 객체를 만들고 해당 객체를 통해 SQL문을 수행 함수에 인수로 전달하는 DB 연결 정보는 마찬가지로 Docker Compose에서 지정한 값을 사용 Copy python # dags/python_with_postgres.py from airflow.sdk import DAG from airflow.providers.standard.operators.python import PythonOperator import pendulum with DAG( dag_id=\u0026#34;python_with_postgres\u0026#34;, schedule=None, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;hook\u0026#34;], ) as dag: def insert_into_postgres(ip: str, port: str, dbname: str, user: str, passwd: str, **kwargs): import psycopg2 from contextlib import closing with closing(psycopg2.connect(host=ip, dbname=dbname, user=user, password=passwd, port=int(port))) as conn: with closing(conn.cursor()) as cursor: dag_id = kwargs.get(\u0026#34;ti\u0026#34;).dag_id task_id = kwargs.get(\u0026#34;ti\u0026#34;).task_id run_id = kwargs.get(\u0026#34;ti\u0026#34;).run_id msg = \u0026#34;INSERT INTO 수행\u0026#34; sql = \u0026#34;INSERT INTO dag_run VALUES (%s,%s,%s,%s);\u0026#34; cursor.execute(sql,(dag_id,task_id,run_id,msg)) conn.commit() postgres_task = PythonOperator( task_id=\u0026#34;postgres_task\u0026#34;, python_callable=insert_into_postgres, op_args=[\u0026#34;172.28.0.3\u0026#34;, \u0026#34;5432\u0026#34;, \u0026#34;minyeamer\u0026#34;, \u0026#34;minyeamer\u0026#34;, \u0026#34;minyeamer\u0026#34;] ) DAG을 실행한 후 DBeaver에서 dag_run 테이블 조회 시 아래와 같이 하나의 행이 올라온 것을 확인 문제점 및 해결방법 # 문제점 : DB 접속정보가 노출되고 접속정보가 변경되면 대응하기가 어려움 해결방법 Variable 이용 Hook 이용 Connection \u0026amp; Hook # Connection : Airflow에서 외부와 연동하기 위해 설정하는 기본 정보 Hook Airflow에서 외부 솔루션의 기능을 사용할 수 있도록 미리 구현된 메서드를 가진 클래스 Connection 정보를 통해 생성되는 객체로, 접속정보가 코드상 노출되지 않음 외부의 특정 솔루션을 다룰 수 있는 메서드가 구현되어 있음 Operator와 같이 Task를 만들어내지는 못하기 때문에 Operator 내 함수에서 사용 Postgres Provider 문서 보기 # apache-airflow-providers-postgres \u0026amp;mdash; apache-airflow-providers-postgres …airflow.apache.org Providers에 속한 Postgres 문서에서 Connection 접속 과정을 조회 가능 Copy python def get_conn(self) -\u0026gt; connection: \u0026#34;\u0026#34;\u0026#34;Establish a connection to a postgres database.\u0026#34;\u0026#34;\u0026#34; conn = deepcopy(self.connection) # check for authentication via AWS IAM if conn.extra_dejson.get(\u0026#34;iam\u0026#34;, False): conn.login, conn.password, conn.port = self.get_iam_token(conn) conn_args = { \u0026#34;host\u0026#34;: conn.host, \u0026#34;user\u0026#34;: conn.login, \u0026#34;password\u0026#34;: conn.password, \u0026#34;dbname\u0026#34;: self.database or conn.schema, \u0026#34;port\u0026#34;: conn.port, } raw_cursor = conn.extra_dejson.get(\u0026#34;cursor\u0026#34;, False) if raw_cursor: conn_args[\u0026#34;cursor_factory\u0026#34;] = self._get_cursor(raw_cursor) if self.options: conn_args[\u0026#34;options\u0026#34;] = self.options for arg_name, arg_val in conn.extra_dejson.items(): if arg_name not in self.ignored_extra_options: conn_args[arg_name] = arg_val self.conn = psycopg2.connect(**conn_args) return self.conn Connection으로부터 host, login, password, database, port 정보를 읽어서 DB 연결에 대한 파라미터로 활용하는 것을 확인 마지막 줄에는 psycopg2.connect() 연결에 대한 psycopg2.extensions.connection 세션 객체를 반환 Connection 추가 # postgres 타입의 Connection을 새로 생성 연결 정보로 Docker Compose에서 지정한 값을 입력 Connection에 입력한 각 항목은 앞서 확인한 get_conn() 메서드에서 PostgreSQL 연결 시 사용 PostgresHook # Provider 패키지를 설치하고, 기존에 psycopg2 라이브러리로 직접 PostgreSQL에 연결하던 부분을 PostgresHook 으로 변경 Copy bash pip install apache-airflow-providers-postgres Copy python # dags/python_with_postgres_hook.py from airflow.sdk import DAG from airflow.providers.standard.operators.python import PythonOperator import pendulum with DAG( dag_id=\u0026#34;python_with_postgres_hook\u0026#34;, schedule=None, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;hook\u0026#34;], ) as dag: def insert_into_postgres(postgres_conn_id: str, **kwargs): from airflow.providers.postgres.hooks.postgres import PostgresHook from contextlib import closing postgres_hook = PostgresHook(postgres_conn_id) with closing(postgres_hook.get_conn()) as conn: with closing(conn.cursor()) as cursor: dag_id = kwargs.get(\u0026#34;ti\u0026#34;).dag_id task_id = kwargs.get(\u0026#34;ti\u0026#34;).task_id run_id = kwargs.get(\u0026#34;ti\u0026#34;).run_id msg = \u0026#34;INSERT INTO 수행\u0026#34; sql = \u0026#34;INSERT INTO dag_run VALUES (%s,%s,%s,%s);\u0026#34; cursor.execute(sql,(dag_id,task_id,run_id,msg)) conn.commit() postgres_task = PythonOperator( task_id=\u0026#34;postgres_task\u0026#34;, python_callable=insert_into_postgres, op_kwargs={\u0026#34;postgres_conn_id\u0026#34;:\u0026#34;conn-db-postgres-custom\u0026#34;} ) DAG을 실행한 후 DBeaver에서 dag_run 테이블 조회 시 아래와 같이 두 번째 행이 추가된 것을 확인 bulk_load # Postgres Provider 문서 보기 # airflow.providers.postgres.hooks.postgres \u0026amp;mdash; …airflow.apache.org PostgreSQL에 데이터를 업로드하는 bulk_load() 메서드에 대해 살펴보기 Copy python def bulk_load(self, table: str, tmp_file: str) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Load a tab-delimited file into a database table.\u0026#34;\u0026#34;\u0026#34; self.copy_expert(f\u0026#34;COPY {table} FROM STDIN\u0026#34;, tmp_file) bulk_load() 메서드는 내부적으로 copy_expert() 메서드를 사용하는데, 해당 소스코드를 조회 Copy python def copy_expert(self, sql: str, filename: str) -\u0026gt; None: self.log.info(\u0026#34;Running copy expert: %s, filename: %s\u0026#34;, sql, filename) if not os.path.isfile(filename): with open(filename, \u0026#34;w\u0026#34;): pass with open(filename, \u0026#34;r+\u0026#34;) as file, closing(self.get_conn()) as conn, closing(conn.cursor()) as cur: cur.copy_expert(sql, file) file.truncate(file.tell()) conn.commit() copy_expert() 메서드는 get_conn() 메서드를 통해 세션 객체를 얻어오고, 세션 객체로부터 cursor 객체를 만들어서 해당 객체가 가지고 있는 copy_expert() 메서드를 수행\nPsycopg 공식 문서에 따르면, copy_expert() 메서드는 file 객체를 전달받아서 COPY TO 문법에 대응되는 SQL문을 실행해 테이블에 업로드 수행\nThe sql statement should be in the form COPY table TO STDOUT to export table to the file object passed as argument or COPY table FROM STDIN to import the content of the file object into table.\nCopy python copy_expert(sql, file, size=8192) Copy python \u0026gt;\u0026gt;\u0026gt; cur.copy_expert(\u0026#34;COPY test TO STDOUT WITH CSV HEADER\u0026#34;, sys.stdout) id,num,data 1,100,abc\u0026#39;def 2,,dada ... Postgres 테이블 생성 # 저번에 만든 Custom Operator로 가져온 네이버 쇼핑 검색 결과 shop.csv 파일을 PostgreSQL에 업로드할 계획을 가지고 테이블 구조를 정의 CSV 파일과 동일한 열을 TEXT 타입으로 가지는 nshopping.search 테이블을 생성 Copy sql CREATE TABLE nshopping_search( rank text, title text, link text, image text, lprice text, hprice text, mallName text, productId text, productType text, brand text, maker text, category1 text, category2 text, category3 text, category4 text ); DBeaver에서 nshopping 스키마를 만들고 SQL문을 수행하면 아래와 같이 테이블 열 목록을 확인 가능 CSV 파일 가공 # Load a tab-delimited file into a database table.\n앞서 bulk_load() 메서드의 주석에 Tab 으로 구분된 파일을 업로드한다고 명시되어 있기 때문에, shop.csv 파일을 , 대신 Tab 으로 구분한 shop_with_tab.csv 파일을 같은 위치에 생성 vi 편집기로 파일을 열었다면 %s/,/\\t/g 명령어를 입력해 , 를 Tab 으로 변경 가능 ranktitlelinkimagelpricehpricemallNameproductIdproductTypebrandmakercategory1category2category3category4 1삼성 갤럭시북 인강용 사무용 업무용 가성비 윈도우11 저가 싼 태블릿 \u0026lt;b\u0026gt;노트북\u0026lt;/b\u0026gt; 추천 기본팩https://smartstore.naver.com/main/products/10407884292https://shopping-phinf.pstatic.net/main_8795238/87952389253.11.jpg428000삼성공식파트너 코인비엠에스879523892532갤럭시북삼성전자디지털/가전노트북2LG전자 울트라PC 라이젠5 사무용 인강용 저렴한 8GB NVMe256GB LG\u0026lt;b\u0026gt;노트북\u0026lt;/b\u0026gt;https://smartstore.naver.com/main/products/6174236911https://shopping-phinf.pstatic.net/main_8371873/83718736488.14.jpg599000제이 씨앤에스837187364882LG전자LG전자디지털/가전노트북3LG그램 \u0026lt;b\u0026gt;노트북\u0026lt;/b\u0026gt; 14그램 Ultra5 16GB 256GB 인텔Arc GPU 인강용https://smartstore.naver.com/main/products/9091504708https://shopping-phinf.pstatic.net/main_8663600/86636005031.7.jpg1149000온라인총판대리점866360050312LG그램LG전자디지털/가전노트북4삼성전자 갤럭시북4 NT750XGR-A51A 16GB 256GBhttps://search.shopping.naver.com/catalog/52631236642https://shopping-phinf.pstatic.net/main_5263123/52631236642.20250124094900.jpg799000네이버526312366421갤럭시북4삼성전자디지털/가전노트북52025 LG그램 15 Ai 라이젠5 16GB 256GB AMD 포토샵 대학생 인강용 \u0026lt;b\u0026gt;노트북\u0026lt;/b\u0026gt;https://smartstore.naver.com/main/products/10133562287https://shopping-phinf.pstatic.net/main_8767806/87678065467.4.jpg1239000창이로운876780654672LG그램LG전자디지털/가전노트북 bulk_load 활용 예시 # PostgresHook 을 사용한 Operator에 이어서 bulk_load() 메서드를 추가 DAG 실행 날짜에 대응되는 디렉토리 아래의 shop_with_tab.csv 파일을 읽어서 nshopping.search 테이블에 업로드될 것을 기대 Copy python from airflow.sdk import DAG from airflow.providers.standard.operators.python import PythonOperator import pendulum with DAG( dag_id=\u0026#34;python_with_postgres_load\u0026#34;, schedule=None, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;hook\u0026#34;], ) as dag: def bulk_load_postgres(postgres_conn_id: str, table_name: str, file_path: str, **kwargs): from airflow.providers.postgres.hooks.postgres import PostgresHook postgres_hook = PostgresHook(postgres_conn_id) postgres_hook.bulk_load(table_name, file_path) postgres_task = PythonOperator( task_id=\u0026#34;postgres_task\u0026#34;, python_callable=bulk_load_postgres, op_kwargs={\u0026#34;postgres_conn_id\u0026#34;:\u0026#34;conn-db-postgres-custom\u0026#34;, \u0026#34;table_name\u0026#34;:\u0026#34;nshopping.search\u0026#34;, \u0026#34;file_name\u0026#34;:\u0026#34;/opt/airflow/files/naverSearch/{{data_interval_end.in_timezone(\\\u0026#34;Asia/Seoul\\\u0026#34;) | ds_nodash }}/shop_with_tab.csv\u0026#34;} ) DAG을 실행하고 DBeaver에서 nshopping.search 테이블을 조회하면 아래와 같이 CSV 파일이 그대로 올라온 것을 확인 하지만, CSV 헤더가 1행으로 들어가는 문제점이 보임 문제점 및 개선방안 # 문제점 : 구분자가 Tab 으로 고정되어 있고, 헤더까지 포함해서 업로드 됨 개선방안 : Custom Hook을 만들어서 구분자를 입력받고 헤더 포함 여부를 선택받게 함 Custom Hook은 다음 게시글에서 구현 Custom Hook # airflow.hooks.base \u0026amp;mdash; Airflow 3.1.3 Documentationairflow.apache.org BaseHook # BaseHook 클래스를 상속받아 직접 만든 Hook을 사용 가능 get_connection()\nConnection 객체를 가져오는 메서드 classmethod 로 설정되어 있어 객체화하지 않고도 호출 가능 Copy python @classmethod def get_connection(cls, conn_id: str) -\u0026gt; Connection: \u0026#34;\u0026#34;\u0026#34; Get connection, given connection id. :param conn_id: connection id :return: connection \u0026#34;\u0026#34;\u0026#34; from airflow.models.connection import Connection conn = Connection.get_connection_from_secrets(conn_id) log.info(\u0026#34;Connection Retrieved \u0026#39;%s\u0026#39;\u0026#34;, conn.conn_id) return conn get_hook()\nConnection 객체로부터 Hook 객체를 반환하는 메서드 classmethod 로 설정되어 있어 객체화하지 않고도 호출 가능 Copy python @classmethod def get_hook(cls, conn_id: str, hook_params: dict | None = None) -\u0026gt; BaseHook: \u0026#34;\u0026#34;\u0026#34; Return default hook for this connection id. :param conn_id: connection id :param hook_params: hook parameters :return: default hook for this connection \u0026#34;\u0026#34;\u0026#34; connection = cls.get_connection(conn_id) return connection.get_hook(hook_params=hook_params) get_conn()\nHook 객체에 대한 연결을 구현하는 메서드로, 반드시 재정의해야 함 Copy python def get_conn(self) -\u0026gt; Any: \u0026#34;\u0026#34;\u0026#34;Return connection for the hook.\u0026#34;\u0026#34;\u0026#34; raise NotImplementedError() PostgresHook 개선점 파악 # 이전에 만들었던 네이버 쇼핑 검색 결과에 대한 CSV 파일을 적재하는 PostgresHook의 기능을 개선\n구분자가 Tab 으로 고정되어 있고, 헤더까지 포함해서 업로드 됨 Psycopg 공식 문서에 따르면, CSV HEADER 구문을 뒤에 붙여 헤더를 제외할 수 있음 또한, COPY 문 뒤에 DELIMITER 구문을 추가해 구분자를 지정할 수도 있음 테이블이 없으면 에러가 발생함, 또한 직접 테이블을 생성하는 것도 불편함 CSV 파일의 헤더를 읽어서 CREATE TABLE 문을 만들고, 위 COPY 문 앞에 붙여서 테이블 생성 또한, 더 정확한 테이블 구성을 위해 모든 열을 text 타입으로 지정하지 않고, CSV 파일의 전체 또는 일부를 읽어서 int4 타입의 열을 추측할 수 있을 것이라 판단 기존엔 데이터를 계속해서 추가했는데, 덮어쓰기 옵션을 통해 이전 데이터를 지울 수도 있으면 더 좋을 것이라 생각함 Hook 기능 정의 # CSV 파일을 PostgreSQL 테이블에 적재하는데, 테이블이 없으면 생성하고, 덮어쓰기도 허용\nCustomPostgresHook # BaseHook 을 상속받는 CustomPostgresHook 를 구현 기본적인 구성은 Postgres Provider 문서에서 제공하는 소스코드를 참고 Hook 재사용을 위해 plugins/ 경로 아래에 추가 Copy python # plugins/hooks/postgres.py from airflow.hooks.base import BaseHook from typing import Literal import psycopg2 class CustomPostgresHook(BaseHook): def __init__(self, postgres_conn_id: str, **kwargs): self.postgres_conn_id = postgres_conn_id self.conn = None self.database = kwargs.get(\u0026#34;database\u0026#34;) def get_conn(self) -\u0026gt; psycopg2.extensions.connection: conn = BaseHook.get_connection(self.postgres_conn_id) conn_args = { \u0026#34;host\u0026#34;: conn.host, \u0026#34;user\u0026#34;: conn.login, \u0026#34;password\u0026#34;: conn.password, \u0026#34;dbname\u0026#34;: self.database or conn.schema, \u0026#34;port\u0026#34;: conn.port, } self.conn = psycopg2.connect(**conn_args) return self.conn def bulk_load(self, table: str, filename: str, encoding=\u0026#34;utf-8\u0026#34;, if_exists: Literal[\u0026#34;append\u0026#34;,\u0026#34;replace\u0026#34;]=\u0026#34;append\u0026#34;, sep=\u0026#39;,\u0026#39;, with_header=True): create = self._create_table_sql(table, filename, encoding, sep, with_header) replace = \u0026#34;TRUNCATE TABLE {};\u0026#34;.format(table) if if_exists == \u0026#34;replace\u0026#34; else str() copy = \u0026#34;COPY {} FROM STDIN DELIMITER \u0026#39;{}\u0026#39; {};\u0026#34;.format(table, sep, (\u0026#34;CSV HEADER\u0026#34; if with_header else \u0026#34;CSV\u0026#34;)) sql = \u0026#39;\u0026#39;.join([create, replace, copy]) self.copy_expert(sql, filename, encoding) def _create_table_sql(self, table: str, filename: str, encoding=\u0026#34;utf-8\u0026#34;, sep=\u0026#39;,\u0026#39;, with_header=True) -\u0026gt; str: if with_header: column_list = self._read_csv_column_list(filename, encoding, sep) return \u0026#34;CREATE TABLE IF NOT EXISTS {}({});\u0026#34;.format(table, column_list) else: return str() def _read_csv_column_list(self, filename: str, encoding=\u0026#34;utf-8\u0026#34;, sep=\u0026#39;,\u0026#39;) -\u0026gt; str: import csv def is_int4_type(value: str) -\u0026gt; bool: return (not value) or (value.isdigit() and (-2147483648 \u0026lt;= int(value) \u0026lt;= 2147483647)) with open(filename, \u0026#34;r+\u0026#34;, encoding=encoding) as file: reader = csv.reader(file, delimiter=sep) header = next(reader) dtypes = [all(map(is_int4_type, values)) for values in zip(*[next(reader) for _ in range(5)])] return \u0026#34;, \u0026#34;.join([\u0026#34;{} {}\u0026#34;.format(col, (\u0026#34;int4\u0026#34; if is_int4 else \u0026#34;text\u0026#34;)) for col, is_int4 in zip(header, dtypes)]) def copy_expert(self, sql: str, filename: str, encoding=\u0026#34;utf-8\u0026#34;) -\u0026gt; None: from contextlib import closing self.log.info(\u0026#34;Running copy expert: %s, filename: %s\u0026#34;, sql, filename) with open(filename, \u0026#34;r+\u0026#34;, encoding=encoding) as file, closing(self.get_conn()) as conn, closing(conn.cursor()) as cur: cur.copy_expert(sql, file) file.truncate(file.tell()) conn.commit() PostgreSQL 연결 메서드 # get_conn()\nPostgresHook 의 get_conn() 메서드와 유사한데, 메서드를 호출할 때마다 Connection 객체를 가져와 연결 정보를 읽어오는데 차이가 있음 psycopg2 라이브러리를 사용해 PostgreSQL에 연결하고 psycopg2.extensions.connection 객체를 반환 Copy python def get_conn(self) -\u0026gt; psycopg2.extensions.connection: conn = BaseHook.get_connection(self.postgres_conn_id) conn_args = { \u0026#34;host\u0026#34;: conn.host, \u0026#34;user\u0026#34;: conn.login, \u0026#34;password\u0026#34;: conn.password, \u0026#34;dbname\u0026#34;: self.database or conn.schema, \u0026#34;port\u0026#34;: conn.port, } self.conn = psycopg2.connect(**conn_args) return self.conn 쿼리문 생성 메서드 # bulk_load()\nf\u0026quot;COPY {table} FROM STDIN\u0026quot; 형식의 단순한 SQL문을 사용하던 기존 bulk_load() 메서드를 개선 create : _create_table_sql() 메서드를 통해 CREATE TABLE 문 생성 replace : if_exists 파라미터 값에 따라 테이블 내용을 모두 삭제하는 구문을 선택적으로 추가 copy : 구분자 또는 헤더 포함 여부 등을 파라미터로 받고 이를 활용하여 COPY 문 생성 Copy python def bulk_load(self, table: str, filename: str, encoding=\u0026#34;utf-8\u0026#34;, if_exists: Literal[\u0026#34;append\u0026#34;,\u0026#34;replace\u0026#34;]=\u0026#34;append\u0026#34;, sep=\u0026#39;,\u0026#39;, with_header=True): create = self._create_table_sql(table, filename, encoding, sep, with_header) replace = \u0026#34;TRUNCATE TABLE {};\u0026#34;.format(table) if if_exists == \u0026#34;replace\u0026#34; else str() copy = \u0026#34;COPY {} FROM STDIN DELIMITER \u0026#39;{}\u0026#39; {};\u0026#34;.format(table, sep, (\u0026#34;CSV HEADER\u0026#34; if with_header else \u0026#34;CSV\u0026#34;)) sql = \u0026#39;\u0026#39;.join([create, replace, copy]) self.copy_expert(sql, filename, encoding) _create_table_sql()\n헤더가 있을 경우에 한정해, _read_csv_column_list() 메서드를 통해 열 목록을 가져오고, CREATE TABLE 문 안에 열 목록을 포맷팅해 반환 IF NOT EXISTS 구문을 추가해 테이블이 이미 존재할 경우는 테이블 생성 생략 헤더가 없을 경우에는 기본적으로 테이블 생성 무시 Copy python def _create_table_sql(self, table: str, filename: str, encoding=\u0026#34;utf-8\u0026#34;, sep=\u0026#39;,\u0026#39;, with_header=True) -\u0026gt; str: if with_header: column_list = self._read_csv_column_list(filename, encoding, sep) return \u0026#34;CREATE TABLE IF NOT EXISTS {}({});\u0026#34;.format(table, column_list) else: return str() _read_csv_column_list()\n헤더와 상위 5개 행을 읽어서 데이터 타입을 추정하고, 이를 바탕으로 테이블의 열 목록을 정의 데이터 타입은 int4 또는 text 두 가지 경우만 판단하며, int4 범위에 있는 숫자형 문자 또는 NULL 값으로만 구성된 열은 int4 타입으로 지정하고, 나머지는 text 타입으로 지정 Copy python def _read_csv_column_list(self, filename: str, encoding=\u0026#34;utf-8\u0026#34;, sep=\u0026#39;,\u0026#39;) -\u0026gt; str: import csv def is_int4_type(value: str) -\u0026gt; bool: return (not value) or (value.isdigit() and (-2147483648 \u0026lt;= int(value) \u0026lt;= 2147483647)) with open(filename, \u0026#34;r+\u0026#34;, encoding=encoding) as file: reader = csv.reader(file, delimiter=sep) header = next(reader) dtypes = [all(map(is_int4_type, values)) for values in zip(*[next(reader) for _ in range(5)])] return \u0026#34;, \u0026#34;.join([\u0026#34;{} {}\u0026#34;.format(col, (\u0026#34;int4\u0026#34; if is_int4 else \u0026#34;text\u0026#34;)) for col, is_int4 in zip(header, dtypes)]) 쿼리문 실행 메서드 # copy_expert()\nPostgresHook 의 copy_expert() 메서드와 동일한데, 한글 CSV 파일은 EUC-KR 등 다른 인코딩이 필요할 수 있어 encoding 파라미터를 추가 Copy python def copy_expert(self, sql: str, filename: str, encoding=\u0026#34;utf-8\u0026#34;) -\u0026gt; None: from contextlib import closing self.log.info(\u0026#34;Running copy expert: %s, filename: %s\u0026#34;, sql, filename) with open(filename, \u0026#34;r+\u0026#34;, encoding=encoding) as file, closing(self.get_conn()) as conn, closing(conn.cursor()) as cur: cur.copy_expert(sql, file) file.truncate(file.tell()) conn.commit() Custom Hook 활용 예시 # DAG 생성 및 실행 # plugins/ 에 정의한 CustomPostgresHook 을 활용 실행 날짜에 생성된 shop.csv 파일을 보고 nshopping.search2 테이블을 생성 및 적재 shop.csv 파일을 굳이 shop_with_tab.csv 파일로 가공할 필요성을 줄여서 편의성 개선 Copy python # dags/python_with_postgres_custom.py from airflow.sdk import DAG from airflow.providers.standard.operators.python import PythonOperator from hooks.postgres import CustomPostgresHook import pendulum with DAG( dag_id=\u0026#34;python_with_postgres_custom\u0026#34;, schedule=None, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;hook\u0026#34;], ) as dag: def bulk_load_postgres(postgres_conn_id: str, table: str, filename: str, **kwargs): custom_postgres_hook = CustomPostgresHook(postgres_conn_id=postgres_conn_id) custom_postgres_hook.bulk_load(table=table, filename=filename, if_exists=\u0026#34;replace\u0026#34;, sep=\u0026#34;,\u0026#34;, with_header=True) bulk_load_postgres = PythonOperator( task_id=\u0026#34;bulk_load_postgres\u0026#34;, python_callable=bulk_load_postgres, op_kwargs={\u0026#34;postgres_conn_id\u0026#34;: \u0026#34;conn-db-postgres-custom\u0026#34;, \u0026#34;table\u0026#34;:\u0026#34;nshopping.search2\u0026#34;, \u0026#34;filename\u0026#34;:\u0026#34;/opt/airflow/files/naverSearch/{{data_interval_end.in_timezone(\\\u0026#34;Asia/Seoul\\\u0026#34;) | ds_nodash }}/shop.csv\u0026#34;} ) 실행 로그 중에서 CustomPostgresHook 이 생성한 SQL문을 확인 가능 if_exists=\u0026quot;replace\u0026quot; 파라미터를 추가했기 때문에, 중간에 TRUNCATE TABLE 구문이 추가 따라서, 여러 번 DAG을 실행해도 매번 테이블이 초기화되어 중복된 데이터가 업로드되지 않음 Copy bash [2025-06-11, 01:24:16] INFO - DAG bundles loaded: dags-folder: source=\u0026#34;airflow.dag_processing.bundles.manager.DagBundlesManager\u0026#34; [2025-06-11, 01:24:16] INFO - Filling up the DagBag from /opt/airflow/dags/python_with_postgres_custom.py: source=\u0026#34;airflow.models.dagbag.DagBag\u0026#34; [2025-06-11, 01:24:16] INFO - Running copy expert: CREATE TABLE IF NOT EXISTS nshopping.search2(rank int4, title text, link text, image text, lprice int4, hprice int4, mallName text, productId text, productType int4, brand text, maker text, category1 text, category2 text, category3 int4, category4 int4);TRUNCATE TABLE nshopping.search2;COPY nshopping.search2 FROM STDIN DELIMITER \u0026#39;,\u0026#39; CSV HEADER;, filename: /opt/airflow/files/naverSearch/20250611/shop.csv: source=\u0026#34;hooks.postgres.CustomPostgresHook\u0026#34; [2025-06-11, 01:24:16] INFO - Secrets backends loaded for worker: count=1: backend_classes=[\u0026#34;EnvironmentVariablesBackend\u0026#34;]: source=\u0026#34;supervisor\u0026#34; [2025-06-11, 01:24:16] INFO - Connection Retrieved \u0026#39;conn-db-postgres-custom\u0026#39;: source=\u0026#34;airflow.hooks.base\u0026#34; [2025-06-11, 01:24:16] INFO - Done. Returned value was: None: source=\u0026#34;airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator\u0026#34; 테이블 조회 # DBeaver에서 nshopping.search2 테이블이 생성되었고, 의도대로 정수형 열을 추측하여 데이터 타입을 구분해서 지정된 것을 확인 테이블 내용을 보면, 기존의 문제였던 헤더가 1행으로 들어갔던게 해결됨이 확인 또한, 추측한 데이터 타입에 맞춰서 값이 정상적으로 들어갔음을 확인 참조한 강의에서는 CSV 파일을 pd.DataFrame 객체로 읽고, SQLAlchemy의 엔진을 사용해 to_sql() 기능으로 PostgreSQL 테이블에 데이터를 적재하는 방식으로 접근 개인적으로는 PostgresHook 을 이해하고자, PostgresHook 의 원형을 최대한 유지하면서 필요한 기능만 추가하기 위해 외부 라이브러리의 사용을 제한함 "},{"id":15,"href":"/blog/airflow-study-6/","title":"Apache Airflow - Http Operator","section":"Posts","content":"HttpOperator # apache-airflow-providers-http \u0026amp;mdash; apache-airflow-providers-http …airflow.apache.org HTTP 요청을 하고 응답 결과를 반환받는 Operator (반환값은 XCom에 저장) HTTP를 이용하여 API를 처리하는 RestAPI 호출 시 사용 가능 Provider 패키지 설치 # HttpOperator 는 Provider 패키지로 별도의 설치가 필요 5.3.0 버전을 기준으로 apache-airflow\u0026gt;=2.10.0 버전을 요구 Copy bash pip install apache-airflow-providers-http HttpOperator 파라미터 # airflow.providers.http.operators.http \u0026amp;mdash; apache-airflow-providers-http …airflow.apache.org http_conn_id : http Connection을 생성해야 하는데 해당 Connection의 ID endpoint : Connection에 등록한 Host 뒤에 붙는 경로 method : HTTP 메서드 (GET, POST, PUT, DELETE 등) data : POST 요청 시 전달할 데이터 또는 GET 요청 시 전달할 파라미터 headers : HTTP 요청 헤더 response_check : HTTP 응답 결과가 정상인지 확인하는 함수 (True 반환) response_filter : HTTP 응답 결과에 대한 전처리 함수 네이버 쇼핑 검색 API # 검색 \u0026gt; 쇼핑 - Search API 검색 \u0026gt; 쇼핑 쇼핑 검색 개요 개요 사전 준비 사항 쇼핑 검색 API 레퍼런스 쇼핑 검색 결과 조회 오류 코드 검색 API 쇼핑 검색 구현 예제 쇼핑 검색 개요 개요 사전 준비 사항 개요 검색 API와 쇼핑 검색 개 … developers.naver.com HttpOperator 활용을 위해 간단한 API를 예제로 사용 네이버 쇼핑 검색 API는 네이버에서 제공하는 Open API 중 하나로 네이버 쇼핑 페이지에서 검색한 결과를 XML 형식 또는 JSON 형식으로 반환하는 RestAPI 하나의 키워드를 전달하여 정확도순으로 1위에 노출되는 상품 정보를 출력하는 기능을 구현 API 키 생성 # 네이버 개발자 센터에서 로그인 후 상단의 메뉴 중 Application-\u0026gt;애플리케이션 등록 으로 이동 아래 이미지와 같이 사용 API로 \u0026ldquo;검색\u0026quot;을 선택하고, 임의로 애플리케이션 이름과 서비스 환경을 입력하여 애플리케이션 등록 생성한 애플리케이션 정보에서 Client ID 와 Client Secret 을 조회 가능 Connection 추가 # 문서에 명시된 요청 URL을 참고하여 Host 를 입력 API가 별도로 알려지지 않은 포트를 사용할 경우 Port 에 포트 번호를 입력하지만, 네이버 Open API는 표준 HTTPS 포트를 사용하기 때문에 미표기 Variable 추가 # API키는 Variable로 추가하는데, Client ID 와 Client Secret 을 각각의 Variable로 등록 Client Secret 의 키 명칭에는 \u0026ldquo;secret\u0026rdquo; 이 포함되어 UI에서 자동으로 마스킹 처리되는 것을 확인 HttpOperator 활용 예시 # 앞서 등록한 Connection의 ID를 http_conn_id 에 입력하고, endpoint 에 Host를 제외한 나머지 경로를 입력 data 에는 GET 요청에 대한 파라미터를 전달하는데, query 로 원하는 키워드를 입력 headers 에는 Jinja 템플릿을 활용하여 Client ID 와 Client Secret 에 대한 Variable 값을 추가 HTTPS 요청 후 반환되는 Response 객체를 JSON 파싱하여 items 배열 내 첫 번째 항목, 즉 \u0026ldquo;노트북\u0026rdquo; 키워드로 검색한 결과에서 1위로 노출되는 상품을 딕셔너리 타입으로 반환 다음 단계의 Task print_product_task 에서는 XCom에서 반환된 값을 꺼내 단순 출력 Copy python # dags/http_operator.py from airflow.sdk import DAG, task from airflow.providers.http.operators.http import HttpOperator from airflow.models.taskinstance import TaskInstance import pendulum with DAG( dag_id=\u0026#34;http_operator\u0026#34;, start_date=pendulum.datetime(2025, 6, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), schedule=None, catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;http\u0026#34;], ) as dag: def parse(response): return response.json()[\u0026#34;items\u0026#34;][0] nshopping_search_task = HttpOperator( task_id=\u0026#34;nshopping_search_task\u0026#34;, http_conn_id=\u0026#34;openapi.naver.com\u0026#34;, endpoint=\u0026#34;/v1/search/shop.json\u0026#34;, method=\u0026#34;GET\u0026#34;, data={ \u0026#34;query\u0026#34;: \u0026#34;노트북\u0026#34;, \u0026#34;display\u0026#34;: 10, \u0026#34;start\u0026#34;: 1, \u0026#34;sort\u0026#34;: \u0026#34;sim\u0026#34; }, headers={ \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-Naver-Client-Id\u0026#34;: \u0026#34;{{var.value.client_id_openapi_naver_com}}\u0026#34;, \u0026#34;X-Naver-Client-Secret\u0026#34;: \u0026#34;{{var.value.client_secret_openapi_naver_com}}\u0026#34; }, response_filter=parse ) @task(task_id=\u0026#34;print_product_task\u0026#34;) def print_product_task(ti: TaskInstance, **kwargs): result = ti.xcom_pull(task_ids=\u0026#34;nshopping_search_task\u0026#34;) from pprint import pprint pprint(result) nshopping_search_task \u0026gt;\u0026gt; print_product_task() DAG 실행 후 print_product_task 의 실행 로그를 확인했을 때, 아래와 같이 nshopping_search_task 에서 반환된 상품에 대한 정보를 조회 가능 Copy bash # print_product_task [2025-06-07, 12:33:35] INFO - DAG bundles loaded: dags-folder: source=\u0026#34;airflow.dag_processing.bundles.manager.DagBundlesManager\u0026#34; [2025-06-07, 12:33:35] INFO - Filling up the DagBag from /opt/airflow/dags/http_operator.py: source=\u0026#34;airflow.models.dagbag.DagBag\u0026#34; [2025-06-07, 12:33:35] INFO - {\u0026#39;brand\u0026#39;: \u0026#39;갤럭시북\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-07, 12:33:35] INFO - \u0026#39;category1\u0026#39;: \u0026#39;디지털/가전\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-07, 12:33:35] INFO - \u0026#39;category2\u0026#39;: \u0026#39;노트북\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-07, 12:33:35] INFO - \u0026#39;category3\u0026#39;: \u0026#39;\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-07, 12:33:35] INFO - \u0026#39;category4\u0026#39;: \u0026#39;\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-07, 12:33:35] INFO - \u0026#39;hprice\u0026#39;: \u0026#39;\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-07, 12:33:35] INFO - \u0026#39;image\u0026#39;: \u0026#39;https://shopping-phinf.pstatic.net/main_8795238/87952389253.11.jpg\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-07, 12:33:35] INFO - \u0026#39;link\u0026#39;: \u0026#39;https://smartstore.naver.com/main/products/10407884292\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-07, 12:33:35] INFO - \u0026#39;lprice\u0026#39;: \u0026#39;428000\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-07, 12:33:35] INFO - \u0026#39;maker\u0026#39;: \u0026#39;삼성전자\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-07, 12:33:35] INFO - \u0026#39;mallName\u0026#39;: \u0026#39;삼성공식파트너 코인비엠에스\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-07, 12:33:35] INFO - \u0026#39;productId\u0026#39;: \u0026#39;87952389253\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-07, 12:33:35] INFO - \u0026#39;productType\u0026#39;: \u0026#39;2\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-07, 12:33:35] INFO - \u0026#39;title\u0026#39;: \u0026#39;삼성 갤럭시북 인강용 사무용 업무용 가성비 윈도우11 저가 싼 태블릿 \u0026lt;b\u0026gt;노트북\u0026lt;/b\u0026gt; 추천 기본팩\u0026#39;}: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-07, 12:33:35] INFO - Done. Returned value was: None: source=\u0026#34;airflow.task.operators.airflow.providers.standard.decorators.python._PythonDecoratedOperator\u0026#34; Custom Operator # Creating a custom Operator \u0026amp;mdash; Airflow 3.1.3 Documentationairflow.apache.org BaseOperator # BaseOperator 클래스를 상속받아 직접 만든 Operator를 사용 가능 BaseOperator 상속 시 두 가지 메서드를 재정의(Overriding)해야 함 def __int__(self, *args, **kwargs) 객체 생성 시 초기화하는 과정을 구현 (명시하지 않은 파라미터는 kwargs에 포함) def execute(self, context) Operator 실행 시 호출되는 메서드로 비즈니스 로직을 구현 template_fields: Sequence[str] = (\u0026quot;name\u0026quot;,) Jinja template을 적용할 파라미터를 지정 Copy python class HelloOperator(BaseOperator): template_fields: Sequence[str] = (\u0026#34;name\u0026#34;,) def __init__(self, name: str, world: str, **kwargs) -\u0026gt; None: super().__init__(**kwargs) self.name = name self.world = world def execute(self, context): message = f\u0026#34;Hello {self.world} it\u0026#39;s {self.name}!\u0026#34; print(message) return message HttpOperator 개선점 파악 # 주요 문제점은 네이버 쇼핑 검색 API의 표시 결과가 최대 100개로 제한되는 것으로, 더 많은 결과를 보려면 파라미터의 start 값을 다르게 한 여러 개의 HttpOperator를 만들어야 함 execute() 메서드 안에서 전체 요청을 100개 단위로 나누고, 반복문을 통해 여러 번 요청 및 응답 결과를 합칠 수 있게 로직을 작성 기존엔 endpoint 를 전부 입력해야 했는데, API 종류를 구분할 수 있는 문자열만 전달해서 endpoint 를 자동으로 만들어주면 더 편리할 것이라 생각함 endpoint 변경이 용이하기 때문에 쇼핑 검색 뿐 아니라 블로그 검색도 같이 수행할 것을 고려 기존엔 JSON 파싱한 결과를 그대로 반환했는데, 로컬 경로에 CSV 파일로 저장하면 결과를 확인하기 더 쉬울 것이라 생각함 Operator 기능 정의 # 네이버 쇼핑 및 블로그 검색 API를 호출하여 전체 데이터를 받은 후 CSV 파일로 저장하기\nNaverSearchToCsvOperator # BaseOperator 를 상속받는 NaverSearchToCsvOperator 를 구현 Operator 재사용을 위해 plugins/ 경로 아래에 추가 Copy python # plugins/operators/nshopping.py from airflow.models import BaseOperator from airflow.hooks.base import BaseHook from typing import Dict, List class NaverSearchToCsvOperator(BaseOperator): template_fields = (\u0026#34;file_path\u0026#34;, \u0026#34;client_id\u0026#34;, \u0026#34;client_secret\u0026#34;) def __init__(self, search_type: str, file_path: str, client_id: str, client_secret: str, keyword: str, display: int = 10, start: int = 1, sort = \u0026#34;sim\u0026#34;, **kwargs): super().__init__(**kwargs) self.http_conn_id = \u0026#34;openapi.naver.com\u0026#34; self.endpoint = f\u0026#34;/v1/search/{search_type}.json\u0026#34; self.method = \u0026#34;GET\u0026#34; self.file_path = file_path if file_path.startswith(\u0026#34;/\u0026#34;) else (\u0026#34;/opt/airflow/files/\u0026#34;+file_path) self.client_id = client_id self.client_secret = client_secret self.keyword = keyword self.display = min(display, 1000) self.start = min(start, 1000) self.sort = sort def execute(self, context): connection = BaseHook.get_connection(self.http_conn_id) url = connection.host + self.endpoint rows = list() for i, start in enumerate(range(self.start, self.display, 100)): display = min(self.display + self.start - start, 100) self.log.info(f\u0026#34;시작: {start}\u0026#34;) self.log.info(f\u0026#34;끝: {start+display-1}\u0026#34;) kwargs = dict(keyword=self.keyword, display=display, start=start, sort=self.sort) rows = rows + self._request_api(url, show_header=(i == 0), **kwargs) self._mkdir(self.file_path) self._to_csv(rows, self.file_path, encoding=\u0026#34;utf-8\u0026#34;, sep=\u0026#39;,\u0026#39;) def _request_api(self, url: str, keyword: str, display: int=10, start: int=1, sort=\u0026#34;sim\u0026#34;, show_header=True) -\u0026gt; List[List]: import requests params = self._get_params(keyword, display, start, sort) headers = self._get_headers(self.client_id, self.client_secret) with requests.request(self.method, url, params=params, headers=headers) as response: return self._parse_api(response, start, show_header) def _get_params(self, keyword: str, display: int=10, start: int=1, sort=\u0026#34;sim\u0026#34;) -\u0026gt; Dict: return {\u0026#34;query\u0026#34;: keyword, \u0026#34;display\u0026#34;: min(display, 100), \u0026#34;start\u0026#34;: min(start, 1000), \u0026#34;sort\u0026#34;: sort} def _get_headers(self, client_id: str, client_secret: str) -\u0026gt; Dict[str,str]: return { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-Naver-Client-Id\u0026#34;: client_id, \u0026#34;X-Naver-Client-Secret\u0026#34;: client_secret } def _parse_api(self, response, start: int=1, show_header=True) -\u0026gt; List[List]: contents = response.json()[\u0026#34;items\u0026#34;] if contents: header = ([[\u0026#34;rank\u0026#34;] + list(contents[0].keys())]) if show_header else [] return header + [[start+i]+list(content.values()) for i, content in enumerate(contents)] else: return list() def _mkdir(self, file_path: str): import os dir_path = os.path.split(file_path)[0] if not os.path.exists(dir_path): os.system(f\u0026#34;mkdir -p {dir_path}\u0026#34;) def _to_csv(self, rows: List[List], file_path: str, encoding=\u0026#34;utf-8\u0026#34;, sep=\u0026#39;,\u0026#39;): def clean(value: str) -\u0026gt; str: return str(value).replace(sep, \u0026#39;\u0026#39;) with open(file_path, \u0026#39;w\u0026#39;, encoding=encoding) as file: for row in rows: file.write(sep.join(map(clean, row))+\u0026#39;\\n\u0026#39;) 초기화 파라미터 # search_type: str\n네이버 쇼핑 검색 API의 endpoint 는 /v1/search/shop.json 인데, 블로그 검색 또는 뉴스 검색과 같은 다른 API를 확인했을 때, URL에서 {{host}}/v1/search/{{search_type}}.json 와 같은 패턴이 있음을 확인 가능 따라서, endpoint 대신에 search_type 을 입력으로 받고 endpoint 를 자동으로 생성 file_path: str\nnaverSearch/ 경로 아래 실행 날짜별로 폴더를 만들어서 아래에 CSV 파일을 저장할 계획인데, data_interval_end 변수를 사용하기 위해 template_fields 에 해당 파라미터를 등록 Root(/) 경로를 지정하지 않았을 경우 /opt/airflow/files 경로를 Root 경로로 사용하도록 지정 이 때, Airflow 웹서버 컨테이너에 파일을 저장해도 로컬 컴퓨터에서 확인할 수 없기 때문에 docker-compose.yaml 파일에 컨테이너와 로컬 컴퓨터 경로를 연결짓는 구문을 입력 Copy yaml # docker-compose.yaml x-airflow-common: volumes: - ${AIRFLOW_PROJ_DIR:-.}/files:/opt/airflow/files client_id: str, client_secret: str\n두 개의 API 계정 정보 또한 Variable에서 값을 꺼내오기 때문에 template_fields 에 등록 요청 헤더에 게정 정보를 추가하기 위해 사용 keyword: str, display: int, start: int, sort: str\nAPI 요청 파라미터를 구성하는 대표적인 키값을 입력 받음 display 는 최대 100까지 허용되지만, 반복문을 통해 나눠서 요청할 수 있기 때문에 1000까지 확장 start 는 API 문서에 명세된 것과 같이 최대 1000까지의 값으로 제한 sort 는 API 종류에 따라 허용되는 값이 다르지만, 공통적으로 정확도순 정렬(sim)을 사용할 것이기에 별도로 검증하지 않음 Copy python template_fields = (\u0026#34;file_path\u0026#34;, \u0026#34;client_id\u0026#34;, \u0026#34;client_secret\u0026#34;) def __init__(self, search_type: str, file_path: str, client_id: str, client_secret: str, keyword: str, display: int = 10, start: int = 1, sort = \u0026#34;sim\u0026#34;, **kwargs): super().__init__(**kwargs) self.http_conn_id = \u0026#34;openapi.naver.com\u0026#34; self.endpoint = f\u0026#34;/v1/search/{search_type}.json\u0026#34; self.method = \u0026#34;GET\u0026#34; self.file_path = file_path if file_path.startswith(\u0026#34;/\u0026#34;) else (\u0026#34;/opt/airflow/files/\u0026#34;+file_path) self.client_id = client_id self.client_secret = client_secret self.keyword = keyword self.display = min(display, 1000) self.start = min(start, 1000) self.sort = sort API 요청 메서드 # execute()\n고정된 Connection ID를 가지고 Connection에 등록된 Host를 조회 Host와 endpoint 를 조합하여 API 요청 URL을 생성 start 부터 display 까지의 범위를 100개 단위로 구분하여 반복문을 구성하고, 반복문마다 파라미터를 다르게 하여 API 요청 메서드 _request_api() 에 전달 Copy python def execute(self, context): connection = BaseHook.get_connection(self.http_conn_id) url = connection.host + self.endpoint rows = list() for i, start in enumerate(range(self.start, self.display, 100)): display = min(self.display + self.start - start, 100) self.log.info(f\u0026#34;시작: {start}\u0026#34;) self.log.info(f\u0026#34;끝: {start+display-1}\u0026#34;) kwargs = dict(keyword=self.keyword, display=display, start=start, sort=self.sort) rows = rows + self._request_api(url, show_header=(i == 0), **kwargs) ... _request_api()\n직접적으로 API에 요청을 보내는 메서드 다른 메서드를 통해 파라미터와 헤더를 만들고, 전달받은 URL에 대해 GET 요청을 보냄 응답 결과인 requests.Response 객체를 그대로 파싱 메서드 _parse_api() 로 전달 Copy python def _request_api(self, url: str, keyword: str, display: int=10, start: int=1, sort=\u0026#34;sim\u0026#34;, show_header=True) -\u0026gt; List[List]: import requests params = self._get_params(keyword, display, start, sort) headers = self._get_headers(self.client_id, self.client_secret) with requests.request(self.method, url, params=params, headers=headers) as response: return self._parse_api(response, start, show_header) _get_params()\n전달받은 파라미터를 딕셔너리 객체로 맵핑하여 반환하는 메서드 Copy python def _get_params(self, keyword: str, display: int=10, start: int=1, sort=\u0026#34;sim\u0026#34;) -\u0026gt; Dict: return {\u0026#34;query\u0026#34;: keyword, \u0026#34;display\u0026#34;: min(display, 100), \u0026#34;start\u0026#34;: min(start, 1000), \u0026#34;sort\u0026#34;: sort} _get_headers()\nAPI 계정 정보를 추가한 요청 헤더를 딕셔너리 객체로 반환하는 메서드 Copy python def _get_headers(self, client_id: str, client_secret: str) -\u0026gt; Dict[str,str]: return { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;X-Naver-Client-Id\u0026#34;: client_id, \u0026#34;X-Naver-Client-Secret\u0026#34;: client_secret } API 응답 결과 파싱 메서드 # execute()\n반복문 밖에서 리스트 객체 rows 를 만들고, 반복문마다 _request_api() 메서드가 반환하는 각각의 리스트 객체를 하나로 결합 첫 번째 반복문에서만 CSV의 헤더를 표시하기 위해 show_header 파라미터를 활용 Copy python def execute(self, context): ... rows = list() for start in range(self.start, self.display, 100): ... rows = rows + self._request_api(url, show_header=(i == 0), **kwargs) ... _parse_api()\nrequests.Response 객체를 인수로 전달받아 JSON 파싱한 후 items 키에서 배열을 꺼냄 CSV 파일로 저장하기 편한 구조로 가공하기 위해 딕셔너리에서 값만 꺼내 배열로 구성 execute() 에서 전달한 show_header 파라미터를 통해 첫 번째 반복문에서만 CSV 헤더를 추가해 반환 pd.DataFrame 을 활용할 수도 있지만, 단순하게 배열 타입으로 파싱 Copy python def _parse_api(self, response, start: int=1, show_header=True) -\u0026gt; List[List]: contents = response.json()[\u0026#34;items\u0026#34;] if contents: header = ([[\u0026#34;rank\u0026#34;] + list(contents[0].keys())]) if show_header else [] return header + [[start+i]+list(content.values()) for i, content in enumerate(contents)] else: return list() CSV 파일 저장 메서드 # execute()\nCSV 파일을 저장하기 전에 _mkdir() 메서드를 통해 file_path 검증 API 응답 결과를 가진 rows 객체를 _to_csv() 메서드에 전달해 CSV 파일로 저장 Copy python def execute(self, context): ... self._mkdir(self.file_path) self._to_csv(rows, self.file_path, encoding=\u0026#34;utf-8\u0026#34;, sep=\u0026#39;,\u0026#39;) _mkdir()\nfile_path 가 있는지 확인하고, 없으면 해당 경로를 생성하는 메서드 Copy python def _mkdir(self, file_path: str): import os dir_path = os.path.split(file_path)[0] if not os.path.exists(dir_path): os.system(f\u0026#34;mkdir -p {dir_path}\u0026#34;) _to_csv()\nfile_path 를 열고, 전달받은 rows 객체를 한 줄씩 기록하는 메서드 구분자 , 가 있을 경우 쌍따옴표 \u0026quot; \u0026quot; 로 감싸는 것과 같은 조치가 필요하지만, 내용이 중요한건 아니기 때문에 단순히 값에서 구분자 , 를 모두 제거하는 방식으로 가공하여 저장 Copy python def _to_csv(self, rows: List[List], file_path: str, encoding=\u0026#34;utf-8\u0026#34;, sep=\u0026#39;,\u0026#39;): def clean(value: str) -\u0026gt; str: return str(value).replace(sep, \u0026#39;\u0026#39;) with open(file_path, \u0026#39;w\u0026#39;, encoding=encoding) as file: for row in rows: file.write(sep.join(map(clean, row))+\u0026#39;\\n\u0026#39;) Custom Operator 활용 예시 # DAG 생성 및 실행 # plugins/ 에 정의한 NaverSearchToCsvOperator 를 가져와서 두 개의 Task를 생성 하나는 네이버 쇼핑 검색 API를 사용하는 search_shopping_task, 다른 하나는 네이버 블로그 검색 API를 사용하는 search_blog_task HttpOperator 를 사용했으면 길어졌을 내용을 NaverSearchToCsvOperator 로 구조화하여 DAG 선언을 단순화 Copy python # dags/nsearch_operator.py from airflow.sdk import DAG from operators.nshopping import NaverSearchToCsvOperator import pendulum with DAG( dag_id=\u0026#34;nsearch_operator\u0026#34;, schedule=\u0026#34;0 9 * * *\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;http\u0026#34;], ) as dag: api_keys = dict( client_id=\u0026#34;{{var.value.client_id_openapi_naver_com}}\u0026#34;, client_secret=\u0026#34;{{var.value.client_secret_openapi_naver_com}}\u0026#34;) common_params = dict(keyword=\u0026#34;노트북\u0026#34;, display=1000, start=1, sort=\u0026#34;sim\u0026#34;) \u0026#34;\u0026#34;\u0026#34;네이버 쇼핑 검색\u0026#34;\u0026#34;\u0026#34; search_shopping_task = NaverSearchToCsvOperator( task_id=\u0026#34;search_shopping_task\u0026#34;, search_type=\u0026#34;shop\u0026#34;, file_path=\u0026#34;naverSearch/{{data_interval_end.in_timezone(\\\u0026#34;Asia/Seoul\\\u0026#34;) | ds_nodash }}/shop.csv\u0026#34;, **api_keys, **common_params ) \u0026#34;\u0026#34;\u0026#34;네이버 블로그 검색\u0026#34;\u0026#34;\u0026#34; search_blog_task = NaverSearchToCsvOperator( task_id=\u0026#34;search_blog_task\u0026#34;, search_type=\u0026#34;blog\u0026#34;, file_path=\u0026#34;naverSearch/{{data_interval_end.in_timezone(\\\u0026#34;Asia/Seoul\\\u0026#34;) | ds_nodash }}/blog.csv\u0026#34;, **api_keys, **common_params ) search_shopping_task \u0026gt;\u0026gt; search_blog_task start 1부터 display 1000까지 총 1000개의 검색 결과를 100개 단위로 나눠서 요청하는 과정을 로그로 남겨서 조회 Copy bash [2025-06-07, 19:44:00] INFO - DAG bundles loaded: dags-folder: source=\u0026#34;airflow.dag_processing.bundles.manager.DagBundlesManager\u0026#34; [2025-06-07, 19:44:00] INFO - Filling up the DagBag from /opt/airflow/dags/nsearch_operator.py: source=\u0026#34;airflow.models.dagbag.DagBag\u0026#34; [2025-06-07, 19:44:00] INFO - Secrets backends loaded for worker: count=1: backend_classes=[\u0026#34;EnvironmentVariablesBackend\u0026#34;]: source=\u0026#34;supervisor\u0026#34; [2025-06-07, 19:44:00] INFO - Secrets backends loaded for worker: count=1: backend_classes=[\u0026#34;EnvironmentVariablesBackend\u0026#34;]: source=\u0026#34;supervisor\u0026#34; [2025-06-07, 19:44:00] INFO - Secrets backends loaded for worker: count=1: backend_classes=[\u0026#34;EnvironmentVariablesBackend\u0026#34;]: source=\u0026#34;supervisor\u0026#34; [2025-06-07, 19:44:00] INFO - Connection Retrieved \u0026#39;openapi.naver.com\u0026#39;: source=\u0026#34;airflow.hooks.base\u0026#34; [2025-06-07, 19:44:00] INFO - 시작: 1: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:00] INFO - 끝: 100: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:00] INFO - 시작: 101: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:00] INFO - 끝: 200: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:01] INFO - 시작: 201: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:01] INFO - 끝: 300: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:01] INFO - 시작: 301: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:01] INFO - 끝: 400: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:01] INFO - 시작: 401: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:01] INFO - 끝: 500: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:02] INFO - 시작: 501: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:02] INFO - 끝: 600: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:02] INFO - 시작: 601: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:02] INFO - 끝: 700: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:02] INFO - 시작: 701: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:02] INFO - 끝: 800: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:02] INFO - 시작: 801: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:02] INFO - 끝: 900: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:03] INFO - 시작: 901: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; [2025-06-07, 19:44:03] INFO - 끝: 1000: source=\u0026#34;airflow.task.operators.operators.nshopping.NaverSearchToCsvOperator\u0026#34; CSV 파일 확인 # 모든 Task가 success 로 처리된 후, 로컬 컴퓨터의 files/ 경로에 두 개의 CSV 파일이 생성됨을 확인 Copy bash % tree files -F files/ └── naverSearch/ └── 20250607/ ├── blog.csv └── shop.csv shop.csv # ranktitlelinkimagelpricehpricemallNameproductIdproductTypebrandmakercategory1category2category3category4 1삼성 갤럭시북...https://...https://...428000삼성공식파트너 코인비엠에스879523892532갤럭시북삼성전자디지털/가전노트북2삼성전자 갤럭시북4...https://...https://...799000네이버526312366421갤럭시북4삼성전자디지털/가전노트북3LG전자 울트라PC...https://...https://...599000제이 씨앤에스837187364882LG전자LG전자디지털/가전노트북4삼성전자 갤럭시북4...https://...https://...728000네이버526477942781갤럭시북4삼성전자디지털/가전노트북5LG그램 \u0026lt;b\u0026gt;노트북\u0026lt;/b\u0026gt;...https://...https://...1149000온라인총판대리점866360050312LG그램LG전자디지털/가전노트북 blog.csv # ranktitlelinkdescriptionbloggernamebloggerlinkpostdate 1\u0026lt;b\u0026gt;노트북\u0026lt;/b\u0026gt;에서...https://...버리고 \u0026lt;b\u0026gt;노트북\u0026lt;/b\u0026gt;으로...란디의 하늘색 꿈blog.naver.com/fksel33202506012고비넘긴 삼성\u0026lt;b\u0026gt;노트북\u0026lt;/b\u0026gt;...https://...노원구 공릉동에 위치한...하드웨어수리닷컴(엘존)blog.naver.com/yzenn202506033MSI 사무용\u0026lt;b\u0026gt;노트북\u0026lt;/b\u0026gt;...https://...MSI\u0026lt;b\u0026gt;노트북\u0026lt;/b\u0026gt;중에서...Modulestudioblog.naver.com/modulestudio202506024아직 이걸 안 써?...https://...아직 이걸 안 써?...짜루의 이것저것 리뷰blog.naver.com/skdaksdptn202505205가성비 초경량 \u0026lt;b\u0026gt;노트북\u0026lt;/b\u0026gt;...https://...그런 \u0026lt;b\u0026gt;노트북\u0026lt;/b\u0026gt;을...삼성Mall 한사랑씨앤씨 공식 블로그blog.naver.com/hansarangcnc20250527 "},{"id":16,"href":"/blog/airflow-study-5/","title":"Apache Airflow - Trigger Dag Run Operator","section":"Posts","content":"Trigger Rule # Dags \u0026amp;mdash; Airflow 3.1.3 Documentationairflow.apache.org 상위 Task들의 상태에 따라 수행여부를 결정하고 싶을 때 사용 기본적으로는 상위 Task가 모두 성공해야 실행 Trigger Rule 종류 # 옵션 설명 all_success 기본값, 상위 Task가 모두 성공하면 실행 all_failed 상위 Task가 모두 failed 상태면 실행 all_done 상위 Task가 모두 수행되면 실행 (성공 또는 실패) all_skipped 상위 Task가 모두 skipped 상태면 실행 one_failed 상위 Task 중 하나 이상 실패하면 실행 one_success 상위 Task 중 하나 이상 성공하면 실행 one_done 상위 Task 중 하나 이상 수행되면 실행 (성공 또는 실패) none_failed 상위 Task 중에 failed 상태가 없으면 실행 none_failed_min_one_success 상위 Task 중에 failed 상태가 없고 성공한 Task가 1개 이상이면 실행 none_skipped 상위 Task 중에 skipped 상태가 없으면 실행 always 항상 실행 all_done 예시 # all_done 의 동작을 확인하기 위한 예시 DAG 작성 3개의 상위 Task 중 2번째 Task에서 의도적으로 예외를 발생시켜서 failed 상태를 유발 하위 Task downstream_task 에 trigger_rule 파라미터로 all_done 전달 Copy python # dags/trigger_rule1.py from airflow.sdk import DAG, task from airflow.exceptions import AirflowException from airflow.providers.standard.operators.bash import BashOperator import pendulum with DAG( dag_id=\u0026#34;trigger_rule1\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), schedule=\u0026#34;0 0 * * *\u0026#34;, catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;branch\u0026#34;], ) as dag: upstream_task1 = BashOperator( task_id=\u0026#34;upstream_task1\u0026#34;, bash_command=\u0026#34;echo upstream1\u0026#34; ) @task(task_id=\u0026#34;upstream_task2\u0026#34;) def upstream_task2(): raise AirflowException(\u0026#34;upstream2 Exception\u0026#34;) @task(task_id=\u0026#34;upstream_task3\u0026#34;) def upstream_task3(): print(\u0026#34;정상 처리\u0026#34;) @task(task_id=\u0026#34;downstream_task\u0026#34;, trigger_rule=\u0026#34;all_done\u0026#34;) def downstream_task(): print(\u0026#34;정상 처리\u0026#34;) [upstream_task1, upstream_task2(), upstream_task3()] \u0026gt;\u0026gt; downstream_task() all_done 은 상위 Task가 성공 또는 실패 여부에 관계없이 모두 수행되면 실행하는 옵션으로, upstream_task2 가 실패 처리되어도 downstream_task 가 수행되는 모습을 확인 none_skipped 예시 # none_skipped 의 동작을 확인하기 위한 예시 DAG 작성 3개의 상위 Task 중 랜덤한 한 Task만 수행하고 나머지 Task에선 skipped 상태를 유발 하위 Task downstream_task 에 trigger_rule 파라미터로 none_skipped 전달 Copy python # dags/trigger_rule2.py from airflow.sdk import DAG, task from airflow.providers.standard.operators.bash import BashOperator import pendulum with DAG( dag_id=\u0026#34;trigger_rule2\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), schedule=\u0026#34;0 0 * * *\u0026#34;, catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;branch\u0026#34;], ) as dag: @task.branch(task_id=\u0026#34;branching\u0026#34;) def random_branch(): import random item_lst = [\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;] selected_item = random.choice(item_lst) if selected_item == \u0026#39;A\u0026#39;: return \u0026#34;upstream_task_a\u0026#34; elif selected_item == \u0026#39;B\u0026#39;: return \u0026#34;upstream_task_b\u0026#34; elif selected_item == \u0026#39;C\u0026#39;: return \u0026#34;upstream_task_c\u0026#34; upstream_task_a = BashOperator( task_id=\u0026#34;upstream_task_a\u0026#34;, bash_command=\u0026#34;echo upstream1\u0026#34; ) @task(task_id=\u0026#34;upstream_task_b\u0026#34;) def upstream_task_b(): print(\u0026#34;정상 처리\u0026#34;) @task(task_id=\u0026#34;upstream_task_c\u0026#34;) def upstream_task_c(): print(\u0026#34;정상 처리\u0026#34;) @task(task_id=\u0026#34;downstream_task\u0026#34;, trigger_rule=\u0026#34;none_skipped\u0026#34;) def downstream_task(): print(\u0026#34;정상 처리\u0026#34;) random_branch() \u0026gt;\u0026gt; [upstream_task_a, upstream_task_b(), upstream_task_c()] \u0026gt;\u0026gt; downstream_task() none_skipped 은 상위 Task가 skipped 상태가 아니어야 실행하는 옵션으로, upstream_task1 만 성공하고 나머지는 skipped 처리되었기 때문에, downstream_task 도 수행되지 못하고 skipped 처리 TriggerDagRunOperator # airflow.operators.trigger_dagrun \u0026amp;mdash; Airflow Documentationairflow.apache.org 다른 DAG을 실행시키는 Operator 실행할 다른 DAG의 ID를 지정하여 수행 선행 DAG이 하나만 있을 경우 TriggerDagRunOperator 를 사용하고, 선행 DAG이 2개 이상인 경우는 ExternalTaskSensor 를 사용 권장 run_id # DAG의 수행 방식과 시간을 유일하게 식별해주는 키 수행 방식(Schedule, manual, Backfill)에 따라 키가 달라짐 스케줄에 의해 실행된 경우 scheduled__{{data_interval_start}} 값을 가짐 예시) scheduled__2025-06-01T00:00:00+00:00 TriggerDagRun 활용 # trigger_run_id : DAG을 실행시킬 때 어떤 run_id 로 실행할지 지정 가능 logical_date : DAG이 트리거된 시간을 지정 가능, manual__{{logical_date}} reset_dag_run : run_id 로 수행된 이력이 있어도 실행시키려면 True 로 설정 wait_for_completion : 지정한 DAG이 완료되어야 다음 Task를 실행하고 싶을 경우 True 로 설정 기본적으로는 DAG의 완료 여부에 관계없이 success 로 빠져나가 다음 Task를 실행 poke_interval : 지정한 DAG이 완료되었는지 확인하는 주기 allowed_states : Task가 success 상태가 되기 위한 DAG의 처리 상태 목록 failed_states : Task가 failed 상태가 되기 위한 DAG의 처리 상태 목록 Copy python # dags/trigger_dagrun.py from airflow.sdk import DAG from airflow.providers.standard.operators.trigger_dagrun import TriggerDagRunOperator from airflow.providers.standard.operators.bash import BashOperator import pendulum with DAG( dag_id=\u0026#34;trigger_dagrun\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), schedule=\u0026#34;0 0 * * *\u0026#34;, catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;dagrun\u0026#34;], ) as dag: start_task = BashOperator( task_id=\u0026#34;start_task\u0026#34;, bash_command=\u0026#34;echo \\\u0026#34;start!\\\u0026#34;\u0026#34;, ) trigger_dag_task = TriggerDagRunOperator( task_id=\u0026#34;trigger_dag_task\u0026#34;, trigger_dag_id=\u0026#34;python_operator\u0026#34;, trigger_run_id=None, logical_date=\u0026#34;{{data_interval_start}}\u0026#34;, reset_dag_run=True, wait_for_completion=False, poke_interval=60, allowed_states=[\u0026#34;success\u0026#34;], failed_states=None ) start_task \u0026gt;\u0026gt; trigger_dag_task DAG 실행 # trigger_dag_task 의 실행 로그에서 python_operator 가 호출된 것을 확인 Copy bash [2025-06-07, 10:52:54] INFO - DAG bundles loaded: dags-folder: source=\u0026#34;airflow.dag_processing.bundles.manager.DagBundlesManager\u0026#34; [2025-06-07, 10:52:54] INFO - Filling up the DagBag from /opt/airflow/dags/trigger_dagrun.py: source=\u0026#34;airflow.models.dagbag.DagBag\u0026#34; [2025-06-07, 10:52:54] INFO - Triggering Dag Run.: trigger_dag_id=\u0026#34;python_operator\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-07, 10:52:54] INFO - Dag Run triggered successfully.: trigger_dag_id=\u0026#34;python_operator\u0026#34;: source=\u0026#34;task\u0026#34; 두 번째 이미지인 PythonOperator의 run_id 가 첫 번째 이미지인 TriggerDagRunOperator의 실행 시간과 같다는 것을 알 수 있으며, trigger_run_id 를 지정하지 않았기 때문에 manual 로 지정 TaskGroup # Dags \u0026amp;mdash; Airflow 3.1.3 Documentationairflow.apache.org 여러 Task들을 그룹화하는 개념 UI 상에서 Task들을 모아서 편하게 보고 관리하기 쉽게 하기 위한 목적 TaskGroup 활용 # @task_group 데코레이터 또는 TaskGroup 클래스를 활용하여 TaskGroup을 구현 docstring을 추가해 Airflow UI에서 TaskGroup에 대한 Tooltip을 표시 또는, tooltip 파라미터로 UI에 표시할 내용을 전달할 수도 있음 (파라미터가 docstring보다 우선) Copy python # dags/task_group.py from airflow.sdk import DAG, task, task_group, TaskGroup from airflow.providers.standard.operators.python import PythonOperator import pendulum with DAG( dag_id=\u0026#34;task_group\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), schedule=\u0026#34;0 0 * * *\u0026#34;, catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;branch\u0026#34;], ) as dag: def inner_function2(**kwargs): msg = kwargs.get(\u0026#34;msg\u0026#34;) or str() print(msg) @task_group(group_id=\u0026#34;first_group\u0026#34;) def first_group(): \u0026#34;\u0026#34;\u0026#34; 첫 번째 TaskGroup 에 대한 Tooltip 입니다. \u0026#34;\u0026#34;\u0026#34; @task(task_id=\u0026#34;inner_function1\u0026#34;) def inner_function1(**kwargs): print(\u0026#34;첫 번째 TaskGroup 내 첫 번째 Task 입니다.\u0026#34;) inner_function2 = PythonOperator( task_id=\u0026#34;inner_function2\u0026#34;, python_callable=inner_function2, op_kwargs={\u0026#34;msg\u0026#34;:\u0026#34;첫 번째 TaskGroup 내 두 번째 Task 입니다.\u0026#34;} ) inner_function1() \u0026gt;\u0026gt; inner_function2 with TaskGroup(group_id=\u0026#34;second_group\u0026#34;, tooltip=\u0026#34;두 번째 TaskGroup 에 대한 Tooltip 입니다.\u0026#34;) as second_group: \u0026#34;\u0026#34;\u0026#34; tooltip 파라미터의 내용이 우선적으로 표시됩니다. \u0026#34;\u0026#34;\u0026#34; @task(task_id=\u0026#34;inner_function1\u0026#34;) def inner_function1(**kwargs): print(\u0026#34;두 번째 TaskGroup 내 첫 번째 Task 입니다.\u0026#34;) inner_function2 = PythonOperator( task_id=\u0026#34;inner_function2\u0026#34;, python_callable=inner_function2, op_kwargs={\u0026#34;msg\u0026#34;: \u0026#34;두 번째 TaskGroup 내 두 번째 Task 입니다.\u0026#34;} ) inner_function1() \u0026gt;\u0026gt; inner_function2 first_group() \u0026gt;\u0026gt; second_group TaskGroup 조회 # DAG 실행 후 Graph View에서 두 개의 TaskGroup을 확인 기대와 다르게 지정한 Tooltip이 표시되지 않았는데, Airflow 3.0 버전의 버그인 것으로 추정 TaskGroup을 클릭하면 펼쳐지면서 내부 Task를 표시 Edge Label # Dags \u0026amp;mdash; Airflow 3.1.3 Documentationairflow.apache.org Task 연결에 대한 설명을 추가하는 개념 Task 종속성을 나타내는 \u0026gt;\u0026gt; 또는 \u0026lt;\u0026lt; 연산자 사이에 Label 을 추가 Edge Label 활용 # 첫 번째 Label은 두 개의 단일 Task 사이를 연결 두 번째와 세 번째 Label은 Branch의 시작과 끝을 각각 연결 Copy python # dags/edge_label.py from airflow.sdk import DAG, Label from airflow.providers.standard.operators.empty import EmptyOperator import pendulum with DAG( dag_id=\u0026#34;edge_label\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), schedule=\u0026#34;0 0 * * *\u0026#34;, catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;branch\u0026#34;], ) as dag: empty_1 = EmptyOperator( task_id=\u0026#34;empty_1\u0026#34; ) empty_2 = EmptyOperator( task_id=\u0026#34;empty_2\u0026#34; ) empty_1 \u0026gt;\u0026gt; Label(\u0026#34;라벨\u0026#34;) \u0026gt;\u0026gt; empty_2 empty_3 = EmptyOperator( task_id=\u0026#34;empty_3\u0026#34; ) empty_4 = EmptyOperator( task_id=\u0026#34;empty_4\u0026#34; ) empty_5 = EmptyOperator( task_id=\u0026#34;empty_5\u0026#34; ) empty_6 = EmptyOperator( task_id=\u0026#34;empty_6\u0026#34; ) empty_2 \u0026gt;\u0026gt; Label(\u0026#34;브랜치 시작\u0026#34;) \u0026gt;\u0026gt; [empty_3,empty_4,empty_5] \u0026gt;\u0026gt; Label(\u0026#34;브랜치 종료\u0026#34;) \u0026gt;\u0026gt; empty_6 Edge Label 조회 # Airflow UI의 Graph View에서 Edge Label을 확인 Branch 연결에 대해서는 모든 연결에 동일한 내용의 Label을 표시 "},{"id":17,"href":"/blog/airflow-study-4/","title":"Apache Airflow - Operator (Branch, Email)","section":"Posts","content":"BranchOperator # Branching # Dags \u0026amp;mdash; Airflow 3.1.3 Documentationairflow.apache.org 특정 Task의 결과에 따라 하위 Task를 선별해서 수행시키고 싶을 때 사용 BranchPythonOperator # BranchPythonOperator 에서 랜덤한 조건에 따라 task_a 만 수행하거나, task_b 와 task_c 를 같이 수행하는 분기 처리 Copy python # dags/branch_python.py from airflow.sdk import DAG from airflow.providers.standard.operators.python import PythonOperator from airflow.providers.standard.operators.python import BranchPythonOperator import pendulum with DAG( dag_id=\u0026#34;branch_python\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), schedule=\u0026#34;0 0 * * *\u0026#34;, catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;branch\u0026#34;], ) as dag: def select_random(): import random item_lst = [\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;] selected_item = random.choice(item_lst) if selected_item == \u0026#39;A\u0026#39;: return \u0026#34;task_a\u0026#34; else: return [\u0026#34;task_b\u0026#34;,\u0026#34;task_c\u0026#34;] branch_task = BranchPythonOperator( task_id=\u0026#34;branch_task\u0026#34;, python_callable=select_random ) def print_selected(**kwargs): print(kwargs[\u0026#34;selected\u0026#34;]) task_a = PythonOperator( task_id=\u0026#34;task_a\u0026#34;, python_callable=print_selected, op_kwargs={\u0026#34;selected\u0026#34;:\u0026#39;A\u0026#39;} ) task_b = PythonOperator( task_id=\u0026#34;task_b\u0026#34;, python_callable=print_selected, op_kwargs={\u0026#34;selected\u0026#34;:\u0026#39;B\u0026#39;} ) task_c = PythonOperator( task_id=\u0026#34;task_c\u0026#34;, python_callable=print_selected, op_kwargs={\u0026#34;selected\u0026#34;:\u0026#39;C\u0026#39;} ) branch_task \u0026gt;\u0026gt; [task_a, task_b, task_c] 여러 번 Trigger하여 실행했는데, 의도대로 task_a 만 수행되거나, task_b 와 task_c 가 같이 수행되는 두 가지 경우를 확인 또한, task_a 가 선택되는 작업에서 XCom을 보면 skipmixin_key 키로 {'followed': ['task_a']} 값이 전달되는데, 이를 통해 다른 Task에서도 어떤 분기 처리가 되었는지 확인 가능 마찬가지로 실행 로그에서도 어떤 Task가 선택되었고, 어떤 Task가 Skip되었는지 조회 가능 Copy bash # branch_task [2025-06-06, 11:19:07] INFO - Done. Returned value was: task_a: source=\u0026#34;airflow.task.operators.airflow.providers.standard.operators.python.BranchPythonOperator\u0026#34; [2025-06-06, 11:19:07] INFO - Branch into task_a: source=\u0026#34;airflow.task.operators.airflow.providers.standard.operators.python.BranchPythonOperator\u0026#34; [2025-06-06, 11:19:07] INFO - Following branch {\u0026#39;task_a\u0026#39;}: source=\u0026#34;airflow.task.operators.airflow.providers.standard.operators.python.BranchPythonOperator\u0026#34; [2025-06-06, 11:19:07] INFO - Skipping tasks [(\u0026#39;task_b\u0026#39;, -1), (\u0026#39;task_c\u0026#39;, -1)]: source=\u0026#34;airflow.task.operators.airflow.providers.standard.operators.python.BranchPythonOperator\u0026#34; [2025-06-06, 11:19:07] INFO - Skipping downstream tasks.: source=\u0026#34;task\u0026#34; @task.branch # BranchPythonOperator 대신에 @task.branch 데코레이터를 써서 아래와 같이 표현도 가능 함수를 호출하여 종속성 표현 (select_random() \u0026gt;\u0026gt; [task_a, task_b, task_c]) Copy python # dags/branch_python_decorator.py from airflow.sdk import DAG, task from airflow.providers.standard.operators.python import PythonOperator import pendulum with DAG( dag_id=\u0026#34;branch_python_decorator\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), schedule=\u0026#34;0 0 * * *\u0026#34;, catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;branch\u0026#34;], ) as dag: @task.branch(task_id=\u0026#34;branch_task\u0026#34;) def select_random(): import random item_lst = [\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;] selected_item = random.choice(item_lst) if selected_item == \u0026#39;A\u0026#39;: return \u0026#34;task_a\u0026#34; else: return [\u0026#34;task_b\u0026#34;,\u0026#34;task_c\u0026#34;] def print_selected(**kwargs): print(kwargs[\u0026#34;selected\u0026#34;]) task_a = PythonOperator( task_id=\u0026#34;task_a\u0026#34;, python_callable=print_selected, op_kwargs={\u0026#34;selected\u0026#34;:\u0026#39;A\u0026#39;} ) task_b = PythonOperator( task_id=\u0026#34;task_b\u0026#34;, python_callable=print_selected, op_kwargs={\u0026#34;selected\u0026#34;:\u0026#39;B\u0026#39;} ) task_c = PythonOperator( task_id=\u0026#34;task_c\u0026#34;, python_callable=print_selected, op_kwargs={\u0026#34;selected\u0026#34;:\u0026#39;C\u0026#39;} ) select_random() \u0026gt;\u0026gt; [task_a, task_b, task_c] BaseBranchOperator # airflow.operators.branch \u0026amp;mdash; Airflow Documentationairflow.apache.org Branching 기능을 제공하는 Operator의 기본 클래스 해당 클래스를 상속받을 경우 choose_branch(self, context) 메서드를 구현해야 하고, 분기 처리 로직을 통해 선택되어야 할 Task를 한 개(문자열) 또는 여러 개(리스트)로 반환해야 함 Copy python # dags/branch_base.py from airflow.sdk import DAG from airflow.providers.standard.operators.branch import BaseBranchOperator from airflow.providers.standard.operators.python import PythonOperator import pendulum with DAG( dag_id=\u0026#34;branch_base\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), schedule=\u0026#34;0 0 * * *\u0026#34;, catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;branch\u0026#34;], ) as dag: class CustomBranchOperator(BaseBranchOperator): def choose_branch(self, context): import random item_lst = [\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;] selected_item = random.choice(item_lst) if selected_item == \u0026#39;A\u0026#39;: return \u0026#34;task_a\u0026#34; else: return [\u0026#34;task_b\u0026#34;,\u0026#34;task_c\u0026#34;] custom_branch_task = CustomBranchOperator(task_id=\u0026#34;custom_branch_task\u0026#34;) def print_selected(**kwargs): print(kwargs[\u0026#34;selected\u0026#34;]) task_a = PythonOperator( task_id=\u0026#34;task_a\u0026#34;, python_callable=print_selected, op_kwargs={\u0026#34;selected\u0026#34;:\u0026#39;A\u0026#39;} ) task_b = PythonOperator( task_id=\u0026#34;task_b\u0026#34;, python_callable=print_selected, op_kwargs={\u0026#34;selected\u0026#34;:\u0026#39;B\u0026#39;} ) task_c = PythonOperator( task_id=\u0026#34;task_c\u0026#34;, python_callable=print_selected, op_kwargs={\u0026#34;selected\u0026#34;:\u0026#39;C\u0026#39;} ) custom_branch_task \u0026gt;\u0026gt; [task_a, task_b, task_c] EmailOperator # airflow.operators.email_operator \u0026amp;mdash; Airflow Documentationairflow.apache.org 이메일을 전송해주는 Operator SMTP 프로토콜을 통해 개인 Gmail에서 다른 주소로 메일을 보내는 기능 구현 Gmail 옵션에서 IMAP 사용 설정 및 앱 비밀번호 생성이 선행되어야 함 docker-compose 수정 # Email Configuration \u0026amp;mdash; Airflow 3.1.3 Documentationairflow.apache.org docker-compose.yaml 파일 내에 아래와 같은 항목을 추가 AIRFLOW__SMTP__SMTP_USER 에는 앱 비밀번호를 생성한 구글 계정을 입력 AIRFLOW__SMTP__SMTP_PASSWORD 에는 발급받은 앱 비밀번호를 공백없이 입력 AIRFLOW__SMTP__SMTP_MAIL_FROM 에는 메일을 보내는 계정을 입력 Copy yaml # docker-compose.yaml x-airflow-common: environment: AIRFLOW__SMTP__SMTP_HOST: smtp.gmail.com AIRFLOW__SMTP__SMTP_USER: ${SMTP_USER} AIRFLOW__SMTP__SMTP_PASSWORD: ${SMTP_PASSWORD} AIRFLOW__SMTP__SMTP_PORT: 587 AIRFLOW__SMTP__SMTP_MAIL_FROM: ${SMTP_USER} Connection 추가 # Managing Connections \u0026amp;mdash; Airflow 3.1.3 Documentationairflow.apache.org Airflow 3.0 버전부터는 SMTP 설정을 환경변수나 설정 파일에서 가져오는 것이 아닌, Connection을 활용하도록 권장 Airflow UI의 사이드바에서 Admin-\u0026gt;Connections 메뉴로 이동한 후, Add Connection 버튼을 클릭하여 Connection 추가 아래 이미지와 같이 Connection Type 으로 smtp 를 선택하고, docker-compose 파일에 추가했던 것처럼 구글 계정과 앱 비밀번호를 포함한 메일 연결 설정을 입력 Extra Fields 에서 메일을 보내는 계정 등 추가적인 정보를 입력 가능 EmailOperator 활용 예시 # 생성한 Connection을 사용하기 위해 conn_id 로 지정한 Connection ID 를 입력 대상 이메일 주소를 to 에 입력하고, 제목은 subject, 내용은 html_content 에 입력 참조를 추가할 시 cc 파라미터로 추가로 입력 가능 Copy python from airflow.sdk import DAG from airflow.providers.smtp.operators.smtp import EmailOperator import pendulum with DAG( dag_id=\u0026#34;email_operator\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), schedule=\u0026#34;0 9 1 * *\u0026#34;, catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;email\u0026#34;], ) as dag: send_email_task = EmailOperator( task_id=\u0026#34;send_email_task\u0026#34;, conn_id=\u0026#34;gmail\u0026#34;, to=\u0026#34;example@gmail.com\u0026#34;, subject=\u0026#34;Airflow 테스트\u0026#34;, html_content=\u0026#34;Airflow 작업이 완료되었습니다.\u0026#34; ) SSLError # 정상적으로 메일이 보내질 것을 기대했지만, 예상치못한 SSLError 가 발생 Copy bash SSLError: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1010) ... File \u0026#34;/usr/local/lib/python3.12/smtplib.py\u0026#34;, line 1022 in __init__ File \u0026#34;/usr/local/lib/python3.12/smtplib.py\u0026#34;, line 255 in __init__ File \u0026#34;/usr/local/lib/python3.12/smtplib.py\u0026#34;, line 341 in connect File \u0026#34;/usr/local/lib/python3.12/smtplib.py\u0026#34;, line 1029 in _get_socket File \u0026#34;/usr/local/lib/python3.12/ssl.py\u0026#34;, line 455 in wrap_socket File \u0026#34;/usr/local/lib/python3.12/ssl.py\u0026#34;, line 1041 in _create File \u0026#34;/usr/local/lib/python3.12/ssl.py\u0026#34;, line 1319 in do_handshake 원인 파악은 못했지만, Connection에서 SSL 비활성화 후 재시도하니 정상적으로 메일 전송 메일 발송 확인 # 정상적으로 메일이 전송되었을 때, 대상 메일에 접속하면 EmailOperator 에서 지정한 것과 동일한 제목과 내용의 메일이 전송된 것을 확인 가능 Copy html \u0026lt;h2\u0026gt;Airflow 테스트\u0026lt;/h2\u0026gt; \u0026lt;!-- 제목 --\u0026gt; \u0026lt;div\u0026gt;Airflow 작업이 완료되었습니다.\u0026lt;/div\u0026gt; \u0026lt;!-- 내용 --\u0026gt; 실행 로그에서도 메일 전송을 위해 임의로 생성한 Connection gmail 을 사용한 것을 확인 Copy bash [2025-06-06, 21:10:46] INFO - DAG bundles loaded: dags-folder: source=\u0026#34;airflow.dag_processing.bundles.manager.DagBundlesManager\u0026#34; [2025-06-06, 21:10:46] INFO - Filling up the DagBag from /opt/airflow/dags/email_operator.py: source=\u0026#34;airflow.models.dagbag.DagBag\u0026#34; [2025-06-06, 21:10:46] INFO - Secrets backends loaded for worker: count=1: backend_classes=[\u0026#34;EnvironmentVariablesBackend\u0026#34;]: source=\u0026#34;supervisor\u0026#34; [2025-06-06, 21:10:46] INFO - Connection Retrieved \u0026#39;gmail\u0026#39;: source=\u0026#34;airflow.hooks.base\u0026#34; "},{"id":18,"href":"/blog/airflow-study-3/","title":"Apache Airflow - Jinja Template, XCom, Variable","section":"Posts","content":"Jinja 템플릿 # Templates reference \u0026amp;mdash; Airflow 3.1.3 Documentationairflow.apache.org 파이썬 기반 웹 프레임워크 Flask, Django에서 주로 사용 HTML 템플릿을 만들고 화면에 보여질 때 값을 렌더링해서 출력 Airflow에서는 파라미터 입력 시 중괄호 2개 {{ }} 를 이용해 변수를 치환된 값으로 입력 변수 목록 (중괄호 생략) # data_interval_start : 스케줄의 시작 날짜이며,pendulum.DateTime 타입 data_interval_end : 스케줄의 종료 날짜(= 배치일)이며,pendulum.DateTime 타입 logical_date : DAG가 실행 중인 시점의 날짜이며,pendulum.DateTime 타입 ds : logical_date 를 YYYY-MM-DD 형태의 문자열로 변환한 값 ds 에서 - 을 제거한 YYYYMMDD 형태의 문자열 ds_nodash 변수도 제공 ts : logical_date 를 2018-01-01T00:00:00+00:00 형태의 문자열로 변환한 값 ts_nodash_with_tz 또는 ts_nodash 등의 변형된 변수도 지원 ds 또는 ts 등은 {{ logical_date | ds }} 의 형태로도 표현 가능 적용 대상 # BashOperator에서는 bash_command, env 파라미터에 템플릿 적용 가능 PythonOperator에서는 templates_dict, op_args, op_kwargs 파라미터에 템플릿 적용 가능 Airflow의 각 Operator 문서에서 Templating 부분 참고 Jinja 템플릿 변수 활용 # BashOperator # Jinja 템플릿 변수를 그대로 출력하는 명령어를 실행하는 DAG 구성 첫 번째 Task는 변수를 그대로 출력하고, 두 번째 Task는 env로 파라미터를 전달해서 출력 Copy python # dags/bash_template.py from airflow.sdk import DAG from airflow.providers.standard.operators.bash import BashOperator import pendulum with DAG( dag_id=\u0026#34;bash_template\u0026#34;, schedule=\u0026#34;0 0 * * *\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;template\u0026#34;], ) as dag: bash_task1 = BashOperator( task_id=\u0026#34;bash_task1\u0026#34;, bash_command=\u0026#34;echo \\\u0026#34;End date is {{ data_interval_end }}\\\u0026#34;\u0026#34;, ) bash_task2 = BashOperator( task_id=\u0026#34;bash_task2\u0026#34;, env={ \u0026#34;START_DATE\u0026#34;: \u0026#34;{{ data_interval_start | ds }}\u0026#34;, \u0026#34;END_DATE\u0026#34;: \u0026#34;{{ data_interval_end | ds }}\u0026#34; }, bash_command=\u0026#34;echo \\\u0026#34;Start date is $START_DATE \\\u0026#34; \u0026amp;\u0026amp; echo \\\u0026#34;End date is $END_DATE\\\u0026#34;\u0026#34;, ) bash_task1 \u0026gt;\u0026gt; bash_task2 DAG 실행 후 bash_task1 의 실행 로그에서 data_interval_end 가 시간대를 포함하여 전체 출력된 것을 확인 bash_task2 의 실행 로그에서는 data_interval_start 와 data_interval_end 이 YYYY-MM-DD 형태의 문자열로 출력된 것을 확인 Copy bash # bash_task1 [2025-06-01, 19:59:27] INFO - DAG bundles loaded: dags-folder: source=\u0026#34;airflow.dag_processing.bundles.manager.DagBundlesManager\u0026#34; [2025-06-01, 19:59:27] INFO - Filling up the DagBag from /opt/airflow/dags/bash_template.py: source=\u0026#34;airflow.models.dagbag.DagBag\u0026#34; [2025-06-01, 19:59:27] INFO - Running command: [\u0026#39;/usr/bin/bash\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo \u0026#34;End date is 2025-05-31 15:00:00+00:00\u0026#34;\u0026#39;]: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; [2025-06-01, 19:59:27] INFO - Output:: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; [2025-06-01, 19:59:27] INFO - End date is 2025-05-31 15:00:00+00:00: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; [2025-06-01, 19:59:27] INFO - Command exited with return code 0: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; [2025-06-01, 19:59:27] INFO - Pushing Copy bash [2025-06-01, 19:59:28] INFO - DAG bundles loaded: dags-folder: source=\u0026#34;airflow.dag_processing.bundles.manager.DagBundlesManager\u0026#34; [2025-06-01, 19:59:28] INFO - Filling up the DagBag from /opt/airflow/dags/bash_template.py: source=\u0026#34;airflow.models.dagbag.DagBag\u0026#34; [2025-06-01, 19:59:28] INFO - Running command: [\u0026#39;/usr/bin/bash\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo \u0026#34;Start date is $START_DATE \u0026#34; \u0026amp;\u0026amp; echo \u0026#34;End date is $END_DATE\u0026#34;\u0026#39;]: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; [2025-06-01, 19:59:28] INFO - Output:: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; [2025-06-01, 19:59:28] INFO - Start date is 2025-05-31: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; [2025-06-01, 19:59:28] INFO - End date is 2025-05-31: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; [2025-06-01, 19:59:28] INFO - Command exited with return code 0: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; [2025-06-01, 19:59:28] INFO - Pushing PythonOperator (1) # keyword argument로 전달되는 Jinja 템플릿 변수를 출력하는 명령어를 실행하는 DAG 구성 이전에 한번 **kwargs 내용을 출력한적이 있었는데, 직접 전달하지 않았음에도 출력되었던 값들이 바로 Jinja 템플릿 변수에 해당 Copy python # dags/python_template1.py from airflow.sdk import DAG, task import pendulum with DAG( dag_id=\u0026#34;python_template1\u0026#34;, schedule=\u0026#34;0 0 * * *\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;template\u0026#34;], ) as dag: @task(task_id=\u0026#34;python_task\u0026#34;) def show_templates(**kwargs): from pprint import pprint pprint(kwargs) show_templates() DAG 실행 후 python_task 의 실행 로그에서 data_interval_end, data_interval_start 등 Jinja 템플릿 변수가 출력된 것을 확인 Copy bash [2025-06-01, 20:18:19] INFO - DAG bundles loaded: dags-folder: source=\u0026#34;airflow.dag_processing.bundles.manager.DagBundlesManager\u0026#34; [2025-06-01, 20:18:19] INFO - Filling up the DagBag from /opt/airflow/dags/python_template.py: source=\u0026#34;airflow.models.dagbag.DagBag\u0026#34; [2025-06-01, 20:18:19] INFO - {\u0026#39;conn\u0026#39;: \u0026lt;ConnectionAccessor (dynamic access)\u0026gt;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;dag\u0026#39;: \u0026lt;DAG: python_template\u0026gt;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;dag_run\u0026#39;: DagRun(dag_id=\u0026#39;python_template\u0026#39;, run_id=\u0026#39;scheduled__2025-05-31T15:00:00+00:00\u0026#39;, logical_date=datetime.datetime(2025, 5, 31, 15, 0, tzinfo=TzInfo(UTC)), data_interval_start=datetime.datetime(2025, 5, 31, 15, 0, tzinfo=TzInfo(UTC)), data_interval_end=datetime.datetime(2025, 5, 31, 15, 0, tzinfo=TzInfo(UTC)), run_after=datetime.datetime(2025, 5, 31, 15, 0, tzinfo=TzInfo(UTC)), start_date=datetime.datetime(2025, 6, 1, 11, 18, 19, 250768, tzinfo=TzInfo(UTC)), end_date=None, clear_number=0, run_type=\u0026lt;DagRunType.SCHEDULED: \u0026#39;scheduled\u0026#39;\u0026gt;, conf={}, consumed_asset_events=[]),: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;data_interval_end\u0026#39;: DateTime(2025, 5, 31, 15, 0, 0, tzinfo=Timezone(\u0026#39;UTC\u0026#39;)),: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;data_interval_start\u0026#39;: DateTime(2025, 5, 31, 15, 0, 0, tzinfo=Timezone(\u0026#39;UTC\u0026#39;)),: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;ds\u0026#39;: \u0026#39;2025-05-31\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;ds_nodash\u0026#39;: \u0026#39;20250531\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;inlet_events\u0026#39;: InletEventsAccessors(_inlets=[], _assets={}, _asset_aliases={}),: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;inlets\u0026#39;: [],: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;logical_date\u0026#39;: DateTime(2025, 5, 31, 15, 0, 0, tzinfo=Timezone(\u0026#39;UTC\u0026#39;)),: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;macros\u0026#39;: \u0026lt;MacrosAccessor (dynamic access to macros)\u0026gt;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;map_index_template\u0026#39;: None,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;outlet_events\u0026#39;: \u0026lt;airflow.sdk.execution_time.context.OutletEventAccessors object at 0xffffaacaa810\u0026gt;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;outlets\u0026#39;: [],: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;params\u0026#39;: {},: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;prev_data_interval_end_success\u0026#39;: \u0026lt;Proxy at 0xffffaad2b140 with factory \u0026lt;function RuntimeTaskInstance.get_template_context.\u0026lt;locals\u0026gt;.\u0026lt;lambda\u0026gt; at 0xffffaad3cfe0\u0026gt;\u0026gt;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - Done. Returned value was: None: source=\u0026#34;airflow.task.operators.airflow.providers.standard.decorators.python._PythonDecoratedOperator\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;prev_data_interval_start_success\u0026#39;: \u0026lt;Proxy at 0xffffaad2b0b0 with factory \u0026lt;function RuntimeTaskInstance.get_template_context.\u0026lt;locals\u0026gt;.\u0026lt;lambda\u0026gt; at 0xffffaad3cea0\u0026gt;\u0026gt;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;prev_end_date_success\u0026#39;: \u0026lt;Proxy at 0xffffaacde870 with factory \u0026lt;function RuntimeTaskInstance.get_template_context.\u0026lt;locals\u0026gt;.\u0026lt;lambda\u0026gt; at 0xffffaad0c7c0\u0026gt;\u0026gt;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;prev_start_date_success\u0026#39;: \u0026lt;Proxy at 0xffffaacde8d0 with factory \u0026lt;function RuntimeTaskInstance.get_template_context.\u0026lt;locals\u0026gt;.\u0026lt;lambda\u0026gt; at 0xffffaad0c720\u0026gt;\u0026gt;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;run_id\u0026#39;: \u0026#39;scheduled__2025-05-31T15:00:00+00:00\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;task\u0026#39;: \u0026lt;Task(_PythonDecoratedOperator): python_task\u0026gt;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;task_instance\u0026#39;: RuntimeTaskInstance(id=UUID(\u0026#39;01972b36-c56a-7d54-b52e-45bb8feb6594\u0026#39;), task_id=\u0026#39;python_task\u0026#39;, dag_id=\u0026#39;python_template\u0026#39;, run_id=\u0026#39;scheduled__2025-05-31T15:00:00+00:00\u0026#39;, try_number=1, map_index=-1, hostname=\u0026#39;f6f932b48199\u0026#39;, context_carrier={}, task=\u0026lt;Task(_PythonDecoratedOperator): python_task\u0026gt;, bundle_instance=LocalDagBundle(name=dags-folder), max_tries=0, start_date=datetime.datetime(2025, 6, 1, 11, 18, 19, 315990, tzinfo=TzInfo(UTC)), end_date=None, state=\u0026lt;TaskInstanceState.RUNNING: \u0026#39;running\u0026#39;\u0026gt;, is_mapped=False, rendered_map_index=None),: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;task_instance_key_str\u0026#39;: \u0026#39;python_template__python_task__20250531\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;task_reschedule_count\u0026#39;: 0,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;templates_dict\u0026#39;: None,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;ti\u0026#39;: RuntimeTaskInstance(id=UUID(\u0026#39;01972b36-c56a-7d54-b52e-45bb8feb6594\u0026#39;), task_id=\u0026#39;python_task\u0026#39;, dag_id=\u0026#39;python_template\u0026#39;, run_id=\u0026#39;scheduled__2025-05-31T15:00:00+00:00\u0026#39;, try_number=1, map_index=-1, hostname=\u0026#39;f6f932b48199\u0026#39;, context_carrier={}, task=\u0026lt;Task(_PythonDecoratedOperator): python_task\u0026gt;, bundle_instance=LocalDagBundle(name=dags-folder), max_tries=0, start_date=datetime.datetime(2025, 6, 1, 11, 18, 19, 315990, tzinfo=TzInfo(UTC)), end_date=None, state=\u0026lt;TaskInstanceState.RUNNING: \u0026#39;running\u0026#39;\u0026gt;, is_mapped=False, rendered_map_index=None),: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;triggering_asset_events\u0026#39;: TriggeringAssetEventsAccessor(_events=defaultdict(\u0026lt;class \u0026#39;list\u0026#39;\u0026gt;, {})),: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;ts\u0026#39;: \u0026#39;2025-05-31T15:00:00+00:00\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;ts_nodash\u0026#39;: \u0026#39;20250531T150000\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;ts_nodash_with_tz\u0026#39;: \u0026#39;20250531T150000+0000\u0026#39;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;var\u0026#39;: {\u0026#39;json\u0026#39;: \u0026lt;VariableAccessor (dynamic access)\u0026gt;,: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-01, 20:18:19] INFO - \u0026#39;value\u0026#39;: \u0026lt;VariableAccessor (dynamic access)\u0026gt;}}: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; PythonOperator (2) # 이번에는 PythonOperator에 Jinja 템플릿 변수를 전달하여 출력하는 python_task1 정의 keyword argument로 전달되는 Jinja 템플릿 변수 중 일부 항목만 선택해서 출력하는 python_task2 정의 Copy python # dags/python_template2.py from airflow.sdk import DAG, task import pendulum with DAG( dag_id=\u0026#34;python_template2\u0026#34;, schedule=\u0026#34;0 0 * * *\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;template\u0026#34;], ) as dag: def print_period(start_date, end_date, **kwargs): print(start_date) print(end_date) python_task1 = PythonOperator( task_id=\u0026#34;python_task1\u0026#34;, python_callable=print_period, op_kwargs={ \u0026#34;start_date\u0026#34;: \u0026#34;{{ data_interval_start | ds }}\u0026#34;, \u0026#34;end_date\u0026#34;: \u0026#34;{{ data_interval_end | ds }}\u0026#34; }, ) @task(task_id=\u0026#34;python_task2\u0026#34;) def python_task2(**kwargs): for __key in [\u0026#34;ds\u0026#34;, \u0026#34;ts\u0026#34;, \u0026#34;data_interval_start\u0026#34;, \u0026#34;data_interval_end\u0026#34;]: if __key in kwargs: print(f\u0026#34;{__key}: {kwargs[__key]}\u0026#34;) python_task1 \u0026gt;\u0026gt; python_task2() DAG 실행 후 python_task1 의 실행 로그에서 전달한 data_interval_start, data_interval_end 값이 YYYY-MM-DD 형태의 문자열로 출력된 것을 확인 python_task2 의 실행 로그에서는 ds, ts, data_interval_start, data_interval_end 값을 keyword argument로부터 꺼내서 그대로 출력 Copy bash # python_task1 [2025-06-02, 00:12:27] INFO - DAG bundles loaded: dags-folder: source=\u0026#34;airflow.dag_processing.bundles.manager.DagBundlesManager\u0026#34; [2025-06-02, 00:12:27] INFO - Filling up the DagBag from /opt/airflow/dags/python_template2.py: source=\u0026#34;airflow.models.dagbag.DagBag\u0026#34; [2025-06-02, 00:12:27] INFO - Done. Returned value was: None: source=\u0026#34;airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator\u0026#34; [2025-06-02, 00:12:27] INFO - 2025-06-01: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-02, 00:12:27] INFO - 2025-06-01: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; Copy bash # python_task2 [2025-06-02, 00:12:27] INFO - DAG bundles loaded: dags-folder: source=\u0026#34;airflow.dag_processing.bundles.manager.DagBundlesManager\u0026#34; [2025-06-02, 00:12:27] INFO - Filling up the DagBag from /opt/airflow/dags/python_template2.py: source=\u0026#34;airflow.models.dagbag.DagBag\u0026#34; [2025-06-02, 00:12:27] INFO - Done. Returned value was: None: source=\u0026#34;airflow.task.operators.airflow.providers.standard.decorators.python._PythonDecoratedOperator\u0026#34; [2025-06-02, 00:12:27] INFO - ds: 2025-06-01: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-02, 00:12:27] INFO - ts: 2025-06-01T15:00:00+00:00: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-02, 00:12:27] INFO - data_interval_start: 2025-06-01 15:00:00+00:00: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-02, 00:12:27] INFO - data_interval_end: 2025-06-01 15:00:00+00:00: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; Macro 변수 # Templates reference \u0026amp;mdash; Airflow 3.1.3 Documentationairflow.apache.org Jinja 템플릿 변수 기반으로 다양한 날짜 연산이 가능하도록 연산 모듈을 제공 Macro 변수 목록 # macros.datetime : datetime.datetime 라이브러리 기반 연산 제공 macros.timedelta : datetime.timedelta 라이브러리 기반 연산 제공 macros.dateutil : dateutil 라이브러리 기반 연산 제공 Copy python import datetime as dt from dateutil.relativedelta import relativedelta today = dt.date.today() # 1일로 변경 first_date = today.replace(day=1) # datetime 연산 first_date = today + relativedelta(day=1) # dateutil 연산 # 1일 빼기 yesterday = today - dt.timedela(days=1) # timedela 연산 yesterday = today - relativedelta(days=1) # dateutil 연산 Macro 변수 활용 예시 # 첫 번째 DAG bash_macros1 은 매월 말에 실행되도록 스케줄을 지정하고, 직전 배치일에서 1일을 추가한 날짜를 START_DATE, 배치일을 END_DATE 로 설정하여 1일부터 말일이 출력되기를 기대 Copy python # dags/bash_macros1.py from airflow.sdk import DAG from airflow.providers.standard.operators.bash import BashOperator import pendulum with DAG( dag_id=\u0026#34;bash_macros1\u0026#34;, schedule=\u0026#34;0 0 L * *\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;template\u0026#34;], ) as dag: bash_task1 = BashOperator( task_id=\u0026#34;bash_task1\u0026#34;, env={ \u0026#34;START_DATE\u0026#34;: \u0026#34;{{ (data_interval_start.in_timezone(\\\u0026#34;Asia/Seoul\\\u0026#34;) + macros.dateutil.relativedelta.relativedelta(days=1)) | ds }}\u0026#34;, \u0026#34;END_DATE\u0026#34;: \u0026#34;{{ data_interval_end.in_timezone(\\\u0026#34;Asia/Seoul\\\u0026#34;) | ds }}\u0026#34; }, bash_command=\u0026#34;echo \\\u0026#34;Start date is $START_DATE \\\u0026#34; \u0026amp;\u0026amp; echo \\\u0026#34;End date is $END_DATE\\\u0026#34;\u0026#34;, ) bash_task1 두 번째 DAG bash_macros2 는 매월 둘째주 토요일에 실행되도록 스케줄을 지정하고, 직전 배치일과 배치일을 1일로 변경해서 전월 1일과 당월 1일이 출력되기를 기대 Copy python # dags/bash_macros2.py from airflow.sdk import DAG from airflow.providers.standard.operators.bash import BashOperator import pendulum with DAG( dag_id=\u0026#34;bash_macros2\u0026#34;, schedule=\u0026#34;0 0 * * 6#2\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;template\u0026#34;], ) as dag: bash_task2 = BashOperator( task_id=\u0026#34;bash_task2\u0026#34;, env={ \u0026#34;START_DATE\u0026#34;: \u0026#34;{{ (data_interval_end.in_timezone(\\\u0026#34;Asia/Seoul\\\u0026#34;) + macros.dateutil.relativedelta.relativedelta(day=1)) | ds }}\u0026#34;, \u0026#34;END_DATE\u0026#34;: \u0026#34;{{ (data_interval_end.in_timezone(\\\u0026#34;Asia/Seoul\\\u0026#34;) + macros.dateutil.relativedelta.relativedelta(day=1)) | ds }}\u0026#34; }, bash_command=\u0026#34;echo \\\u0026#34;Start date is $START_DATE \\\u0026#34; \u0026amp;\u0026amp; echo \\\u0026#34;End date is $END_DATE\\\u0026#34;\u0026#34;, ) bash_task2 하지만 실행 로그를 확인했을 때 기대와 다른 결과가 확인되었는데, 마치 data_interval_start, data_interval_end 가 동일한 값을 가지고 있다고 생각됨 Copy bash # bash_task1 (bash_macros1) [2025-06-03, 10:54:43] INFO - Start date is 2025-06-01: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; [2025-06-03, 10:54:43] INFO - End date is 2025-05-31: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; Copy bash # bash_task2 (bash_macros2) [2025-06-03, 10:57:23] INFO - Start date is 2025-05-01: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; [2025-06-03, 10:57:23] INFO - End date is 2025-05-01: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; 실제로 bash_macros1 의 마지막 실행 내역에 대해 Details 탭에서 실행 정보를 조회했을 때, data_interval_start, data_interval_end 값이 모두 동일한 배치일로 나타나는 것을 확인 참고 자료로 활용한 강의에서 사용했던 Airflow 2.x 버전과, 현재 사용하는 Airflow 3.x 버전에서 data_interval_start, data_interval_end 를 결정하는 기준이 변경된 것을 인지 Airflow 3.0 업데이트 # 2.9 버전에서 data_interval 계산 알고리즘에 영향을 주는 create_cron_data_intervals 파라미터가 도입되었는데, 3.0 버전부터 기본값이 기존 False 에서 True 로 변경되면서, 기존 CronDataIntervalTimetable 대신 CronTriggerTimetable 알고리즘이 사용되도록 변경됨 즉, 3.0 버전부터 기본적으로 data_interval 을 고려하지 않도록 변경되어 data_interval_start 와 data_interval_end 값이 모두 실제 DAG이 실행된 날짜로 표현 The create_cron_data_intervals configuration is now False by default. This means that the CronTriggerTimetable will be used by default instead of the CronDataIntervalTimetable\nUpgrading to Airflow 3 \u0026amp;mdash; Airflow 3.1.3 Documentationairflow.apache.org CronDataIntervalTimetable # 이전 버전의 알고리즘인 CronDataIntervalTimetable 의 경로를 파악해서 bash_macros1 DAG의 스케줄을 재설정 실행 로그를 조회했을 때, 초기 의도대로 START_DATE 는 배치일 기준 1일, END_DATE 는 배치일인 말일이 출력되는 것을 확인 Copy python # dags/bash_macros1.py from airflow.sdk import DAG from airflow.providers.standard.operators.bash import BashOperator from airflow.timetables.interval import CronDataIntervalTimetable import pendulum with DAG( dag_id=\u0026#34;bash_macros1\u0026#34;, schedule=CronDataIntervalTimetable(\u0026#34;0 0 L * *\u0026#34;, timezone=\u0026#34;Asia/Seoul\u0026#34;), start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;template\u0026#34;], ) as dag: bash_task1 = BashOperator( task_id=\u0026#34;bash_task1\u0026#34;, env={ \u0026#34;START_DATE\u0026#34;: \u0026#34;{{ (data_interval_start.in_timezone(\\\u0026#34;Asia/Seoul\\\u0026#34;) + macros.dateutil.relativedelta.relativedelta(days=1)) | ds }}\u0026#34;, \u0026#34;END_DATE\u0026#34;: \u0026#34;{{ data_interval_end.in_timezone(\\\u0026#34;Asia/Seoul\\\u0026#34;) | ds }}\u0026#34; }, bash_command=\u0026#34;echo \\\u0026#34;Start date is $START_DATE \\\u0026#34; \u0026amp;\u0026amp; echo \\\u0026#34;End date is $END_DATE\\\u0026#34;\u0026#34;, ) bash_task1 Copy bash # bash_task1 (bash_macros1) [2025-06-03, 12:10:27] INFO - Start date is 2025-05-01: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; [2025-06-03, 12:10:27] INFO - End date is 2025-05-31: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; CronTriggerTimetable # 3.0 버전 이후에서 사용되는 CronTriggerTimetable 알고리즘으로도 data_interval 을 적용할 수 있는 방법이 있는데, interval 파라미터로 timedelta 를 전달하면 가능 bash_macros2 DAG의 스케줄에 CronTriggerTimetable 알고리즘을 적용하면서, interval 파라미터로 1주의 간격을 지정 (초기 의도인 매월 둘째주 토요일과는 다르게 매주 토요일로 변경) 실행 로그를 조회했을 때, END_DATE 는 배치일인 5월 31일, START_DATE 는 직전 배치일인 5월 24일이 출력되는 것을 확인 Copy python # dags/bash_macros2.py from airflow.sdk import DAG from airflow.providers.standard.operators.bash import BashOperator from airflow.timetables.trigger import CronTriggerTimetable import datetime as dt import pendulum with DAG( dag_id=\u0026#34;bash_macros2\u0026#34;, schedule=CronTriggerTimetable( \u0026#34;0 0 * * 6\u0026#34;, timezone=\u0026#34;Asia/Seoul\u0026#34;, interval=dt.timedelta(weeks=1), ), start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;template\u0026#34;], ) as dag: bash_task2 = BashOperator( task_id=\u0026#34;bash_task2\u0026#34;, env={ \u0026#34;START_DATE\u0026#34;: \u0026#34;{{ data_interval_start.in_timezone(\\\u0026#34;Asia/Seoul\\\u0026#34;) | ds }}\u0026#34;, \u0026#34;END_DATE\u0026#34;: \u0026#34;{{ data_interval_end.in_timezone(\\\u0026#34;Asia/Seoul\\\u0026#34;) | ds }}\u0026#34; }, bash_command=\u0026#34;echo \\\u0026#34;Start date is $START_DATE \\\u0026#34; \u0026amp;\u0026amp; echo \\\u0026#34;End date is $END_DATE\\\u0026#34;\u0026#34;, ) bash_task2 Copy bash # bash_task2 (bash_macros2) [2025-06-03, 12:20:03] INFO - Start date is 2025-05-24: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; [2025-06-03, 12:20:03] INFO - End date is 2025-05-31: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; XCom # XComs \u0026amp;mdash; Airflow 3.1.3 Documentationairflow.apache.org Cross Communication이란 의미로, Airflow DAG 내 Task 간 데이터 공유를 위해 사용되는 기술 주로 작은 규모의 데이터 공유를 위해 사용 (XCom 내용은 메타 DB의 xcom 테이블에 값이 저장) 1GB 이상의 대용량 데이터 공유를 위해서는 외부 솔루션 사용 필요 XCom 사용법 # keyword arguments로 전달되는 ti (task_instance) 객체를 활용 Copy python @task(task_id=\u0026#34;task1\u0026#34;) def task1(**kwargs): ti = kwargs[\u0026#34;ti\u0026#34;] ti.xcom_push(key=\u0026#34;key1\u0026#34;, value=\u0026#34;value1\u0026#34;) @task(task_id=\u0026#34;task2\u0026#34;) def task2(**kwargs): ti = kwargs[\u0026#34;ti\u0026#34;] value1 = ti.xcom_pull(key=\u0026#34;key1\u0026#34;) print(value1) 만약 서로 다른 Task 에서 동일한 키값을 push 한 후, 단순히 해당 키값을 pull로 꺼낼 때, 가장 마지막에 push된 키값이 반환 안전하게 키값을 꺼내오기 위해서는 대상 Task를 가리키는 task_ids 파라미터를 명시할 수 있음 주의) Airflow 3.0 버전에서는 task_ids 가 반드시 명시되어야 함\nCopy python @task(task_id=\u0026#34;task2\u0026#34;) def task2(**kwargs): ti = kwargs[\u0026#34;ti\u0026#34;] value1 = ti.xcom_pull(key=\u0026#34;key1\u0026#34;, task_ids=\u0026#34;task1\u0026#34;) print(value1) return 값 활용 # @task 데코레이터 사용 시 return 값은 자동으로 XCom에 return_value 키로 저장 다음 단계의 Task에서 이전 단계의 return 값을 꺼낼 수 있음 Copy python @task(task_id=\u0026#34;task1\u0026#34;) def task1(**kwargs): return \u0026#34;value1\u0026#34; @task(task_id=\u0026#34;task2\u0026#34;) def task2(**kwargs): ti = kwargs[\u0026#34;ti\u0026#34;] value1 = ti.xcom_pull(key=\u0026#34;return_value\u0026#34;, task_ids=\u0026#34;task1\u0026#34;) print(value1) task1() \u0026gt;\u0026gt; task2() 또는, 데코레이터 사용 시 함수의 출력값을 다음 함수의 입력값으로 직접 전달하는 표현을 통해 return 값을 인수로 전달할 수도 있음 Copy python @task(task_id=\u0026#34;task1\u0026#34;) def task1(**kwargs): return \u0026#34;value1\u0026#34; @task(task_id=\u0026#34;task2\u0026#34;) def task2(value1, **kwargs): print(value1) task2(task1()) Xcom 활용 # PythonOperator (1) # 앞서 서술한 코드를 DAG 안에서 Task로 구현 두 개의 xcom_push_task 에서 동일한 키값을 XCom에 push하고, xcom_pull_task 에서 Xcom으로부터 키값을 pull하여 출력 Copy python # dags/python_xcom1.py from airflow.sdk import DAG, task from airflow.models.taskinstance import TaskInstance import pendulum with DAG( dag_id=\u0026#34;python_xcom1\u0026#34;, schedule=\u0026#34;0 0 * * *\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;xcom\u0026#34;], ) as dag: @task(task_id=\u0026#34;xcom_push_task1\u0026#34;) def xcom_push_task1(ti: TaskInstance, **kwargs): ti.xcom_push(key=\u0026#34;key1\u0026#34;, value=\u0026#34;value1\u0026#34;) ti.xcom_push(key=\u0026#34;key2\u0026#34;, value=[1,2,3]) @task(task_id=\u0026#34;xcom_push_task2\u0026#34;) def xcom_push_task2(ti: TaskInstance, **kwargs): ti.xcom_push(key=\u0026#34;key1\u0026#34;, value=\u0026#34;value2\u0026#34;) ti.xcom_push(key=\u0026#34;key2\u0026#34;, value=[4,5,6]) @task(task_id=\u0026#34;xcom_pull_task\u0026#34;) def xcom_pull_task(ti: TaskInstance, **kwargs): value1 = ti.xcom_pull(key=\u0026#34;key1\u0026#34;) value2 = ti.xcom_pull(key=\u0026#34;key2\u0026#34;, task_ids=\u0026#34;xcom_push_task1\u0026#34;) print(value1) print(value2) xcom_push_task1() \u0026gt;\u0026gt; xcom_push_task2() \u0026gt;\u0026gt; xcom_pull_task() 두 개의 xcom_push_task 실행 내역의 XCom 탭에서 key1 과 key2 에 대한 값이 지정됨을 확인 xcom_pull_task 에서는 task_ids 를 지정하지 않았을 때 마지막으로 push된 \u0026ldquo;value2\u0026quot;가 출력될 것을 기대했지만, Airflow 3.0에서 발생한 업데이트로 인해 None 값이 출력 Copy bash # xcom_pull_task [2025-06-03, 15:27:41] INFO - None: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-06-03, 15:27:41] INFO - [1, 2, 3]: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; Airflow 3.0 업데이트 # Airflow 3.0부터는 task_ids 를 반드시 명시하도록 변경됨 kwargs[\u0026quot;ti\u0026quot;].xcom_pull(key=\u0026quot;key\u0026quot;) 와 같은 구문은 더 이상 작동하지 않음 In Airflow 2, the xcom_pull() method allowed pulling XComs by key without specifying task_ids, \u0026hellip;, leading to unpredictable behavior. Airflow 3 resolves this inconsistency by requiring task_ids when pulling by key.\nRelease Notes \u0026amp;mdash; Airflow 3.1.3 Documentationairflow.apache.org PythonOperator (2) # xcom_return_task 에서 문자열 \u0026ldquo;Success\u0026quot;를 반환하고, 두 개의 xcom_pull_task 에서 서로 다른 방식으로 return 값을 받아 출력 Copy python # dags/python_xcom2 from airflow.sdk import DAG, task from airflow.models.taskinstance import TaskInstance import pendulum with DAG( dag_id=\u0026#34;python_xcom2\u0026#34;, schedule=\u0026#34;0 0 * * *\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;xcom\u0026#34;], ) as dag: @task(task_id=\u0026#34;xcom_return_task\u0026#34;) def xcom_return_task(**kwargs) -\u0026gt; str: return \u0026#34;Success\u0026#34; @task(task_id=\u0026#34;xcom_pull_task1\u0026#34;) def xcom_pull_task1(ti: TaskInstance, **kwargs): status = ti.xcom_pull(key=\u0026#34;return_value\u0026#34;, task_ids=\u0026#34;xcom_return_task\u0026#34;) print(f\u0026#34;\\\u0026#34;xcom_return_task\\\u0026#34; 함수의 리턴 값: {status}\u0026#34;) @task(task_id=\u0026#34;xcom_pull_task2\u0026#34;) def xcom_pull_task2(status: str, **kwargs): print(f\u0026#34;\\\u0026#34;xcom_return_task\\\u0026#34; 함수로부터 전달받은 값: {status}\u0026#34;) return_value = xcom_return_task() return_value \u0026gt;\u0026gt; xcom_pull_task1() xcom_pull_task2(return_value) 두 개의 xcom_pull_task 에서 모두 정상적으로 return 값을 받아서 동일한 결과가 출력됨을 확인 Copy bash # xcom_pull_task1 [2025-06-03, 15:36:10] INFO - \u0026#34;xcom_return_task\u0026#34; 함수의 리턴 값: \u0026#34;Success\u0026#34;: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; Copy bash # xcom_pull_task2 [2025-06-03, 15:36:10] INFO - \u0026#34;xcom_return_task\u0026#34; 함수로부터 전달받은 값: \u0026#34;Success\u0026#34;: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; BashOperator # Jinja 템플릿 문법을 통해 ti.xcom_push 또는 ti.xcom_pull 사용이 가능 마지막 출력문은 자동으로 return_value 로 전달 Copy python # dags/bash_xcom.py from airflow.sdk import DAG from airflow.providers.standard.operators.bash import BashOperator import pendulum with DAG( dag_id=\u0026#34;bash_xcom\u0026#34;, schedule=\u0026#34;0 0 * * *\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;xcom\u0026#34;], ) as dag: bash_push_task = BashOperator( task_id=\u0026#34;bash_push_task\u0026#34;, bash_command=\u0026#34;echo START \u0026amp;\u0026amp; echo XCOM PUSHED {{ ti.xcom_push(key=\u0026#39;bash_pushed\u0026#39;, value=\u0026#39;bash_message\u0026#39;) }} \u0026amp;\u0026amp; echo COMPLETE\u0026#34;, ) bash_pull_task = BashOperator( task_id=\u0026#34;bash_pull_task\u0026#34;, env={ \u0026#34;PUSHED_VALUE\u0026#34;: \u0026#34;{{ ti.xcom_pull(key=\u0026#39;bash_pushed\u0026#39;, task_ids=\u0026#39;bash_push_task\u0026#39;) }}\u0026#34;, \u0026#34;RETURN_VALUE\u0026#34;: \u0026#34;{{ ti.xcom_pull(key=\u0026#39;return_value\u0026#39;, task_ids=\u0026#39;bash_push_task\u0026#39;) }}\u0026#34; }, bash_command=\u0026#34;echo $PUSHED_VALUE \u0026amp;\u0026amp; echo $RETURN_VALUE\u0026#34;, ) bash_push_task \u0026gt;\u0026gt; bash_pull_task bash_push_task 의 실행 내역에서 직접 push한 bash_pushed 가 XCom에 들어있고, 마지막 출력문도 return_value 로 저장되어 있음을 조회 bash_pull_task 에서 첫 번째로는 XCom에서 bash_pushed 키를 가지고 꺼낸 PUSHED_VALUE 값을 출력하여, 실행 로그에 \u0026ldquo;bash_message\u0026rdquo; 가 출력됨을 확인 두 번째로는 Xcom에서 return_value 키를 가지고 꺼낸 RETURN_VALUE 값을 출력하여, 실행 로그에 bash_push_task 의 마지막 출력문 \u0026ldquo;COMPLETE\u0026rdquo; 가 출력됨을 확인 Copy bash # bash_pull_task [2025-06-03, 16:05:09] INFO - bash_message: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; [2025-06-03, 16:05:09] INFO - COMPLETE: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; Python → Bash 전달 # PythonOperator에서 딕셔너리 객체를 반환했을 경우, BashOperator에서 XCom을 통해 딕셔너리 내 특정 값을 꺼낼 수 있음 반대로, BashOperator에서 push한 값 또는 마지막 출력문을 PythonOperator에서 XCom을 통해 꺼낼 수도 있음 Copy python @task(task_id=\u0026#34;python_push\u0026#34;) def python_push_xcom(): return {\u0026#34;status\u0026#34;:\u0026#34;Success\u0026#34;, \u0026#34;data\u0026#34;:[1,2,3]} bash_pull = BashOperator( task_id=\u0026#34;bash_pull\u0026#34;, env={ \u0026#34;STATUS\u0026#34;: \u0026#34;{{ ti.xcom_pull(key=\\\u0026#34;return_value\\\u0026#34;, task_ids=\\\u0026#34;python_push\\\u0026#34;)[\\\u0026#34;status\\\u0026#34;] }}\u0026#34;, \u0026#34;DATA\u0026#34;: \u0026#34;{{ ti.xcom_pull(key=\\\u0026#34;return_value\\\u0026#34;, task_ids=\\\u0026#34;python_push\\\u0026#34;)[\\\u0026#34;data\\\u0026#34;] }}\u0026#34; }, bash_command=\u0026#34;echo $STATUS \u0026amp;\u0026amp; echo $DATA\u0026#34; ) Variable # Variables \u0026amp;mdash; Airflow 3.1.3 Documentationairflow.apache.org 모든 DAG에서 공유하는 전역 변수 Airflow UI에서 Admin 메뉴를 통해 접근 및 생성 가능 Variable 가져오기 # Variable 라이브러리를 통해 전역 변수를 꺼내는 방법 해당 방법은 DAG를 파싱할 때마다 DB 연결을 발생시켜 불필요한 부하가 발생 (스케줄러 과부하 원인) Copy python from airflow.models import Variable var = Variable.get(\u0026#34;key\u0026#34;) Jinja 템플릿을 이용해 Operator 내부에서 가져오는 방법 실제 실행할때만 DB에 접근하기 때문에 상대적으로 부하가 적음 (Airflow에서 권장하는 방법) Copy python from airflow.operators.bash import BashOperator bash_task = BashOperator( task_id=\u0026#34;bash_task\u0026#34;, bash_command=f\u0026#34;echo {{var.value.key}}\u0026#34; ) Variable 활용 # 앞서 서술한 코드를 DAG 안에서 Task로 구현 첫 번째 Task에서는 Variable 라이브러리로 꺼낸 전역 변수를 출력하고, 두 번째 Task에서는 Jinja 템플릿을 이용해 전역 변수를 출력 Copy python # dags/bash_variable.py from airflow.sdk import DAG, Variable from airflow.providers.standard.operators.bash import BashOperator import pendulum with DAG( dag_id=\u0026#34;bash_variable\u0026#34;, schedule=\u0026#34;0 0 * * *\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;xcom\u0026#34;], ) as dag: var = Variable.get(\u0026#34;sample_key\u0026#34;) bash_var_task1 = BashOperator( task_id=\u0026#34;bash_var_task1\u0026#34;, bash_command=f\u0026#34;echo variable: \\\u0026#34;{var}\\\u0026#34;\u0026#34;, ) bash_var_task2 = BashOperator( task_id=\u0026#34;bash_var_task2\u0026#34;, bash_command=\u0026#34;echo variable: \\\u0026#34;{{ var.value.sample_key }}\\\u0026#34;\u0026#34;, ) 기대했던 것과 달리, 실행 로그에서는 전역 변수가 마스킹 처리되어 출력 airflow.cfg 설정에서 sensitive_var_conn_names 항목을 확인해보고, Web UI 컨테이너에 들어가서 airflow variables get 명령어로 전역 변수가 마스킹된 채로 저장되어 있는지도 확인해보고, BashOperator 안에서 비교 연산자로 설정한 것과 동일한 값이 가져와지는지도 출력해서 확인해봤는데, 모두 정상적이고 실행 로그에서 전역 변수를 직접 출력할 때만 마스킹 처리됨 어떻게든 출력해보려고 했지만, 모든 사람이 접근할 수 있는 환경 변수를 평문으로 출력시키지 않으려는 의도가 있다고 짐작하고 추가적인 시도를 중지함 Copy bash # bash_var_task1 [2025-06-03, 16:52:07] INFO - variable: ***: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; Masking sensitive data \u0026amp;mdash; Airflow 3.1.3 Documentationairflow.apache.org "},{"id":19,"href":"/blog/airflow-study-2/","title":"Apache Airflow - Operator (Bash, Python)","section":"Posts","content":"Operator란? # 특정 행위를 할 수 있는 기능을 모아 놓은 클래스 Task : Operator를 객체화하여 DAG에서 실행 가능한 오브젝트 Bash Operator : 쉘 스크립트 명령을 수행하는 Operator Python Operator : 파이썬 함수를 실행하는 Operator 개발 환경 설정 # Git \u0026amp; Github # Copy bash % git init % git add .gitignore % git commit -m \u0026#34;init\u0026#34; % git remote add origin https://github.com/\u0026lt;username\u0026gt;/\u0026lt;repository\u0026gt; % git push Git 설치 및 Github에 리포지토리를 생성 로컬 Airflow 설치 경로에서 Git 저장소를 생성하고 원격 저장소와 연결 Airflow에서 제공하는 .gitignore 를 로컬 Airflow 설치 경로에 복제하여 불필요한 /logs 경로 등을 Git에서 제외 airflow/.gitignore at main · apache/airflow Apache Airflow - A platform to programmatically author, schedule, and monitor workflows - airflow/.gitignore at main · … GitHub 디렉토리 구조 # Copy bash % tree -a -F . ./ ├── .env ├── .git/ ├── .gitignore ├── config/ │ └── airflow.cfg ├── dags/ ├── docker-compose.yaml ├── logs/ └── plugins/ Example DAGs 삭제 # Copy yaml # docker-compose.yaml AIRFLOW__CORE__LOAD_EXAMPLES: \u0026#39;false\u0026#39; 기본 설정으로 Airflow 실행 시 거슬리는 Example DAG를 삭제 컨테이너를 재시작해도 여전히 예제가 남아있어 airflow db reset 등 여러가지 초기화 방법을 탐색했지만, 결과적으로 localhost:8080 에 대한 크롬 브라우저 캐시를 삭제하니 해결 BashOperator # BashOperator \u0026amp;mdash; apache-airflow-providers-standard Documentationairflow.apache.org BashOperator 정의 # dags/ 경로 아래에 bash_operator.py 파일을 생성 Airflow에서 제공하는 예시 example_bash_operator 를 복제 및 일부를 수정하여 DAG 선언 Copy python # dags/bash_operator.py from airflow.sdk import DAG from airflow.providers.standard.operators.bash import BashOperator import pendulum with DAG( dag_id=\u0026#34;bash_operator\u0026#34;, schedule=\u0026#34;0 0 * * *\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;bash\u0026#34;], ) as dag: bash_task1 = BashOperator( task_id=\u0026#34;bash_task1\u0026#34;, bash_command=\u0026#34;echo whoami\u0026#34;, ) bash_task2 = BashOperator( task_id=\u0026#34;bash_task2\u0026#34;, bash_command=\u0026#34;echo $HOSTNAME\u0026#34;, ) bash_task1 \u0026gt;\u0026gt; bash_task2 DAG 파라미터 # dag_id : UI에서 표시되는 DAG 명칭, 관리의 용이성을 위해 파일 명칭과 동일하게 지정 schedule : 크론탭(Crontab)을 사용하여 스케줄 지정 start_date : 작업 시작일을 지정, 예시에는 시간대가 UTC로 적용되어 Asia/Seoul로 변경 catchup : 시작일(start_date)부터 현재까지 실행되지 않은 작업을 일괄로 실행 (True) dagrun_timeout : DAG 실행 시간을 제한 tags : UI에서 DAG를 구별하기 위한 태그 지정 BashOperator 파라미터 # BashOperator 를 사용하여 두 개의 bash 명령어를 실행하는 Task를 정의 task_id : UI에서 표시되는 Task 명칭, 관리의 용이성을 위해 객체명과 동일하게 지정 bash_command : 실행할 bash 명령어 Task 간 \u0026gt;\u0026gt; 기호로 연결하여 종속성을 표시 여러 개의 Task를 묶고 싶을 때는 배열을 사용 (task1 \u0026gt;\u0026gt; [task2, task3]) 종속성은 연속해서 표시 가능 (task1 \u0026gt;\u0026gt; task2 \u0026gt;\u0026gt; task3) airflow/providers/standard/tests/system/standard/example_bash_operator.py at … Apache Airflow - A platform to programmatically author, schedule, and monitor workflows - … GitHub BashOperator 실행 # Airflow 컨테이너를 중지 후 다시 실행해 UI에서 DAG가 올라온 것을 확인 DAG를 실행하여 정상적으로 수행되는지 확인 DAG를 클릭해서 이동하는 페이지에서 bash_task 과 bash_task2 의 관계를 그래프로 조회 가능 bash_task1 과 bash_task2 에 대해 각각의 로그를 확인 bash_task1 에는 echo whoami 명령어가 전달되어 whoami 를 결과로 출력 bash_task2 에는 echo $HOSTNAME 명령어가 전달되어 컨테이너의 HOSTNAME 을 출력 Copy bash # bash_task1 [2025-05-30, 00:16:56] INFO - Running command: [\u0026#39;/usr/bin/bash\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo whoami\u0026#39;]: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; [2025-05-30, 00:16:56] INFO - Output:: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; [2025-05-30, 00:16:56] INFO - whoami: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; Copy bash # bash_task2 [2025-05-30, 00:16:57] INFO - Running command: [\u0026#39;/usr/bin/bash\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $HOSTNAME\u0026#39;]: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; [2025-05-30, 00:16:57] INFO - Output:: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; [2025-05-30, 00:16:57] INFO - wha439072926: source=\u0026#34;airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook\u0026#34; PythonOperator # PythonOperator \u0026amp;mdash; apache-airflow-providers-standard Documentationairflow.apache.org PythonOperator 종류 # PythonOperator : 파이썬 함수를 실행시키기 위한 Operator PythonVirtualenvOperator : 파이썬 가상환경 생성 후 작업을 수행하고 마무리되면 가상환경을 삭제하는 Operator ExternalPythonOperator : 기존에 존재하는 파이썬 가상환경에서 작업을 수행하는 Operator BranchPythonOperator : 파이썬 함수 실행 결과에 따라 다음 Task를 선택적으로 실행시킬 수 있는 Operator BranchPythonVirtualenvOperator : 가상환경 생성/삭제 및 브랜치 처리 기능이 있는 Operator BranchExternalPythonOperator : 기존 가상환경을 사용하면서 브랜치 처리 기능이 있는 Operator PythonOperator 정의 # Copy python # dags/python_operator.py from airflow.sdk import DAG from airflow.providers.standard.operators.python import PythonOperator import pendulum import random with DAG( dag_id=\u0026#34;python_operator\u0026#34;, schedule=\u0026#34;0 0 * * *\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;python\u0026#34;], ) as dag: def select_country(): COUNTRIES = [ \u0026#34;Argentina\u0026#34;, \u0026#34;Australia\u0026#34;, \u0026#34;Brazil\u0026#34;, \u0026#34;Canada\u0026#34;, \u0026#34;China\u0026#34;, \u0026#34;France\u0026#34;, \u0026#34;Germany\u0026#34;, \u0026#34;India\u0026#34;, \u0026#34;Indonesia\u0026#34;, \u0026#34;Italy\u0026#34;, \u0026#34;Japan\u0026#34;, \u0026#34;Mexico\u0026#34;, \u0026#34;Russia\u0026#34;, \u0026#34;Saudi Arabia\u0026#34;, \u0026#34;South Africa\u0026#34;, \u0026#34;South Korea\u0026#34;, \u0026#34;Turkey\u0026#34;, \u0026#34;United Kingdom\u0026#34;, \u0026#34;United States\u0026#34; ] print(random.choice(COUNTRIES)) python_task = PythonOperator( task_id=\u0026#34;python_task\u0026#34;, python_callable=select_country, ) 앞서 BashOperator 예제를 가져오고 Task를 PythonOperator 로 대체 G20 국가 중 랜덤한 국가를 선택해 출력하는 함수 select_country() 를 정의 select_country() 함수를 실행하는 python_task 를 단일 Task로 사용 PythonOperator 실행 # Airflow UI에서 python_operator DAG가 올라온 것을 확인 DAG를 실행하여 정상적으로 수행되는지 확인 python_task 의 첫 번째 실행 로그에서는 \u0026ldquo;Argentina\u0026rdquo; 국가가 선택되어 출력 UI에서 Trigger 버튼을 눌러 python_task 를 수동 실행한 로그에서는 \u0026ldquo;Indonesia\u0026rdquo; 국가가 선택되어 출력 Copy bash # scheduled run (first) [2025-05-31, 11:33:06] INFO - DAG bundles loaded: dags-folder: source=\u0026#34;airflow.dag_processing.bundles.manager.DagBundlesManager\u0026#34; [2025-05-31, 11:33:06] INFO - Filling up the DagBag from /opt/airflow/dags/python_operator.py: source=\u0026#34;airflow.models.dagbag.DagBag\u0026#34; [2025-05-31, 11:33:06] INFO - Done. Returned value was: None: source=\u0026#34;airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator\u0026#34; [2025-05-31, 11:33:06] INFO - Argentina: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; Copy bash # manual run (second) [2025-05-31, 11:36:38] INFO - DAG bundles loaded: dags-folder: source=\u0026#34;airflow.dag_processing.bundles.manager.DagBundlesManager\u0026#34; [2025-05-31, 11:36:38] INFO - Filling up the DagBag from /opt/airflow/dags/python_operator.py: source=\u0026#34;airflow.models.dagbag.DagBag\u0026#34; [2025-05-31, 11:36:38] INFO - Done. Returned value was: None: source=\u0026#34;airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator\u0026#34; [2025-05-31, 11:36:38] INFO - Indonesia: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; Plugins # Plugins \u0026amp;mdash; Airflow 3.1.3 Documentationairflow.apache.org plugins/ 경로에 파이썬 함수를 작성하고 외부에서 활용 DAG 선언 시 함수를 가져오기만 해도 되어서 깔끔해지고 같은 함수를 재활용할 수 있어서 편리 예시로, plugins/ 경로 아래에 현재 시간을 출력하는 print_now() 함수를 정의 Copy python # plugins/common/common_func.py import datetime as dt def print_now(): print(dt.datetime.now()) Plugins 활용 예시 # Copy python # dags/python_plugins.py from airflow.sdk import DAG from airflow.providers.standard.operators.python import PythonOperator from common.common_func import print_now import pendulum with DAG( dag_id=\u0026#34;python_plugins\u0026#34;, schedule=\u0026#34;0 0 * * *\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;python\u0026#34;], ) as dag: python_plugins_task = PythonOperator( task_id=\u0026#34;python_plugins_task\u0026#34;, python_callable=print_now, ) plugins/ 에 정의한 함수를 import 해서 사용하는 python_plugins DAG 및 단일 Task를 생성 이 때, VSCode 상에서는 plugins/ 경로를 생략한 import 문을 인식하지 못하기 때문에 .env 파일에 PYTHONPATH 를 추가 Copy bash # .env WORKSPACE_FOLDER=/Users/.../airflow PYTHONPATH=${WORKSPACE_FOLDER}/plugins 마찬가지로, Airflow 컨테이너를 재시작하면 python_plugins DAG가 올라온 것을 확인 DAG를 실행한 후 python_plugins_task 의 실행 로그에서 현재 시간이 출력된 결과를 조회 별도로 시간대를 지정하지 않았는데, UTC 시간대를 기준으로 시간이 가져와진 것을 확인 Copy bash [2025-05-31, 12:10:35] INFO - DAG bundles loaded: dags-folder: source=\u0026#34;airflow.dag_processing.bundles.manager.DagBundlesManager\u0026#34; [2025-05-31, 12:10:35] INFO - Filling up the DagBag from /opt/airflow/dags/python_plugins.py: source=\u0026#34;airflow.models.dagbag.DagBag\u0026#34; [2025-05-31, 12:10:35] INFO - Done. Returned value was: None: source=\u0026#34;airflow.task.operators.airflow.providers.standard.operators.python.PythonOperator\u0026#34; [2025-05-31, 12:10:35] INFO - 2025-05-31 03:10:35.099945: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; Decorator # Airflow 공식 문서에서 PythonOperator 사용하는 것보다는 코드가 짧고 가독성이 좋은 @task 데코레이터를 활용하는 것을 권장 Decorator 활용 # Copy python # dags/python_decorator.py from airflow.sdk import DAG, task import pendulum with DAG( dag_id=\u0026#34;python_decorator\u0026#34;, schedule=\u0026#34;0 0 * * *\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;python\u0026#34;], ) as dag: @task(task_id=\u0026#34;python_decorator_task\u0026#34;) def print_input(__input): print(__input) python_decorator_task = print_input(\u0026#34;@task 데코레이터 실행\u0026#34;) Airflow에서 제공하는 예시 example_python_decorator 를 복제 및 일부를 수정하여 DAG 선언 단순히 함수에 @task 데코레이터를 추가하고 task_id 파라미터를 부여 airflow/providers/standard/tests/system/standard/example_python_decorator.py at … Apache Airflow - A platform to programmatically author, schedule, and monitor workflows - … GitHub DAG를 실행한 후 python_decorator_task 의 실행 로그에서 전달한 인수가 출력된 것을 확인 Copy bash [2025-05-31, 12:29:22] INFO - DAG bundles loaded: dags-folder: source=\u0026#34;airflow.dag_processing.bundles.manager.DagBundlesManager\u0026#34; [2025-05-31, 12:29:22] INFO - Filling up the DagBag from /opt/airflow/dags/python_decorator.py: source=\u0026#34;airflow.models.dagbag.DagBag\u0026#34; [2025-05-31, 12:29:22] INFO - Done. Returned value was: None: source=\u0026#34;airflow.task.operators.airflow.providers.standard.decorators.python._PythonDecoratedOperator\u0026#34; [2025-05-31, 12:29:22] INFO - @task 데코레이터 실행: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; Parameter # 파이썬에서는 함수에 순서대로 인자를 전달하거나 키-값의 형태로 파라미터를 전달 가능 순서대로 전달되는 인자를 배열로 받을 때는 *args 와 같이 * 를 붙여서 Tuple 타입의 객체를 받을 수 있음 키-값의 형태로 전달되는 파라미터를 받을 때는 **kwargs 와 같이 ** 를 붙여서 딕셔너리 타입의 객체를 받을 수 있음 인자와 파라미터를 모두 받아서 출력하는 함수 regist() 를 아래와 같이 구현 Copy python # plugins/common/common_func.py def regist(name: str, age: int, *args, **kwargs): print(f\u0026#34;이름: {name}\u0026#34;) print(f\u0026#34;나이: {age}\u0026#34;) for __key, __value in kwargs.items(): print(f\u0026#34;{__key}: {__value}\u0026#34;) if args: print(f\u0026#34;기타 정보: {args}\u0026#34;) op_args, op_kwargs # PythonOperator 도 함수를 실행할 때 인자 또는 파라미터를 전달할 수 있는 방법을 제공 인자를 전달할 때는 op_args 파라미터에 배열 객체를 전달 파라미터를 전달할 때는 op_kwargs 파라미터에 딕셔너리 객체를 전달 Copy python # dags/python_parameter.py from airflow.sdk import DAG from airflow.providers.standard.operators.python import PythonOperator from common.common_func import regist import pendulum with DAG( dag_id=\u0026#34;python_parameter\u0026#34;, schedule=\u0026#34;0 0 * * *\u0026#34;, start_date=pendulum.datetime(2025, 1, 1, tz=\u0026#34;Asia/Seoul\u0026#34;), catchup=False, tags=[\u0026#34;example\u0026#34;, \u0026#34;python\u0026#34;], ) as dag: regist_task = PythonOperator( task_id=\u0026#34;regist_task\u0026#34;, python_callable=regist, op_args=[\u0026#34;김철수\u0026#34;, 20, \u0026#34;서울\u0026#34;, \u0026#34;대한민국\u0026#34;], op_kwargs={\u0026#34;이메일\u0026#34;:\u0026#34;su@example.com\u0026#34;, \u0026#34;전화번호\u0026#34;:\u0026#34;010-1234-5678\u0026#34;}, ) DAG를 실행한 후 regist_task 의 실행 로그에서 전달한 인자와 파라미터가 출력된 것을 확인 실제로는 직접 전달한 파라미터 외에 ds 또는 ts 등 Airflow에서 만들어지는 파라미터가 같이 전달되는 것 같은데 앞으로 Airflow를 알게되면서 활용할 수 있을 것을 기대 Copy bash [2025-05-31, 15:04:22] INFO - DAG bundles loaded: dags-folder: source=\u0026#34;airflow.dag_processing.bundles.manager.DagBundlesManager\u0026#34; [2025-05-31, 15:04:22] INFO - Filling up the DagBag from /opt/airflow/dags/python_parameter.py: source=\u0026#34;airflow.models.dagbag.DagBag\u0026#34; [2025-05-31, 15:04:23] INFO - 이름: 김철수: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; ... [2025-05-31, 15:04:23] INFO - 나이: 20: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; ... [2025-05-31, 15:04:23] INFO - 이메일: su@example.com: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; [2025-05-31, 15:04:23] INFO - 전화번호: 010-1234-5678: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; ... [2025-05-31, 15:04:23] INFO - 기타 정보: (\u0026#39;서울\u0026#39;, \u0026#39;대한민국\u0026#39;): chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; "},{"id":20,"href":"/blog/airflow-study-1/","title":"Apache Airflow - 설치하기 (Docker Compose)","section":"Posts","content":"Airflow란? # 워크플로우를 만들고 관리하기 위한 목적의 파이썬 기반 오픈소스 플랫폼 워크플로우는 DAG(Directed Acyclic Graph) 구조를 가짐 Cron 기반으로 작업 스케줄링 웹 UI를 통해 작업을 모니터링하고 실패 시 재실행이 가능 Airflow 장점 # 파이썬에서 지원되는 다양한 라이브러리를 활용 가능 대규모 환경에서 부하 증가 시 수평적 확장이 가능한 Kubenetes 등 아키텍처 지원 오픈소스 플랫폼의 이점을 살려 원하는 기능을 커스터마이징 가능 Airflow 단점 # 실시간 워크플로우 관리에 적합하지 않음 (최소 분 단위 실행) 워크플로우가 많아질수록 모니터링이 어려움 DAG(Directed Acyclic Graph) # DAG는 Task 간의 종속성과 순서를 지정 Task : DAG 내에서 어떠한 행위를 할 수 있는 객체 DAG는 1개 이상의 Task로 구성 Task 간에 순한되지 않고 방향성을 가짐 Task에 대한 종속성은 \u0026gt;\u0026gt; 또는 \u0026lt;\u0026lt; 연산자를 사용해 선언 예시) first_task \u0026gt;\u0026gt; second_task Airflow Workflow # Scheduler : 예약된 일정에 워크플로우를 Executor에게 넘겨 Task를 실행 Executor : Scheduler 내부의 모든 작업을 실행하며, 모든 Task가 순차적으로 실행되게 관리 Worker : 실제 Task를 실행하는 주체 Metadata Database : Scheduler, Executor, Webserver가 상태를 저장하는데 사용 DAG Directory : 파이썬으로 작성한 DAG 파일을 저장하는 공간 Webserver : User Interface를 통해 Scheduler와 DAG 실행 과정을 시각화해 표시 User Interface # DAG와 각각의 작업이 수행되는 내용을 시각적으로 확인 DAG 실행을 직접 트리거 DAG 실행 로그를 확인하고 제한적인 디버깅 수행 Architecture Overview \u0026amp;mdash; Airflow Documentationairflow.apache.org Airflow 설치 환경 # macOS Sequoia 15.5 Docker Desktop 4.41.2 Airflow 3.0.1 Docker란? # 애플리케이션을 독립적인 환경에서 실행시키는 기술 가상화 서버(VM)와 비교했을 때, Guest OS를 요구하지 않아 더 효율적인 자원 활용이 가능 docker-compose 를 이용해 Airflow를 운영하는데 필요한 여러 서비스(컨테이너)를 실행 Docker Descktop 설치 # Mac Install Docker Desktop for Mac to get started. This guide covers system requirements, where to download, and … Docker Documentation 운영체제에 맞는 Docker Descktop 파일을 내려받아 설치 설치 후 실행하면 아래와 같은 초기 화면을 확인 Airflow 설치하기 # Running Airflow in Docker \u0026amp;mdash; Airflow 3.1.3 Documentationairflow.apache.org 1. docker-compose.yaml # Copy bash % curl -LfO \u0026#39;https://airflow.apache.org/docs/apache-airflow/3.0.1/docker-compose.yaml\u0026#39; % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 12766 100 12766 0 0 38365 0 --:--:-- --:--:-- --:--:-- 38336 Airflow 문서로부터 docker-compose.yaml 경로를 복사해 터미널에서 요청 내려받은 docker-compose.yaml 파일을 보면 아래와 같은 컨테이너가 설치될 것임을 짐작 가능 Copy yaml services: postgres: image: postgres:13 redis: image: redis:7.2-bookworm airflow-apiserver: ... airflow-scheduler: ... airflow-dag-processor: ... airflow-worker: ... airflow-triggerer: ... airflow-init: ... airflow-cli: ... flower: ... 2. 환경 설정 # Copy bash mkdir -p ./dags ./logs ./plugins ./config echo -e \u0026#34;AIRFLOW_UID=$(id -u)\u0026#34; \u0026gt; .env Airflow 문서에 따라 필요한 디렉토리를 생성 AIRFLOW_UID가 설정되지 않았다는 경고를 피하기 위해 .env 파일을 생성 3. Airflow 초기화 # Copy bash docker compose up airflow-init exited with code 0 가 출력되면 정상적으로 초기화되어 다음 단계로 진행 4. Airflow 실행 # Copy text docker compose up Docker Desktop 상에서 아래 이미지와 같이 모든 컨테이너들이 실행되고 있는지 확인 가능 또는 터미널에서 docker ps 명령어를 입력해 컨테이너 실행 상태를 조회 가능 Copy bash % docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a0431bfa0d1f apache/airflow:3.0.1 \u0026#34;/usr/bin/dumb-init …\u0026#34; 1 minutes ago Up 1 minutes (healthy) 8080/tcp airflow-airflow-worker-1 4d32f0c45d4a apache/airflow:3.0.1 \u0026#34;/usr/bin/dumb-init …\u0026#34; 1 minutes ago Up 1 minutes (healthy) 8080/tcp airflow-airflow-triggerer-1 f4cd1b0887de apache/airflow:3.0.1 \u0026#34;/usr/bin/dumb-init …\u0026#34; 1 minutes ago Up 1 minutes (healthy) 8080/tcp airflow-airflow-dag-processor-1 3213192935b6 apache/airflow:3.0.1 \u0026#34;/usr/bin/dumb-init …\u0026#34; 1 minutes ago Up 1 minutes (healthy) 8080/tcp airflow-airflow-scheduler-1 e933dc415a73 apache/airflow:3.0.1 \u0026#34;/usr/bin/dumb-init …\u0026#34; 1 minutes ago Up 1 minutes (healthy) 0.0.0.0:8080-\u0026gt;8080/tcp airflow-airflow-apiserver-1 94f88f2e5507 postgres:13 \u0026#34;docker-entrypoint.s…\u0026#34; 6 minutes ago Up 6 minutes (healthy) 5432/tcp airflow-postgres-1 3f82ba15433e redis:7.2-bookworm \u0026#34;docker-entrypoint.s…\u0026#34; 6 minutes ago Up 6 minutes (healthy) 6379/tcp airflow-redis-1 Airflow 둘러보기 # Airflow 로그인 # 컨테이너가 정상적으로 실행 중인 경우 브라우저에서 localhost:8080 주소로 접속하면 Airflow UI에 접근 가능 초기 계정은 Username 과 Password 모두 airflow 입력 정상적으로 로그인 시 아래와 같은 홈페이지로 이동 Example DAG 실행 # 사이드바에서 Dags 메뉴를 클릭하면 DAG 목록이 있는 페이지로 이동 첫 번째에 있는 tutorial_taskflow_templates DAG를 클릭하여 상세 페이지 조회 상단의 DAG 제목 옆에 토글 스위치를 클릭하면 DAG 활성화 좌측에서 DAG 내 Task 간 종속성을 그래프로 확인 (해당 DAG는 단일 Task로 구성) 내비게이션 메뉴 중에서 Runs 메뉴를 클릭하면 실행 이력을 조회 실행된 결과를 하나 클릭하면 하위 Task 목록을 조회 가능 State 항목을 통해 처리 상태를 알 수 있는데, success 상태는 Task가 성공적으로 처리됨을 의미 Task 하나를 클릭해서 Logs 메뉴에 들어가면 아래와 같이 context 라는 객체를 출력하는 구문이 확인됨 앞서 DAG 페이지의 내비게이션 메뉴 중 Code 메뉴에서 이미 코드를 확인했는데, get_current_context() 라는 함수를 통해 context 라는 딕셔너리 객체를 가져와 출력하는 작업으로 추측 Copy text [2025-05-27, 23:31:26] INFO - context: {\u0026#39;dag\u0026#39;: \u0026lt;DAG: tutorial_taskflow_templates\u0026gt;, ...}: chan=\u0026#34;stdout\u0026#34;: source=\u0026#34;task\u0026#34; Airflow 중지 # Copy bash % docker compose stop [+] Stopping 8/8 ✔ Container airflow-airflow-dag-processor-1 Stopped 1.1s ✔ Container airflow-airflow-triggerer-1 Stopped 0.9s ✔ Container airflow-airflow-worker-1 Stopped 2.8s ✔ Container airflow-airflow-scheduler-1 Stopped 0.8s ✔ Container airflow-airflow-apiserver-1 Stopped 0.6s ✔ Container airflow-airflow-init-1 Stopped 0.0s ✔ Container airflow-postgres-1 Stopped 0.1s ✔ Container airflow-redis-1 Stopped 0.2s docker-compose.yaml 파일이 있는 경로에서 서비스를 명시하지 않고 명령어를 실행하면 모든 Airflow 컨테이너를 중지 Airflow 컨테이너 삭제 # Copy bash % docker compose down --volumes --rmi all [+] Running 13/13 ✔ Container airflow-airflow-dag-processor-1 Removed 1.5s ✔ Container airflow-airflow-triggerer-1 Removed 1.2s ✔ Container airflow-airflow-worker-1 Removed 3.4s ✔ Container airflow-airflow-scheduler-1 Removed 1.0s ✔ Container airflow-airflow-apiserver-1 Removed 0.9s ✔ Container airflow-airflow-init-1 Removed 0.1s ✔ Container airflow-postgres-1 Removed 0.2s ✔ Container airflow-redis-1 Removed 0.2s ✔ Image apache/airflow:3.0.1 Removed 0.9s ✔ Image postgres:13 Removed 0.9s ✔ Image redis:7.2-bookworm Removed 0.9s ✔ Volume airflow_postgres-db-volume Removed 0.0s ✔ Network airflow_default Removed 0.5s docker-compose.yaml 파일이 있는 경로에서 서비스를 명시하지 않고 명령어를 실행하면 모든 Airflow 컨테이너를 삭제 --rmi 옵션을 추가하여 관련 이미지까지 모두 삭제 "},{"id":21,"href":"/blog/10000-recipe/","title":"[Python] 만개의 레시피 데이터 수집","section":"Posts","content":"최근 레시피 생성을 목적으로 한 사이드 프로젝트에 참여하게 되었는데\n모델 학습을 위한 만개의 레시피 데이터 크롤링을 진행해보았습니다.\n스키마 구성 # 기존엔 레시피 명칭과 음식 재료 정보만을 수집할 계획이었지만,\n만개의 레시피의 각 페이지를 살펴보면서 추가적으로 가져갈만한 데이터가 있음을 확인하여\n우선적으로 테이블 관계 및 스키마를 구성해보았습니다.\n초기에 만개의 레시피와 공공데이터를 데이터 소스로 삼았기 때문에,\n만개의 레시피에 대한 DB _10000, 공공데이터에 대한 DB food로 구성했습니다.\n_10000 DB 내 테이블은 만개의 레시피 내 각각의 페이지에서 가져온 데이터로 구성되며,\n크게 카테고리, 레시피, 사용자 단위로 구분할 수 있습니다.\n만개의 레시피 데이터 수집 # 크롤링에서 데이터 요청 및 가공을 위해 정의된 유틸리티 함수들이 있는데,\n별도로 코드를 보여주지는 않고 해당 함수가 호출될 때 간단히 어떤 동작을 하는지만 전달드립니다.\n카테고리 추출 # 만개의 레시피 카테고리는 레시피 검색 페이지에서 간단하게 추출할 수 있으므로,\n개발자 도구 또는 requests에 대한 응답에서 카테고리에 해당하는 부분을 가져옵니다.\n여기서 get_headers() 함수는 User-Agent 등 기본적인 브라우저 정보가 담긴 헤더를 반환합니다.\nCopy python url = \u0026#34;https://www.10000recipe.com/recipe/list.html\u0026#34; headers = get_headers(url, referer=url) headers[\u0026#34;upgrade-insecure-requests\u0026#34;] = \u0026#39;1\u0026#39; response = session.get(url, params=params, headers=headers) source = BeautifulSoup(response.text, \u0026#34;html.parser\u0026#34;) cate_list = source.select_one(\u0026#34;div.cate_list\u0026#34;) pattern = \u0026#34;javascript:goSearchRecipe([\\d\\w()\u0026#39;,]+)\u0026#34; raw_cat = [(re_get(pattern, cat.attrs[\u0026#34;href\u0026#34;]),cat.text) for cat in cate_list.select(\u0026#34;a\u0026#34;) if \u0026#34;href\u0026#34; in cat.attrs] cat_map = lambda catType, catId, catName: {\u0026#34;categoryId\u0026#34;:catId, \u0026#34;categoryType\u0026#34;:catType, \u0026#34;categoryName\u0026#34;:catName} categories = [cat_map(*literal_eval(data), name) for data, name in raw_cat] categories = pd.DataFrame(categories) categories = categories[categories[\u0026#34;categoryId\u0026#34;]!=\u0026#39;\u0026#39;] categories.head() 데이터 수집 결과 아래와 같은 구조의 데이터를 획득할 수 있습니다.\ncategoryId categoryType categoryName 63 cat4 밑반찬 56 cat4 메인반찬 54 cat4 국/탕 55 cat4 찌개 60 cat4 디저트 레시피 목록 추출 # 레시피 검색 페이지는 검색어, 정렬 기준, 페이지, 카테고리를 쿼리로 받습니다.\n레시피 목록을 추출하는데 검색어나 카테고리는 필요하지 않고 동일한 정렬 기준에서 수집하기 때문에\n데이터 수집 시에는 페이지에 반복문을 적용하여 데이터가 존재하는 범위를 가져올 것입니다.\nCopy python ORDER_MAP = {\u0026#34;정확순\u0026#34;:\u0026#34;accuracy\u0026#34;, \u0026#34;최신순\u0026#34;:\u0026#34;date\u0026#34;, \u0026#34;추천순\u0026#34;:\u0026#34;reco\u0026#34;} get_params = lambda **kwargs: {k:v for k,v in kwargs.items() if v} uri = \u0026#34;https://www.10000recipe.com/recipe/\u0026#34; def fetch(session: requests.Session, query=str(), sortType=\u0026#34;추천순\u0026#34;, page=1, cat1=str(), cat2=str(), cat3=str(), cat4=str(), **kwargs) -\u0026gt; List[str]: url = uri+\u0026#34;list.html\u0026#34; params = get_params(q=query, order=ORDER_MAP[sortType], page=page, cat1=cat1, cat2=cat2, cat3=cat3, cat4=cat4) headers = get_headers(url, referer=url) headers[\u0026#34;upgrade-insecure-requests\u0026#34;] = \u0026#39;1\u0026#39; response = session.get(url, params=params, headers=headers) return parse(response.text, **kwargs) def parse(response: str, **kwargs) -\u0026gt; List[str]: source = BeautifulSoup(response, \u0026#39;html.parser\u0026#39;) uris = source.select(\u0026#34;a.common_sp_link\u0026#34;) ids = [uri.attrs[\u0026#34;href\u0026#34;].split(\u0026#39;/\u0026#39;)[-1] for uri in uris if \u0026#34;href\u0026#34; in uri.attrs] return ids 데이터 수집 결과로는 문자열 타입의 레시피 ID 목록을 획득할 수 있습니다.\n레시피 정보 추출 # 레시피 ID로 접근할 수 있는 레시피 상세 정보 페이지에서\n레시피 정보에 대한 데이터를 추출합니다. 소스코드 내에서 레시피 정보가 JSON 형식으로 존재하기 때문에\n일일히 HTML 태그를 파싱할 필요 없이 데이터를 한번에 JSON 오브젝트로 가져올 수 있습니다.\n데이터를 가공하는 map_recipe() 함수 내에서\ncast_int()는 데이터를 정수형으로 변환할 때 에러가 발생하면 기본값 0을 반환하는 함수이고,\nhier_get()은 중첩 딕셔너리의에 단계별 키 목록에 대한 값을 안전하게 가져오기 위한 함수입니다.\nCopy python uri = \u0026#34;https://www.10000recipe.com/recipe/\u0026#34; def fetch(session: requests.Session, recipeId: str, **kwargs) -\u0026gt; Dict: url = uri+recipeId # https://www.10000recipe.com/recipe/6997297 headers = get_headers(url, referer=uri+\u0026#34;list.html\u0026#34;) headers[\u0026#34;upgrade-insecure-requests\u0026#34;] = \u0026#39;1\u0026#39; response = session.get(url, headers=headers) return parse(response.text, recipeId, **kwargs) def parse(response: str, recipeId: str, **kwargs) -\u0026gt; Dict: source = BeautifulSoup(response, \u0026#39;html.parser\u0026#39;) raw_json = source.select_one(\u0026#34;script[type=\\\u0026#34;application/ld+json\\\u0026#34;]\u0026#34;).text try: data = json.loads(raw_json) except: data = literal_eval(raw_json) return map_recipe(data, recipeId, source, **kwargs) def map_recipe(data: Dict, recipeId: str, source=None, **kwargs) -\u0026gt; Dict: recipe_info = {\u0026#34;recipeId\u0026#34;: recipeId} recipe_info[\u0026#34;name\u0026#34;] = data.get(\u0026#34;name\u0026#34;) recipe_info[\u0026#34;author\u0026#34;] = hier_get(data, [\u0026#34;author\u0026#34;,\u0026#34;name\u0026#34;]) recipe_info[\u0026#34;ratingValue\u0026#34;] = cast_int(hier_get(data, [\u0026#34;aggregateRating\u0026#34;,\u0026#34;ratingValue\u0026#34;])) recipe_info[\u0026#34;reviewCount\u0026#34;] = cast_int(hier_get(data, [\u0026#34;aggregateRating\u0026#34;,\u0026#34;reviewCount\u0026#34;])) recipe_info[\u0026#34;totalTime\u0026#34;] = data.get(\u0026#34;totalTime\u0026#34;) recipe_info[\u0026#34;recipeYield\u0026#34;] = data.get(\u0026#34;recipeYield\u0026#34;) try: recipe_info[\u0026#34;recipeIngredient\u0026#34;] = \u0026#39;,\u0026#39;.join(data[\u0026#34;recipeIngredient\u0026#34;]) except: recipe_info[\u0026#34;recipeIngredient\u0026#34;] = extract_ingredient(source, **kwargs) recipe_info[\u0026#34;recipeInstructions\u0026#34;] = \u0026#39;\\n\u0026#39;.join( [step.get(\u0026#34;text\u0026#34;,str()) for step in data.get(\u0026#34;recipeInstructions\u0026#34;,list()) if isinstance(step, dict)]) recipe_info[\u0026#34;createDate\u0026#34;] = data.get(\u0026#34;datePublished\u0026#34;) return recipe_info def extract_ingredient(source: Tag, **kwargs) -\u0026gt; str: cont_ingre = source.select_one(\u0026#34;div.cont_ingre\u0026#34;) if cont_ingre: return [ingre.split() for ingre in cont_ingre.select_one(\u0026#34;dd\u0026#34;).text.split(\u0026#39;,\u0026#39;)] else: return str() 데이터 수집 결과 아래와 같이 정리된 딕셔너리를 얻을 수 있습니다.\nCopy python { \u0026#34;recipeId\u0026#34;: \u0026#34;6997297\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;두부짜조\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;호이호이\u0026#34;, \u0026#34;ratingValue\u0026#34;: 5, \u0026#34;reviewCount\u0026#34;: 1, \u0026#34;totalTime\u0026#34;: \u0026#34;PT20M\u0026#34;, \u0026#34;recipeYield\u0026#34;: \u0026#34;1 servings\u0026#34;, \u0026#34;recipeIngredient\u0026#34;: \u0026#34;두부 30g,라이스페이퍼 2장,돼지고기 5g,...\u0026#34;, \u0026#34;recipeInstructions\u0026#34;: \u0026#34;부위는 상관없지만 저는 저렴하고...\u0026#34;, \u0026#34;createDate\u0026#34;: \u0026#34;2023-02-19T13:37:04+09:00\u0026#34; } 실질적으로 활용할 데이터는 레시피명 name과 재료명인 recipeIngredient이며,\n평점, 리뷰 수, 조리순서 등도 추가적인 분석을 통해 활용성을 기대해볼 수 있습니다.\n요리 후기 추출 # 동일한 레시피 상세 정보 페이지에서 요리 후기에 대한 데이터를 추출할 수 있습니다.\n단, 요리 후기는 JSON 형식으로 정리되어 있지 않기 때문에\nHTML 소스를 파싱하여 대상 문자열을 추출해야 합니다.\n데이터를 가공하는 map_review() 함수 내에서\nre_get()은 정규표현식 패턴에 매칭되는 문자열을 추출하는 함수이고,\nselect_text()는 BeautifulSoup 태그에서\nCSS Selector로 안전하게 문자열을 추출하는 함수입니다.\nCopy python GENDER = {\u0026#34;info_name_m\u0026#34;:\u0026#34;M\u0026#34;, \u0026#34;info_name_f\u0026#34;:\u0026#34;F\u0026#34;} uri = \u0026#34;https://www.10000recipe.com/recipe/\u0026#34; rid_ptn = \u0026#34;replyReviewDiv_(\\d+)\u0026#34; uid_ptn = \u0026#34;/profile/review.html\\?uid=([\\d\\w]+)\u0026#34; date_ptn = \u0026#34;(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\u0026#34; def fetch(session: requests.Session, recipeId: str, **kwargs) -\u0026gt; List[Dict]: url = uri+recipeId headers = get_headers(url, referer=uri+\u0026#34;list.html\u0026#34;) headers[\u0026#34;upgrade-insecure-requests\u0026#34;] = \u0026#39;1\u0026#39; response = session.get(url, headers=headers) return parse(response.text, recipeId, **kwargs) def parse(response: str, recipeId: str, **kwargs) -\u0026gt; List[Dict]: source = BeautifulSoup(response, \u0026#39;html.parser\u0026#39;) reply_divs = source.select(\u0026#34;div.view_reply\u0026#34;) review_div = [div for div in reply_divs if div.select_one(\u0026#34;div.reply_tit\u0026#34;).text.strip().startswith(\u0026#34;요리 후기\u0026#34;)] if review_div: review_list = review_div[0].select(\u0026#34;div.reply_list\u0026#34;) return [map_review(review, recipeId, **kwargs) for review in review_list] else: return list() def map_review(data: Tag, recipeId: str, **kwargs) -\u0026gt; Dict: review_info = dict() review_info[\u0026#34;reviewId\u0026#34;] = re_get(rid_ptn, data.select(\u0026#34;div\u0026#34;)[-1].attrs.get(\u0026#34;id\u0026#34;)) review_info[\u0026#34;recipeId\u0026#34;] = recipeId review_info[\u0026#34;userId\u0026#34;] = re_get(uid_ptn, data.select_one(\u0026#34;a\u0026#34;).attrs.get(\u0026#34;href\u0026#34;)) review_info[\u0026#34;contents\u0026#34;] = select_text(data, \u0026#34;p.reply_list_cont\u0026#34;) detail = data.select_one(\u0026#34;h4.media-heading\u0026#34;) if detail: review_info[\u0026#34;userName\u0026#34;] = select_text(detail, \u0026#34;b\u0026#34;) gender = detail.select_one(\u0026#34;b\u0026#34;).attrs.get(\u0026#34;class\u0026#34;) review_info[\u0026#34;userGender\u0026#34;] = GENDER.get(gender[0]) if gender else None review_info[\u0026#34;createDate\u0026#34;] = re_get(date_ptn, detail.text) return review_info 데이터 수집 결과 아래와 같이 정리된 딕셔너리를 얻을 수 있습니다.\n여기서 요리 후기와 별도로 사용자 명칭과 성별을 추출할 수 있습니다.\nCopy python { \u0026#34;reviewId\u0026#34;: \u0026#34;395018\u0026#34;, \u0026#34;recipeId\u0026#34;: \u0026#34;6843136\u0026#34;, \u0026#34;userId\u0026#34;: \u0026#34;58031746\u0026#34;, \u0026#34;contents\u0026#34;: \u0026#34;정말 간단한데 중불로하니 좀 태워먹었... 맛은 있네욬ㅋㅋㅋㅋㅋ다음엔 중불이랑 약불 사이로 함 더해바야겠어욬ㅋㅋㅋㄱㅋㅋ감삼둥..♡♡\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;나찡as\u0026#34;, \u0026#34;userGender\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;createDate\u0026#34;: \u0026#34;2020-11-09 17:14:02\u0026#34; } 댓글 추출 # 레시피 상세 정보 페이지에서 댓글은 미리보기만이 제공되며\n전체 댓글을 확인하기 위해서는 별도의 페이지에 접속해야 합니다.\n해당 페이지의 출력 결과에서도 요리 후기와 같은 방식으로\nHTML 소스를 파싱하여 대상 문자열을 추출해야 합니다.\nCopy python GENDER = {\u0026#34;info_name_m\u0026#34;:\u0026#34;M\u0026#34;, \u0026#34;info_name_f\u0026#34;:\u0026#34;F\u0026#34;} uri = \u0026#34;https://www.10000recipe.com/recipe/\u0026#34; cid_ptn = \u0026#34;replyCommentDiv_(\\d+)\u0026#34; uid_ptn = \u0026#34;/profile/recipe_comment.html\\?uid=([\\d\\w]+)\u0026#34; date_ptn = \u0026#34;(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2})\u0026#34; def fetch(session: requests.Session, recipeId: str, page=1, **kwargs) -\u0026gt; List[Dict]: url = uri+\u0026#34;ajax.html\u0026#34; params = dict(q_mode=\u0026#34;getListComment\u0026#34;, seq=recipeId, page=page) headers = get_headers(url, referer=uri+recipeId) headers[\u0026#34;upgrade-insecure-requests\u0026#34;] = \u0026#39;1\u0026#39; response = session.get(url, params=params, headers=headers) return parse(response.text, recipeId, **kwargs) def parse(response: str, recipeId: str, **kwargs) -\u0026gt; List[Dict]: source = BeautifulSoup(response, \u0026#39;html.parser\u0026#39;) comment_list = source.select(\u0026#34;div.reply_list\u0026#34;) return [map_comment(comment, recipeId, **kwargs) for comment in comment_list] def map_comment(data: Tag, recipeId: str, **kwargs) -\u0026gt; Dict: comment_info = dict() comment_info[\u0026#34;commentId\u0026#34;] = re_get(cid_ptn, data.select(\u0026#34;div\u0026#34;)[-1].attrs.get(\u0026#34;id\u0026#34;)) comment_info[\u0026#34;recipeId\u0026#34;] = recipeId comment_info[\u0026#34;userId\u0026#34;] = re_get(uid_ptn, data.select_one(\u0026#34;a\u0026#34;).attrs.get(\u0026#34;href\u0026#34;)) comment_info[\u0026#34;contents\u0026#34;] = select_text(data, \u0026#34;div.media-body\u0026#34;).split(\u0026#39;|\u0026#39;)[-1] detail = data.select_one(\u0026#34;h4.media-heading\u0026#34;) if detail: comment_info[\u0026#34;userName\u0026#34;] = select_text(detail, \u0026#34;b\u0026#34;) gender = detail.select_one(\u0026#34;b\u0026#34;).attrs.get(\u0026#34;class\u0026#34;) comment_info[\u0026#34;userGender\u0026#34;] = GENDER.get(gender[0]) if gender else None comment_info[\u0026#34;createDate\u0026#34;] = re_get(date_ptn, detail.text) return comment_info review = fetch(session, \u0026#34;6843136\u0026#34;) review[0] 데이터 수집 결과 아래와 같이 정리된 딕셔너리를 얻을 수 있습니다.\n데이터 구조는 요리 후기와 동일합니다.\nCopy python { \u0026#34;commentId\u0026#34;: \u0026#34;39693405\u0026#34;, \u0026#34;recipeId\u0026#34;: \u0026#34;6843136\u0026#34;, \u0026#34;userId\u0026#34;: \u0026#34;89382542\u0026#34;, \u0026#34;contents\u0026#34;: \u0026#34;신고그러네여..재료양이..ㅜ\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;휘아여\u0026#34;, \u0026#34;userGender\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;createDate\u0026#34;: \u0026#34;2022-03-18 00:02\u0026#34; } "},{"id":22,"href":"/blog/smartstore-login-3/","title":"[Python] requests로 네이버 스마트스토어센터 로그인 구현하기 (3)","section":"Posts","content":"앞선 네이버 로그인 구현 과정을 통해 네이버 로그인에 대해 이해하고\n스마트스토어센터 로그인 결과로 얻을 수 있는 쿠키 값의 일부를 획득했습니다.\nCopy python cookies = { \u0026#34;NNB\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;nid_inf\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_AUT\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_JKL\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_CHK\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NSI\u0026#34;: \u0026#34;...\u0026#34;, } 하지만, 스마트스토어센터에서 데이터를 가져오기 위해 필요한 쿠키 값은\nCBI_SES, CBI_CHK, NSI 세 가지 값이기 때문에\n지금까지는 준비 과정에 불과했다고 할 수 있습니다.\n이번 게시글에서는 스마트스토어센터 로그인 과정을 이해하고\n직접 구현해보면서 SmartstoreLogin 클래스를 완성해보겠습니다.\n스마트스토어센터 로그인 이해 # 지금까지 스마트스토어센터의 두 가지 로그인 방식 중\n네이버 로그인 방식으로 로그인을 수행하기 위해,\n실제 네이버 로그인에 대한 이해 및 구현을 진행했습니다.\n요청 내역 탐색 시 주의사항 # 새 창에서 띄워지는 네이버 로그인 페이지는\n로그인이 완료되면 닫혀버리기 때문에 네트워크 요청 내역을 확인하기 어렵습니다.\n이 경우 개발자 도구 Sources 탭에서 Event Listener Breakpoints 메뉴 아래\nWindow \u0026gt; window.close 부분을 선택하면 창이 닫히는 순간에 중단시킬 수 있습니다.\n네이버 로그인과의 차이점 # 스마트스토어센터 로그인에서의 네이버 로그인은 기존 방식과 다소의 차이점이 존재합니다.\n아래는 스마트스토어센터 로그인 POST 요청에서 확인할 수 있는 데이터입니다.\nCopy json { \u0026#34;localechange\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;dynamicKey\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;logintp\u0026#34;: \u0026#34;oauth2\u0026#34;, \u0026#34;encpw\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;enctp\u0026#34;: 1, \u0026#34;svctype\u0026#34;: 64, \u0026#34;smart_LEVEL\u0026#34;: 1, \u0026#34;bvsd\u0026#34;: { \u0026#34;uuid\u0026#34;:\u0026#34;...\u0026#34;, \u0026#34;encData\u0026#34;:\u0026#34;...\u0026#34; }, \u0026#34;encnm\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;locale\u0026#34;: \u0026#34;ko_KR\u0026#34;, \u0026#34;client_id\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://nid.naver.com/oauth2.0/authorize?response_type=code\u0026amp;state=...\u0026amp;client_id=...\u0026amp;redirect_uri=https%3A%2F%2Faccounts.commerce.naver.com%2Foauth%2Fcallback\u0026amp;locale=ko_KR\u0026amp;inapp_view=\u0026amp;oauth_os=\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;pw\u0026#34;: \u0026#34;\u0026#34; } 기존의 네이버 로그인 데이터와 비교했을 때 3개의 값이 추가되었음을 알 수 있습니다.\nlogintp의 경우 \u0026quot;oauth2\u0026quot;로 고정된 값으로 보이지만,\nurl 내 state와 client_id는 지금까지의 과정에서는 얻을 수 없었던\n새로운 값으로 로그인을 위해 추가적인 동작이 필요해 보입니다.\nOAuth URL 가져오기 # state와 client_id의 경우 네이버 로그인 페이지를 불러오는 과정에서\n이미 전달되는 값이기 때문에 해당 페이지 안에서는 출처를 찾을 수 없었습니다.\n따라서 네이버 로그인 페이지로 이동하기 위해 거치는 스마트스토어센터 로그인 페이지에서\n네이버 로그인 페이지를 띄우는 과정에 집중하여 두 값이 발생하는 지점을 찾아보았고,\ngraphql 주소로 보낸 POST 요청에 대한 응답으로 url에 해당하는 authUrl 값을 받는 것을 확인했습니다.\n이렇게 구한 client_id 및 url 값을 로그인 데이터에 담아 요청을 보낼 경우\n일반적인 네이버 로그인 결과로 얻을 수 있는 NID_AUT 등의 쿠키 값을 획득할 수 있습니다.\nGraphQL 로그인 분석 # 스마트스토어센터 로그인은 네이버 로그인에서 그치지 않고\nCBI_SES, CBI_CHK, NSI 쿠키 값을 추가로 얻어야 합니다.\n이 중에서 CBI_SES를 응답 파일 내에서 검색했을 때 graphql 주소에 대한 응답으로\nCBI_SES와 CBI_CHK 값을 반환하는 것을 알 수 있었습니다.\n해당 주소는 앞서 인증 주소를 가져오는 과정에서 보았던 것인데\n당시 snsLoginBegin라는 명칭의 쿼리와는 다른 snsLoginCallback 쿼리를 사용하여\n추가적인 로그인을 수행하는 것임을 짐작할 수 있습니다.\n변수로 전달되는 state의 경우 앞에서 구한 것과 동일한 값이지만,\ncode는 아직까지 본 적 없는 값입니다.\n하지만, code는 어떠한 응답 파일 내에서도 출처를 찾아볼 수 없고,\ncode의 값 자체를 검색했을 때 oauth_token이라는 키와 동일한 값을 사용한다는 것 말고는\n별다른 단서를 찾을 수 없었습니다.\n이 경우 네이버 로그인 후에 연속적으로 진행되는 다른 요청 내역을 직접 들여다봐야 했고,\n다행히 바로 아래의 주소에 대한 응답 내역에서 oauth_token 값을 받아볼 수 있었습니다.\nCopy html \u0026lt;html\u0026gt; \u0026lt;script language=javascript nonce=\u0026#34;4SzeR1mCGzDbnzr3s5rjQ1Li\u0026#34;\u0026gt; location.replace(\u0026#34;https://nid.naver.com/login/noauth/allow_oauth.nhn?oauth_token=...\u0026amp;with_pin\u0026amp;step=agree_term\u0026amp;inapp_view=\u0026amp;oauth_os=\u0026#34;); \u0026lt;/script\u0026gt; \u0026lt;/html\u0026gt; oauth_token의 값을 code에 넣어서 state와 함께 graphql 주소에 요청할 경우\n응답 헤더의 Set-Cookie에서 볼 수 있는 CBI_SES와 CBI_CHK를 받게 됩니다.\n2단계 인증 분석 # 스마트스토어센터는 최초 로그인 시 반드시 2단계 인증을 거쳐야 합니다.\n마지막 남은 NSI 값 또한 해당 2단계 인증을 거쳐야 얻을 수 있을 것이라 걱정했지만,\n다행히 2단계 인증을 거치지 않아도 네트워크 응답 내역에서 NSI를 확인할 수 있었습니다.\nPOST 요청이지만 전달되는 데이터는 아래와 같이 단순했기에\n추가적인 분석 없이 마지막 NSI 값을 획득했습니다.\nCopy json {\u0026#34;url\u0026#34;: \u0026#34;https://sell.smartstore.naver.com/#/home/dashboard\u0026#34;} 스마트스토어센터 로그인 구현 # 지금까지의 과정을 통해 스마트스토어센터에서 데이터를 가져오기 위해 필요한 CBI_SES, CBI_CHK, NSI 값을 획득하는 방법을 파악했습니다.\n이를 SmartstoreLogin 클래스의 메소드로 구현해보겠습니다.\n네이버 로그인 구현 # 기존의 네이버 로그인 기능에 OAuth URL을 가져오는 부분을 추가시킨\nnid_login() 및 fetch_oauth_url() 메소드를 정의합니다.\nCopy python SMARTSTORE_URL = \u0026#34;https://sell.smartstore.naver.com/\u0026#34; SLOGIN_URL = \u0026#34;https://accounts.commerce.naver.com\u0026#34; GRAPHQL_DATA = str({ \u0026#34;operationName\u0026#34;: \u0026#34;snsLoginBegin\u0026#34;, \u0026#34;variables\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;login\u0026#34;, \u0026#34;snsCd\u0026#34;: \u0026#34;naver\u0026#34;, \u0026#34;svcUrl\u0026#34;: \u0026#34;https://sell.smartstore.naver.com/#/login-callback\u0026#34;}, \u0026#34;query\u0026#34;: \u0026#34;mutation snsLoginBegin($mode: String!, $snsCd: String!, $svcUrl: String!, \\ $oneTimeLoginSessionKey: String, $userInfos: [UserInfoEntry!]) {\\n snsBegin(\\n \\ snsLoginBeginRequest: {mode: $mode, snsCd: $snsCd, svcUrl: $svcUrl, oneTimeLoginSessionKey: \\ $oneTimeLoginSessionKey, userInfos: $userInfos}\\n ) {\\n authUrl\\n __typename\\n }\\n}\\n\u0026#34; }).replace(\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;) class SmartstoreLogin(NaverLogin): def fetch_oauth_url(self): referer = f\u0026#34;{SLOGIN_URL}/login?url={SMARTSTORE_URL}#/login-callback\u0026#34; headers = self.get_headers(host=SLOGIN_URL, referer=referer) response = self.post(urljoin(SLOGIN_URL, \u0026#34;graphql\u0026#34;), data=GRAPHQL_DATA, headers=headers) self.oauth_url = json.loads(response.text)[\u0026#34;data\u0026#34;][\u0026#34;snsBegin\u0026#34;][\u0026#34;authUrl\u0026#34;] self.oauth_params = {k:v.pop() for k,v in parse_qs(urlparse(self.oauth_url).query).items()} if \u0026#34;auth_type\u0026#34; in self.oauth_params: self.oauth_params.pop(\u0026#34;auth_type\u0026#34;) self.oauth_params = dict(self.oauth_params, **{\u0026#34;locale\u0026#34;:\u0026#34;ko_KR\u0026#34;,\u0026#34;inapp_view\u0026#34;:\u0026#39;\u0026#39;,\u0026#34;oauth_os\u0026#34;:\u0026#39;\u0026#39;}) graphql 주소에 대한 요청 데이터를 그대로 구현한 것이 GRAPHQL_DATA이며,\n그 결과로 OAuth URL을 얻을 수 있습니다.\nOAuth URL의 파라미터는 향후 GraphQL 인증 과정에서 재활용되기 때문에\noauth_params 변수에 저장해둡니다.\nCopy python LOGIN_URL = \u0026#34;https://nid.naver.com/nidlogin.login\u0026#34; SLOGIN_DATA = lambda dynamicKey, encpw, bvsd, encnm, client_id: \\ dict(LOGIN_DATA(dynamicKey, encpw, bvsd, encnm), **{\u0026#34;logintp\u0026#34;:\u0026#34;oauth2\u0026#34;,\u0026#34;svctype\u0026#34;:\u0026#34;64\u0026#34;,\u0026#34;client_id\u0026#34;:client_id}) class SmartstoreLogin(NaverLogin): def nid_login(self): self.fetch_keys() self.set_encpw() self.set_bvsd() self.fetch_oauth_url() data = SLOGIN_DATA(self.dynamicKey, self.encpw, self.bvsd, self.encnm, self.oauth_params.get(\u0026#34;client_id\u0026#34;)) headers = self.get_headers(LOGIN_URL, referer=self.oauth_url) headers[\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/x-www-form-urlencoded\u0026#34; headers[\u0026#34;Upgrade-Insecure-Requests\u0026#34;] = \u0026#34;1\u0026#34; response = self.post(LOGIN_URL, data=data, headers=headers) 네이버 로그인 과정에서는 bvsd를 생성한 후 OAuth URL을 추가로 가져오고\nclient_id를 기존의 로그인 데이터 내에 포함시켜 POST 요청을 보냅니다.\n해당 메소드의 결과로 NID_AUT, NID_JKL, NID_SES를 부여받을 수 있습니다.\nOAuth 로그인 구현 # OAuth 로그인은 네이버 로그인과 GraphQL 인증으로 구성됩니다.\n현시점에서 GraphQL 인증에 필요한 것은 oauth_token 뿐이기 때문에\n앞선 네이버 로그인 과정에서 획득한 주소로부터 oauth_token을 가져오는 메소드 fetch_oauth_token()과\n전체적인 OAuth 로그인 과정을 구현한 oauth_login() 메소드를 정의합니다.\nCopy python OAUTH_URL = \u0026#34;https://nid.naver.com/oauth2.0/authorize\u0026#34; class SmartstoreLogin(NaverLogin): def fetch_oauth_token(self): headers = self.get_headers(LOGIN_URL, referer=LOGIN_URL, cookies=self.get_cookies()) response = self.get(OAUTH_URL, headers=headers, params=self.oauth_params) if re.search(\u0026#34;(?\u0026lt;=oauth_token\\=)(.*?)(?=\u0026amp;)\u0026#34;, response.text): self.oauth_token = re.search(\u0026#34;(?\u0026lt;=oauth_token\\=)(.*?)(?=\u0026amp;)\u0026#34;, response.text).group() Copy python OAUTH_DATA = lambda code, state: str({ \u0026#34;operationName\u0026#34;:\u0026#34;snsLoginCallback\u0026#34;, \u0026#34;variables\u0026#34;: { \u0026#34;code\u0026#34;: code, \u0026#34;state\u0026#34;: state}, \u0026#34;query\u0026#34;:\u0026#34;mutation snsLoginCallback($code: String!, $state: String!) \\ {\\n snsCallback(snsLoginCallbackRequest: {code: $code, state: $state}) \\ {\\n statCd\\n loginStatus\\n nextUrl\\n sessionKey\\n snsCd\\n \\ idNo\\n realnm\\n age\\n email\\n __typename\\n }\\n}\\n\u0026#34; }).replace(\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;) class SmartstoreLogin(NaverLogin): def oauth_login(self): self.nid_login() self.fetch_oauth_token() code, state = self.oauth_token, self.oauth_params.get(\u0026#34;state\u0026#34;) referer = SLOGIN_URL+f\u0026#34;/oauth/callback?code={code}\u0026amp;state={state}\u0026#34; headers = self.get_headers(host=SLOGIN_URL, referer=referer, cookies=self.get_cookies()) response = self.post(urljoin(SLOGIN_URL, \u0026#34;graphql\u0026#34;), data=OAUTH_DATA(code, state), headers=headers) 2단계 인증 구현 # 2단계 인증을 직접 수행할 필요는 없습니다.\nNSI 쿠키 값을 할당받을 수 있는 주소로 POST 요청을 보내는\ntwo_factor_login() 메소드를 정의합니다.\nCopy python TWOLOGIN_URL = SMARTSTORE_URL+\u0026#34;api/login?url=https%3A%2F%2Fsell.smartstore.naver.com%2F%23%2Fhome%2Fdashboard\u0026#34; TWOLOGIN_DATA = {\u0026#34;url\u0026#34;: \u0026#34;https://sell.smartstore.naver.com/#/home/dashboard\u0026#34;} class SmartstoreLogin(NaverLogin): def two_factor_login(self): headers = self.get_headers(SMARTSTORE_URL, referer=SMARTSTORE_URL, cookies=self.get_cookies()) headers[\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/json;charset=UTF-8\u0026#34; headers[\u0026#34;x-current-state\u0026#34;] = \u0026#34;https://sell.smartstore.naver.com/#/login-callback\u0026#34; headers[\u0026#34;x-current-statename\u0026#34;] = \u0026#34;login-callback\u0026#34; headers[\u0026#34;x-to-statename\u0026#34;] = \u0026#34;login-callback\u0026#34; response = self.post(TWOLOGIN_URL, data=TWOLOGIN_DATA, headers=headers) 로그인 메소드 구현 # SmartstoreLogin 객체를 사용할 때는 login() 메소드를 활용합니다.\nCopy python class SmartstoreLogin(NaverLogin): def login(self): email_pattern = re.compile(\u0026#34;[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\u0026#34;) self.seller_login() if email_pattern.search(self.userid) else self.oauth_login() self.two_factor_login() 향후 판매자 계정으로 로그인 하는 경우를 고려해\nuserid가 이메일인 경우 seller_login() 이라는 미구현된 메소드를 실행하도록 정의했습니다.\n일반적인 네이버 아이디를 사용할 경우엔 OAuth 로그인과 2단계 인증을 거쳐\n처음 목적으로 했던 아래의 모든 쿠키 값을 획득하게 됩니다.\nCopy python cookies = { \u0026#34;NNB\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;nid_inf\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_AUT\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_JKL\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_CHK\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NSI\u0026#34;: \u0026#34;...\u0026#34;, } 해당 쿠키를 가진 SmartstoreLogin 객체를 세션 객체로 활용한다면\n스마트스토어센터 내 어떤 데이터라도 파이썬 requests 모듈로 가져올 수 있게 됩니다.\n"},{"id":23,"href":"/blog/smartstore-login-2/","title":"[Python] requests로 네이버 스마트스토어센터 로그인 구현하기 (2)","section":"Posts","content":"이번 게시글에서는 스마트스토어센터 페이지에서 데이터를 수집하는 자동화 프로그램을 제작하기 위한\n첫 번째 과정으로 네이버 로그인을 구현할 것입니다.\n앞선 게시글에서 데이터를 수집하는 방식에 대해 알아보면서\n로그인이 필요한 페이지에 접근하기 다음과 같은 쿠키 값이 필요함을 확인했습니다.\nCopy python cookies = { \u0026#34;NNB\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;nid_inf\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_AUT\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_JKL\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_CHK\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NSI\u0026#34;: \u0026#34;...\u0026#34;, } 위 키값들은 앞으로 로그인 프로세스를 파악하는 과정에서 중요하게 활용됩니다.\n네이버 로그인 이해 # 네이버 스마트스토어센터 로그인 과정에서 진행되는 네이버 로그인은\n일반적인 네이버 로그인과는 다른 과정으로 진행됩니다.\n따라서 우선 일반적인 네이버 로그인 과정을 알아보겠습니다.\n해당 파트는 아래 게시글을 참고해 작성되었습니다.\n파이썬#76 - 파이썬 크롤링 requests 로 네이버 로그인 하기\n네이버 로그인 요청 분석 # 네이버 로그인 과정을 분석하기 위해서는 우선 네이버 로그인을 요청을 시도하여\n전달되는 값을 확인해야 합니다.\n네이버 로그인 페이지에서 로그인을 수행하는 과정에서\n발견할 수 있는 POST 요청을 살펴보면 다음과 같은 데이터가 전달됨을 발견할 수 있습니다.\n암호화된 값을 생략하고 키로 전달되는 내용을 확인하면 다음과 같습니다.\nCopy json { \u0026#34;localechange\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;dynamicKey\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;encpw\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;enctp\u0026#34;: 1, \u0026#34;svctype\u0026#34;: 1, \u0026#34;smart_LEVEL\u0026#34;: 1, \u0026#34;bvsd\u0026#34;: { \u0026#34;uuid\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;encData\u0026#34;: \u0026#34;...\u0026#34; }, \u0026#34;encnm\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;locale\u0026#34;: \u0026#34;ko_KR\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://www.naver.com\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;pw\u0026#34;: \u0026#34;\u0026#34; } 공백이나 고정된 값을 가진 키를 제외하면 결과적으로\ndynamicKey, encpw, bvsd, encnm를 밝혀내는 것이 중요할 것이라 판단됩니다.\n네이버 로그인 폼 분석 # 키의 명칭만으로는 무엇을 의미하는지 알 수 없기 때문에\n로그인 페이지 소스에서 키명칭을 검색하였고 네이버 로그인 폼에서 하나의 단서를 찾을 수 있었습니다.\ndynamicKey의 경우 로그인 폼에 동적으로 부여되는 값임을 알 수 있습니다.\n하지만 나머지 encpw, bvsd, encnm의 값은 비어있기 때문에\n다른 자바스크립트 응답을 분석해야 합니다.\n네이버 로그인 RSA 암호화 # encpw 값에 대한 단서를 찾기 위해 전체 검색을 수행했을 때\ncommon_202201.js 내부에서 RSA 암호화 처리를 통해 값을 생성함을 알 수 있습니다.\n그 중에서 가장 처음 단계로 실행될 것이라 추측되는 것이 아래 confirmSubmit() 함수입니다.\n해당 함수는 아이디와 비밀번호의 여부를 체크하고 encryptIdPw() 함수의 결과를 반환합니다.\n바로 밑에서 확인할 수 있는 encryptIdPw() 함수의 내용은 다음과 같습니다.\nCopy js function encryptIdPw() { var id = $(\u0026#34;id\u0026#34;); var pw = $(\u0026#34;pw\u0026#34;); var encpw = $(\u0026#34;encpw\u0026#34;); var rsa = new RSAKey; if (keySplit(session_keys)) { rsa.setPublic(evalue, nvalue); try{ encpw.value = rsa.encrypt( getLenChar(sessionkey) + sessionkey + getLenChar(id.value) + id.value + getLenChar(pw.value) + pw.value); } catch(e) { return false; } $(\u0026#39;enctp\u0026#39;).value = 1; id.value = \u0026#34;\u0026#34;; pw.value = \u0026#34;\u0026#34;; return true; } else { getKeyByRuntimeInclude(); return false; } return false; } 해당 함수는 session_keys라는 값을 처리하고 RSA 암호화한 결과를\nencpw의 값으로 대체하는 것을 알 수 있습니다.\n마찬가지로 해당 명칭을 검색했을 때\nsession_keys는 Ajax 통신의 응답 결과를 받아오는 것을 확인할 수 있습니다.\n하지만 네이버 로그인 페이지에서 svctype=262144를 추가적인 파라미터로 입력할 경우\n접근할 수 있는 모바일 로그인 페이지에서 해당 값을 확인할 수 있었습니다.\n다시 encryptIdPw() 함수로 돌아가서 session_keys를 처리하기 위해\nkeySplit() 함수를 찾아보았습니다.\nCopy js function keySplit(a) { keys = a.split(\u0026#34;,\u0026#34;); if (!a || !keys[0] || !keys[1] || !keys[2] || !keys[3]) { return false; } sessionkey = keys[0]; keyname = keys[1]; evalue = keys[2]; nvalue = keys[3]; $(\u0026#34;encnm\u0026#34;).value = keyname; return true } 모바일 페이지에서 볼 수 있는 session_keys 값은 콤마를 기준으로\n4개의 값으로 구분되어 있었는데 해당 함수에서는 각각을\nsessionKey, encnm, evalue, nvalue으로 분리했습니다.\n여기서 encnm 값을 우선적으로 가져올 수 있었고,\n다음으로 encpw 값을 찾기 위해 RSA 암호화 부분을 탐색해봅니다.\nCopy js rsa.setPublic(evalue, nvalue); encpw.value = rsa.encrypt( getLenChar(sessionkey) + sessionkey + getLenChar(id.value) + id.value + getLenChar(pw.value) + pw.value); session_keys에서 분리된 evalue와 nvalue로 RSA 공개키를 생성하고\n마찬가지로 session_keys에 포함된 sessionKey 및 아이디, 비밀번호의 조합을\n암호화한 결과가 encpw임을 확인할 수 있습니다.\n파이썬에서는 공개키 생성을 rsa.PublicKey() 함수로 수행할 수 있으며\nrsa.encrypt() 함수로 RSA 암호화를 진행할 수 있습니다.\n해당 과정은 아래와 같이 구현됩니다.\nCopy python publicKey = rsa.PublicKey(int(nvalue,16), int(evalue,16)) value = \u0026#39;\u0026#39;.join([chr(len(key))+key for key in [sessionKey, id, pw]]) encpw = rsa.encrypt(value.encode(), publicKey).hex() 여기까지의 과정으로 dynamicKey, encpw, encnm의 값을 얻을 수 있습니다.\nbvsd 값 생성하기 # 마지막으로 필요한 bvsd 값에 대한 단서는 응답 문서 내에서\nbvsd.1.3.8.min.js란 명칭으로 알기 쉽게 확인할 수 있지만\n그 내용은 가독성 면에서 쉽게 해석하기 어려웠습니다.\n다른 자료를 참고했을 때 bvsd는 브라우저가 정상적인지 여부를 파악하기 위한 값으로\n해당 값이 없을 경우 로그인 과정에서 캡차를 발생시킨다는 것을 알 수 있었습니다.\nbvsd.1.3.8.min.js에서 주목할 부분은 uuid 및 encData를 생성하는 부분인데\n아래 코드에서 encData는 o라는 값을 인코딩하는 것으로 추측됩니다.\no 값을 코드 내에서 찾아보니 아래와 같이 디바이스의 마우스 상태 등을\n기록한 값임을 확인할 수 있었습니다.\nCopy js o = { a: n, b: \u0026#34;1.3.8\u0026#34;, c: (0, m[\u0026#34;default\u0026#34;])(), d: r, e: this._deviceOrientation.get(), f: this._deviceMotion.get(), g: this._mouse.get(), j: this._fpDuration || y.NOT_YET, h: this._fpHash || \u0026#34;\u0026#34;, i: this._fpComponent || [] }; 하지만 각각의 값을 해석하고 생성하는 것은 쉽지 않았기에\n이미 완성된 코드를 참고하여 set_bvsd() 메소드를 정의했습니다.\nencData의 인코딩에는 lzstring 모듈의\nLZString.compressToEncodedURIComponent() 함수를 활용했습니다.\nCopy python from lzstring import LZString import uuid ENC_DATA = lambda uuid, userid, passwd: str({ \u0026#34;a\u0026#34;: f\u0026#34;{uuid}-4\u0026#34;, \u0026#34;b\u0026#34;: \u0026#34;1.3.4\u0026#34;, \u0026#34;d\u0026#34;: [{ \u0026#34;i\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;b\u0026#34;: {\u0026#34;a\u0026#34;: [\u0026#34;0\u0026#34;, userid]}, \u0026#34;d\u0026#34;: userid, \u0026#34;e\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;f\u0026#34;: \u0026#34;false\u0026#34; }, { \u0026#34;i\u0026#34;: passwd, \u0026#34;e\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;f\u0026#34;: \u0026#34;false\u0026#34; }], \u0026#34;h\u0026#34;: \u0026#34;1f\u0026#34;, \u0026#34;i\u0026#34;: {\u0026#34;a\u0026#34;: \u0026#34;Mozilla/5.0\u0026#34;} }).replace(\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;) class NaverLogin(LoginSpider): def set_bvsd(self): uuid4 = str(uuid.uuid4()) encData = LZString.compressToEncodedURIComponent(ENC_DATA(uuid4, self.userid, self.passwd)) self.bvsd = str({\u0026#34;uuid\u0026#34;:uuid4, \u0026#34;encData\u0026#34;:encData}).replace(\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;) 네이버 로그인 구현 # 지금까지의 과정을 통해 네이버 로그인에 필요한\ndynamicKey, encpw, bvsd, encnm 값을 생성하는 법을 파악했습니다.\n이를 NaverLogin 클래스의 메소드로 구현해보겠습니다.\nRSA 암호화 구현 # 먼저 dynamicKey와 함께 encpw, encmn 생성에 필요한\nsession_keys를 가져오기 위한 메소드 fetch_keys()와,\nRSA 암호화를 통해 encpw 값을 구하는 set_encpw() 메소드를 정의합니다.\nCopy python from bs4 import BeautifulSoup import rsa LOGIN_URL = \u0026#34;https://nid.naver.com/nidlogin.login\u0026#34; class NaverLogin(LoginSpider): def fetch_keys(self): response = self.get(LOGIN_URL, headers=self.get_headers(host=LOGIN_URL), params={\u0026#34;svctype\u0026#34;:\u0026#34;262144\u0026#34;}) source = BeautifulSoup(response.text, \u0026#39;lxml\u0026#39;) keys = source.find(\u0026#34;input\u0026#34;, {\u0026#34;id\u0026#34;:\u0026#34;session_keys\u0026#34;}).attrs.get(\u0026#34;value\u0026#34;) self.sessionKey, self.encnm, n, e = keys.split(\u0026#34;,\u0026#34;) self.dynamicKey = source.find(\u0026#34;input\u0026#34;, {\u0026#34;id\u0026#34;:\u0026#34;dynamicKey\u0026#34;}).attrs.get(\u0026#34;value\u0026#34;) self.publicKey = rsa.PublicKey(int(n,16), int(e,16)) session_keys의 경우 모바일 로그인 페이지에서만 가져올 수 있기 때문에\nsvctype=262144를 GET 요청의 파라미터로 전달해 모바일 로그인 페이지를 가져옵니다.\nnvalue와 evalue는 별도의 변수로 저장하지 않고\npublicKey를 생성해 클래스 변수로 저장합니다.\nCopy python class NaverLogin(LoginSpider): def set_encpw(self): value = \u0026#34;\u0026#34;.join([chr(len(key))+key for key in [self.sessionKey, self.userid, self.passwd]]) self.encpw = rsa.encrypt(value.encode(), self.publicKey).hex() 앞에서 가져온 sessionKey와 함께 미리 초기화된 네이버 아이디 및 비밀번호를\n조합 및 암호화하여 encpw를 생성합니다.\nPOST 요청 구현 # 미리 정의한 set_bvsd() 메소드를 포함해 모든 준비 과정이 마무리되었습니다.\n클래스 변수로 저장된 암호화된 값들을 데이터에 담아 POST 로그인 요청을 보내는\nlogin() 메소드는 다음과 같이 정의할 수 있습니다.\nCopy python NAVER_URL = \u0026#34;https://www.naver.com\u0026#34; LOGIN_DATA = lambda dynamicKey, encpw, bvsd, encnm: { \u0026#34;localechange\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;dynamicKey\u0026#34;: dynamicKey, \u0026#34;encpw\u0026#34;: encpw, \u0026#34;enctp\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;svctype\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;smart_LEVEL\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;bvsd\u0026#34;: bvsd, \u0026#34;encnm\u0026#34;: encnm, \u0026#34;locale\u0026#34;: \u0026#34;ko_KR\u0026#34;, \u0026#34;url\u0026#34;: quote_plus(NAVER_URL), \u0026#34;id\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;pw\u0026#34;: \u0026#34;\u0026#34;, } class NaverLogin(LoginSpider): def login(self): self.fetch_keys() self.set_encpw() self.set_bvsd() data = LOGIN_DATA(self.dynamicKey, self.encpw, self.bvsd, self.encnm) headers = self.get_headers(LOGIN_URL, referer=LOGIN_URL) headers[\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/x-www-form-urlencoded\u0026#34; headers[\u0026#34;Upgrade-Insecure-Requests\u0026#34;] = \u0026#34;1\u0026#34; self.post(LOGIN_URL, data=data, headers=headers) POST 요청 시 전달되었던 데이터와 동일한 값을 반환하는 LOGIN_DATA 함수를 생성하고\n암호화된 값을 전달해 최종적인 POST 데이터를 만들었습니다.\n해당 데이터로 요청을 보낼 경우 정상적인 응답을 받게 되고\nNaverLogin 세션 객체의 쿠키 값을 확인하면 아래와 같은 결과를 확인할 수 있습니다.\nCopy python naver = NaverLogin(\u0026#34;userid\u0026#34;, \u0026#34;passwd\u0026#34;) naver.login() naver.get_cookies() ====================================== \u0026#39;NID_AUT=...; NID_JKL=...; NID_SES=...; nid_inf=1228467713\u0026#39; 또한 해당 결과는 개발자 도구에서도 응답 헤더의 set-cookie 값에서 찾아볼 수 있습니다.\n지금까지의 과정으로 네이버 로그인 과정을 거쳤을 때,\n게시글의 서두에서 언급한 쿠키 값의 목록 중에서 일부 값을 획득할 수 있습니다.\nCopy python cookies = { \u0026#34;NNB\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;nid_inf\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_AUT\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_JKL\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_CHK\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NSI\u0026#34;: \u0026#34;...\u0026#34;, } 이 중에서 NNB의 경우 네이버 페이지 접속 시 기본적으로 부여되는 값이기 때문에 무시하고\nNID_AUT, NID_JKL, NID_SES가 채워졌습니다.\n나머지 값들은 스마트스토어센터 로그인 과정에서 얻을 수 있기 때문에\n다음 게시글에서 다뤄보도록 하겠습니다.\n"},{"id":24,"href":"/blog/smartstore-login-1/","title":"[Python] requests로 네이버 스마트스토어센터 로그인 구현하기 (1)","section":"Posts","content":"네이버 스마트스토어센터에서는 매출 향상에 도움을 주는 유용한 통계 데이터를 제공해줍니다.\n쇼핑몰 데이터를 분석하는 입장에서 무료로 제공되는 이런 데이터는 큰 도움이 되지만,\n대부분이 엑셀 파일 다운로드를 지원하지 않고 빈번하게 수치가 바뀌는 데이터를 각각의 메뉴에서 매번 확인하기도 어렵습니다.\n이런 데이터를 자동화 프로그램으로 수집 및 적재할 수 있다면 업무 효율을 크게 향상시킬 수 있을 것입니다.\n이번 게시글에서는 실제 네이버 스마트스토어 로그인 구현에 앞서\n데이터 수집에 대한 간단한 설명을 진행하고 네이버 로그인 구현의 바탕이 되는 클래스와 메소드를 정의합니다.\n데이터 수집 개요 # 네이버 웹사이트에서 데이터를 수집할 때 활용할 수 있는 방안은 2가지가 있습니다.\n첫 번째는 CSS Selector 또는 XPath를 활용해 웹사이트 특정 위치의 값을 가져오는 것,\n두 번째는 API에 요청을 보내 JSON 형태의 데이터를 가져오는 것입니다.\n특정 위치의 값을 가져오는 첫 번째 방식은 UI에 의존적이어서 코드의 지속성을 보장하기 어렵고\n원하는 데이터와 관련없는 웹 소스 전체를 불러오기 때문에 속도 면에서도 단점이 있습니다.\n따라서, API를 제공하는 경우 두 번째 방식을 이용하는 것이 효율적입니다.\n데이터 수집 시나리오 # 네이버 쇼핑에서 표시되는 상품의 순위는 검색인기도를 기준으로 결정됩니다.\n키워드별 상위권 상품의 검색인기도를 가져오는 것을 예시로 데이터 수집을 진행해보겠습니다.\n위 이미지에서 왼쪽 부분은 실제 UI, 오른쪽 부분은 HTML 소스 입니다.\n해당 소스에서 데이터를 가져온다면 div.popularity-product \u0026gt; div.box-border 위치에서\ndd 태그를 순서대로 지정해서 각각의 종합, 적합도, 인기도 값을 가져올 수 있습니다.\n해당 데이터를 분석에 활용하기 위해서는 인기도 수치를 구성하는 클릭수, 판매실적 등도 필요하기 때문에\n상세보기 페이지를 확인해야하고 결과적으로 하나의 상품에 대한 데이터를 보기 위해 두 개의 페이지를 방문해야 합니다.\n하지만 네이버의 대부분의 웹페이지는 API를 기반으로 가져온 데이터로 구성되기 때문에\n해당 API를 활용할 수 있다면 더욱 효율적인 데이터 수집이 가능합니다.\n서버에서 가져오는 데이터를 확인할 때는 주로 개발자 도구의 네트워크 탭을 활용합니다.\n웹페이지 로드 시 가져오는 문서를 확인하다보면 위 이미지와 같이 목표로 하는 데이터를 보내주는 API를 발견할 수 있습니다.\n새 탭에서 해당 API 주소를 요청하면 위 이미지 내 오른쪽 부분과 같은 JSON 형식의 데이터를 받을 수 있습니다.\n실제 UI에서 가져오고자 하는 종합, 적합도, 인기도 수치도 해당 데이터에서 확인할 수 있습니다.\n여기에는 추가로 클릭수, 판매실적 등에 대한 수치 데이터도 포함되어 있기 때문에\n해당 API를 활용하면 다수의 페이지에 요청을 보낼 수고도 줄어들게 됩니다.\n로그인이 필요한 페이지의 데이터 가져오기 # 여기까지는 간단해보이지만 네이버 스마트스토어센터 데이터를 requests 모듈로 가져오는데는\n하나의 추가적인 문제가 존재합니다.\n단순한 GET 요청일지라도 로그인 정보를 갖고 있지 않다면 데이터를 받을 수 없습니다.\n스마트스토어센터에 로그인하지 않은 상태에서 위 API 주소로 요청을 보내게 된다면\n아래와 같은 에러 메시지를 받아볼 수 있습니다.\nCopy json { \u0026#34;error\u0026#34;: \u0026#34;Full authentication is required to access this resource\u0026#34; } 이 문제에 대한 해결방법은 헤더에 있습니다.\n개발자 도구 네트워크 탭에서 하나의 문서를 클릭하고 Headers 탭에서 스크롤을 내리면\n아래와 같은 Request Headers 정보를 확인할 수 있습니다.\n서버와 클라이언트 간 네트워크 요청 시 서버는 클라이언트의 정보를 확인할 목적으로\n클라이언트에 쿠키라는 암호화된 인증 정보를 남깁니다.\n클라이언트가 해당 정보를 헤더에 담아 요청을 보내는 경우에만 서버가 올바른 응답을 전달합니다.\nrequests 모듈에서는 이러한 과정을 다음과 같이 구현할 수 있습니다.\nCopy python headers = {\u0026#34;cookie\u0026#34;: \u0026#34;...\u0026#34;} response = requests.get(url, headers=headers) 하지만 일반적인 쿠키 값은 30분의 유통기한이 있기 때문에, 매번 쿠키 값을 갱신해야 하는데\n자동화 프로그램을 돌리기 전에 직접 로그인해서 쿠키 값을 갱신하는 것은 바람직하지 못합니다.\n결과적으로 로그인이 필요한 스마트스토어 페이지의 데이터를 가져오기 위해서는\n자동화된 로그인 과정을 거쳐서 쿠키 값을 갱신할 필요가 있습니다.\n쿠키 확인하기 # 클라이언트에서 요청하는 헤더 내역에서 확인할 수 있는 정보는 표현하면 다음과 같습니다.\nCopy json { \u0026#34;NNB\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;nid_inf\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_AUT\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_JKL\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_CHK\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NSI\u0026#34;: \u0026#34;...\u0026#34; } 이는 앞으로 스마트스토어센터 로그인을 구현하는데서 반드시 확인해야할 목록입니다.\n지금은 이 값들이 어떤 의미를 가지고 어디서 발생하는 값인지 알 수는 없지만,\n서버로부터 해당 값들을 받아오는 것에 집중하여 로그인 프로세스를 파악하고\n로그인 진행 과정을 쿠키 값을 통해 시각적으로 점검할 것입니다.\n스마트스토어센터 로그인 개요 # 스마트스토어센터 로그인을 구현하기 위해 로그인 페이지를 탐색할 필요가 있습니다.\n메인 페이지에서 로그인하기 버튼을 클릭했을 때 이동하는 로그인 페이지에서 실제 로그인이 이루어집니다.\n스마트스토어센터 로그인에는 판매자 아이디로 로그인하는 방식과\n네이버 아이디로 로그인하는 방식이 있습니다.\n우선적으로 네이버 아이디로 로그인하는 방식을 알아보겠습니다.\n네이버 로그인을 구현하는 것에 관해선 좋은 선례가 있어 많은 부분을 참고했습니다.\n해당 내용은 아래 링크를 참고할 수 있습니다.\n파이썬#76 - 파이썬 크롤링 requests 로 네이버 로그인 하기\n클래스 정의 # 네이버 로그인 기능은 자동화 프로그램에서 지속적으로 활용될 것이기 때문에\n별도의 클래스에서 메소드로 구현할 필요가 있습니다.\n먼저 requests 모듈의 Session 클래스를 상속받는 NaverLogin 클래스를 정의합니다.\nNaverLogin은 네이버 ID와 비밀번호를 초기화하는 단순한 기능만을 구현했지만\nrequests.Session 클래스를 상속받았기 때문에\n웹페이지 요청과 관련된 다양한 기능을 가지고 있습니다.\nCopy python class NaverLogin(requests.Session): def __init__(self, userid: str, passwd: str, **kwargs): super().__init__(**kwargs) self.userid = userid self.passwd = passwd 그리고 NaverLogin을 상속받는 SmartstoreLogin 클래스를 정의합니다.\n일반적인 네이버 로그인과 스마트스토어센터에서 진행되는 네이버 로그인이 다르기 때문에\nNaverLogin 메소드의 일부를 변경할 필요가 있을 것입니다.\nCopy python class SmartstoreLogin(NaverLogin): def __init__(self, userid=str(), passwd=str(), **kwargs): super().__init__(userid, passwd, **kwargs) 추가적으로 로그인 페이지 요청 과정에서 빈번하게 정의해야 하는 매개변수 생성을\n간단하게 할 수 있는 메소드를 정의하겠습니다.\n헤더 생성 메소드 정의 # requests 모듈은 기본적으로 헤더를 갖고 있지 않는데\n이 상태로 다수의 웹페이지에 요청을 보낸다면 로봇으로 간주당해 차단당할 것입니다.\n임의의 웹페이지에 요청을 보낼 때 확인할 수 있는 요청 헤더 HEADERS를 기본 바탕으로,\n웹페이지 별로 최적화된 헤더를 생성하는 get_headers() 메소드를 정의합니다.\nCopy python HEADERS = { \u0026#34;Accept\u0026#34;: \u0026#34;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\u0026#34;, \u0026#34;Accept-Encoding\u0026#34;: \u0026#34;gzip, deflate, br\u0026#34;, \u0026#34;Accept-Language\u0026#34;: \u0026#34;ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\u0026#34;, \u0026#34;Connection\u0026#34;: \u0026#34;keep-alive\u0026#34;, \u0026#34;sec-ch-ua\u0026#34;: \u0026#39;\u0026#34;Chromium\u0026#34;;v=\u0026#34;106\u0026#34;, \u0026#34;Google Chrome\u0026#34;;v=\u0026#34;106\u0026#34;, \u0026#34;Not;A=Brand\u0026#34;;v=\u0026#34;99\u0026#34;\u0026#39;, \u0026#34;sec-ch-ua-mobile\u0026#34;: \u0026#34;?0\u0026#34;, \u0026#34;sec-ch-ua-platform\u0026#34;: \u0026#39;\u0026#34;Windows\u0026#34;\u0026#39;, \u0026#34;Sec-Fetch-Dest\u0026#34;: \u0026#34;empty\u0026#34;, \u0026#34;Sec-Fetch-Mode\u0026#34;: \u0026#34;cors\u0026#34;, \u0026#34;Sec-Fetch-Site\u0026#34;: \u0026#34;same-origin\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\u0026#34;, } Copy python from urllib.parse import urlparse class NaverLogin(requests.Session): def get_headers(self, authority=str(), referer=str(), cookies=str(), host=str(), **kwargs) -\u0026gt; Dict[str,Any]: headers = HEADERS.copy() if authority: headers[\u0026#34;Authority\u0026#34;] = urlparse(authority).hostname if host: headers[\u0026#34;Host\u0026#34;] = urlparse(host).hostname if referer: headers[\u0026#34;Referer\u0026#34;] = referer if cookies: headers[\u0026#34;Cookie\u0026#34;] = cookies return dict(headers, **kwargs) 호스트명을 의미하는 Authority 또는 Host, 리다이렉트 전 경로를 의미하는 Referer,\n그리고 쿠키를 의미하는 Cookie 등의 값은 수시로 변하기 때문에 별도의 입력으로 지정합니다.\n쿠키 생성 메소드 정의 # 헤더와 함께 활용되는 쿠키는 헤더와 마찬가지로 웹페이지 요청 시 빈번히 활용되는데\nrequests 모듈의 쿠키 자료형인 RequestsCookieJar를 헤더에 직접 포함시킬 수 없기 때문에,\n쿠키를 적절한 형태의 문자열로 변환하는 get_cookies() 메소드를 정의합니다.\nCopy python from requests.cookies import RequestsCookieJar class NaverLogin(requests.Session): def get_cookies(self, **kwargs) -\u0026gt; str: return self.parse_cookies(dict(self.cookies, **kwargs)) def parse_cookies(self, cookies: RequestsCookieJar) -\u0026gt; str: return \u0026#34;; \u0026#34;.join([str(key)+\u0026#34;=\u0026#34;+str(value) for key,value in cookies.items()]) 마치며 # 이번 게시글에서는 두 가지 데이터 수집 방식을 예시를 통해 알아보았고\n스마트스토어센터 로그인의 바탕이 되는 클래스와 메소드를 정의했습니다.\n다음 게시글에서는 네이버 로그인을 본격적으로 구현해보겠습니다.\n"},{"id":25,"href":"/blog/hugo-blog-old-3/","title":"Hugo 블로그 만들기 [2022년] (3) - 테마 커스터마이징","section":"Posts","content":"Hugo 블로그 만들기 (3) - 테마 커스터마이징 # 블로그를 구성할 때 기술적, 시간적 한계 때문에 이미 만들어진 테마를 사용하게 됩니다.\n제가 Hugo 블로그를 만들 때도 이러한 문제 때문에 PaperMod 테마를 사용했지만,\n블로그를 보다보면 만족스럽지 못한 부분이 발견됩니다.\n이번 포스트에서는 제가 PaperMod 테마를 커스터마이징한 과정을 안내해드리겠습니다.\nArchive, Search 추가하기 # PaperMod 테마를 가져오면서 가장 신경쓰였던 부분은\n메인 메뉴가 Categories, Tags 두 개 뿐이었단 점입니다.\nArchive는 그렇다쳐도 Search 기능은 빼먹을 수 없는 부분이라 생각하기 때문에,\nHugo 및 PaperMod 내 이슈를 참고하여 관련된 내용을 탐색했습니다.\n다행히 PaperMod 테마에서 해당 기능을 연결하지 않았을 뿐,\n기능에 대한 레이아웃은 존재하기 때문에 content/ 디렉토리 아래 다음과 같은 파일을 추가했습니다.\nCopy yaml # content/archive.md --- title: \u0026#34;Archive\u0026#34; layout: \u0026#34;archives\u0026#34; url: \u0026#34;/archive\u0026#34; summary: \u0026#34;archive\u0026#34; --- Copy yaml # content/search.md --- title: \u0026#34;Search\u0026#34; layout: \u0026#34;search\u0026#34; url: \u0026#34;/search\u0026#34; summary: \u0026#34;search\u0026#34; --- 추가로, 설정에서도 해당 파일을 인식해야되기 때문에 다음과 같은 설정을 추가했습니다.\npost/ 외에 다른 디렉토리를 등록하고 싶은 경우에도 해당 키값을 활용할 수 있습니다.\nCopy yaml params: mainsections: [\u0026#34;page\u0026#34;, \u0026#34;post\u0026#34;, \u0026#34;archive\u0026#34;, \u0026#34;search\u0026#34;] 마지막으로, 메인 메뉴에서 해당 링크로 이동하기 위한 바로가기를 추가했습니다.\n여기에는 카테고리, 태그 등이 있을건데 weight 값을 통해 적절하게 위치를 조정할 수 있습니다.\nCopy yaml menu: main: - identifier: archive name: Archive url: /archive/ weight: 10 - identifier: search name: Search url: /search/ weight: 20 위와 같은 과정을 통해 Archive, Search 기능을 추가했습니다.\n검색 엔진 등록하기 # 검색 엔진에 등록하기 위한 과정은 해당 영상을 참고해주시기 바랍니다.\n저는 위 과정에서 블로그 내에 추가해야 할 Site Verification Tag를 추가하는 법을 전달드리겠습니다.\nPaperMod 테마에서는 아래처럼 해당 부분이 만들어져 있기 때문에 크게 걱정할 필요는 없습니다.\n아래는 layouts/partials/ 내에 head.html 파일에서 가져왔습니다.\nCopy html {{- if site.Params.analytics.google.SiteVerificationTag }} \u0026lt;meta name=\u0026#34;google-site-verification\u0026#34; content=\u0026#34;{{ site.Params.analytics.google.SiteVerificationTag }}\u0026#34;\u0026gt; {{- end }} {{- if site.Params.analytics.yandex.SiteVerificationTag }} \u0026lt;meta name=\u0026#34;yandex-verification\u0026#34; content=\u0026#34;{{ site.Params.analytics.yandex.SiteVerificationTag }}\u0026#34;\u0026gt; {{- end }} {{- if site.Params.analytics.bing.SiteVerificationTag }} \u0026lt;meta name=\u0026#34;msvalidate.01\u0026#34; content=\u0026#34;{{ site.Params.analytics.bing.SiteVerificationTag }}\u0026#34;\u0026gt; {{- end }} {{- if site.Params.analytics.naver.SiteVerificationTag }} \u0026lt;meta name=\u0026#34;naver-site-verification\u0026#34; content=\u0026#34;{{ site.Params.analytics.naver.SiteVerificationTag }}\u0026#34;\u0026gt; {{- end }} 구글, 네이버 외에 Bing, Yandex를 지원하며 저는 다음과 같이 구글과 네이버만 설정했습니다.\nCopy yaml params: analytics: google: SiteVerificationTag: \u0026lt;YOUR-VERIFICATION-TAG\u0026gt; naver: SiteVerificationTag: \u0026lt;YOUR-VERIFICATION-TAG\u0026gt; 번외로 Google Tag 등 head에 추가로 입력할 부분이 있다면,\n동일한 위치에 extend_head.html을 사용할 수 있습니다.\n아래는 제가 extend_head.html 내에 Google Tag를 위한 스크립트를 추가한 부분입니다.\nCopy html {{- if site.GoogleAnalytics }} {{- /* Google tag (gtag.js) */}} \u0026lt;script async src=\u0026#34;https://www.googletagmanager.com/gtag/js?id={{ site.GoogleAnalytics }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\u0026#39;js\u0026#39;, new Date()); gtag(\u0026#39;config\u0026#39;, \u0026#39;{{ site.GoogleAnalytics }}\u0026#39;); \u0026lt;/script\u0026gt; {{- end }} KaTex 추가하기 # KaTex는 웹에서 수식을 표현하기 위한 방식입니다.\n제 과거 게시글엔 KaTex 표기법을 사용한 것이 존재하는데 이것이 제대로 표시되지 않는 문제를 발견했습니다.\n저는 공식 문서 대신 Stack Overflow 등을 참고해 아래 코드를 extend_head.html에 추가했는데,\n아쉽게도 출처는 남겨두지 못했습니다.\nCopy html \u0026lt;script\u0026gt; MathJax = { tex: { inlineMath: [[\u0026#39;$\u0026#39;, \u0026#39;$\u0026#39;], [\u0026#39;\\\\(\u0026#39;, \u0026#39;\\\\)\u0026#39;]], displayMath: [[\u0026#39;$$\u0026#39;,\u0026#39;$$\u0026#39;], [\u0026#39;\\\\[\u0026#39;, \u0026#39;\\\\]\u0026#39;]], processEscapes: true, processEnvironments: true }, options: { skipHtmlTags: [\u0026#39;script\u0026#39;, \u0026#39;noscript\u0026#39;, \u0026#39;style\u0026#39;, \u0026#39;textarea\u0026#39;, \u0026#39;pre\u0026#39;] } }; window.addEventListener(\u0026#39;load\u0026#39;, (event) =\u0026gt; { document.querySelectorAll(\u0026#34;mjx-container\u0026#34;).forEach(function(x){ x.parentElement.classList += \u0026#39;has-jax\u0026#39;}) }); \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://polyfill.io/v3/polyfill.min.js?features=es6\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; id=\u0026#34;MathJax-script\u0026#34; async src=\u0026#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Cover 간편하게 지정하기 # 저는 Github 저장소 내에 업로드한 이미지 주소를 속성값에 연결해 블로그 이미지를 표시하는데,\n게시글을 작성할 때마다 지정하게 되는 Cover 이미지의 경우 매번 전체 링크를 지정하는게 불편했습니다.\n대표적으로 해당 게시글의 Cover 이미지 주소는 다음과 같습니다.\nCopy html https://github.com/minyeamer/til/blob/main/.media/covers/hugo-logo.png?raw=true 저는 여기서 hugo-logo.png를 제외한 앞뒤의 요소가 불필요하다는 것을 인식했고\n설정 파일에 다음과 같이 prefix, suffix라는 키값으로 지정하게 처리했습니다.\nCopy yaml params: cover: prefix: \u0026#34;https://github.com/minyeamer/til/blob/main/.media/covers/\u0026#34; suffix: \u0026#34;?raw=true\u0026#34; 그리고 해당 설정을 적용시키기 위해 실질적으로 Cover 이미지를 표시하는 layouts/partials/ 아래 cover.html 파일을 수정했습니다.\n주석으로 지정된 부분이 원본이며, image 키값의 앞뒤로 prefix와 suffix를 덧붙였습니다.\nCopy html \u0026lt;!-- {{- if $addLink }}\u0026lt;a href=\u0026#34;{{ (.Params.cover.image) | absURL }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34;\u0026gt;{{ end -}} \u0026lt;img loading=\u0026#34;lazy\u0026#34; src=\u0026#34;{{ (.Params.cover.image) | absURL }}\u0026#34; alt=\u0026#34;{{ $alt }}\u0026#34;\u0026gt; --\u0026gt; {{- if $addLink }}\u0026lt;a href=\u0026#34;{{ if site.Params.cover.prefix }}{{ site.Params.cover.prefix }}{{ end }}{{ .Params.cover.image }}{{ if site.Params.cover.suffix }}{{ site.Params.cover.suffix }}{{ end }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34;\u0026gt;{{ end -}} \u0026lt;img loading=\u0026#34;lazy\u0026#34; src=\u0026#34;{{ if site.Params.cover.prefix }}{{ site.Params.cover.prefix }}{{ end }}{{ .Params.cover.image }}{{ if site.Params.cover.suffix }}{{ site.Params.cover.suffix }}{{ end }}\u0026#34; alt=\u0026#34;{{ $alt }}\u0026#34;\u0026gt; 기타 설정 # 너비 설정 # 초기에 PaperMod 테마를 사용할 때 너비가 좁아 불편한 느낌이 있었습니다.\n해당 설정은 css 파일로 지정할 것이라 생각했고,\nassets/css/core/ 경로에 있는 theme-vars.css 파일을 발견해 다음과 같이 수정했습니다.\n기존 720px에서 900px로 늘어나 쾌적하게 블로그를 볼 수 있게 되었습니다.\nCopy css :root { --main-width: 900px; 새 탭에서 링크 열기 # 다음으로 관심을 가진 건 깃허브에서 매번 불편하게 생각했던 링크 오픈 방식인데,\n개인적으로는 현재 탭이 아닌 새 탭에서 열리는 방식을 선호하기 때문에 해당 부분의 수정이 필요했습니다.\n다행히 Hugo 이슈 내용 중 다음과 같은 답변을 참고해 파일을 추가했습니다.\n아래는 layouts/_default/_markup/ 경로에 추가한 render-link.html 파일입니다.\nCopy html \u0026lt;a href=\u0026#34;{{ .Destination | safeURL }}\u0026#34;{{ with .Title}} title=\u0026#34;{{ . }}\u0026#34;{{ end }}{{ if strings.HasPrefix .Destination \u0026#34;http\u0026#34; }} target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34;{{ end }}\u0026gt;{{ .Text | safeHTML }}\u0026lt;/a\u0026gt; 포스트 수정 # 마지막으로 포스트 수정 버튼에 문제를 인식했습니다.\n물론, 모든 포스트는 로컬에서 작성하고 수정하지만, 오류가 발생하는 버튼을 그냥 놔둘 수는 없습니다.\nGo에 대해 잘 알지 못해 최선의 기능이라고 생각하지는 않지만,\n검색을 통해 발견한 replace 함수를 사용해 기존 경로에서 오류를 일으키는 부분을 제거했습니다.\nCopy html {{- if or .Params.editPost.URL site.Params.editPost.URL -}} {{- $fileUrlPath := path.Join .File.Path }} {{- if or .Params.author site.Params.author (.Param \u0026#34;ShowReadingTime\u0026#34;) (not .Date.IsZero) .IsTranslated }}\u0026amp;nbsp;|\u0026amp;nbsp;{{- end -}} \u0026lt;a href=\u0026#39;{{ .Params.editPost.URL | default site.Params.editPost.URL }}{{ if .Params.editPost.appendFilePath | default ( site.Params.editPost.appendFilePath | default false ) }}/{{ replace $fileUrlPath site.Params.editPost.ignoreFilePath \u0026#34;\u0026#34; 1 }}{{ end }}\u0026#39; rel=\u0026#34;noopener noreferrer\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt; {{- .Params.editPost.Text | default (site.Params.editPost.Text | default (i18n \u0026#34;edit_post\u0026#34; | default \u0026#34;Edit\u0026#34;)) -}} \u0026lt;/a\u0026gt; {{- end }} 개선사항 # 현재 PaperMod 테마의 카테고리는 아래 그림처럼 태그와 동일한 리스트 템플릿을 사용하는데,\n개인적으로는 트리 형태의 계층식 카테고리를 선호합니다.\n언제나처럼 PaperMod 이슈를 탐색하던 중 해당 이슈를 발견했는데,\n아래 그림처럼 제가 머릿속에 그리던 방식을 그대로 표현하여 큰 관심을 가졌습니다.\n해당 기능을 구현한 분께 메일을 보내 참고 자료를 얻었지만,\n아직까진 시간적 여유가 부족해 해당 작업을 처리하지 못한 상태입니다.\n향후 개선되기를 희망하는 부분입니다.\n마치며 # Hugo 블로그 만들기 시리즈의 마지막으로 커스터마이징 과정을 소개했습니다.\n커스터마이징은 그때그때 필요하다고 생각하는 부분을 수정하는 것이기 때문에\n본인의 입맛에 맛는 블로그를 만들기 위해서는 테마의 구조를 이해해야 합니다.\n아직 Go에 대해서도 잘 몰라 검색을 통해 요령껏 찾아내는 수준이지만,\nGo에 익숙해지게 된다면 동적 TOC 등 기능의 개선을 기대해 볼 수 있을 것입니다.\n해당 게시글을 통해 Hugo 블로그 만들기에 도움이 되었으면 좋겠습니다.\n참고 자료 # EP09. 구글, 네이버 검색엔진 등록하기 KaTex Simple way to open in a new tab [Feature][Discussion] Tree-style category list page "},{"id":26,"href":"/blog/hugo-blog-old-2/","title":"Hugo 블로그 만들기 [2022년] (2) - Utterances 댓글 적용","section":"Posts","content":"Hugo 블로그는 기본적으로 댓글 기능을 제공하지는 않습니다.\n제가 사용하는 PaperMod 테마에서는 서드파티 서비스인 Disqus를 위한 레이아웃이 존재하지만,\n저는 기본적인 블로그 운영을 Github 플랫폼 내에서 구성하고 싶기 때문에 다른 기능을 사용해보려 합니다.\n이번 포스트에서는 Utterances 댓글 기능을 추가하는 방법을 안내해드리겠습니다.\nUtterances 설치하기 # Utterances는 Github issues 기반으로 댓글을 관리하는 기능입니다.\n무료 플랜에서 광고가 붙는 Disqus와 다르게 별도의 유료 플랜이 없어 간편하게 사용할 수 있습니다.\nUtterances 설치는 단순히 레이아웃 상에서 댓글이 위치할 곳에 자바스크립트 코드를 삽입하면 됩니다.\n하지만, 선행적으로 해당 링크를 통해 Utterances와 연동시킬 저장소를 등록해야 합니다.\n무료 플랜 선택 후 Utterances를 적용할 저장소를 선택하게 되는데\n모든 저장소를 지정해도 되지만, 저는 댓글을 관리할 저장소만 지정하겠습니다.\n간단하게 Utterances 적용이 완료되면 아래 공식 문서 페이지로 이동합니다.\nhttps://utteranc.es/ 공식 문서에서 저장소 이름, 이슈 맵핑 방식 등을 지정하면 해당하는 스크립트가 생성됩니다.\n저는 포스트 제목이 변경될 수 있기 때문에 pathname을 기준으로 이슈를 생성하고,\n사용자 시스템 설정에 호환되는 Preferred Color Scheme 테마를 사용합니다.\nCopy html \u0026lt;script src=\u0026#34;https://utteranc.es/client.js\u0026#34; repo=\u0026#34;[ENTER REPO HERE]\u0026#34; issue-term=\u0026#34;pathname\u0026#34; theme=\u0026#34;github-light\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; 스크립트 삽입하기 # PaperMod 테마에는 layouts/partials/ 위치에 comments.html이라는 레이아웃이 존재합니다.\n테마 별로 레이아웃이 다르기 때문에 다른 테마의 경우 이슈 등을 참고하여 구조를 파악할 필요가 있습니다.\nCopy html {{- /* Comments area start */ -}} {{- /* to add comments read =\u0026gt; https://gohugo.io/content-management/comments/ */ -}} {{- if $.Site.Params.utteranc.enable -}} \u0026lt;script src=\u0026#34;https://utteranc.es/client.js\u0026#34; repo=\u0026#34;{{ .Site.Params.utteranc.repo }}\u0026#34; issue-term=\u0026#34;{{ .Site.Params.utteranc.issueTerm }}\u0026#34; {{- if $.Site.Params.utteranc.label -}}label=\u0026#34;{{ .Site.Params.utteranc.label }}\u0026#34;{{- end }} theme=\u0026#34;{{ .Site.Params.utteranc.theme }}\u0026#34; crossorigin=\u0026#34;{{ .Site.Params.utteranc.crossorigin }}\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; {{- end }} {{- /* Comments area end */ -}} 단순하게 레이아웃에 스크립트를 붙여넣어도 되지만,\n향후 속성값을 변경하기 위해 불필요하게 테마를 수정하는 경우를 방지하기 위해\n설정 파일을 통해 동적으로 속성값을 집어넣도록 설정했습니다.\nHugo HTML 코드 내에 이중 중괄호({{ }})는 Go 템플릿을 코딩하는 부분으로,\n아래와 같은 설정 파일을 읽어서 각각의 키에 해당하는 값을 할당합니다.\n이에 대한 자세한 사용법은 Hugo 공식 문서를 참조할 수 있습니다.\nCopy yaml params: utteranc: enable: true repo: \u0026#34;minyeamer/minyeamer.github.io\u0026#34; issueTerm: \u0026#34;pathname\u0026#34; label: \u0026#34;comments\u0026#34; theme: \u0026#34;preferred-color-scheme\u0026#34; crossorigin: \u0026#34;anonymous\u0026#34; 정상적으로 스크립트가 삽입되었다면 아래와 같이 댓글을 입력하는 부분이 표시됩니다.\n댓글 기능이 정상적으로 적용되는지 확인하기 위해 실험적으로 댓글을 작성해봅니다.\n저도 과거 게시글에 댓글을 작성하여 아래와 같이 올라온 이슈를 확인했습니다.\n마치며 # Hugo 블로그를 통한 소통을 기대하여 댓글 기능을 추가해보았습니다.\n생각보다 간단하기 때문에 깃허브 블로그를 꾸미면서 댓글 기능을 희망하시는 분들이라면\nUtterances를 적극 활용해보시기를 추천드립니다.\n마지막 포스트로는 PaperMod 테마를 수정한 과정을 안내해드리겠습니다.\nHugo 테마끼리 공통적인 부분이 있기 때문에 다른 테마를 사용하시더라도 도움이 될 것입니다.\n참고 자료 # Utterances Documents Introduction to Hugo Templating "},{"id":27,"href":"/blog/hugo-blog-old-1/","title":"Hugo 블로그 만들기 [2022년] (1) - Hugo 기본 구성","section":"Posts","content":"얼마 전, 티스토리 블로그에서 Jekyll 블로그로 이동했는데,\n처음 기대했던 submodule을 활용한 효율적인 저장소 연동에서 어려움을 겪고 다른 대안을 탐색하게 되었습니다.\nJekyll 블로그를 사용함에 있어서, Ruby 언어로 구성된 블로그 구조에 대해 이해하기 어려운데다가\n로컬 환경에서 Jekyll 블로그를 실행하면서 발생하는 에러를 처리하는데도 난항을 겪었는데,\n웹상에서 자동 배포가 이루어지는 과정에서 submodule인 TIL 저장소를 포스트로 인식하지 못하는 문제가 있었습니다.\nJekyll 블로그의 대안으로 Hexo 및 Hugo 프레임워크에 주목했고,\n두 제품의 장단점을 비교하여 상대적으로 배포가 빠르고 현재까지도 업데이트가 이루어지는 Hugo를 선택했습니다.\n이번 포스트에서는 제가 Hugo 블로그를 구성한 과정을 간략한 설명과 함께 안내해드리겠습니다.\n테마 선택하기 # 블로그의 모든 페이지 레이아웃을 만들 계획이 아니라면 블로그 선택에 있어 테마 선정이 필요합니다.\nHugo는 아래 페이지에서 다양한 테마를 제공하며, 태그를 통해 블로그 외에도 목적에 맞는 테마를 찾아볼 수 있습니다.\n미리보기만으로 알기 어렵다면 제작자가 제공하는 데모 사이트를 방문해볼 수 있고,\n아래 안내드릴 Hugo 설치를 통해 로컬에서 exampleSite를 확인해 볼 수도 있습니다.\nhttps://themes.gohugo.io/ Jekyll 블로그를 사용했을 당시 적용했던 Chirpy 테마는 사이드 메뉴, 계층식 카테고리, 동적 TOC 등\n제가 추구하는 모든 기능을 가지고 있었는데, Hugo에는 저의 취향을 완벽히 만족시키는 테마가 없었습니다.\n그나마 괜찮았던 LoveIt 테마의 경우 설정 곳곳에 중국어가 포함되어 있어 이해하기 어렵겠다는 생각이 들었습니다.\n결국, 저는 모든 테마를 둘러본 후 다루기 쉬워보이면서 외적으로도 괜찮았던 PaperMod 테마를 선택했습니다.\nHugo 블로그 구성하기 # 이번 Hugo 블로그 구성은 Mac 환경에서 진행되었으며, 다른 환경의 구성 방식은 제공되지 않습니다.\nHugo 설치 # Mac 사용자라면 Homebrew를 통해 쉽게 Hugo를 설치하여 사용할 수 있습니다.\n터미널에 아래 명령어를 입력해 설치가 가능합니다.\nCopy bash brew install hugo 설치가 완료되면, 버전 정보를 출력해서 정상 설치 여부를 확인합니다.\nCopy bash % hugo version hugo v0.102.2+extended darwin/arm64 BuildDate=unknown Github 저장소 생성 # Hugo는 원본 데이터 및 설정 파일이 포함될 공간과, 렌더링된 페이지가 저장될 공간이 필요합니다.\n일반적으로는 분리된 저장소를 통해 구현하지만, 앞서 Jekyll 블로그를 구성해보면서\n브랜치를 통해 하나의 저장소에서 두 개의 공간을 관리할 수 있을 것이라 판단했습니다.\n하나의 저장소를 main과 gh-pages, 두 개의 브랜치로 나누어 구성할 계획이며,\n우선적으로 \u0026lt;USERNAME\u0026gt;.github.io 명칭의 저장소를 생성합니다.\nHugo 프로젝트 생성 # 일반적인 웹 프레임워크에서 프로젝트를 시작하는 것처럼, Hugo에서도 기본 템플릿을 제공합니다. 아래 명령어를 통해 프로젝트를 생성할 수 있고, 이름은 자유롭게 지정해도 됩니다.\nCopy bash % hugo new site \u0026lt;NAME\u0026gt; 만들어진 프로젝트 구조는 다음과 같습니다.\n만들어진 테마를 사용한다면 대부분의 구성요소들이 themes/ 디렉토리 내에 위치하게 되며,\n포스트를 위한 content/, 이미지 등을 위한 static/ 디렉토리 외엔 거의 사용하지 않습니다.\nCopy bash . ├── archetypes │ └── default.md ├── config.toml ├── content ├── data ├── layouts ├── public ├── static └── themes 저장소 연동 # 테마를 불러오기에 앞서 git 설정이 필요합니다.\n프로젝트 디렉토리로 이동한 후, 아래 명령어를 통해 원격 저장소와 연동합니다.\nCopy bash % git init % git add . % git commit -m \u0026#34;feat: new site\u0026#34; % git branch -M main % git remote add origin https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git % git push -u origin main 추가적으로, 렌더링된 페이지가 저장되고 실질적인 배포가 이루어지는 브랜치를 생성합니다.\nCopy bash % git branch gh-pages main % git checkout gh-pages % git push origin gh-pages % git checkout main Hugo에서 페이지를 렌더링한 결과는 public/ 디렉토리에 저장되며, 이를 gh-pages 브랜치와 연결해야 합니다.\n기존에 존재하는 빈 디렉토리를 제거하고 gh-pages 브랜치를 main 브랜치의 submodule로 연결합니다.\nsubmodule에 대한 개념은 해당 영상을 참고해주시기 바라며, 단순하게 설명하자면 동기화 기능입니다.\nCopy bash % rm -rf public % git submodule add -b gh-pages https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git public % git add public % git add .gitmodules % git commit -m \u0026#34;feat: add submodule for github pages\u0026#34; % git push 테마 불러오기 # git 설정을 완료한 후, 미리 정해두었던 테마를 themes/ 디렉토리 내에 위치시킵니다.\n마찬가지로 submodule을 활용하며, 테마의 디렉토리명은 반드시 테마 설정에 명시된 것과 동일한 이름이어야 합니다.\n커스터마이징을 고려하면 원본 저장소가 아닌 별도로 fork한 저장소를 연결시키는게 좋습니다.\nCopy bash % git submodule add https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod % git add themes/PaperMod % git add .gitmodules % git commit -m \u0026#34;feat: import hugo theme\u0026#34; 만약 fork 저장소를 사용하면서 원본 저장소의 변경사항을 업데이트하고 싶다면,\n원본 저장소를 새로운 원격 저장소로 등록해 pull 작업을 수행합니다.\nCopy python git remote add upstream https://github.com/adityatelange/hugo-PaperMod git fetch upstream git merge upstream/master git commit -m \u0026#34;update: pull upstream\u0026#34; 아래는 PaperMod 테마의 디렉토리 구조입니다.\n테마를 수정할 일이 있다면 아래 구조를 참고해 필요한 파일에 접근해 볼 수 있습니다.\nCopy bash themes/PaperMod ├── LICENSE ├── README.md ├── assets │ ├── css │ │ ├── common │ │ ├── core │ │ ├── extended │ │ ├── hljs │ │ └── includes │ └── js ├── go.mod ├── i18n ├── images ├── layouts │ ├── 404.html │ ├── _default │ │ └── _markup │ ├── partials │ ├── robots.txt │ └── shortcodes └── theme.toml Hugo 설정 # Hugo 블로그 설정은 config 파일에서 지정할 수 있고, toml, yaml, json 형식을 지원합니다.\n테마를 사용할 경우 커스텀 키가 존재할 수 있어 별도의 문서를 참조하는게 좋습니다.\nHugo 공식 설정에 관한 문서와 PaperMod 설정에 관한 문서는 아래를 참고해주시기 바랍니다.\nhttps://gohugo.io/getting-started/configuration/ https://github.com/adityatelange/hugo-PaperMod/wiki/Installation 제 설정 파일의 경우 커스터마이징을 통해 호환되지 않는 키가 존재할 수 있지만,\n동일한 테마를 사용한다면 일부분을 참고해 도움을 받을 수 있을거라 기대합니다.\nHugo 배포 # Hugo는 hugo -t \u0026lt;THEMES\u0026gt; 명령어를 통해 로컬에서 페이지 렌더링을 진행할 수 있고,\n그 결과인 public/ 디렉토리 내 내용을 gh-pages에 push하여 배포를 수행합니다.\n배포에 앞서, 깃허브에서 제공하는 Github Pages가 gh-pages 브랜치를 참고하도록\n아래 그림과 같이 저장소 설정에서 빌드 및 배포 대상 브랜치를 지정해주어야 합니다.\n위와 같이 수동으로 배포할 경우 두 번의 push 과정을 거쳐야 합니다.\n매번 이 과정을 수행하는 것은 불편하기 때문에 쉘 스크립트를 작성하여 작업을 단순화합니다.\n해당 스크립트는 다른 포스트를 참고해 작성했습니다.\nCopy bash #!/bin/bash echo -e \u0026#34;\\033[0;32mDeploying updates to GitHub...\\033[0m\u0026#34; # Build the project. # hugo -t \u0026lt;your theme\u0026gt; hugo -t PaperMod # Go to public folder, submodule commit cd public # Add changes to git. git add . # Commit changes. msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; # Push source and build repos. git push origin gh-pages # Come back up to the project root cd .. # Commit and push to main branch git add . msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; git push origin main 스크립트 파일에 실행 권한을 부여하고 실행해 볼 수 있습니다.\nCopy bash % chmod 777 deploy.sh % ./deploy.sh 배포가 완료되면, https://.github.io 주소로 접속해 블로그를 확인할 수 있습니다.\n포스트 작성하기 # Hugo 포스트는 아래 명령어를 통해 생성할 수 있고,\n별도의 markdown 파일을 content/post/ 경로 내에 추가할 수도 있습니다.\nCopy bash % hugo new post/\u0026lt;FILENAME\u0026gt;.md Front Matter # 제목, 작성일자 등을 지정하기 위해 포스트 상단에 Front Matter라고 하는 토큰을 작성해야 합니다. Front Matter는 설정 파일과 동일하게 toml, yaml, json 형식을 지원하며,\nHugo 공식 문서 또는 PaperMod에서 안내하는 아래형식을 참고할 수 있습니다.\nCopy yaml --- title: \u0026#34;My 1st post\u0026#34; date: 2020-09-15T11:30:03+00:00 # weight: 1 # aliases: [\u0026#34;/first\u0026#34;] tags: [\u0026#34;first\u0026#34;] author: \u0026#34;Me\u0026#34; # author: [\u0026#34;Me\u0026#34;, \u0026#34;You\u0026#34;] # multiple authors showToc: true TocOpen: false draft: false hidemeta: false comments: false description: \u0026#34;Desc Text.\u0026#34; canonicalURL: \u0026#34;https://canonical.url/to/page\u0026#34; disableHLJS: true # to disable highlightjs disableShare: false disableHLJS: false hideSummary: false searchHidden: true ShowReadingTime: true ShowBreadCrumbs: true ShowPostNavLinks: true ShowWordCount: true ShowRssButtonInSectionTermList: true UseHugoToc: true cover: image: \u0026#34;\u0026lt;image path/url\u0026gt;\u0026#34; # image path/url alt: \u0026#34;\u0026lt;alt text\u0026gt;\u0026#34; # alt text caption: \u0026#34;\u0026lt;text\u0026gt;\u0026#34; # display caption under cover relative: false # when using page bundles set this to true hidden: true # only hide on current single page editPost: URL: \u0026#34;https://github.com/\u0026lt;path_to_repo\u0026gt;/content\u0026#34; Text: \u0026#34;Suggest Changes\u0026#34; # edit text appendFilePath: true # to append file path to Edit link --- 게시글 저장소 연동 # 저는 기존 TIL 저장소를 게시글로 활용할 예정이었기에,\ncontent/post/ 디렉토리를 TIL 저장소의 submodule로 대체했습니다.\nCopy bash % git submodule add https://github.com/minyeamer/til.git content/post/ % git add content/post/ % git add .gitmodules % git commit -m \u0026#34;feat: add til repository as post\u0026#34; 이렇게 설정했을 때 장점은 TIL 저장소에 변경사항이 발생했을 경우,\n아래와 같은 단 한 줄의 명령어로 블로그 저장소에서 업데이트할 수 있습니다.\n해당 명령어는 물론, 테마와 같은 다른 submodule에도 적용할 수 있습니다.\nCopy bash % git submodule update --remote 마치며 # Jekyll 블로그와 며칠간 씨름하다 Hugo로 이동해 기존의 목표를 달성할 수 있었습니다.\nChirpy 테마를 활용하지 못하는 것이 아쉽지만, PaperMod의 코드는 알기 쉽게 작성되어 있어\n시간적 여유만 있다면 커스터마이징에서 어려움이 없을 것이라 판단합니다.\n이번 포스트에서는 Hugo 블로그를 구성하고 포스트를 작성하는 과정을 전달했습니다.\n다음엔 Utterances 위젯을 활용해 댓글 기능을 추가하는 방법을 안내해드리겠습니다.\n참고 자료 # Hugo Documents PaperMod Documents 블로그 구축기 (1) Hugo + Github으로 개인 블로그 만들기 저장소 안에 저장소 - git submodule "},{"id":28,"href":"/blog/jekyll-blog/","title":"깃허브 블로그 시작하기","section":"Posts","content":"블로그를 처음 시작함에 있어서 모든 것이 준비된 호스팅 서비스의 편의성은 무시할 수 없습니다.\n저도 처음엔 코드를 직접 건드리는 자유도 높은 방식의 블로그에 진입 장벽을 느끼고\n가볍게 시작할 수 있는 티스토리를 통해 블로그에 입문했습니다.\n하지만, 개발적 지식을 학습하면서 깃허브에 마크다운 문서를 올리는 빈도가 늘어났고,\n깃허브에 올린 문서를 굳이 티스토리로 다시 옮겨 담는 것에 불편함을 느끼게 되었습니다.\n마크다운 문서를 자주 작성하고 깃허브 저장소를 학습 노트로 활용한다면,\n깃허브 블로그를 구성해보는 것이 문서를 통합적으로 관리할 수 있다는 점에서 매력적이라 생각합니다.\n현재는 막 깃허브 블로그를 꾸려서 적응해가는 단계에 불과하지만,\n웹에서 정적 파일을 수집하는 기술을 적용할 수 있다면 중복된 자료를 생성할 필요 없이\nTIL 저장소 자체를 블로그 포스트 저장소로도 활용할 수 있을 것이라 기대합니다.\n블로그를 개설하고 처음 작성하는 이번 포스트에서는 깃허브 블로그를 만든 과정을 소개해드리겠습니다.\n테마 선택 및 가져오기 # 깃허브 블로그를 생성하는데 있어 주로 사용되는 기술이 Jekyll이라는 사이트 생성 엔진 입니다.\nJekyll을 구성하는 Ruby와 쉘 스크립트 작성에 대한 이해가 있다면 더욱 자유도 높은 작업을 할 수 있지만,\n다행히 이를 모를지라도 다른 사용자들이 만든 테마를 가져와 블로그를 구성해 볼 수 있습니다. Jekyll 테마는 아래와 같은 사이트를 참조하여 마음에 드는 UI를 확인할 수 있습니다.\nhttps://jekyllthemes.io http://jekyllthemes.org 무료로 가져다 사용할 수 있는 여러 테마 중 개인적으로 마음에 드는 Chirpy 테마를 활용해 보겠습니다.\n테마 별로 적용 및 활용하는 방식에 다소 차이가 있지만,\nChirpy 같은 경우 아래 튜토리얼 사이트가 만들어져 있어 비교적 쉽게 블로그를 구성할 수 있습니다.\nhttps://chirpy.cotes.page 블로그 배포하기 # Chirpy 테마를 설치하고 배포하는 방법엔 두 가지 방식이 있습니다.\nChirpy Starter를 통해 간단하게 설치하기\n튜토리얼에서는 Jekyll을 전혀 모르는 사용자도 쉽게 테마를 활용할 수 있는 프로젝트 파일이 마련되어 있습니다.\n깃허브 저장소를 생성하는 것과 같은 단순한 버튼 클릭만으로 완성된 사이트를 배포할 수 있습니다.\nGithub에서 소스코드를 fork 받아 직접 설치하기\n스크립트를 실행하는 등 다소의 작업이 추가되지만, 블로그 커스터마이징에 유리한 방식입니다.\nJekyll을 다뤄볼 줄 안다면 직접 설치를 진행하는 것이 취향에 맞는 방식일 수 있습니다.\n저 같은 경우 Jekyll에 친숙한 편이 아니기 때문에 1번째 방법을 통해 설치를 진행했습니다.\n이때, 저장소 이름은 \u0026lt;GH_USERNAME\u0026gt;.github.io 형식으로 지정해야 하며,\n\u0026lt;GH_USERNAME\u0026gt;에는 깃허브 아이디를 입력하면 됩니다.\n위 방식으로 저장소를 생성하면 자동으로 배포가 수행되는데, Actions 탭을 통해 아래처럼 진행사항을 확인할 수 있습니다.\n빌드 및 배포가 완료되면 https://\u0026lt;저장소 이름\u0026gt; 주소를 통해 블로그 페이지에 접근할 수 있는데,\n2022년 8월 기준에서 해당 테마를 가져온 직후엔\n--- layout: home # Index page --- 텍스트만 존재하는 화면을 마주하게 됩니다.\n이것은 현재 Github Pages가 스타일이 적용되지 않는 main 브랜치를 대상으로 하고 있는 것이 원인으로,\nSettings 탭 아래 Pages 메뉴를 클릭했을 때 보이는 Branch 부분을 gh-pages로 수정하면 됩니다.\n블로그 설정하기 # 향후 블로그 호스팅 및 사이트 제목을 수정하는 등의 설정을 위해 _config.yml 파일을 수정할 필요가 있습니다.\n제가 블로그 세팅에 도움을 받은 게시글로부터 일부 항목에 대한 설명을 가져왔습니다.\n항목 값 설명 timezone Asia/Seoul 시간대를 설정하는 부분으로 서울 표준시로 설정합니다. title 블로그 제목 프로필 사진 아래 큰 글씨로 제목이 표시됩니다. tagline 프로필 설명 블로그 제목 아래에 작은 글씨로 부연설명을 넣을 수 있습니다. description SEO 구글 검색에 어떤 키워드로 내 블로그를 검색하게 할 것인가를 정의하는 부분입니다. url https://*.github.io 블로그와 연결된 url을 입력합니다. github Github ID 본인의 github 아이디를 입력합니다. twitter.username Twitter ID 트위터를 사용한다면 아이디를 입력합니다. social.name 이름 포스트 등에 작성자로 표시할 나의 이름을 입력합니다. social.email 이메일 나의 이메일 계정을 입력합니다. social.links 소셜 링크들 트위터, 페이스북 등 내가 사용하고 있는 소셜 서비스의 나의 홈 url을 입력합니다. avatar 프로필 사진 블로그 왼쪽 상단에 표시될 프로필 사진의 경로를 설정합니다. toc true 포스트 오른쪽에 목차를 표시합니다. paginate 10 한 목록에 몇 개의 글을 표시할 것인지 지정합니다. 이 부분은 저의 설정 파일 _config.yml 또는 github 내 검색을 통해 접근할 수 있는\n다른 사용자 분들의 설정 파일을 참고하면 원하는 부분을 수정하는데 도움이 될 것입니다.\n_config.yml 파일이 수정 등 저장소에 변화가 발생하면 자동으로 빌드 및 배포 과정이 수행되며,\n변경사항이 적용되는데 약간 시간이 걸릴 수 있습니다.\n포스트 작성하기 # Jekyll은 마크다운 문법으로 글을 작성할 수 있습니다.\n마크다운 문법에 익숙하지 않다면 해당 게시글을 참고해 주시기 바랍니다.\nVS Code 또는 기타 웹 편집기를 활용하면 마크다운 작성 내용을 실시간으로 렌더링해 확인할 수 있습니다.\n게시글에 대한 마크다운 파일은 _posts 디렉토리 내에 위치시키고,\nyyyy-mm-dd-제목.md의 형식으로 파일 이름을 지정해야 합니다.\n제목에 해당하는 부분은 실제 포스트 제목이 아닌, url로 활용되는 부분이기 때문에\n게시글의 내용을 짐작하게 하는 간단한 단어나 문장을 활용하는게 좋습니다.\n마크다운 파일의 상단엔 Front Matter라고 하는 Jekyll 게시글에서 허용하는 규칙을 통해\n게시글 제목, 작성일자, 카테고리, 태그 등을 지정할 수 있습니다.\n자세한 내용은 튜토리얼을 참조할 수도 있고,\n해당 게시글에 대한 raw 파일을 확인해보셔도 좋습니다.\n마치며 # 과거 깃허브 블로그를 만들려고 했을 때는 Jekyll을 직접 다뤄야 해서 쉽게 접근하지 못했는데,\n이제는 그럴 필요 없이 완성된 패키지를 가져다 쓸 수 있게 되어서 많이 편해졌다고 생각합니다.\n참고 자료 # Chirpy Documents 깃헙(GitHub) 블로그 10분안에 완성하기 Jekyll Chirpy 테마 사용하여 블로그 만들기 Github 블로그 테마적용하기(Chirpy) "}]