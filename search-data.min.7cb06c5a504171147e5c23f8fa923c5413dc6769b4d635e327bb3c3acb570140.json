[{"id":0,"href":"/blog/2023-04-02/","title":"2023-04-02 Log","section":"Posts","content":"Spark UDF # 자신의 기능을 정의할 수 있는 사용자 정의 함수 머신러닝 모델의 내부를 이해하지 않고도 스파크 SQL에서 예측 결과를 쿼리 가능 스파크 SQL이 하위 표현식의 평가 순서를 보장하지 않기 때문에 UDF 내부에서 null 검사 Copy python from pyspark.sql.types import LongType def cubed(s): return s * s * s spark.udf.register(\u0026#34;cubed\u0026#34;, cubed, LongType()) spark.range(1, 9).createOrReplaceTempView(\u0026#34;udf_test\u0026#34;) spark.sql(\u0026#34;SELECT id, cubed(id) AS id_cubed FROM udf_test\u0026#34;).show() Pandas UDF # 입력과 출력이 모두 판다스 인스턴스인 API 지원 Copy python import pandas as pd from pyspark.sql.functions ipmort col, pandas_udf from pyspark.sql.types import LongType def cubed(a: pd.Series) -\u0026gt; pd.Series: return a * a * a cubed_udf = pandas_udf(cubed, returnType=LongType()) df = spark.range(1, 4) df.select(\u0026#34;id\u0026#34;, cubed_udf(col(\u0026#34;id\u0026#34;))).show() Spark Shell # $SPARK_HOME 폴더에서 ./bin/spark-sql 명령어 실행 Copy bash spark-sql\u0026gt; CREATE TABLE people (Name STRING, age INT) JDBC # user, password, url, dbtable, query, driver 연결 속성 지정 파티셔닝을 위한 numPartitions, partitionColumn, lowerBound, upperBound 속성 PostgreSQL # Copy python jdbcDF1 = (spark .read # .write .format(\u0026#34;jdbc\u0026#34;) .opiton(\u0026#34;url\u0026#34;, \u0026#34;jdbc:postgresql://[DBSERVER]\u0026#34;) .option(\u0026#34;dbtable\u0026#34;, \u0026#34;[SCHEMA].[TABLENAME]\u0026#34;). .option(\u0026#34;user\u0026#34;, \u0026#34;[USERNAME]\u0026#34;) .option(\u0026#34;password\u0026#34;, \u0026#34;[PASSWORD]\u0026#34;) .load()) # .save() SQL 고차 함수 # 중첩된 구조를 개별 행으로 분해\nCopy sql SELECT id, collect_list(value = 1) AS values FROM (SELECT id, EXPLODE(values) AS value FROM table) x GROUP BY id 배열 유형 함수 # array_distinct(array): 배열 내의 중복을 제거 array_intersect(array, array): 중복되지 않은 두 배열의 교차점을 반환 array_union(array, array): 중복 항목 없이 두 배열의 결합을 반환 array_except(array, array): 배열1에는 존재하지만 배열2에는 존재하지 않는 요소를 반환 array_join(array, string): 구분 기호를 사용하여 배열 요소를 연결 array_max(array): Null값을 제외한 배열의 최댓값을 반환 array_min(array): Null값을 제외한 배열의 최솟값을 반환 array_position(array, T): 배열에서 지정된 요소의 첫번째 인덱스를 반환 array_remove(array, T): 배열에서 지정된 요소와 동일한 모든 요소를 제거 array_overlap(array, array): Null이 아닌 동일한 값이 두 배열에 있을 경우 true를 반환 array_sort(array): 배열을 오름차순으로 정렬하고 Null은 맨 끝에 위치 concat(array, ...): 문자열, 바이너리, 배열 등을 연결 flatten(array\u0026lt;array\u0026gt;): 배열 안의 배열들을 단일 배열로 플랫화 array_repeat(T, Int): 지정된 요소가 포함된 배열을 지정한 횟수만큼 반환 reverse(array): 문자열의 역순 또는 배열에서 요소의 역순을 반환 sequence(T, T), 시작부터 끝을 포함한 일련의 요소를 생성 shuffle(array): 주어진 배열의 무작위 순열을 반환 slice(array, Int, Int): 배열에서 지정된 시작과 끝 인덱스에 대한 하위 집합을 반환 array_zip(array, array, ...): 병합된 구조 배열을 반환 element_at(array, Int): 지정된 인덱스에서 지정된 배열의 요소를 반환 cardinality(array): 지정된 배열 또는 맵의 크기를 반환 맵 함수 # map_form_arrays(array, array): 주어진 키/값 배열 쌍에서 맵을 생성하여 반환 map_from_entries(array\u0026lt;struct\u0026gt;): 주어진 배열에서 생성된 맵을 반환 map_concat(map, ...): 입력된 맵의 결합을 반환 element_at(map, K): 주어진 키에 대한 값을 반환하고 없을 경우 Null을 반환 cardinality(array): 지정된 배열 또는 맵의 크기를 반환 고차 함수 # transform(values, value -\u0026gt; lambda expression) 형태의 람다식 사용 Copy python # transform(): 배열의 각 요소에 함수를 적용하여 배열을 생성 spark.sql(\u0026#34;\u0026#34;\u0026#34; SELECT celsius, transform(celsius, t -\u0026gt; ((t * 9) div 5) + 32) AS fahrenheit FROM tC \u0026#34;\u0026#34;\u0026#34;).show() # filter(): 입력한 배열의 요소 중 참인 요소만 배열로 반환 spark.sql(\u0026#34;\u0026#34;\u0026#34; SELECT celsius, filter(celsius, t -\u0026gt; t \u0026gt; 38) AS high FROM tC \u0026#34;\u0026#34;\u0026#34;).show() # exists(): 입력한 배열의 요소 중 불린 함수를 만족시키면 참을 반환 spark.sql(\u0026#34;\u0026#34;\u0026#34; SELECT celsius, exists(celsius, t -\u0026gt; t \u0026gt; 38) AS high FROM tC \u0026#34;\u0026#34;\u0026#34;).show() # reduce(): 요소를 병합하고 배열의 요소를 단일값으로 줄임 spark.sql(\u0026#34;\u0026#34;\u0026#34; SELECT celsius, reduce(celsius, 0, (t, acc) -\u0026gt; t + acc, acc -\u0026gt; (acc div size(celsius) * 9 div 5) + 32 ) AS avgFahrenheit FROM tC \u0026#34;\u0026#34;\u0026#34;).show() SQL 작업 # Union # Copy python bar = departureDelays.union(foo) bar.createOrReplaceTempView(\u0026#34;bar\u0026#34;) bar.filter(expr(\u0026#34;\u0026#34;\u0026#34;origin == \u0026#39;SEA\u0026#39; AND destination == \u0026#39;SFO\u0026#39; AND date LIKE \u0026#39;01010%\u0026#39; AND delay \u0026gt; 0\u0026#34;\u0026#34;\u0026#34;)).show() JOIN # Copy python foo.join( airports, airports.IATA == foo.origin ).select(\u0026#34;City\u0026#34;, \u0026#34;State\u0026#34;, \u0026#34;date\u0026#34;, \u0026#34;delay\u0026#34;, \u0026#34;distance\u0026#34;, \u0026#34;destination\u0026#34;).show() Window # rank(), dense_rank(), percent_rank(), ntile(), row_number() cume_dist(), first_value(), last_value(), lag(), lead() Copy sql CREATE TABLE departureDelaysWindow AS SELECT origin, destination, SUM(delay) AS TotalDelays FROM departureDelays WHERE origin IN (\u0026#39;SEA\u0026#39;, \u0026#39;SFO\u0026#39;, \u0026#39;JFK\u0026#39;) AND destination IN (\u0026#39;SEA\u0026#39;, \u0026#39;SFO\u0026#39;, \u0026#39;JFK\u0026#39;, \u0026#39;DEN\u0026#39;, \u0026#39;ORD\u0026#39;, \u0026#39;LAX\u0026#39;, \u0026#39;ATL\u0026#39;) GROUP BY origin, destination; Copy python spark.sql(\u0026#34;\u0026#34;\u0026#34; SELECT origin, destination, TotalDelays, rank FROM ( SELECT origin, destination, TotalDelays, dense_rank() OVER (PARTITION BY origin ORDER BY TotalDelays DESC) AS rank FROM departureDelaysWindow ) t WHERE rank \u0026lt;= 3 \u0026#34;\u0026#34;\u0026#34;).show() Modification # Copy python from pyspark.sql.functions import * # 열 추가 foo2 = (foo.withColumn( \u0026#34;status\u0026#34;, expr(\u0026#34;CASE WHEN delay \u0026lt;= 10 THEN \u0026#39;On-time\u0026#39; ELSE \u0026#39;Delayed\u0026#39; END\u0026#34;) )) # 열 삭제 foo3 = foo2.drop(\u0026#34;delay\u0026#34;) # 칼럼명 바꾸기 foo4 = foo3.withColumnRenamed(\u0026#34;status\u0026#34;, \u0026#34;flight_status\u0026#34;) # 피벗 SELECT * FROM ( SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay FROM departureDelays WHERE origin = \u0026#39;SFA ) PIVOT ( CAST AVG(delay) AS DECIMAL(4, 2) AS AvgDelay, MAX(delay) AS MaxDelay FOR month IN (1 JAN, 2 FEB) ) "},{"id":1,"href":"/blog/10000-recipe/","title":"[Python] 만개의 레시피 데이터 수집","section":"Posts","content":"최근 레시피 생성을 목적으로 한 사이드 프로젝트에 참여하게 되었는데\n모델 학습을 위한 만개의 레시피 데이터 크롤링을 진행해보았습니다.\n스키마 구성 # 기존엔 레시피 명칭과 음식 재료 정보만을 수집할 계획이었지만,\n만개의 레시피의 각 페이지를 살펴보면서 추가적으로 가져갈만한 데이터가 있음을 확인하여\n우선적으로 테이블 관계 및 스키마를 구성해보았습니다.\n초기에 만개의 레시피와 공공데이터를 데이터 소스로 삼았기 때문에,\n만개의 레시피에 대한 DB _10000, 공공데이터에 대한 DB food로 구성했습니다.\n_10000 DB 내 테이블은 만개의 레시피 내 각각의 페이지에서 가져온 데이터로 구성되며,\n크게 카테고리, 레시피, 사용자 단위로 구분할 수 있습니다.\n만개의 레시피 데이터 수집 # 크롤링에서 데이터 요청 및 가공을 위해 정의된 유틸리티 함수들이 있는데,\n별도로 코드를 보여주지는 않고 해당 함수가 호출될 때 간단히 어떤 동작을 하는지만 전달드립니다.\n카테고리 추출 # 만개의 레시피 카테고리는 레시피 검색 페이지에서 간단하게 추출할 수 있으므로,\n개발자 도구 또는 requests에 대한 응답에서 카테고리에 해당하는 부분을 가져옵니다.\n여기서 get_headers() 함수는 User-Agent 등 기본적인 브라우저 정보가 담긴 헤더를 반환합니다.\nCopy python url = \u0026#34;https://www.10000recipe.com/recipe/list.html\u0026#34; headers = get_headers(url, referer=url) headers[\u0026#34;upgrade-insecure-requests\u0026#34;] = \u0026#39;1\u0026#39; response = session.get(url, params=params, headers=headers) source = BeautifulSoup(response.text, \u0026#34;html.parser\u0026#34;) cate_list = source.select_one(\u0026#34;div.cate_list\u0026#34;) pattern = \u0026#34;javascript:goSearchRecipe([\\d\\w()\u0026#39;,]+)\u0026#34; raw_cat = [(re_get(pattern, cat.attrs[\u0026#34;href\u0026#34;]),cat.text) for cat in cate_list.select(\u0026#34;a\u0026#34;) if \u0026#34;href\u0026#34; in cat.attrs] cat_map = lambda catType, catId, catName: {\u0026#34;categoryId\u0026#34;:catId, \u0026#34;categoryType\u0026#34;:catType, \u0026#34;categoryName\u0026#34;:catName} categories = [cat_map(*literal_eval(data), name) for data, name in raw_cat] categories = pd.DataFrame(categories) categories = categories[categories[\u0026#34;categoryId\u0026#34;]!=\u0026#39;\u0026#39;] categories.head() 데이터 수집 결과 아래와 같은 구조의 데이터를 획득할 수 있습니다.\ncategoryId categoryType categoryName 63 cat4 밑반찬 56 cat4 메인반찬 54 cat4 국/탕 55 cat4 찌개 60 cat4 디저트 레시피 목록 추출 # 레시피 검색 페이지는 검색어, 정렬 기준, 페이지, 카테고리를 쿼리로 받습니다.\n레시피 목록을 추출하는데 검색어나 카테고리는 필요하지 않고 동일한 정렬 기준에서 수집하기 때문에\n데이터 수집 시에는 페이지에 반복문을 적용하여 데이터가 존재하는 범위를 가져올 것입니다.\nCopy python ORDER_MAP = {\u0026#34;정확순\u0026#34;:\u0026#34;accuracy\u0026#34;, \u0026#34;최신순\u0026#34;:\u0026#34;date\u0026#34;, \u0026#34;추천순\u0026#34;:\u0026#34;reco\u0026#34;} get_params = lambda **kwargs: {k:v for k,v in kwargs.items() if v} uri = \u0026#34;https://www.10000recipe.com/recipe/\u0026#34; def fetch(session: requests.Session, query=str(), sortType=\u0026#34;추천순\u0026#34;, page=1, cat1=str(), cat2=str(), cat3=str(), cat4=str(), **kwargs) -\u0026gt; List[str]: url = uri+\u0026#34;list.html\u0026#34; params = get_params(q=query, order=ORDER_MAP[sortType], page=page, cat1=cat1, cat2=cat2, cat3=cat3, cat4=cat4) headers = get_headers(url, referer=url) headers[\u0026#34;upgrade-insecure-requests\u0026#34;] = \u0026#39;1\u0026#39; response = session.get(url, params=params, headers=headers) return parse(response.text, **kwargs) def parse(response: str, **kwargs) -\u0026gt; List[str]: source = BeautifulSoup(response, \u0026#39;html.parser\u0026#39;) uris = source.select(\u0026#34;a.common_sp_link\u0026#34;) ids = [uri.attrs[\u0026#34;href\u0026#34;].split(\u0026#39;/\u0026#39;)[-1] for uri in uris if \u0026#34;href\u0026#34; in uri.attrs] return ids 데이터 수집 결과로는 문자열 타입의 레시피 ID 목록을 획득할 수 있습니다.\n레시피 정보 추출 # 레시피 ID로 접근할 수 있는 레시피 상세 정보 페이지에서\n레시피 정보에 대한 데이터를 추출합니다. 소스코드 내에서 레시피 정보가 JSON 형식으로 존재하기 때문에\n일일히 HTML 태그를 파싱할 필요 없이 데이터를 한번에 JSON 오브젝트로 가져올 수 있습니다.\n데이터를 가공하는 map_recipe() 함수 내에서\ncast_int()는 데이터를 정수형으로 변환할 때 에러가 발생하면 기본값 0을 반환하는 함수이고,\nhier_get()은 중첩 딕셔너리의에 단계별 키 목록에 대한 값을 안전하게 가져오기 위한 함수입니다.\nCopy python uri = \u0026#34;https://www.10000recipe.com/recipe/\u0026#34; def fetch(session: requests.Session, recipeId: str, **kwargs) -\u0026gt; Dict: url = uri+recipeId # https://www.10000recipe.com/recipe/6997297 headers = get_headers(url, referer=uri+\u0026#34;list.html\u0026#34;) headers[\u0026#34;upgrade-insecure-requests\u0026#34;] = \u0026#39;1\u0026#39; response = session.get(url, headers=headers) return parse(response.text, recipeId, **kwargs) def parse(response: str, recipeId: str, **kwargs) -\u0026gt; Dict: source = BeautifulSoup(response, \u0026#39;html.parser\u0026#39;) raw_json = source.select_one(\u0026#34;script[type=\\\u0026#34;application/ld+json\\\u0026#34;]\u0026#34;).text try: data = json.loads(raw_json) except: data = literal_eval(raw_json) return map_recipe(data, recipeId, source, **kwargs) def map_recipe(data: Dict, recipeId: str, source=None, **kwargs) -\u0026gt; Dict: recipe_info = {\u0026#34;recipeId\u0026#34;: recipeId} recipe_info[\u0026#34;name\u0026#34;] = data.get(\u0026#34;name\u0026#34;) recipe_info[\u0026#34;author\u0026#34;] = hier_get(data, [\u0026#34;author\u0026#34;,\u0026#34;name\u0026#34;]) recipe_info[\u0026#34;ratingValue\u0026#34;] = cast_int(hier_get(data, [\u0026#34;aggregateRating\u0026#34;,\u0026#34;ratingValue\u0026#34;])) recipe_info[\u0026#34;reviewCount\u0026#34;] = cast_int(hier_get(data, [\u0026#34;aggregateRating\u0026#34;,\u0026#34;reviewCount\u0026#34;])) recipe_info[\u0026#34;totalTime\u0026#34;] = data.get(\u0026#34;totalTime\u0026#34;) recipe_info[\u0026#34;recipeYield\u0026#34;] = data.get(\u0026#34;recipeYield\u0026#34;) try: recipe_info[\u0026#34;recipeIngredient\u0026#34;] = \u0026#39;,\u0026#39;.join(data[\u0026#34;recipeIngredient\u0026#34;]) except: recipe_info[\u0026#34;recipeIngredient\u0026#34;] = extract_ingredient(source, **kwargs) recipe_info[\u0026#34;recipeInstructions\u0026#34;] = \u0026#39;\\n\u0026#39;.join( [step.get(\u0026#34;text\u0026#34;,str()) for step in data.get(\u0026#34;recipeInstructions\u0026#34;,list()) if isinstance(step, dict)]) recipe_info[\u0026#34;createDate\u0026#34;] = data.get(\u0026#34;datePublished\u0026#34;) return recipe_info def extract_ingredient(source: Tag, **kwargs) -\u0026gt; str: cont_ingre = source.select_one(\u0026#34;div.cont_ingre\u0026#34;) if cont_ingre: return [ingre.split() for ingre in cont_ingre.select_one(\u0026#34;dd\u0026#34;).text.split(\u0026#39;,\u0026#39;)] else: return str() 데이터 수집 결과 아래와 같이 정리된 딕셔너리를 얻을 수 있습니다.\nCopy python { \u0026#34;recipeId\u0026#34;: \u0026#34;6997297\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;두부짜조\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;호이호이\u0026#34;, \u0026#34;ratingValue\u0026#34;: 5, \u0026#34;reviewCount\u0026#34;: 1, \u0026#34;totalTime\u0026#34;: \u0026#34;PT20M\u0026#34;, \u0026#34;recipeYield\u0026#34;: \u0026#34;1 servings\u0026#34;, \u0026#34;recipeIngredient\u0026#34;: \u0026#34;두부 30g,라이스페이퍼 2장,돼지고기 5g,...\u0026#34;, \u0026#34;recipeInstructions\u0026#34;: \u0026#34;부위는 상관없지만 저는 저렴하고...\u0026#34;, \u0026#34;createDate\u0026#34;: \u0026#34;2023-02-19T13:37:04+09:00\u0026#34; } 실질적으로 활용할 데이터는 레시피명 name과 재료명인 recipeIngredient이며,\n평점, 리뷰 수, 조리순서 등도 추가적인 분석을 통해 활용성을 기대해볼 수 있습니다.\n요리 후기 추출 # 동일한 레시피 상세 정보 페이지에서 요리 후기에 대한 데이터를 추출할 수 있습니다.\n단, 요리 후기는 JSON 형식으로 정리되어 있지 않기 때문에\nHTML 소스를 파싱하여 대상 문자열을 추출해야 합니다.\n데이터를 가공하는 map_review() 함수 내에서\nre_get()은 정규표현식 패턴에 매칭되는 문자열을 추출하는 함수이고,\nselect_text()는 BeautifulSoup 태그에서\nCSS Selector로 안전하게 문자열을 추출하는 함수입니다.\nCopy python GENDER = {\u0026#34;info_name_m\u0026#34;:\u0026#34;M\u0026#34;, \u0026#34;info_name_f\u0026#34;:\u0026#34;F\u0026#34;} uri = \u0026#34;https://www.10000recipe.com/recipe/\u0026#34; rid_ptn = \u0026#34;replyReviewDiv_(\\d+)\u0026#34; uid_ptn = \u0026#34;/profile/review.html\\?uid=([\\d\\w]+)\u0026#34; date_ptn = \u0026#34;(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\u0026#34; def fetch(session: requests.Session, recipeId: str, **kwargs) -\u0026gt; List[Dict]: url = uri+recipeId headers = get_headers(url, referer=uri+\u0026#34;list.html\u0026#34;) headers[\u0026#34;upgrade-insecure-requests\u0026#34;] = \u0026#39;1\u0026#39; response = session.get(url, headers=headers) return parse(response.text, recipeId, **kwargs) def parse(response: str, recipeId: str, **kwargs) -\u0026gt; List[Dict]: source = BeautifulSoup(response, \u0026#39;html.parser\u0026#39;) reply_divs = source.select(\u0026#34;div.view_reply\u0026#34;) review_div = [div for div in reply_divs if div.select_one(\u0026#34;div.reply_tit\u0026#34;).text.strip().startswith(\u0026#34;요리 후기\u0026#34;)] if review_div: review_list = review_div[0].select(\u0026#34;div.reply_list\u0026#34;) return [map_review(review, recipeId, **kwargs) for review in review_list] else: return list() def map_review(data: Tag, recipeId: str, **kwargs) -\u0026gt; Dict: review_info = dict() review_info[\u0026#34;reviewId\u0026#34;] = re_get(rid_ptn, data.select(\u0026#34;div\u0026#34;)[-1].attrs.get(\u0026#34;id\u0026#34;)) review_info[\u0026#34;recipeId\u0026#34;] = recipeId review_info[\u0026#34;userId\u0026#34;] = re_get(uid_ptn, data.select_one(\u0026#34;a\u0026#34;).attrs.get(\u0026#34;href\u0026#34;)) review_info[\u0026#34;contents\u0026#34;] = select_text(data, \u0026#34;p.reply_list_cont\u0026#34;) detail = data.select_one(\u0026#34;h4.media-heading\u0026#34;) if detail: review_info[\u0026#34;userName\u0026#34;] = select_text(detail, \u0026#34;b\u0026#34;) gender = detail.select_one(\u0026#34;b\u0026#34;).attrs.get(\u0026#34;class\u0026#34;) review_info[\u0026#34;userGender\u0026#34;] = GENDER.get(gender[0]) if gender else None review_info[\u0026#34;createDate\u0026#34;] = re_get(date_ptn, detail.text) return review_info 데이터 수집 결과 아래와 같이 정리된 딕셔너리를 얻을 수 있습니다.\n여기서 요리 후기와 별도로 사용자 명칭과 성별을 추출할 수 있습니다.\nCopy python { \u0026#34;reviewId\u0026#34;: \u0026#34;395018\u0026#34;, \u0026#34;recipeId\u0026#34;: \u0026#34;6843136\u0026#34;, \u0026#34;userId\u0026#34;: \u0026#34;58031746\u0026#34;, \u0026#34;contents\u0026#34;: \u0026#34;정말 간단한데 중불로하니 좀 태워먹었... 맛은 있네욬ㅋㅋㅋㅋㅋ다음엔 중불이랑 약불 사이로 함 더해바야겠어욬ㅋㅋㅋㄱㅋㅋ감삼둥..♡♡\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;나찡as\u0026#34;, \u0026#34;userGender\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;createDate\u0026#34;: \u0026#34;2020-11-09 17:14:02\u0026#34; } 댓글 추출 # 레시피 상세 정보 페이지에서 댓글은 미리보기만이 제공되며\n전체 댓글을 확인하기 위해서는 별도의 페이지에 접속해야 합니다.\n해당 페이지의 출력 결과에서도 요리 후기와 같은 방식으로\nHTML 소스를 파싱하여 대상 문자열을 추출해야 합니다.\nCopy python GENDER = {\u0026#34;info_name_m\u0026#34;:\u0026#34;M\u0026#34;, \u0026#34;info_name_f\u0026#34;:\u0026#34;F\u0026#34;} uri = \u0026#34;https://www.10000recipe.com/recipe/\u0026#34; cid_ptn = \u0026#34;replyCommentDiv_(\\d+)\u0026#34; uid_ptn = \u0026#34;/profile/recipe_comment.html\\?uid=([\\d\\w]+)\u0026#34; date_ptn = \u0026#34;(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2})\u0026#34; def fetch(session: requests.Session, recipeId: str, page=1, **kwargs) -\u0026gt; List[Dict]: url = uri+\u0026#34;ajax.html\u0026#34; params = dict(q_mode=\u0026#34;getListComment\u0026#34;, seq=recipeId, page=page) headers = get_headers(url, referer=uri+recipeId) headers[\u0026#34;upgrade-insecure-requests\u0026#34;] = \u0026#39;1\u0026#39; response = session.get(url, params=params, headers=headers) return parse(response.text, recipeId, **kwargs) def parse(response: str, recipeId: str, **kwargs) -\u0026gt; List[Dict]: source = BeautifulSoup(response, \u0026#39;html.parser\u0026#39;) comment_list = source.select(\u0026#34;div.reply_list\u0026#34;) return [map_comment(comment, recipeId, **kwargs) for comment in comment_list] def map_comment(data: Tag, recipeId: str, **kwargs) -\u0026gt; Dict: comment_info = dict() comment_info[\u0026#34;commentId\u0026#34;] = re_get(cid_ptn, data.select(\u0026#34;div\u0026#34;)[-1].attrs.get(\u0026#34;id\u0026#34;)) comment_info[\u0026#34;recipeId\u0026#34;] = recipeId comment_info[\u0026#34;userId\u0026#34;] = re_get(uid_ptn, data.select_one(\u0026#34;a\u0026#34;).attrs.get(\u0026#34;href\u0026#34;)) comment_info[\u0026#34;contents\u0026#34;] = select_text(data, \u0026#34;div.media-body\u0026#34;).split(\u0026#39;|\u0026#39;)[-1] detail = data.select_one(\u0026#34;h4.media-heading\u0026#34;) if detail: comment_info[\u0026#34;userName\u0026#34;] = select_text(detail, \u0026#34;b\u0026#34;) gender = detail.select_one(\u0026#34;b\u0026#34;).attrs.get(\u0026#34;class\u0026#34;) comment_info[\u0026#34;userGender\u0026#34;] = GENDER.get(gender[0]) if gender else None comment_info[\u0026#34;createDate\u0026#34;] = re_get(date_ptn, detail.text) return comment_info review = fetch(session, \u0026#34;6843136\u0026#34;) review[0] 데이터 수집 결과 아래와 같이 정리된 딕셔너리를 얻을 수 있습니다.\n데이터 구조는 요리 후기와 동일합니다.\nCopy python { \u0026#34;commentId\u0026#34;: \u0026#34;39693405\u0026#34;, \u0026#34;recipeId\u0026#34;: \u0026#34;6843136\u0026#34;, \u0026#34;userId\u0026#34;: \u0026#34;89382542\u0026#34;, \u0026#34;contents\u0026#34;: \u0026#34;신고그러네여..재료양이..ㅜ\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;휘아여\u0026#34;, \u0026#34;userGender\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;createDate\u0026#34;: \u0026#34;2022-03-18 00:02\u0026#34; } "},{"id":2,"href":"/blog/2023-03-25/","title":"2023-03-25 Log","section":"Posts","content":"Spark SQL # 데이터 소스와 JDBC/ODBC 커넥터 또는 스파크 애플리케이션 사이를 연결 SparkSession의 sql() 함수를 통해 SQL 쿼리를 실행 Copy python # 임시뷰 생성 df = (spark.read.format(\u0026#34;csv\u0026#34;) .option(\u0026#34;inferSchema\u0026#34;, \u0026#34;true\u0026#34;) .option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;) .load(csv_file)) df.createOrReplaceTempView(\u0026#34;us_delay_flights_tbl\u0026#34;) # 임시뷰를 기반으로 쿼리 spark.sql(\u0026#34;\u0026#34;\u0026#34;SELECT distance, origin, destination FROM us_delay_flights_tbl WHERE distance \u0026gt; 1000 ORDER BY distance DESC\u0026#34;\u0026#34;\u0026#34;).show(10) SQL Table # 관리형 테이블: 메타데이터와 파일 저장소의 데이터를 모두 관리 비관리형 테이블: 메타데이터만 관리하고 외부 데이터 소스에서 데이터를 직접 관리 관리형 테이블에서 DROP TABLE과 같은 SQL 명령은 실제 데이터를 삭제 Copy python # 관리형 테이블 생성 spark.sql(\u0026#34;CREATE DATABASE learn_spark_db\u0026#34;) spark.sql(\u0026#34;USE learn_spark_db\u0026#34;) spark.sql(\u0026#34;\u0026#34;\u0026#34;CREATE TABLE managed_us_delay_flights_tbl ( data STRING, delay INT, distance INT, origin STRING, destination STRING)\u0026#34;\u0026#34;\u0026#34;) # 비관리형 테이블 생성 spark.sql(\u0026#34;\u0026#34;\u0026#34;CREATE TABLE us_delay_flights_tbl ( date STRING, delay INT, distance INT, origin STRING, destination STRING) USING csv OPTIONS (PATH \u0026#39;data.csv\u0026#39;)\u0026#34;\u0026#34;\u0026#34;) SQL View # 기존 테이블을 토대로 뷰를 만들 수 있으며, 스파크 애플리케이션이 종료되면 사라짐 임시 뷰: 스파크 애플리케이션 내 단일 SparkSession에 연결 전역 임시 뷰: 스파크 애플리케이션 내 여러 SparkSession을 만들 수 있음 Copy sql -- SQL 예제 CREATE OR REPLACE GLOBAL TEMP VEIW us_origin_airport_SFO_global_tmp_view AS SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = \u0026#39;SFO\u0026#39; Copy python # 파이썬 예제 df_sfo = spark.sql(\u0026#34;\u0026#34;\u0026#34;SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = \u0026#39;SFO\u0026#39;\u0026#34;\u0026#34;\u0026#34;) df_sfo.createOrReplaceGlobalTempView(\u0026#34;us_origin_airport_SFO_global_tmp_view\u0026#34;) Data Source # DataFrameReader # 아래와 같이 권장되는 사용 패턴이 존재\nDataFrameReader.format(args).option(\u0026quot;Key\u0026quot;, \u0026quot;value\u0026quot;).schema(args).loads() format(): \u0026ldquo;parquet\u0026rdquo;, \u0026ldquo;csv\u0026rdquo;, \u0026ldquo;json\u0026rdquo; 등이 가능하며 기본적으로는 \u0026ldquo;parquet\u0026rdquo; option(): 키/값 쌍을 지정하며 기본 모드로 PERMISSIVE 적용 schema(): 스키마를 유추할 수 있는 DDL 문자열 또는 StructType 제공 load(): 데이터 소스의 경로이며, option()에 지정된 경우 비워둘 수 있음 DataFrameWriter # 아래와 같이 권장되는 사용 패턴이 존재\nDataFrameWriter.format(args).option(args).bucketBy(args).partitionBy(args).save(path) format(): \u0026ldquo;parquet\u0026rdquo;, \u0026ldquo;csv\u0026rdquo;, \u0026ldquo;json\u0026rdquo; 등이 가능하며 기본적으로는 \u0026ldquo;parquet\u0026rdquo; option(): 키/값 쌍을 지정하며 기본 모드 옵션은 error 또는 errorifexists bucketBy(): 버킷 개수 및 버킷 기준 칼럼명 save(): 데이터 소스의 경로 saveAsTable(): 저장할 테이블 Parquet # 다양한 I/O 최적화를 제공하는 오픈소스 칼럼 기반 파일 형식 파케이 파일은 데이터 파일, 메타데이터, 여러 압축 파일 및 일부 상태 파일이 포함 JSON # 단일 라인 모드와 다중 라인 모드가 있고, 다중 라인 모드는 multiline을 true로 설정 compression, dateFormat, multiline, allowUnquoted 등 옵션 사용 가능 CSV # 기본적으로 쉼표로 각 데이터를 구분하며, 다른 구분 기호로 필드를 분리할 수 있음 inferSchema, sep, escape, header 등 옵션 사용 가능 Avro # 스파크 2.4에 내장된 데이터 소스로 소개된 형식으로 카프카에서 메시지를 직렬화할 때 사용 JSON에 대한 직접 매핑, 속도와 효율성, 다양한 언어에서 사용할 수 있는 바인딩 등 이점 제공 avroSchema, recordName, recordNamespace, ignoreExtension 등 옵션 사용 가능 ORC # 스파크 2.x는 벡터화된 ORC 리더를 지원 벡터화된 리더는 행 블록(1024개 단위)을 읽어 작업을 간소화하고\n검색, 필터, 집계, 조인과 같은 집중적인 작업에 대한 CPU 사용량 줄임 Image # 딥러닝 및 머신러닝 프레임워크를 지원하기 위해 이미지 파일을 도입 Copy bash root |-- image: struct (nullable = true) | |-- origin: string (nullable = true) | |-- height: integer (nullable = true) | |-- width: integer (nullable = true) | |-- nChannels: integer (nullable = true) | |-- mode: integer (nullable = true) | └-- data: binary (nullable = true) └-- label: integer (nullable = true) Binary File # 아래와 같은 열이 있는 데이터 프레임 생성 path: StringType modificationTime: TimestampType length: LongType content: BinaryType recursiveFileLookup이 \u0026ldquo;true\u0026quot;로 설정된 경우 label 컬럼이 없음 "},{"id":3,"href":"/blog/2023-03-21/","title":"2023-03-21 Log","section":"Posts","content":"Map Reduce # MR API는 많은 양의 기본 셋업 코드가 필요하고,\n장애 대응이 불안정하며 반복적인 디스크 I/O 작업 발생 머신러닝, 스트리밍 등 동적이고 반복적인 컴퓨팅 작업에서 효율 개선을 위해 스파크 개발 Spark # DAG의 스케줄러와 질의 최적화 모듈을 통해 병렬 수행 모든 결과는 메모리에 유지되며, 디스크 I/O를 제한적으로 사용 데이터 프레임과 같은 고수준 데이터 추상화 계층 아래 단순한 논리 자료구조 구축 스파크 SQL, 스파크 MLlib, 스파크 정형화 스트리밍 GraphX 등 모듈 지원 저장과 연산을 분리하여 데이터 소스를 메모리에서 처리 Spark Components # 어떠한 코드로 작성해도 고도로 경량화된 바이트코드로 변환되어 워커 노드의 JVM에서 실행 스파크 SQL: SQL 계통의 질의를 써서 데이터를 데이터 프레임으로 읽어들이는 기능 스파크 MLlib: 경사 하강법 최적화를 포함한 저수준 ML 기능 포함 스파크 정형화 스트리밍: 카프카 등 실시간 연결 및 반응을 위한 모델 GraphX: 그래프 병렬 연산을 수행하기 위한 라이브러리 Spark Architecture # 스파크 드라이버: 스파크 이그제큐터를 위한 자원 요청 및 태스크 분배 SparkSession: 모든 스파크 연산과 데이터에 대한 통합 연결 채널, 스파크 SQL 질의 가능 클러스터 매니저: 클러스터에서 자원을 관리 및 할당하는 책임 스파크 이그제큐터: 클러스터의 각 워커 노드에서 동작하며 태스크를 실행하는 역할 Spark Application # 스파크 애플리케이션의 핵심에는 스파크 드라이버가 있으며, 이 드라이버는 SparkSession 객체를 생성 스파크 잡: 드라이버는 스파크 애플리케이션을 하나 이상의 스파크 잡으로 변환 스파크 스테이지: 스파크 연산을 여러 스테이지로 나뉘어 실행, 최소 실행 단위 스파크 태스크: 개별 CPU 코어에 할당되어 작업 Spark Calculation # 트랜스포메이션: 원본 데이터를 수정하지 않고 새로운 데이터 프레임으로 변형\n(select()나 filter() 같은 연산으로 원본 데이터 프레임을 수정하지 않음) 트랜스포메이션은 즉시 계산되지 않고 리니지 형태로 기록,\n액션이 실행될 때 트랜스포메이션끼리 재배열하거나 합쳐서 더 효율적으로 최적화 트랜스포메이션 연산: orderBy(), groupBy(), filter(), select(), join() 액션 연산: show(), take(), count(), collect(), save() 좁은 트랜스포메이션: 하나의 입력 파티션을 연산하여 하나의 결과 파티션을 생성 (filter, contains) 넓은 트랜스포메이션: 다른 파티션으로부터 데이터를 읽어 들여서 합치고 디스크에 작성 (groupBy, orderBy) Spark RDD # 의존성: 어떤 입력을 필요로 하고 현재의 RDD가 어떻게 만들어지는지 파티션: 작업을 나눠 파티션별로 병렬 연산할 수 있는 능력을 부여 연산 함수: RDD에 저장되는 데이터를 Iterator[T] 형태로 변환 스파크 2.x는 고수준 연산을 사용하여 명료함과 단순함을 가짐\n예시로 #1과 같은 그룹화는 #2로 간소화할 수 있음\nCopy python #1 agesRDD = (dataRDD. map(lambda x: (x[0], (x[1], 1)))) .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) .map(lambda x: (x[0], x[1][0]/x[1][1])) Copy python #2 avgDf = dataDf.groupby(\u0026#34;name\u0026#34;).agg(avg(\u0026#34;age\u0026#34;)) DataFrame # 스파크는 이전 변경 내역을 보관하여 이를 보존한 채로 칼럼의 이름이나 타입을 변경 가능 스파크의 기본 데이터 타입은 String, Byte, Integer, Float 등 (파이썬 데이터 타입도 지원) 복합 데이터 분석을 위한 Map, Array, Struct, Date 타입 등의 복합 타입 및 정형화 타입 지원 Schema # 데이터 프레임을 위해 칼럼 이름과 연관된 데이터 타입을 정의한 것 스파크가 데이터 타입을 추측해야 하는 책임을 덜어주어 이에 대한 별도의 잡을 만드는 것을 방지 데이터가 스키마와 맞지 않는 경우, 조기에 문제를 발견 가능 프로그래밍 스타일 또는 DDL을 사용하여 스키마 정의 Copy python from pyspark.sql.types import * # 프로그래밍 스타일 schema = StructType([ StructField(\u0026#34;author\u0026#34;, StringType(), False), StructField(\u0026#34;title\u0026#34;, StringType(), False), StructField(\u0026#34;pages\u0026#34;, IntegerType(), False)]) # DDL schema = \u0026#34;author STRING, title STRING, pages INT\u0026#34; # 스키마로 데이터 프레임 생성 df = spark.createDataFrame(data, schema) # 데이터 프레임의 스키마 출력 print(df.printSchema()) Column # Copy python df.select(expr(\u0026#34;Hits * 2\u0026#34;)) # 표현식을 사용한 계산 df.select(col(\u0026#34;Hits\u0026#34;) * 2) # 칼럼명을 사용한 계산 # 기존 칼럼을 연결하여 새로운 칼럼을 생성 df .withColumn(\u0026#34;AuthorsId\u0026#34;, (concat(expr(\u0026#34;First\u0026#34;), expr(\u0026#34;Last\u0026#34;), expr(\u0026#34;Id\u0026#34;)))) .select(col(\u0026#34;AuthorsId\u0026#34;)) # 칼럼값에 따라 정렬 df.sort(col(\u0026#34;Id\u0026#34;).desc) df.sort($\u0026#34;Id\u0026#34;.desc) # $는 칼럼을 Column 타입으로 변환해주는 스파크의 함수 Row # Copy python blog_row = Row(6, \u0026#34;Reynold\u0026#34;, \u0026#34;2015-03-02\u0026#34;) blog_row[1] # \u0026gt;\u0026gt; \u0026#39;Reynold\u0026#39; ; 인덱스로 개별 Row에 접근 # Row 객체들을 데이터 프레임으로 변환 rows = [Row(\u0026#34;Matei\u0026#34;, \u0026#34;CA\u0026#34;), Row(\u0026#34;Reynold\u0026#34;, \u0026#34;CA\u0026#34;)] df = spark.createDateFrame(rows, [\u0026#34;Authors\u0026#34;, \u0026#34;State\u0026#34;]) DataFrameReader # JSON, CSV 등 다양한 포맷의 데이터 소스에서 데이터를 읽어 데이터 프레임으로 가져옴 동일하게 데이터 프레임을 내보내기 위해 DataFrameWriter 사용 DataFrameWriter의 포맷은 기본으로 parquet, 데이터 압축에서는 snapy 사용 Copy python from pyspark.sql.types import * # 데이터 읽기 schema = StructType([StructField(...), ...]) file = \u0026#34;data.csv\u0026#34; df = spark.read_csv(file, header=True, schema=schema) # 데이터 쓰기 df.write.format(\u0026#34;parquet\u0026#34;).save(path) Projection # 필터를 이용해 특정 관계 상태와 매칭되는 행들만 반환 filter()나 where() 메서드로 표현 Copy python from pyspark.sql.functions import * # 앞에서 5개 행 반환 df.show(5, truncate=False) # CallType의 unique한 개수 반환 (df .select(\u0026#34;CallType\u0026#34;) .where(col(\u0026#34;CallType\u0026#34;).isNotNull()) # null 타입 제외 .agg(countDistinct(\u0026#34;CallType\u0026#34;).alias(\u0026#34;DistinctCallTypes\u0026#34;)) .show()) # 칼럼명 변경 df.withColumnRenamed(\u0026#34;Delay\u0026#34;, \u0026#34;ResponseDelayedinMins\u0026#34;) # 칼럼 내용 변경 (df .withColumn(\u0026#34;IncidentDate\u0026#34;, to_timestamp(col(\u0026#34;CallDate\u0026#34;), \u0026#34;MM/dd/yyyy\u0026#34;)) .drop(\u0026#34;CallDate\u0026#34;)) # 날짜 칼럼 질의 (df .select(year(\u0026#34;IncidentDate\u0026#34;)) .distinct() .orderBy(year(\u0026#34;IncidentDate\u0026#34;)) .show()) Aggregation # count(), min(), max(), sum(), avg() 등 연산 Copy python (df .select(\u0026#34;CallType\u0026#34;) .groupBy(\u0026#34;CallType\u0026#34;) .count() .orderBy(\u0026#34;count\u0026#34;, ascending=False) .show(n=10, truncate=False)) Dataset API # 정적 타입 API와 동적 타입 API로 구분 데이터 프레임은 Dataset[Row]으로 표현되며,\nRow는 서로 다른 타입의 값을 저장할 수 있는 포괄적 JVM 객체 데이터세트는 Dataset[T]로 표현되며, 엄격하게 타입이 정해진 JVM 객체의 집합 동적으로 타입이 부여되는 파이썬과 R에서는 데이터프레임, 자바 등에서는 데이터세트 사용 스칼라에서는 케이스 클래스로 데이터세트 정의 Copy scala case class DeviceIoTData (battery_level: Long, ...) ds = spark.read.json(\u0026#34;devices.json\u0026#34;).as[DeviceIoTData] // 데이터세트에서도 트랜스포메이션 또는 액션 수행 가능 // 람다 함수의 인자는 DeviceIoT의 JVM 객체로 개별 데이터 필드에 접근 가능 ds.filter(d =\u0026gt; d.temp \u0026gt; 30 \u0026amp;\u0026amp; d.humidity \u0026gt; 70) Spark SQL # 정형화 데이터 관련 작업을 단순화할 수 있도록 추상화 빠른 데이터 탐색을 위한 대화형 스파크 SQL 셸을 제공 표준 데이터베이스의 JDBC/ODBC 커넥터를 통해 외부의 도구들과 연결할 수 있는 중간 역할 최적화된 질의 계획과 JVM을 위한 최적화된 코드 생성 Catalyst Optimizer # 연산 쿼리를 실행 계획으로 변환하기 위해\n분석 \u0026gt; 논리적 최적화 \u0026gt; 물리 계획 수립 \u0026gt; 코드 생성 과정을 거침 언어에 상관없이 사용자가 실행한 작업은 동일한 과정을 거쳐 바이트 코드로 변환 파이썬에서 df.explain(True) 함수를 통해 스테이지별 상세 내용 확인 가능 Optimization Flow # 분석: 쿼리를 위한 추상 문법 트리 생성 논리적 최적화: 여러 계획들을 수립하고 비용 기반 옵티마이저를 통해 최적화 물리 계획 수립: 논리 계획을 바탕으로 물리적 연산자를 사용해 최적화된 물리 계획 생성 코드 생성: 각 머신에서 실행할 효율적인 자바 바이트 코드를 생성 포괄 코드 생성을 통해 전체 쿼리를 하나의 함수로 합치고 CPU 레지스터 사용을 없앰 "},{"id":4,"href":"/blog/2023-02-19/","title":"2023-02-19 Log","section":"Posts","content":"Collaborative Filtering # 사용자와 아이템 간의 상호 상관 관계를 분석하여\n새로운 새로운 사용자-아이템 관계를 찾는 추천 시스템 A와 B가 유사한 그룹으로 묶일 경우 A가 구매하지 않은 아이템 중 B가 선호하는 아이템을 추천 Memory-based CF # 근접 이웃 방법 아이템 또는 사용자 간의 관계를 계산에 중점으로 두는 방식 사용자가 아직 평가하지 않은 아이템을 예측하기 위함 Item-based CF # 동일한 사용자의 이웃 아이템 점수를 기반으로 아이템에 대한 사용자의 선호도평가 이미 평가했거나 상호작용한 사용자를 대상으로 하는 아이템과 유사한 아이템을 탐색 User-based CF # 새로운 아이템을 평가할 때, 유사한 아이템에 대해 비슷한 점수를 매긴\n다른 사용자들을 찾고, 해당 사용자가 상호 사용한 적 없는 아이템에 대한 사용자 점수를 예측 Model-based CF # 사용자-아이템의 숨겨진 특성 값을 계산하여 학습 확장성과 예측 속도가 빠르지만, 예측 정확도가 떨어질 수 있음 Latent Factor Models, MDP, Decision Tree, Bayesian Network 등 Latent Factor Model # 점수 패턴에서 추론된 20-100개의 벡터로 아이템들과 사용자들을 모두 특성화하여 점수를 설명 Matrix를 사용자-잠재요인, 아이템-잠재요인으로 각각 분해하여 학습 행렬분해(Matrix Factorization) 방법 등 데이터가 클수록 나타나는 성능이 좋음 평가 지표 # 상관계수 코사인 유사도 타니모토 계수 협업 필터링 한계점 # Cold Start Problem: 기존의 경험이 없는 사용자나 아이템에 대한 추천이 어려움 Long Tail: 사용자들의 관심을 많이 받은 소수의 아이템에 집중되는 비대칭적 쏠림 현상 발생 계산량이 많은 알고리즘이기 때문에, 사용자가 증가할수록 계산 시간이 길어져 효율성 저하 References # Blossomindy\u0026rsquo;s Research Blog 추천시스템1 - 추천시스템이란?, 추천 알고리즘의 종류 "},{"id":5,"href":"/blog/2023-02-18/","title":"2023-02-18 Log","section":"Posts","content":"추천 시스템 # 협업 필터링(Collaborative Filtering, CF) 내용 기반 필터링(Content-Based Filtering, CBF) 지식 기반 필터링(Knowledge-Based Filtering, KBF) 딥러닝(Deep Learning) 하이브리드 필터링(협업필터링 \u0026amp; 딥러닝) 협업 필터링 개요 # 구매 및 소비한 제품에 대한 소비자의 평가 패턴이 비슷한 집단 속에서\n서로 접하지 않은 제품을 추천 소비자들의 평가 정도가 필요 (신규, 휴면 고객)\n-\u0026gt; 조회, 클릭 등을 통해 간접적으로 데이터화 내용 기반 필터링 개요 # 제품의 내용을 분석해서 추천 지식 기반 필터링 개요 # 특정 분야 전문가의 도움을 받아 그 분야에 대한 전체적인 지식 구조를 만들어서 활용 협업 필터링 # 협업 필터링 알고리즘 # 모든 사용자 간 평가의 유사도 계산 (코사인 유사도 등) 추천 대상과 다른 사용자간 유사도 추출 추천 대상이 평가하지 않은 아이템에 대한 예상 평가값 계산\n(평가값 = 다른 사용자 평가 * 다른 사용자 유사도) 아이템 중에서 예상 평가값 가장 높은 N개 추천 이웃을 고려한 CF # KNN 방법 Thresholding 방법 사용자 기반 CF # 데이터가 풍부한 경우 정확한 추천 결과에 대한 위험성 존재 데이터 크기가 적고 사용자에 대한 정보가 있는 경우 아이템 기반 CF # 계산이 빠름 업데이트에 대한 결과 영향이 적음 데이터 크기가 크고 충분한 정보가 없는 경우 성과측정지표 # 정확도(accuracy) = 올바르게 예측된 아이템 수 / 전체 아이템 수 정밀도(precision) = 올바르게 추천된 아이템 수 / 전체 아이템 수 재현율(recall) = 올바르게 추천된 아이템 수 / 사용자가 실제 선택한 전체 아이템 수 조화평균(F1 score) = (2 * 정밀도 * 재현율) / (정밀도 + 재현율) 범위(coverage) = 추천이 가능한 사용자(아이템) 수 / 전체 사용자(아이템) 수 "},{"id":6,"href":"/blog/2023-02-12/","title":"2023-02-12 Log","section":"Posts","content":"1강 - 채권시장에 몰리는 개인투자자들 왜? # 📌 국내 채권시장, 올해 들어 뜨거운 상황 # 개인투자자들의 채권 순매수 규모는 연간 4조원 내외로 주식에 비해 매우 작음금리\n→ 상품은 채권이 아니어도 이미 많이 투자하고 있기 때문 올해 들어서는 9월 중순까지 이미 13조원 순매수. 지난 몇 년간 연간 순매수 규모의 3배를 넘어서는 상황 이러한 현상은 국내 뿐 아니라 해외에서도 나타남. 채권으로의 자금 이동 커짐 📌 채권에 대한 관심이 높아진 가장 큰 이유는 채권의 가격이 낮아졌기 때문 # 채권의 가격은 금리와 반대 방향으로 움직임. 즉 금리가 오른 것이 채권 투자에 대한 관심이 늘어난 주된 이유 자산시장에서 한 자산의 매력만으로 관심이 늘어나는 것은 아님.\n대안적인 투자 대상 의 매력도가 떨어지는 경우에도 그런 현상 나타남 채권의 가장 큰 대안적 투자 대상은 주식인데,\n올해(2022 기준) 들어 한국뿐 아니라 글로벌 주식시장은 불안한 움직임을 보이고 있음 아이러니하게도, 최근 주가 하락의 주된 이유가 긴축과 금리 상승. 채권가격과 주식 가격이 동시에 하락.\n그렇다면 두 가지 모두 매력이 커졌어야 하는데 왜 채권이 더 각광을 받을까? ✅ 불확실성이 커졌기 때문. 채권은 주식에 비해서는 불확실성이 확연히 작은 자산\n📌 부동산 시장의 불안도 채권으로의 자금 이동을 부추기고 있음 # 우리나라 자산가들의 투자 패턴은 부동산 보유 후 팔고 예금.\n이후에 시장 상황에 따라서 다시 부동산이나 주식으로.\n즉, 고정이자형 상품은 대기성 자금의 형태 그러나 지금은 주식과 함께 부동산도 위험하다는 인식 퍼져 있음. 지난 5년간 급격하게 상승했고,\n자산가뿐 아니라 청년층 주택 매수 급증으로 추가적인 매수 수요 실종된 상황 또한 우리나라는 가계부채 비율이 높은 수준으로 한번 부동산 시장이 불안해지면,\n회복에 시간이 걸릴 가능성 역시 높음 ✅ 결국 금리 상승 외에 개인들이 접근할 수 있는 자산 중\n대표적인 주식과 부동산의 기대수익률이 낮아진 것이 채권에 대한 관심 급증의 이유\n📌 글로벌 채권시장에서 잘 알려진 채권왕 제프리 건들락 # 최근 인터뷰에서 장기채에 적극적으로 투자하라고 밝힘 제프리 건들락은 원래 가수였는데, 돈을 벌기 위해 투자를 시작.\n지금은 더블라인 캐피탈이라는 회사의 CEO로 140조원의 자금을 굴리고 있고, 개인 자산은 2.6조원에 달함 사실 2021년 초까지만 해도 그는 인플레이션 우려가 심해질 것이기 때문에 금리가 오를 것을 예견,\n채권을 매도하라고 권유했었음. 그 당시만 해도 물가에 대한 관심은 소수만 가짐 하지만, 이제는 반대로 채권을 매수해서 금리가 하락할 때 매도해 10~15% 이익을 얻을 수 있을 것이라 전망. 주식 종목을 잘 고르면 이보다 수익률이 높겠지만,\n채권의 경우 혹시 금리가 올라도 만기 보유하면 금리를 받을 수 있으니 좋은 투자 대안 📌 금리가 올랐다고 채권 투자할 때 아무렇게나 하는 것은 금물 # 지금은 인플레이션 시대. 언제까지 어느 정도로 인플레이션이 높아질 것인지 판단하기 어려운 상태에서\n채권을 잘 모르고 투자하는 것은 자기 자산의 실질 가치를 낮추는 행위가 될 수 있음 예를 들어 OECD는 우리나라의 물가상승률을 5% 이상으로 전망했는데, 지금 3년만기 회사채의 경우 금리가 4.7%대임.\n만약 앞으로 3년간 물가가 계속 5% 이상을 유지하면 이자는 받지만, 내가 투자한 원금의 실질 가치는 떨어지는 것.\n지금 100만원으로 살 수 있는 물건을 3년 후에는 못 사게 됨 따라서 투자 기간이나, 금리 전망, 또 신용위험 등에 대한 적절한 분석을 통해서 가급적 고정 이자가 높고,\n중도 매도 등을 활용해 수익률을 극대화할 수 있는 능력이 있어야 함. 이는 채권을 본질을 알아야 가능한 일 제프리 건들락이 4%도 되지 않는 미국채 금리 환경에서 10~15% 채권 투자 수익률을 낼 수 있다고 생각하는 것은\n채권과 관련된 각종 정보를 효율적으로 이용해 매매에 활용할 수 있기 때문 채권 투자, 왜 어렵게 느껴지나? # 사실 많은 사람들이 채권에 투자한 적이 없다고 생각하지만, 이는 반은 맞고 반은 틀린 얘기.\n채권과 비슷한 성격을 갖는 금융상품에는 이미 많이 투자해 왔기 때문. 사실 예금도 채권의 한 종류라고 볼 수 있음. 중간 매도시 자본이득을 얻을 수는 없지만,\n해지하면 되므로 유동성이 없는 것도 아님 채권형 펀드도 마찬가지. 최근에는 개인의 채권형 펀드 가입이 거의 줄어들었지만,\nMMF나 증권사의 CMA 상품에는 가입되어 있는 경우가 많을 것. 직접은 아니지만 간접적인 형태로 투자하고 있는 셈 또한 이미 많은 사람들이 자신의 목적에 따라 채권에 효율적으로 투자하고 있음.\n많은 사람들이 채권으로 자신의 금융적 목표를 이루는 것은 결국 채권의 본질에 대해 잘 알기 때문.\n지금이라도 채권을 배울 이유는 충분 📌 채권을 잘 공부하면, 금융시장 전반을 이해하는 데 큰 도움이 됨 # 사실 채권은 주식에 비해서 훨씬 더 오래된 금융상품. 주식의 시작은 주식회사와 함께 출발하는데,\n17세기 네덜란드동인도회사가 첫 사례로 여겨지고 있음. 그런데 채권은 이보다 더 오래 전에 전쟁과 함께 시작. 과거 왕국들은 전쟁을 많이 치뤘는데,\n전쟁에는 돈이 많이 들어가고 세금으로 이를 감당할 수 없었음.\n따라서 왕국들은 채권을 발행해 자금을 조달하고 나중에 원금과 이자를 지급하는 방식을 활용했음 하지만, 채권의 가격인 금리는 현대적인 의미의 채권보다도 훨씬 더 긴 역사를 가짐.\n어찌 보면 인간 역사에서 잉여가 발생한 후에 가장 먼저 나타난 개념이 금리일 수 있음 ✅ 이 같은 점은 채권과 채권의 가격이 금리를 잘 이해하면,\n그 경제가 어떤 상황에 처해 있는지를 알 수 있음을 시사함\n📌 금리는 어떤 방식으로 경제 상황을 판단하게 해 줄까? # 금리는 어떤 경제의 건강한 정도를 나타내는 척도가 됨 금융시장에 위험이 없고 물가가 안정된 상태에서 금리가 높다는 얘기는 그 나라가 성장률이 높다는 점을 의미함.\n즉, 돈을 빌려가는 사람들이 높은 금리에 빌려가도 투자 등을 통해 장사를 잘 해서 갚을 수 있는 상황임을 의미함 하지만 이와 함께 위험이 커지면서 나타나는 금리 급등은 그 나라 또는 그 나라의 경제 주체 중\n상당 수가 자신의 부채를 갚지 못할 수 있음을 의미.\n갚지 못할 위험이 위험 프리미엄의 형태로 금리에 반영되는 것 ✅ 금리가 중요한 이유 중 하나는 정책당국이 금리를 통해 정책을 수행한다는 점.\n주식은 거래하는 사람들이 제한되어 있는 데다, 실제로 주가에 적용을 받는 주체는 기업과 주식 투자자 뿐.\n하지만, 금리는 광범위한 사람에게 영향. 부채가 많은 정부도 영향 우리가 때때로 겪는 경제 위기가 주로 채권시장으로부터 출발한다는 점도 중요. 자기 돈을 잃는 것보다 남에게 갚지 못하는 것이 경제에서는 더 큰 충격이기 때문. ✅ 결국 채권시장의 상황과 금리를 공부하면 현재 경제의 건강성, 정책 기조,\n나아가 경제 위기 발생 가능성 등을 가늠하는 데도 도움이 될 것\n2강 - 알야야 번다, 채권이란? # 📌 채권은 한마디로 돈을 빌리면서 발행한 증권 # 돈을 빌리려면 결국 언제 갚을 건지, 빌린 기간 동안 얼마나 이자를 지급할 건지,\n언제 이자를 지급해야 할 건지가 정해져야 함 이를 채권 투자 용어로 설명하면 → 만기와 현금 흐름의 조건이 사전에 결정된 유가증권.\n여기서 중요한 것은 사전에 결정된다는 점. 발행 시점에 결정되어 모든 것이 정해져 있다는 게 다름.\n주식과 비교해 보면 주식은 배당을 주지만, 배당은 정해져 있지 않음. 또한 만기도 없음 채권 발행은 결국 남의 돈을 빌리는 것으로부터 출발하기 때문에 주식과 발행자가 다를 수 있음.\n주식의 경우에는 민간 기업만 발행 가능. 📌 왜? → 주식은 이익이 많이 나면 그것이 유보되어 기업 가치에 포함되거나, 배당으로 지급되는 개념 # 공공기관은 이익 극대화가 첫번째 목표가 아님.\n예를 들어 정부는 세금을 걷고 써서 돈을 많이 남기는 것이 목표가 아님.\n이러한 기관이 발행한 주식의 기대 배당은 0. 따라서 주가가 형성될 수 없음. 채권은 이자와 원금을 갚을 것이라는 약속만 하면 발행할 수 있음. 정부처럼 세금을 걷어서 갚을 수도 있고,\n가지고 있는 자산을 팔아서 갚을 수도 있음. → 채권 발행자는 주식 발행자에 비해 훨씬 광범위 📌 용어 중에 제일 기본적인 것은 채권을 구성하는 요소 # ✅ 액면은 돈을 빌려 줄 때 단위로 해석할 수 있는데, 가격 계산의 단위이기도 함.\n주식에서 발행할 때 액면가라는 개념이 있는데, 이는 보통 5000원이고, 요즘은 500원으로도 발행.\n그리고 배당의 기준이 됨. 이처럼 금리는 이 액면에 대한 비율을 의미함 ✅ 만기는 주식과 다른 점. 주식은 만기가 없음. 기업이 살아 있는 한 주식회사의 주식은 계속 존재.\n의도적으로 상장 폐지를 시키는 경우에도 주식은 존재함. 주식회사가 없어져야 주식도 없어짐.\n그렇지만, 빌리는 돈은 언젠가 갚아야 함. 빌려주는 사람도 원금을 돌려 받기 때문에 빌려줌. ✅ 표면이율, 또는 표면금리도 중요함.\n표면금리는 발행할 때 앞으로 특정 시점에 지급할 이자를 계산하는 금리임.\n보통 발행 당시의 시장금리와 비슷한 수준에서 결정됨. 그래야 매수자가 매수할 수 있기 때문.\n표면금리를 시장금리보다 낮추거나 높이는 경우가 있는데, 이 경우에는 그만큼 더 싸거나 비싸게 발행됨. 표면이율은 시장금리가 변해도 만기 때까지 변하지 않음. 즉 고정된 이자임.\n이것이 있기 때문에 채권 투자의 의미가 있는 것. 주식 배당이 시기에 따라 다른 것과 큰 차이 ✅ 경과 기간과 잔존 기간이라는 용어도 알아야 함.\n채권이 발행된 이후에는 시간이 갈수록 처음에 약속한 만기가 점점 가까워짐.\n즉 현재부터 원금 상환 때까지의 기간은 점점 짧아짐.\n만약 처음에 3년 만기로 발행된 채권이 1년 지났다면 경과 기간은 1년이고, 잔존 기간은 2년임.\n이 시점에서 거래가 될 경우 금리는 2년만기 금리를 적용받게 됨 📌 가격 계산에서 가장 중요한 원리 # 어떤 자산의 가격이라는 것이 그 자산을 보유함으로써 얻게 되는 현금을 현재가치로 환산한 것이란 점. 현재가치라는 것은 미래 발생하는 현금 흐름이 지금 얼마로 평가되어야 하는가를 의미함. ex) 1년 후에 받는 100만원은 현재로 보면 그보다 낮게 평가되어야 함.\n시장금리에 따라서 현재 95만원을 투자해 놓으면 1년 후에 100만원이 될 수 있기 때문.\n반대로 지금 100만원을 1년간 투자해 놓으면 105만원이 될 수 있음.\n이 경우 1년 후 105만원의 현재가치가 100만원이라고 표현할 수 있음 채권은 종류에 따라 만기까지 여러 번 이자를 주는 채권도 있고 그렇지 않은 채권도 있음.\n하지만, 이를 일반화해서 쓰면 위와 같은 식으로 표현할 수 있음.\n표를 보면 가격 P는 앞으로 들어올 이자와 원금을 각각의 기간에 대한 할인률도 현재가치화해서 합한 것임 그런데 채권의 경우에는 가격 계산이 더 쉬움. 일반적인 채권의 경우 C가 모두 같기 때문.\n그리고 우리가 얘기하는 시장금리는 y인데, 채권 계산할 때 y도 같음. 엄밀하게 말하면 y가 다 달라야겠지만,\n우리가 쓰는 시장금리는 이것을 가중평균해 놓은 금리이기 때문에 한 가지만 쓰면 됨 그런데 이 식으로부터 재미 있는 사실을 유추할 수 있음. 금리 y가 클수록 가격 P는 작아진다는 점.\ny가 분모에 있으니 당연한 얘기. 또한 표면이자 C가 클수록 가격 P도 커짐 ✅ 그렇다면 실제로 채권 가격을 계산할 때 이것을 다 계산기로 계산해야 하나? → 그렇지 않음.\n시장에서는 시장금리로 거래되고, 가격 계산은 기계가 알아서 함\n채권, 생각보다 넓은 기대수익과 위험 # 앞의 가격 산식에서 짐작할 수 있듯이 기본적으로 채권은 주식에 비해서 불확실성이 작은 자산으로 볼 수 있음.\n만기, 표면금리 등 미리 정해진 것들이 많기 때문 하지만, 실제로 자산별 위험과 수익의 구조를 보면 채권은 상당히 넓은 위험을 가지는 것으로 평가됨.\n그래프를 보면 특정한 예금보다 더 안전한 채권이 있는 반면, 주식보다 더 위험이 큰 채권도 있음. 이러한 위험, 수익 구조가 나타나는 것은 채권 발행자가 워낙 다양하고, 만기도 천차만별이기 때문.\n물론 불확실성이라는 측면에서 보면 주식이 불확실성한 요소가 더 많은 것은 분명.\n하지만, 실제적으로 가격이 움직일 수 있는 범위나 투자수익률의 관점에서 보면 주식의 가격 변동 범위,\n투자수익률보다 더 큰 변동성과 손실을 내는 채권들도 존재 ✅ 예금보다 안정적인 채권은 단기 국채를 들 수 있음.\n은행은 극단적인 경우 망하고 예금을 돌려줄 수 없는 경우가 있지만,\n자국 통화 국채는 그런 경우가 나타날 가능성이 없고,\n단기 국채의 경우 금리 변동에 따른 가격 변동성 자체도 작기 때문\n📌 채권과 주식의 차이점 # 채권은 이익 극대화를 목표로 하는 기관이 아니더라도 발행할 수 있다는 점이 주식과 다름.\n주식은 주식회사만 발행. 이 때문에 채권 종목이 너무 많아져 복잡하게 느껴지긴 하지만,\n반대로 그렇기 때문에 채권은 더 많은 기회를 제공해 주기도 함 채권과 주식은 각각 타인 자본, 자기 자본으로 불림. 예를 들어 회사 입장에서 볼 때 채권은 빌린 돈이지만,\n주식은 스스로 주인으로 참여하겠다고 들어온 돈 주식을 보유하면 특정되지 않은 배당을 받는 반면 채권을 보유하면 이자를 받으며,\n만기가 도래하면 원금도 돌려받는다는 특징이 있음. 주식은 회사가 망하지 않는 한,\n그리고 매입 소각되지 않는 한 만기가 되지 않음 ✅ 채권은 워낙 다양한 면모를 가지기 때문에 주식보다 더 위험하고, 예금보다 더 안전한 채권도 존재함.\n그만큼 채권 투자자에게는 기회가 많다고 볼 수 있음\n3강 - 채권, 어떤 종류가 있나요? # 📌 채권의 발행 주체 분류 # 하나의 자산군은 다양한 기준으로 분류가 가능.\n그 중에서도 가장 대표적인 분류은 발행자가 누구냐에 따라 분류하는 것. 채권의 발행자는 크게는 공공기관이 발행한 채권과 민간기업이 발행한 채권이 있음.\n⚠️ 공공기관에는 정부와 중앙은행도 포함.\n이외에도 공사채나 지방자치단체가 발행한 채권도 공공기관 발행 채권에 포함.\n민간기업의 경우에는 일반 회사와 금융기관을 나누고,\n금융기관을 다시 은행과 기타 금융기관으로 나누는 것이 일반적인 분류법 보증 유무별로도 채권을 분류할 수 있음.\n보증이라는 것은 발행자가 원리금 상환을 하지 못하게 된 경우에 누군가가 그걸 대신해 주는 경우를 얘기.\n보증자는 수수료를 받고 그 일을 해줌. 예를 들어서 보증 보험회사가 그런 일을 해 줄 수 있음. 이렇게 되면 사는 사람 입장에서는 당연히 더 안전해짐.\n하지만, 최근 들어 보증채권은 거의 발행되지 않는데,\n보증이 들어간 채권은 결국 보증자의 신용으로 발행된 채권과 마찬가지여서 금리가 낮을 수 밖에 없기 때문. 이외에 담보채권도 있는데, 이것은 보증채권과 달리 발행자가 갚을 수가 없을 때\n보유하고 있는 자산 중 일부를 팔아서 갚도록 처음부터 정해진 채권임.\n이렇게 되면 이 담보에 대해서는 우선권을 갖기 때문에 투자자는 선호.\n그런데 이 경우에도 금리가 낮아서 발행은 거의 없음 채권의 구성 요소 중 하나인 만기, 즉 상환기간 별로도 나눌 수 있음.\n주로 장기, 중기, 단기로 나뉘는데, 미국의 경우 기준은 보통 1년 10년을 기준으로 분류하고,\n우리나라의 경우는 1년 5년 정도를 기준으로 함. 다만, 이 같은 분류는 정해진 것이 아니기 때문에\n유연하게 분류될 수도 있어 주의할 필요 📌 국채는 채권이 출발한 시작점이자 한 나라 채권의 기준이 되는 채권. # 주로 해당국 통화로 발행되기 때문에 가격은 떨어질 수 있지만, 부도 위험이 없음.\n물론 정부가 외화가 필요할 때는 달러나 유로로 발행하기도 하지만, 규모는 크지 않음 국채는 다양한 만기로 발행됨. 우리나라에서는 과거에 3년이 주된 발행 채권이었지만,\n지금은 2년, 5년, 10년, 20년, 30년, 길게는 50년 만기 채권도 발행.\n→ 수요자들이 원한 것도 있지만, 정부가 자기들의 부채를 만기별로 나눠 놓기 위한 것.\n한번에 상환 규모가 커지면 부담이 커지기 때문 📌 국채 중에 특별한 것으로는 소비자물가가 오를 때 지급되는 이자도 같이 올라가는 물가연동채권 # 이 채권은 물가가 오를 때 비싸지고 내릴 때 싸짐. 주로 10년 만기로 발행됨 국채는 기관투자자들이나, 자산가들이 극도로 안정적이면서\n수익에 크게 개의치 않을 때 사는 채권으로 개인투자자들에게는 적합하다고 볼 수 없음. ✅ 국채 금리는 그 나라 전체 채권 시장의 기준 금리가 되고,\n가장 많이 거래되기 때문에 채권 투자자라면 국채의 거래 상황에 관심을 가질 필요가 있음\n📌 국채와 비슷한 채권으로 한국은행이 발행하는 통안증권 # 통안증권은 기본적으로 한국은행이 시중에 돈이 많이 풀렸다고 생각될 때 발행하는 채권으로\n주로 1년과 2년 만기로 발행됨. 즉 통화량 조절 목적의 채권임. 한국은행 역시 정부에 가깝기 때문에 상환을 못할 위험은 없음. 돈을 찍어서 갚으면 되기 때문.\n하지만, 국채와 비슷하게 금리가 낮기 때문에 개인 투자자 입장에서는 투자 매력이 크지 않음 📌 개인투자자들이 관심을 가질 만한 채권 3인방 # 공사채 # 특수채라고도 하는데, 특별법에 의해서 만들어진 공사,\n예를 들어 수자원공사, 도로공사, 주택금융공사 등이 발행한 채권을 의미함. 이러한 채권은 국채보다는 금리가 조금 높음.\n하지만, 특별법 자체에 정부가 해당 공사에 손실이 발생했을 때\n이를 보전하는 항목이 있는 경우가 대부분이기 때문에 사실상 부도 위험이 매우 낮다고 할 수 있음. ✅ 안전성을 중시하는 투자자들 중 국채보다는 조금 더 높은 금리를 원하는 투자자들은 공사채 투자 가능\n금융채 # 금융채는 말 그대로 금융기관이 발행한 채권 공공기관의 성격을 가지면서 금융기관이기도 한 산업은행, 중소기업은행, 수출입은행 등이 발행한 채권도 있지만,\n시중은행이 발행하는 채권도 있고, 카드사와 캐피탈사 등 여신전문업체가 발행하는 채권도 있음. 여신전문업체는 수신 기능 예를 들어 예금을 받는 기능은 없고\n채권이나 어음 발행을 통해서만 자금을 조달할 수 있는 금융기관. ✅ 이러한 다양한 금융기관은 다 위험도가 다름.\n따라서 금융채 내에서 적절한 채권을 찾을 경우 안전성과 모두 달성할 수 있음\n회사채 # 일반 기업이 발행하는 회사채 역시 투자자에게는 매력적인 채권. 기업들은 투자나 운영 자금을 마련하기 위해서 회사채를 발행하는데,\n크고 우량한 회사가 발행한 채권일수록 안전한 대신 금리가 낮고,\n다소 작고 신용도가 떨어지는 채권은 조금 덜 안전한 대신 금리가 높음. 이러한 위험은 채권이 발행될 때 신용평가회사가 부여한 등급으로 확인되는데,\n공사채, 금융채에도 마찬가지로 등급이 부여되어 투자에 도움을 주고 있음 📌 회사채의 종류와 특징 # 회사채는 일반적인 채권 이외에도 많은 종류가 있음.\n기본적으로 발행하는 기업의 신용등급별로 종류가 나뉘지만,\n여러 가지 조건들이 부여되면서 다양한 투자 기회를 제공해 줌 증권회사에서 발행하는 ELS나 DLS는 회사채로 분류되는데, 일반적인 회사채와는 다른 형태 기본적으로 발행하는 증권사의 신용도에 따라 가격이 결정되기도 하지만,\n특정한 조건이 만족될 경우 더 높은 금리를 주는 경우가 많기 때문에 투자자들에게 관심. ELS는 주가지수나 특정 기업의 주가 변동에 따라서 이자 지급의 형태가 달라지는 채권이고,\nDLS는 주가 이외에 금리나 원자재 가격 등 다양한 상품의 가격을 기초로 이자지급 형태가 달라지는 채권임. ✅ 다만, 이러한 채권은 기초자산의 가격이 크게 움직일 경우 이자를 지급받지 못하거나\n원금도 받지 못할 가능성이 있다는 측면에서 일반적인 채권과 다름.\n따라서 투자에 있어서 매우 주의를 기울여야 함\n주식과 채권의 중간 형태로 전환사채나 신주인수권부사채 등도 발행됨. 보통 작은 기업이 좋은 신용등급을 받을 수 없기 때문에 특정한 조건이 만족되면 주식으로 전환되거나\n신주를 받을 수 있는 형태로 발행하는 채권인데, 주가가 정해진 수준보다 오르면 전환하거나\n신주를 받아서 팔 수 있기 때문에 투자자로서는 일종의 선택권을 갖게 됨. 만약 주가가 안 오르면 채권의 형태로 보유할 수도 있음.\n다만, 이러한 채권을 발행하는 기업은 일반적으로 신용등급이 낮고,\n주식을 준다는 점을 강조해 표면이자를 낮게 발행하는 경우가 많아 개별 종목별로 잘 선별해서 투자해야 함 최근에는 금융기관들이 발행하는 신종자본증권이 있음. 금융기관들은 감독원의 자기자본 기준을 맞추기 위해서 증자도 하지만,\n때로는 자본으로 인정받는 채권을 발행하기도 함. 이것이 신종자본증권 자본으로 인정받기 위해서는 만기가 길어야 하기 때문에 10년 또는 그 이상으로 발행되고,\n금융기관이 부도가 났을 경우 일반적인 채권보다 상환 우선순위가 낮는데,\n보통 자본으로서 인정받지 못하는 시기가 되면 금융기관이 되사가고,\n부채비율이 높을 수 밖에 없는 금융기관의 일반 채권과 신종자본증권의 상환우선 순위 차이는\n큰 의미가 없기 때문에 개인투자자들이 관심을 갖는 채권임. 특히 금리가 높다는 점이 매력 양도성정기예금 # CD라고 하는데, 은행이 발행. 일반 예금과 다른 점은 매매가 가능하다는 점.\n기업어음은 기업이 발행하는 1년 이내 어음이고,\n표지어음은 과거에는 종합금융사가 지금은 대형 증권사가 발행하는 단기 자금 조달 수단임. 환매조건부채권 # 투자자 입장에서는 채권을 잠시 보유하는 대가로 이자를 받는 상품이고,\n정해진 기간이 되면 원래 보유하고 있던 기관이 다시 사 가는 거래 보통 채권을 보유한 기관이 단기로 자금을 조달할 때 사용 콜 # 매일매일 들어오고 나가는 자금이 맞아야 하는 금융기관들이 단기 자금 과부족,\n잉여 상태일 때 초단기로 거래하는 상품 ✅ 이러한 단기금융상품을 알아 두면 내가 돈을 맞긴 금융기관이\n단기 유동성을 어떻게 관리하는지 알 수 있기 대문에 관심을 가질 필요가 있음\n📌 국채 선물 시장 # 국채 선물시장은 주가지수선물처럼 국채 가격이 변할 때 같이 변하는 선물을 의미 작은 증거금만 내고 거래가 가능한데 3년 5년 10년 국채 선물시장이 있음 주식시장에서 선물을 매도함으로써 주가가 떨어질 때 이익을 얻을 수 있는 것처럼,\n채권시장에서도 국채선물을 매도하면 금리가 오를 때, 즉 채권가격이 떨어질 때 이익을 얻을 수 있음 ✅ 즉, 채권시장에서도 헷지가 가능함. 또한 국채선물시장에는 지금도 개인투자자들이 진입해 있음.\n4강 - 채권투자, 변수는 전망! 채권 가격 전망 비법 공개합니다 # 📌 시장금리 전망은 주가지수, 환율 전망과 함께 금융시장의 움직임을 가늠하는 데 가장 중요 # 시장금리 그 자체가 채권의 가격을 의미하기 때문. 채권의 가격 산식은 표면이자와 만기, 그리고 시장금리로 이뤄져 있음.\n그런데 다른 두 가지가 정해져 있고, 시장금리만 시장에서 결정되며 시시각각 변화하므로\n채권의 가격을 변동시키는 유일한 요소는 시장금리라고 할 수 있음 모든 자산에서 그렇듯 가격을 전망하는 것은 최적의 투자 의사결정을 내리기 위함\n→ ✅ 즉, 시장금리 전망을 해야 채권을 사고 파는 최적의 투자 의사결정을 내릴 수 있음 📌 피셔 방정식 # 피셔 방정식을 보면 우리가 채권의 가격을 계산하는 금리인 명목 금리는\n개념적으로 실질금리와 기대인플레이션의 합으로 이뤄져 있다는 점을 알 수 있음. 이러한 관계는 이미 현대 경제학의 시작과 함께 제시된 개념.\n기본적으로 누군가에 돈을 빌려줄 때는 앞으로 물가가 얼마나 오를 것인지를 감안하기 때문. 즉, 적어도 물가가 오르는 만큼 보상받지 못하면 돈을 빌려줄 이유가 없음.\n그런데 이 같은 보상은 최소한도임. 돈을 빌려줄 때는 그 이상의 보상을 받고 싶기 때문임.\n내가 돈을 쓰지 못하는 기간의 기회 비용에 대한 보상을 받아야 돈을 빌려줄 의지가 생김 ✅ 이 수식에서 기대인플레이션과 기회 비용에 대한 보상 모두 경제적 상황이 영향을 준다는 점을 알 수 있음.\n📌 실질금리는 기회 비용에 대한 이야기 # 돈을 빌려 주는 사람부터 생각해 보면, 이 사람은 돈을 빌려줄 수도 있지만, 직접 사용할 수도 있음. ex) 어떤 가계가 열심히 저축해 1억원의 돈이 있다고 가정.\n이 경우 이 사람은 이것을 채권을 사는데 사용할 수도 있지만, 직접 가게를 차리는 데 사용할 수도 있음 만약 직접 가게를 차려서 벌수 있는 순수한 이익이 1년에 1천만원이면\n이 사람은 1억원으로 20%의 수익률을 올렸다고 볼 수 있음.\n여기에서 순수한 이익은 각종 원재료 값은 물론 나와 종업원의 인건비 등을 모두 합한 것임 그런데 위험이 크지 않은 시장금리가 10%에 크게 미치지 못하는 수준,\n예를 들어 5%면 이 사람은 돈을 빌려주는 것보다 내가 직접 사업을 하는 것이\n나은 선택일 수 있음 즉, 더 많은 이자를 주는 경우에 돈을 빌려줄 것임 반대로 돈을 빌리는 쪽은 5%로 빌려서 가게를 차리고 10%를 벌 수 있다면 좋은 일임.\n하지만, 그렇지 않더라도, 예를 들어 6%로 빌리는 경우에도 남는 것이 있기 때문에\n더 높은 시장금리를 받아들일 수 있을 것임 ✅ 결국 이러한 의사 결정은 앞으로 경제가 어떤 모습을 보일 것인가에 의존하게 됨.\n경제가 좋아질 것으로 예상되면 가게를 차려서 얻을 수 있는 이익이 더 커질 것이라고 예상할 수 있기 때문\n✅ 명목금리 중 실질금리 부분은 경기 사이클과 함께 상승과 하락을 반복할 것이라고 예상할 수 있음.\n기대 인플레이션 # 도표를 보면 가운데 표시된 원이 경기 사이클을 의미. 오른쪽 위부터 회복, 확장, 수축, 침체기로 회전하는데,\n각각의 경우 실질금리가 어떤 방향의 압력을 받는지가 네모 박스 안에 표시되어 있음 그런데 점선 네모 박스를 보면 저점과 고점이 각각 경기의 고점, 저점보다 먼저 기록됨을 알 수 있음.\n이는 채권시장도 주식처럼 실제 상황을 사전 반영하기 때문. 시차는 경기 사이클마다 다르지만, 선행성은 분명히 나타남 통화 재정 정책도 영향을 미침 한편, 이러한 경기 사이클과 함께 중요하게 고려해야 하는 부분이 있음. 바로 정부의 정책임.\n정부는 통화, 재정정책을 통해 경기 사이클의 진폭을 안정시키려 하기 때문에\n때때로 경기 사이클에 따른 금리 변동에 영향을 미침 특히 통화정책의 경우 단기금리의 진폭을 더 크게 만드는 성격을 갖는데,\n경기 확장이 과열로 이어질 경우 중앙은행이 과열을 막기 위해 금리를 크게 올려 대응하기 때문. 반면 경기 확장이 침체로 이어질 경우에는 금리를 크게 내려 대응해 단기금리도 크게 내리는 현상이 나타남.\n즉, 통화정책은 경기 사이클에 따른 금리 변동 폭을 조금 더 키우는 역할을 하는 것이 일반적 재정정책은 반대 방향으로 영향을 미치는 경우가 많음.\n경기가 좋을 때는 세수가 많이 걷혀 국채 발행이 줄어드는 반면,\n경기가 나쁘면 세수가 줄어드는 데다 재정정책을 쓰기 위해 국채 발행을 늘리기 때문 한편, 이러한 정책 역시 실제 정책만큼이나 정책에 대한 기대가 영향을 미친다는 점을 기억해야 함.\n예를 들어 통화정책의 경우 앞으로 긴축을 하겠다는 발언 만으로도 시장금리에 영향을 미칠 수 있음.\n따라서 실제 정책을 수행하는 것 만큼이나 시장에서 형성된 정책 기대를 살펴야 할 필요가 있음 통화정책은 경기 사이클에 연동하고, 이에 따라 경기 확장 때는 금리 상승 방향으로\n수축 때는 하락 방향으로 영향을 미친다는 점을 표시. 다만, 앞서 지적한 대로 진폭을 크게 할 수 있다는 점과\n실제 정책만큼 기대가 영향을 미친다는 점에 는는 유의할 필요가 있음 📌 수익률 곡선 # 수익률곡선은 시장금리와 채권 만기까지의 기간을 축으로 만든 그래프.\n시장에서는 1년만기 금리와 3년만기, 10년만기 금리가 각각 다르기 때문 이 같은 수익률곡선을 살펴보는 이유는, 나중에 전략에도 도움이 되지만,\n근본적으로는 시장에서 단기금리와 장기금리가 늘 같은 방향으로 움직이지는 않을 수 있다는 점을 강조하기 위해서. 만기별로 금리에 영향을 미치는 요소들을 살피는 것도 채권 가격을 전망하는 데 중요 기본적으로 단기금리는 통화정책에, 장기금리는 경제 전망과 채권 수급 등의 영향을 더 받는 경향이 있음.\n특히 2년 또는 3년 만기 이하 채권 금리는 정책금리를 올리거나 올릴 것이 확실해지면 같이 오르고,\n반대의 경우 내리는 경향이 강함 반면, 장기금리는 통화정책 방향과 같이 움직이는 경우도 있지만, 금리를 올리는 것이\n결국 경제를 나쁘게 할 것이라는 기대로 이어지면 떨어지기도 함. 이에 따라 수익률곡선의 기울기는 완만해지거나,\n심지어 장단기금리가 역전되기도 함. 또한 - 장기채권에 대한 수요가 강력할 때도 비슷한 현상이 나타남 ✅ 따라서 여러 요인을 동시에 감안해 장단기 금리를 전망해야 함\n5강 - 알야야 성공한다, 채권투자 위험: 금리 위험과 신용위험 # 투자에 있어서 위험이란 무엇일까? # 모든 자산 투자는 위험을 내포함. 즉, 투자의 결과가 생각한 대로 나타나지 않을 가능성이 큼.\n사거나 팔 당시 현재 가격은 정해지지만, 미래 가격이나 상황은 정해지지 않은 미지의 영역이기 때문 채권은 기본적으로 안전한 자산이지만, 극단적으로는 국채도 상환받지 못할 가능성을 완전히 배제할 수는 없음.\n0.000001%의 확률이라도 가능성이 0이라고는 볼 수 없다는 얘기 하지만, 투자의 위험을 사전에 어느 정도 측정할 수 있음.\n예를 들어 주식시장에서 성장주는 조금 더 가격 변동성이 크고,\n부동산 시장에서 특정 지역의 부동산은 다른 지역의 부동산보다 조금 더 위험하다는 판단을 내릴 수 있음 이러한 특징을 알고 있다면, 내 성격 즉 내가 위험을 감수하고 높은 수익률을 올리고 싶은지,\n낮지만 안전한 수익률을 원하는지에 따라 알맞은 자산을 선택할 수 있음 채권 투자, 비교적 안전하지만 위험 존재 # 채권 투자는 앞서 자산별 기대수익과 위험 도표에서 살펴봤듯이 넓은 범위의 위험을 내포하고 있지만,\n대체로 주식 투자에 비해서는 덜 위험하다고 여겨짐.\n특히 만기까지 보유할 경우에는 고정된 이자가 지급되기 때문에 매도하지 않는다면 위험이 통제됨.\n또한 그러한 성격 때문에 가격 변동성도 주식에 비해 작은 편. 그렇지만, 채권을 적극적으로 투자하고, 수익률을 높이고 싶은 경우에는 각종 위험을 감수할 수 있음.\n그리고 대표적인 위험은 가격 변동 위험을 의미하는 금리 위험과 장단기금리 차이가 서로 다르게 움직여서 나타나는 위험,\n치명적으로 작용할 수 있는 부도위험과 부도위험의 크기로 변하는 신용위험,\n원하는 시기에 매도를 할 수 없는 유동성 위험 등이라고 할 수 있음 그런데 이 같은 위험 중 빈도 수 측면에서는 금리 위험이, 위험의 크기 측면에서는 신용위험이 가장 큼.\n따라서 이번 시간에는 이 두가지 위험을 사전에 측정하는 방법을 소개하겠음.\n중간에 수식이 하나 나오지만, 시스템에서 계산되기 때문에 여러분은 개념만 잘 이해하면 됨 금리 위험과 듀레이션 # 앞서 우리는 시장금리가 채권 가격을 움직이는주된 요인이고,\n금리를 전망하면 채권 가격 변동을 예상할 수 있다고 했음.\n그런데 중요한 것은 이러한 시정금리 변동에 따른 채권 가격 변동이 채권별로 다르다는 점.\n즉 시장금리가 1% 오르거나 내릴 때 각각 채권의 가격은 다르게 움직임.\n그런데 이러한 정보를 알고 있으면 어떤 장점이 있을까? 만약 금리가 올라 채권 가격이 떨어질 것이라고 전망이 확실하면 우리는 어떻게 대응해야 할까?\n채권을 팔거나 뭔가 금리가 오를 때 가격이 같이 오르는 자산을 사면 될 것임.\n주식시장에서 주가가 떨어질 것 같으면 선물을 매도하는 것과 같은 원리. 또한 드물긴 하지만,\n채권의 경우에는 시장금리가 오를 때 받게 되는 표면 이자도 같이 오르는 채권을 사면 될 것임 하지만, 금리 전망은 기본적으로 확실하지 않음. 따라서 처음에 채권을 살 때\n내가 금리가 변화될 위험에 대해 어떤 생각을 갖고 있는지를 감안해 사는 채권을 선택할 수 있음.\n즉 위험을 회피하는 사람이면 금리가 변해도 가격이 덜 변하는 - 채권을,\n고수익을 원하는 사람이면 금리가 조금 변해도 가격이 더 크게 변하는 채권을 선택하면 됨 이러한 경우에 사용되는 개념이 듀레이션임.\n듀레이션은 1%의 금리 변동에 대해 채권 가격이 얼마나 변하는가를 측정한 지표로 각 채권별로 다름 듀레이션의 개념 # 듀레이션은 어떻게 계산될까/ 앞서서 얘기한 것처럼, 듀레이션 수식을 외우거나 할 필요는 없음.\n매매하는 시스템이 개별 채권의 듀레이션을 다 계산해 줌. 다만, 이해를 돕기 위해 수식을 소개함.\n개념적으로 보면 듀레이션은 채권 현금흐름의 가중평균 상환기간임 처음에 투자자들이 수식을 만든 것은 채권에 투자한 사람으로서 갖는 기본적인 질문,\n이 채권을 사서 얼마 정도 지나면 내가 처음에 투자한 원금 부분이 회수되는 걸까?에 대해 답하기 위해서임.\n채권은 중간에 이자를 지급하는 경우가 대부분이기 때문에 이 수치는 만기까지의 기간보다 짧게 됨.\n만약 만기 때만 이자와 원금을 주는 채권의 경우에는 만기까지의 기간과 듀레이션이 같음.\n그런데 식이 복잡해진 것은, 미래에 들어오는 돈을 다 현재가치화 해야하기 때문.\n내일 받는 10원의 오늘의 10원과 실질적으로 다르다는 점을 반영 그런데 투자자들은 이러한 식을 조금 변형하면 1%의 금리 변동에 따른 채권가격 변동분으로 바뀐다는 점을 발견했음.\n즉 금리 변동에 따른 채권 가격의 민감도를 계산하는 데 사용할 수 있음을 발견 그림의 원점에 대해 볼록한 곡선은 어떤 채권의 가격과 금리와의 관계를 나타냄.\n곡선이 이렇게 생긴 것은 수식이 다항식이기 때문. 여기에서 듀레이션은 금리가 금리1에서 금리2로 오를 때 가격이 가격1에서 가격2로 움직이는 직선의 기울기를 의미함.\n조금 어려운 표현으로는 금리 1 상황에서 1차 미분값, 즉 가격곡선과 접하는 직선의 기울기를 의미.\n따라서 금리가 조금 변한다면 거의 정확하게 금리 변동에 대한 가격변동을 사전에 측정 가능 주의할 점이 있음. 금리가 크게 변하면 가격 변동분을 듀레이션으로 다 게산할 수 없음.\n기관투자자들은 이러한 오차를 2차 미분값으로 또 게산. 하지만, 개인 입장에서는 듀레이션 만으로도\n어떤 채권이 어느 정도 금리 민감도를 갖는지 알 수 있다는 점에서 매우 유용한 지표 신용 위험이란 무엇일까? # 이제 나타나는 경우는 많이 없으나, 한번 발생하면 투자자에게 큰 타격을 줄 수 있는 신용위험에 대해 살펴보겠음.\n신용위험은 민간 발행자가 발행하는 채권이 갖는 고유의 위험으로 다른 말로 표현하면\n상환 가능성의 변화에 따른 채권 가격 변화 가능성을 의미함 우리는 주변에서 돈을 꼭 갚을 것으로 믿어지는 사람에게 돈을 빌려 줄 때는\n심지어 원금만 돌려 받기로 하고 빌려줄 때가 있음. 반면 갚을 가능성이 작은 사람한테는 돈을 숫제 안 빌리 주거나,\n빌려준다고 해도 높은 금리를 요구함. 이러한 금리 차이는 결국 신용위험이 금리로 표현된다는 점을 의미.\n채권시장에서는 이러한 보상을 신용 프리미엄이라고 부르는데, 발행자의 위험이 클수록 이 프리미엄은 커짐 이러한 프리미엄은 곧 이자수익이기도 함. 따라서 적절한 채권을 선택한다면\n무조건 안전한 채권을 선택할 경우보다 높은 수익률 달성.\n다만, 너무 위험한 채권을 선택한다면 이자를 받기는커녕 원금도 손실을 볼 수 있음 그런데 이러한 상황으로부터 투자자를 보호하기 위해 채권 발행자는 발행시에\n해당 채권의 신용도를 신용평가회사에 의해 부여 받아야 함. 신용평가 회사는 특정 채권의 채무 변제능력,\n즉 상환 가능성을 상대적, 절대적 기준으로 등급을 매기는 회사인데, 처음 발행될 때와 1년에 한번씩,\n그리고 어떤 특정한 이슈가 발생할 때 신용등급을 부여하거나 변화시킴 채권 투자자라면 꼭 알아야 할 신용등급 # 따라서 국채나 아닌 공공 부문과 민간 부문이 발행한 채권에 투자하는 투자자라면 신용등급을 꼭 알아야 함 신용등급은 기본적으로 상환 능력에 초점을 맞추기 때문에,\n얼마나 돈을 많이 버는가와 함께 지금 얼마나 많은 현금성 자산을 가지고 있는지,\n갚아야 할 부채는 얼마나 많은지 등을 토대로 부여됨.\n미래의 매출과 이익 성장성에 초점을 맞추는 주식과는 다른 접근법이 필요함 신용평가 회사의 신용등급 표현 # AAA등급부터 D등급 까지 있으며,\nAAA와 D 등급을 제외한 나머지 등급은 +, 0, -를 붙여 3개의 하위등급으로 나뉨 개인투자자들은 이 중 BBB등급 이상 채권만을 접근하길 권함.\n일반적으로 BBB등급 이상을 투자등급, BB등급 이하부터는 투기등급으로 분류 여기에서 중요한 것은 BBB등급. 일반적으로 A등급 이상이 부여된 채권은 부도 확률이 매우 작음.\n그런데 BBB 등급을 보면 환경 변화가 지급 확실성을 저하한다고 표현되어 있음.\n이에 따라 부도율 역시 A등급 이상보다는 의미 있게 높아지기 시작함.\n이를 반영해 금리가 높아 위험 선호도에 따라 투자 여부가 갈리는 등급이라고 할 수 있음 신용등급에만 의존하지 말자 # 그런데 신용평가 회사의 신용등급 조정은 많은 경우 시기적으로 늦음. 재무제표를 확인해야 하고,\n등급 변경은 기업에게 매우 중요한 사건이기 때문에 신중을 기하는 것이 일반적 따라서 투자자들은 보유하고 있는 채권의 신용등급 변화 가능성을 스스로 체크할 능력을 가지는 것이 좋고,\n이는 중간 중간 발표되는 신용평가 회사의 신용분석 리포트와\n증권회사에서 제공하는 신용분석 리포트를 통해서 얻을 수 있음 해당 리포트들은 신용등급이 부여된 채권들의 거래를 분석하고, 여러 환경의 변화, 개별 기업의 문제,\n특정 채권이나 예를 들어 A등급 회사채 처럼 특정 범주의 채권들에서 가격이 어떻게 변화하는가 등을 다뤄서\n채권투자자들에게 도움을 줌 그런데 이러한 신용분석을 잘 이해하기 위해서는 분석 대상을 알아야 함.\n일반적으로 외부환경의 변화와 상환의지, 지급 능력, 재무 상태, 담보 등이 분석 대상인데,\n투자자들은 리포트를 읽을 때 이러한 부분들이 꼼꼼하게 분석되어 있는지를 살펴볼 필요가 있음 한편, 신용평가회사의 등급 부여 방법론도 참고할 필요가 있는데,\n도표를 보면 이들은 글로벌 트렌드와 더불어 규제, 경영진의 질적인 수준으로부터,\n발행 규모까지 매우 다양한 정보를 활용함을 알 수 있음.\n따라서 신용위험이 큰 채권에 투자하려는 투자자라면 발행 시의 신용등급 리포트를 꼼꼼하게 살펴볼 것을 권함 6강 - 채권 어떻게 투자해야 내돈 벌고 지킬까? 채권 투자 전략과 방법 # 어떤 투자에서도 중요한 시점 포착 # 투자에 있어서 가장 중요한 것은 무엇일까? 너무 많은 요인들이 있지만,\n투자자 입장에서 보면 역시 사고 파는 시점을 잘 아는 것이 제일 중요함.\n낮은 가격에 사고 비싼 가격에 파는 것은 모든 투자자들이 원하는 것 채권의 경우 기본적으로 만기까지 보유하는 분이 많지만,\n이 경우에도 가격이 싼 시점에 사는 것과 비싼 시점에 사는 것은 매우 큰 차이가 있으며,\n시점을 잘 포착해 낼 수만 있다면 만기 보유를 선택할 수도, 팔아서 자본이득을 내고,\n또 다시 투자에 나설 수 있다는 옵션이 생김 그렇기 때문에 지난 4일차에서 배운 채권 가격, 즉 시장금리 전망의 기초를 잘 활용해\n투자 시점을 선택하는 것 중요. 특히 이러한 기본을 알고 있다면,\n시장에서 흔히 나타나는 오버슈팅과 언더슈팅을 수익률을 높이는 기회로 삼을 수 있음 그런데 투자에 있어서는 보유 기간도 매우 중요함. 자금의 성격도 투자자들의 성격도 다 다르기 때문.\n어떤 사람은 채권을 사서 만기까지 보유하고,\n어떤 사람은 계속 채권을 거래해 자본 이득을 극대화하려 할 수 있음.\n따라서 투자 기간과 전망의 기간을 잘 조화할 필요가 있음 만약 만기 보유를 비롯한 3년 이상의 기간을 고려하는 장기 투자자라면 단기적인 가격 흐름보다는\n국내외 경제의 구조적인 문제를 이해할 필요가 있음. 예를 들면 최근 나타나고 있는 달러화 강세와\n그보다 더 긴 인구 고령화 문제, 미중 패권 전쟁과 같은 구조적 문제를 이해할 필요가 있는데,\n이러한 문제들은 모두 채권 가격, 시장금리의 장기적인 추세를 결정함.\n따라서 내가 채권을 보유하는 기간 동안에 가격에 계속해서 영향을 미침.\n만약 고령화가 성장률을 떨어뜨려 금리를 내리는 힘으로 작용한다고 믿어지면,\n채권을 사서 장기 보유하면 이익을 얻을 수 있음 이와 마찬가지로 중기와 단기에도 각각의 기간에 더 크게 영향을 미치는 요인들이 있음.\n다만, 개인투자자라면 유동성 문제 등을 감안해 단기 투자보다는\n경기 순환 사이클 전망을 바탕으로 한 중기 투자를 권함. 경기 전망도 어려운 것은 마찬가지지만\n채권가격에 가장 확실한 영향을 미치기 때문.\n또한 경기 순환 사이클을 알면 통화정책 전망 등을 채권투자에 활용할 수도 있음.\n예를 들어 현재의 금리 인상 사이클이 마무리될 무렵에 중기적 관점에서\n채권을 매수하고 금리가 떨어질 때까지 기다리는 것은 가장 일반적인 투자 방법임 만기보유와 중도 매도는 어떻게 판단할까? # 한편, 채권을 사는 개인들은 때때로 만기 보유와 중도 매도의 판단 기로에 놓일 수 있음.\n금리가 올랐다고 판단해서 채권을 매수한 경우 이후에 금리가 더 올라 손해를 볼 수도,\n내려서 이득을 볼 수도 있는데, 이 경우 원래 산 채권을 만기까지 그냥 보유할 것인지,\n아니면 중도에 팔고 다시 매수에 나설 것인지 등을 고민하게 됨 이와 관련해서 중요한 개념은 내가 의도한 투자 기간 중 발생하는 총수익률임.\n채권투자에 있어서 총수익률은 기간 중 채권 매매를 통해 얻게 되는 이자소득과 자본이득을 더한 값인데,\n한번 투자하고 만기까지 보유한다면 처음 결정된 이자소득이 총수익률이 되고,\n중간에 매매를 한다면 이자소득과 자본이득들의 합이 총수익률이 됨 예를 들어 채권을 사고 금리가 더 올라 손해인 상태를 가정해 보면,\n투자자는 만기까지 보유해 확정된 이자소득만 노릴 수도 있지만, 중간에 손해를 보고서라도 팔고,\n더 높은 금리에 채권을 매수해 전체 기간의 이자소득을 더 높일 수 있음.\n단, 이러한 전략은 이렇게 해서 얻게 되는 추가적인 이자소득과 첫번째 매도에서 발생한\n자본손실을 비교해 이자소득이 큰 경우에만 의미가 있음 한편, 만기 보유와 중도 매도 의사 결정을 할 때는 둘재 시간에 채권 용어에서 설명한 경과 기간이 중요.\n채권을 사고 경과 기간이 많이 지난 경우 채권의 듀레이션은 짧아지게 되는데,\n이렇게 되면 금리가 올라서 나타나는 자본 손실이 작아짐.\n그 상태에서 나머지 기간에 높은 이지소득을 얻을 수 있다면 이익이 될 수 있음 장기채권, 단기채권 어떤 채권에 투자할까? # 한편, 앞에서 배운 수익률곡선을 채권 투자에 활용할 수 있음.\n수익률곡선은 앞선 강의에서 시장금리와 만기 간의 관계를 의미한다고 설명했는데,\n이러한 수익률곡선의 현재 모양과 앞으로 수익률곡선이 어떻게 바뀔 것인지에 대해\n적절히 예상하면 채권 투자를 통해 돈을 벌고 지키는 데 도움이 됨 일반적으로 수익률곡선은 우상향의 모양을 갖지만, 때때로 평평하거나 역전되는 경우가 있음.\n만약 수익률곡선이 평평하다면, 단기 채권에 투자하고, 보유하고 있는 장기 채권을 파는 것이 이득임.\n장기채권에 투자할 경우 만기가 길어서 발생하는 자연스러운 프리미엄을 받을 수 없기 때문 물론 자본이득이 목적인 투자자는 금리 전망에 근거해 장기채권을 살 수 있음.\n그러나, 수익률곡선은 일반적으로 우상향의 경향성을 갖기 때문에 정상화되기 시작하면\n장기금리 상승 폭이 빠를 수 있다는 점을 고려해야 함. 게다가 장기채권은 듀레이션이 크기 때문에\n장기금리 상승 폭이 빠르면 가격 하락 폭도 훨씬 크게 나타날 수 있음 수익률곡선이 역전되어 있을 때와 수익률곡선의 기울기가 일반적인 경우보다 가파를 때\n각각 어떻게 의사 결정을 내리는 것이 바람직할지를 설명했음.\n일단 역전되어 있을 때는 정책금리 인상이 빠르게 나타날 때임을 감안할 필요가 있음.\n따라서 단기 투자로는 이자소득이 많고 금리가 올라도 타격이 작은 단기 채권에 집중해야 함.\n다만, 이러한 상황은 금리 인상 사이클의 말미에 경기 침체가 나타나면서 전반적으로\n금리가 크게 떨어질 수 있음을 의미함. 따라서 경기 침체로 금리 인상이 중단될 것으로\n예상되는 시점부터는 장기채권을 매수해 자본이득을 극대화할 필요가 있음 수익률곡선이 일반적인 경우보다 가파른 경우는 주로 정책금리를 큰 폭으로 내릴 경우임.\n이 경우는 이자소득 측면에서 장기채권이 상대적으로 유리함.\n다만, 인하 사이클의 마지막이 되면 전체적인 금리는 오르게 되고,\n특히 인하가 마무리되는 주된 이유는 앞으로의 물가 상승이 예상되는 경우이기 때문에\n상당 기간 장기채권 투자를 중단해야 함 롤링 효과 # 한편, 수익률곡선을 이용하는 방법 중에는 수익률곡선이 일반적인 기울기를 가질 때\n\u0026lsquo;롤링 효과\u0026rsquo;를 이용한 방법이 있음 조금 어려운 개념이긴 하지만, 롤링 효과는 수익률곡선이 초단기부터 어느 정도 만기까지는 가파른 곡선이다가,\n해당 만기를 지나고 나면 평평해지는 일반적인 형태를 갖는 효과를 의미함.\n이는 연금이나 보험 등 장기채에 대한 특수한 수요가 존재하고, 장기채일수록 가격 변동성이 커서\n공격적인 투자자들이 참여하기 때문으로 알려졌는데, 개인투자자들은 이를 이용할 수 있음 특히 여러 채권을 놓고 만기와 금리간 관계를 점으로 찍어 보면 이러한 가운데에서도\n상대적으로 저평가된 종목을 찾을 수 있음. 이러한 저평가는 다양한 이유에 의한 것인데,\n신용 위험도 있을 수 있지만, 일시적인 수급 불일치와 단기적인 유동성 부족 등이\n이유가 될 수도 있어 투자자에게는 기회임 수익률 곡선의 모양을 이용한 채권투자 # 가운데 곡선은 일반적인 수익률곡선이고, 점들은 각각 채권의 만기, 금리를 나타낸 것임.\n그런데 중간 위쪽에 보면 under valuation이라는 점이 있음\n그 점은 어떤 채권이 일반적인 수익률곡선보다 높은 금리로 거래되고 있는 경우임.\n그리고 만기가 짧아질수록 만기 변화보다 금리 변화가 크게 나타남.\n따라서 under valuation에 포함된 채권을 사고,\n롤링 효과라고 표현된 기간만큼 보유한 후 매도하면 수익률을 극대화할 수 있음 물론 전체적으로 수익률곡선이 상향하는 경우에는 이러한 전략으로도 손실이 발생할 수 있음.\n하지만, 이 경우에도 손실 폭은 다른 투자 대안보다 나은 성과로 이어질 가능성이 높음 개인 투자자, 너무 큰 신용 위험을 지지 말자 # 신용 위험과 관련해서 개인투자자들에 대한 권고는 가급적 안전한 채권\n특히 A 등급 이상 채권에 투자하자는 것임. 발생했을 때의 타격이 너무 크기 때문 물론 과거에는 BBB, BB 등 위험채권 투자도 많았음.\n부도가 나도 채권단들이 개인들에게 대체로 원리금을 지급해 줬기 때문. 하지만, 이제는 그렇지 않음 특히 개인은 기관투자자와 달리 포트폴리오를 구성하기 어려움.\n100개의 위험한 채권을 사면 1~2개만 부도가 날 때 전체 손실이 크지 않지만,\n개인이 한 두개의 채권을 사고 하나가 부도가 나면 손실이 매우 큼.\n이러한 점을 고려해 A등급 이상 채권에만 투자할 것으로 권함 개인 채권투자, 어떤 방법을 사용할까? # 마지막으로 개인의 채권 투자 방법을 간단하게 소개하겠음.\n개인이 채권에 투자하는 방법은 크게 직접 투자와 간접 투자로 나뉨 이 중 직접 투자는 증권사와 은행을 이용할 수 있음. 특히 증권사 이용을 권하는데,\n증권사에 따라서 HTS나 MTS를 이용해 거래소에서 거래되는 채권에 투자할 수 있도록 연결해 주기 때문.\n이 외에도 증권사는 본인들이 인수하거나 사 놓은 채권을 개인들에게 팔기도 함.\n이러한 채권은 증권사 HTS/MTS에도 소개가 되어 있기 때문에 골라서 매수할 수 있음.\n예를 들어 최근 증권사들은 2년이 남지 않은 카드사나 캐피탈사 채권을 5%가 넘는 금리에 팔고 있음 은행에서도 특정금전신탁 계약을 통해 채권에 직접 투자할 수 있음.\n특정금전신탁은 돈을 맡기고 특정한 자산을 사 달라고 요청할 수 있는 신탁임.\n하지만, 은행 창구를 방문해 계약을 맺고 운용지시를 내려야 하는 복잡한 과정을 거치게 됨.\n큰 차이는 아닐 수도 있지만, 같은 채권도 은행에서는 조금 더 비싸게 매수하는 경향이 있음 간접적인 투자 방법임. 이는 자산운용사의 펀드나 채권ETF를 이용한 것인데,\n특히 채권ETF를 이용하면 수시로 큰 비용없이 거래가 가능하다는 장점이 있음.\n다만, ETF는 특정 채권을 매수한 경우와 달리 계속해서 듀레이션이 유지됨.\n예를 들어 3년만기 채권 ETF는 기초자산을 바꿔 계속 3년 만기를 유지함.\n따라서 시간에 걸쳐 가격 변동성이 작아지는 직접 채권과 달리 가격 변동성이 계속 높게 유지된다는 점을 감안해야 함 "},{"id":7,"href":"/blog/2023-02-05/","title":"2023-02-05 Log","section":"Posts","content":"인스타 마케팅 # 웃긴 글귀 등으로 모은 팔로워는 스토어 홍보를 위해 유지할 수 없음 홍보용 사진을 먼저 올려놓고 소비자를 대상으로 팔로우와 좋아요를 눌러서 끌어들이기 오프라인 매장의 경우 태그를 통해 특정 지역의 사람을 검색 인스타그램 봇에 어뷰징에 걸리더라도 타게팅용 전단지 계정이기 때문에 상관없음 어떻게 사진 찍을지 모를 경우 해시태그로 다른 스토어를 참고해서 벤치마킹 유튜브 노출 # 상품 카테고리와 연관이 있는 유튜버 찾기 (업로드 날짜 최신순) (중위 노출) 유튜버 이메일 주소를 통해 체험단 제안서 전달 또는 다른 유튜브 계정으로 사입 상품 영상을 올리기 관련 동영상 노출 조건 - 제목, 설명, 언어 자막 음성 인식 텍스트, 태그, 시청 기록 카페/블로그 마케팅 # 대가성이 높은 블로그보다는 카페가 유용 카페에서는 일상적 포스트도 같이 올려 사이사이에 홍보 (다른 아이디로 댓글 달기) 카페 노출 시 카테고리별 최적화 고려 (게시글 내용에 키워드 작업) 가입 즉시 회원의 경우 네이버 알고리즘에 의해 노출이 어려움 쿠키 삭제, VPN으로 어뷰징 회피 최적화 블로그를 만드는 것보다는 컨택하는 것이 나음 네이버 폼을 통해 블로그 체험단 제안 사진 최소 5장 이상, 글 15003000자 이상, 노출 키워드 35회 이상 언급, 영상 최소 1개 박리다매 # 후발주자가 진입하기 어려워 수익성뿐 아니라 안정성도 보장 키워드 # 이미 잘나가는 상품이 있다면 특정 키워드 내에서 상품 지수가 높음 저효율 상품명을 지우기 (소형 키워드를 쳐내 기존 키워드의 비중을 늘리기) 새로운 중소 키워드와 대형 키워드를 추가하기 단어 배열도 중요 (중요한 메인 키워드는 거리를 가깝게) 키워드는 있는데 노출이 되지 않는 경우 카테고리를 확인 (또는 키워드의 카테고리 연관도, 카테고리를 명확하게) 쿠팡은 키워드 선호도의 비중이 높음 (검색어별로 상위 노출 지수가 다름, 주기적으로 확인) 쿠팡은 메인 키워드가 영어일 수 있음 (외국인 유입량 늘리기) 대량 등록 vs 개별 등록 # 대량 등록 = 장사, 개별 등록 = 사업 기본기부터 갖추기 (개별 등록) 대량 등록의 재고 관리, CS 처리, 검색 최적화 등 어려움 대량 등록의 잘나가는 상품만 찾아서 다른 사람이 등록시킬 수 있음 마케팅 회사 # CPC 단가 기준으로 블로그 노출은 남는 장사 구매 작업, 리뷰 작업 어뷰징 (리뷰 알바) 키워드 검색 후 상품을 구매해서 검색 최적화 상품 찜하기 및 체류 시간 증가(3분)로 어뷰징 회피 동일한 구매자로 잡히기 않게 처리 가구매는 신고가 들어올 시 판매 정지 등 위험이 있음 레드오션 시장 # 새로운 키워드를 주기적으로 분석 기존 모델 벤치마킹, 평점 낮은 순을 기준으로 소비자가 추구하는 가치 파악 (배송상태, 배송속도, 품질, 소음, 냄새 등) 단점 대응과 주문 및 제작 판매 채널 리스팅 판매 채널에 대응하는 키워드 리스팅 (네이버, 쿠팡 등 별로 연관 검색어) 최적화 블로그와 카페 리스팅 업로드 표준 규격 설정 매출 늘리기 # 단순 노동 시간을 가공하기 (한건 처리할 시간으로 한번에 처리, 쇼핑몰 관리 솔루션, 직원) 상품 등록 시 대량 등록 프로그램 도매 납품처를 찾아서 안정성을 높이기 (세금계산서, 견적서 양식) 직원 채용 # 우선 프리랜서 후 정규 직원 채용 계속해서 일을 잘하는지 케어하고 관리 정으로 운영하지 않기 직원 채용 시 기계보다는 부품을 사고 정보다는 이성을 활용 3PL # 포장 인력, 사무실이 없다면 3PL 활용 경기도권, 인천, 부산 등 배대지에 따라 결정 화물량에 따라 다른 경우 등 애매한 경우는 믿지 않기 시간대별 주문 마감 확인 여러 쇼핑몰 발주를 한번에 관리 (통합 관리 솔루션이 있는 업체) 조건 제시할 때 녹음 필수, 계약서 견적서 받기, 물류가 줄어들면 가격 인상 여부 확인 "},{"id":8,"href":"/blog/2023-02-04/","title":"2023-02-04 Log","section":"Posts","content":"상품 페이지 # 브랜드 가치 설명 다른 상품과 다른 이유 설명 (전문적인 용어 사용, 유사품 주의 안내) 제품의 가치를 증명 (추상적이지 않은 재질, 효과, 인과관계, 픽토그램, 타겟팅 등) 배송 정책 및 클레임 대비 (해외 직배송 기간, 초기불량, A/S 관련 문의 등) 리뷰 관리 # 상품 검수 이미지 전달, 통관 완료 안내, 포토리뷰 안내, 복붙 금지 아동 타겟 제품의 상품 페이지에 KC 인증 문제가 있을 경우 리뷰를 상세 페이지 대신 활용 고객의 잘못된 클레임은 이성적으로 설명, 후단 조치는 고객의 만족을 위해 왕복 배송비 손해를 감수하고 반품 욕설 리뷰는 신고 처리 마음에 들지 않을 시 7일 이내 무료 반품 택배 # GS Postbox 사업자 등록 (2kg 이하 2600원) 하루 발주량이 많아지면 택배 계약 (하루 50-100건 판매 중이라고 전달, 미래 성장 제시) 제품의 사이즈에 맞는 박스를 중국 판매자에게 미리 요구 파손 위험이 있는 경우 에어캡 또는 에어캡 봉투 갈색 박스는 투명 테이프보다 유색 테이프 사용 (취급주의 등 프린팅된 테이프, 내부재는 투명 테이프) 진상 처리 # 자신의 직책을 낮추기, 직원이라서 자신의 한계를 전달 상대가 반품 제품을 보내지 않으면 내용 증명을 보내기 좋은 사람에게는 사장으로 할인 혜택을 주고 진상한테는 대리로 보상할 방안이 없다고 전달 진상한테 잘 대해줄 필요가 없음, 억지로 서비스의 강도를 높이지 않기 CS가 늦어지는 이유를 설명할 수 있어야 함 (순차적으로 처리 안내, 톡톡 상담 자동 응답 메시지) 전문성과 친숙함을 동시에 갖춰야 함 전화 상담 # 전화 상담 첫마디는 \u0026lsquo;안녕하세요 XXX 스토어 입니다\u0026rsquo; \u0026lsquo;판매처가 여러 곳 있는데 어디셔 연락주셨어요?\u0026rsquo;, \u0026lsquo;상품명 말씀해주세요\u0026rsquo; 많은 상품을 취급하는 것을 알려서 제품 정보를 찾는데 걸리는 시간을 늦추기 \u0026lsquo;기다려주셔서 감사합니다\u0026rsquo; 마무리 멘트 \u0026lsquo;혹시라도 또다른 문의가 있으시다면 톡톡 상담을 이용햊 주십시오\u0026rsquo; \u0026lsquo;늦은 시간에 연락드려 죄송합니다. XXX 스토어 입니다\u0026rsquo; 온라인 상담 # 안녕하세요 XXX 스토어 입니다 \u0026hellip; 오래 기다려 주셔서 감사합니다 \u0026hellip; 항상 좋은 하루 되십시오 시작과 끝은 확실하게, 시장판처럼 보이게 하지 않기 답장 주기가 늦어지는 경우 전화 상담으로 처리 [네이버 스마트플레이스를 활용한 가상번호 만들기]\nCopy text 휴대폰 번호를 스토어 번호에 입력해서, 개인정보 유출이 우려되시는 분들이 많으실 텐데요. https://smartplace.naver.com/에서 업체 등록을 하면, \u0026#39;스마트콜\u0026#39; 서비스를 통해 0507으로 시작하는 가상 번호를 만들어서 휴대폰과 연동시킬 수 있습니다. 이편에서 오는 전화는 \u0026#39;네이버 스마트플레이스에서 연결됩니다\u0026#39;라고 처음에 인트로 음성이 나와서 \u0026#39;스마트스토어 보고 연락 왔구나\u0026#39; 확인하실 수 있어요~ 이 인트로 음악이 안 나오면 주변 지인이라고 생각하면 되고, 인트로 음악이 나오면 \u0026#39;안녕하세요 (스토어명)입니다\u0026#39; 라고 하실 수 있습니다. 준비된 자세의 상담원과 \u0026#39;누구세요?\u0026#39;라고 묻는 상담원은 첫인상부터가 다르니까요. 광고 # CPM (노출당), CPA,CPS (전환당), CPC (클릭당) 일정 광고 금액 이상 시 전환율은 그대론데 광고비가 많아 오히려 마진 대비 광고 효율 감소\n(효율적인 골든 스팟 찾기) 판매당 순이익, 전환율, 입찰가를 통해 광고 마진을 계산 광고가 필요한 유형 = MOQ 증가 (큰 마진을 낼 필요가 없음) 광고비는 PC 최소 노출 입찰가, 모바일 1위,2위,3위 평균 입찰가 등 설정 (자동 규칙 만들기) 모의 시뮬레이션 # 물류비 절감 (중국 내륙 운송비, 한국 운송비, 한국 내륙 운송비) 사업자 통관 \u0026gt; 세관 인보이스, 화뮬 정보 엑셀, 원산지 정보, KC 인증\n(운송 중 화물 분실 위험) 관세, 부가세, 해외송금 수수료 아이템 위너 방어 위해 브랜딩 필수 유사 브랜딩 # 1688 썸네일 대신 직접 찍은 사진, 또는 사은품을 넣은 사진 (세트 상품) 모델에 상표를 각인하기, 대량 주문 시 공장에 상표 각인 (추가금 말하지 않기, 단조롭게 제안, 첫주문 허들 낮추기) 포장에 상표를 각인하기 (포장을 바꿔서 모델명을 바꾸기) 국내 인증으로 차별화를 두기 (KC 인증) 주문 제작 시 우선 샘플을 받아보고 결정 사입 과정 # 박스에 Made in China 표시 필수 (관세법령정보포털에서 원산지표시대상 Y 찾기) 지적재산권 침해 상품 수입 금지 (브랜드 상품 등) 물류비 책정 (부피 기준으로 가격 책정하는 상품 등), 수입(소형 화물),\nCBM(가로세로높이 단위), LCL(다른 물품과 같이 운송), FCL(본인 상품만 운송) 파손률을 줄이기 위해 30% 선금, 샘플 확인 후 70% 후불, 파손 상품은 다시 받는 조건으로 계약 국내 인증 # 1381 인증 표준 정보 센터에 전화해 수입 물품에 필요한 인증 및 시험소 위치 확인 내부 서류 등이 필요한 경우 제조사에 요청 인증을 통과하지 못할 시 제조사에 부적합 요청서를 전달해서 제품을 다시 받기 대행사를 통해 인증 받기 (KC 인증 대행, 다른 업체와 비교) 가져오기 힘든 상품은 선발주자가 되기 상품 노출 # 외부몰을 이용하면 네이버 검색엔진에서도 표시 가능 (가격비교 매칭 요청) 최저가 경쟁 # 100원, 1000원 차이는 의미가 없기 떄문에 가격을 신경쓰지 않게 하는 기대요소를 높이기 다른 상품과의 비교 사진, 가격 대비 차별화된 이유 알리기, 정직함, 신뢰도 당일 배송, 내일 도착 보장, \u0026lsquo;지역에 따라 배송일자에 차이가 있을 수 있습니다\u0026rsquo; 낮은 가격의 미끼 상품을 추가상품으로 첨부 (기존 상품과 배송비를 포함시키기) 어느정도의 가격이 넘어가면 가격보다는 만족감이 중요 브랜딩이 없을 겨웅 썸네일을 바꾸고 최대한 다른 상품임을 인지시키기 무조건 선발주자를 이길 필요가 없고 비교도 괜찮음 상품페이지를 배낀 경우 네이버 저작권 신고 좋은 상품이 있다면 여러 채널에 놓고 팔기 (옥션, 11번가, 티몬, 위메프 등) 반품이 들어온 상품은 당근마켓, 번개장터 등에서 처리 제조사와 우호적인 관계를 유지 (알리왕왕보다는 위챗으로 소통, 신제품 출시에 대한 티저 받기) "},{"id":9,"href":"/blog/2023-02-02/","title":"2023-02-02 Log","section":"Posts","content":"스마트스토어 시뮬레이션 # 주문이 들어온 경우 취소되기 전에 발주 확인을 눌러주기 \u0026gt; 고객에게 반송배송비 청구\n단, 이미 발송이 간 경우 취소 거부 재고가 없어 발송이 늦을 경우 발송지연 누르기 상담은 1차로 톡톡 상담 (주말에 부재중 설정, 문제가 있으면 전화로 처리) 쿠팡 시뮬레이션 # 반품 배송비를 높여서 반품률 낮추기 발주서 엑셀에 배대지 주소 입력하고 발주서 업로드 유자본 창업 시 일반배송 변경 후 배송비 변경 배송 지연이 없기 때문에 품절을 걸지 말고 출고소요기간을 늘리면서 계속 주문 받기 상품 문의에서 문의 받기 (전화 통해 고객 만족도 올리기) 위탁판매 # 제조업체부터 도매업체 단계를 고려 (중국 제조업체 \u0026gt; 중국 도매업체 \u0026gt; 국내 도매업체 \u0026gt; MD 등) 중소기업현황정보시스템에서 제조업체 파악 제조업체와 가까이 있는 것이 중요 1688 사용 시 알리왕왕 통해 대화 (장미 등 이모티콘 통해 우호적 감정 표시) 알리왕왕으로 샘플 주문, 추가 동영상 요구 배송대행지 # 상담 첫 개시/마감 시간 전화를 받는지 확인 (아니면 거름, 보통 2개의 상담 시간 존재) 도착한 당일에 한국으로 출고가 가능한지 여부 중국 센터에 한국인 관리자가 있는지 여부 (지점이 한국에 있는 경우 거름 = 02/032 번호, 내용 전달 과정이 많음) 세관에서 발생할 수 있는 문제 대처하기\nCopy text 세관에서 문제 발생 시, 유니패스에서 운송장 번호 입력 이후 나오는 세관장/포워딩업체에 전화해서 문제를 확인할 수 있습니다. 일반적으로는 먼저 전화가 옵니다. 보통 배대지에 물어보면 관련 정보를 얻을 수 있으나, 직접 문제를 확인하고 싶다면 해당 방법으로 진행하시면 됩니다. 원산지표기, 생산 정보 표기 등 필수로 표기해야 하는 사항을 위반했다면 세관에 제품이 묶이게 됩니다. 이 때 보수 작업 요청을 진행해야 하며, 비용이 소모되며 비용은 작업량마다 다릅니다. 해당 표시 기준을 여러 번 준수하지 않았을 시 벌금이 부과될 수 있습니다. 네이버 상품 등록 # 네이버는 상품명에 중복되는 단어를 최대한 제거 (2번 반복까지 허용) 단어 조합을 강조하고 싶으면 단어 간 거리를 가깝게 지정 할인율은 20-30%로 맞춰서 판매가 조정 부가세는 과세상품 재고수량은 넉넉하게 네이버 알고리즘은 옵션명도 잡기 때문에 추가로 입력 1688에서 썸네일을 바로 가져오기 보다 상세 페이지를 캡쳐해서 가져오기 (중국어가 없는 사진 요청 또는 타오바오) 영상을 넣으면 동영상 타이틀 지정 가능 모바일 화면 기준으로 상세 페이지 작성 상세 페이지 등록 시 이름도 지정 (노출 키워드를 내용에 자연스럽게 담기) 브랜드, 제조사는 다른 업체들이 따라할 수 없는 것으로 (문제될 시 공장 이름) 국내 배송 시 오늘 배송으로 경쟁력 높이기 해외 배송 시 반품배송비 높이기 포인트는 50원이라도 지정해서 리뷰 유도 태그는 키워드 그대로 지정 Page Title은 상품명 그대로 지정 Meta description은 20자에서 50자 내외로 노출 키워드를 활용해서 제품 소개 쿠팡 상품 등록 # 마지막 카테고리까지 설정 브랜드, 제조사는 네이버와 동일 구매대행의 경우 인당최대구매 수량을 0으로 맞춰 무제한 구매 가능하게 쿠팡 옵션도 최대한 정확하게 배송방법에 구매대행 선택 시 인보이스 영수증에 흰배경 사진 올려도 가능 Copy text 상품 업로드 전에 상위 노출된 상품군들 사이에서 평균적으로, 공통적으로 어떤 상품 속성이 설정되어 있는지 유심히 보세요. 내 상품의 품질 지수 높이는데에 도움이 됩니다. 무조건적으로 속성을 많이 넣는게 도움이 되진 않습니다. 그러나 어느 정도의 공통점을 캐치하고, 이걸 바탕으로 속성을 추가하면 노출 지수를 더 높일 수 있습니다. 이 업로드 과정과 키워드 캐치 과정은 분명히 오래 걸리지만, 이 구간에서 계속해서 신경써야 유입이 일어나고, 판매로 전환됩니다. 판매량 = 좋은 상세페이지 X 좋은 키워드가 기반입니다. 여기서 업로드 할 때 신경써야 하는 키워드를 대충 작업하면 절대 판매가 일어날 수가 없습니다. "},{"id":10,"href":"/blog/2023-01-29/","title":"2023-01-29 Log","section":"Posts","content":"스마트스토어 장점/특징 # 네이버 사용자가 잠재고객 판매 수수료가 굉장히 낮으며(약 4%) 정산이 빠름(7일 이내)\n(쿠팡 약 5-10%, 이베이 약 8-12%) 안전거래 보증 각종 마케팅 데이터 제공 (통계 \u0026gt; 마케팅 분석) 판매자 존중 (답변 지연 등 배려) 기획전, 럭키투데이 등으로 노출 증가 톡톡으로 간편하게 CS 처리\n(톡톡 문의는 즉시 답변이 어려울 경우 업무 중 순차적으로 진행된다고 알림) 리뷰에 코멘트를 달아서 감사,반박 등 적합도 중요 (브랜드, 카테고리, 상품명, 이미지 등) 쉽기 때문에 경쟁 강도가 높음 쿠팡 장점/특징 # 트래픽이 많음 (플랫폼 신뢰도로 인해 로켓배송 없이도 구매율 높음) 키워드 셋팅이 자유로움 (쿠팡은 검색어를 직접 지정, 네이버는 상품명과 카테고리 기반) 상품명은 브랜드, 제품명, 제조사, 등록상품명 순으로 적용 (네이버 상품명 = 쿠팡 노출상품명) 쿠팡은 구매자 편애 구조 구매자 취소 시 배송비는 판매자가 부담 주문 즉시 업체직송으로 처리해서 배송 등록 구매자 파손 후 반품해도 손해배상 불가, 직접 부담해야 함\n(상품 페이지에서 반품 시 청구 비용을 고객에게 명시) Copy text 2020.07.07 최근 쿠팡에서 \u0026#39;업체 직송\u0026#39;으로 인한 강제 배송 처리를 악용하는 판매자가 많아져, 잦은 사용 시 어뷰징으로 분류해 판매 지수를 떨어뜨리는 것으로 확인되었습니다. 네이버에서 \u0026#39;직접 전달\u0026#39; 처리하는 건 문제가 없으나, 쿠팡의 경우 \u0026#39;업체 직송\u0026#39;을 이용한 주문 취소 방어가 불가능하므로 참고해 주세요. 그러나 앞으로 주문 취소에 대한 응대가 불가능한 건 아닙니다. 방법을 알려 드리겠습니다. (1) 아무렇게나 송장 번호를 뽑고 나서, 택배는 보내지 마세요. (2) 그러면 그 송장 번호는 존재하지만 배송이 되지 않는, \u0026#39;가송장번호\u0026#39;가 됩니다. 이 번호를 다른 주문 건에 송장번호로 등록시키면 \u0026#39;배송 지시\u0026#39;편에 머물러 있고 그 때 고객은 임의로 주문 취소를 할 수 없게 됩니다. (3) 이후에 실제 그 고객에 대한 송장 번호를 발급받게 되면 \u0026#39;배송 지시\u0026#39;편에 들어가 다시 송장번호를 등록시켜 주시면 됩니다. (4) 그러면 품질 저하/어뷰징 없이 정상적으로 송장번호 등록/집하 및 배송 처리가 가능합니다. 정산 주기가 느림 (30일-45일, 선정산 서비스 등 금융 상품 있음) 경쟁을 부추기는 플랫폼 (아이템 위너 시스템: 똑같은 제품에 가격이 저렴하면 1위를 뺏김) 트래픽 외에는 모든 것이 단점 상세페이지 수정 # 핵심요약 # 포토샵, remove.bg: 누끼따기 라이트룸: 사진 보정 망고보드: 상세페이지 마무리 작업 구글 크롬: 이미지와 동영상 다운로드 알툴바: 이미지 대량 다운로드 fanyi.baidu.com: 중국어 이미지 번역 누끼따기 관련 강좌 # 🔗 [초급] 포토샵으로 누끼따기! - 프디랩Freelancer Designer LAB\nhttps://www.youtube.com/watch?v=TnLe2TEgGqw 🔗 [★10분팁★]포토샵 이미지따기 100%완벽하기 따기 강좌[MaDia] - Madia Designer\nhttps://www.youtube.com/watch?v=j-nL0fPAaME\u0026t=4s 🔗 너무 쉬-움, 포토샵에서 \u0026lsquo;나무 누끼따기\u0026rsquo; - 컨트롤에쓰\nhttps://www.youtube.com/watch?v=2L6568VzzR4 🔗 포토샵 강좌 #46 - 펜툴로 사진 누끼 쉽게 따기 - 롤스토리디자인연구소\nhttps://www.youtube.com/watch?v=cpGMMXPiH6M\u0026t=56s 상품 소싱 # 선발 주자: 국내에 없는 상품을 미리 올리고 대기, 또는 블로거 유튜버를 통해 상품을 알림 후발 주자: 선발 주자를 따라 상품을 올리기 때문에 안정감 있지만, 먹을 수 있는 파이가 적음 팔리는 상품을 찾기 위해서는 이해하고 공감할 수 있는 상품을 선정 (ex. 코로나 웹캠 등) 불편함을 해결할 수 있는 상품을 키워드화해서 상품을 소싱 (이슈를 찾을 수 있는 키워드) 해외직구 후기, 알리익스프레스 후기, 알리 내돈내산, 해외직구 내돈내산, 해외직구 직접 등 위탁 판매 (국내) # 다음, 밴드에 도매업자 찾기 (위탁 유통) 새우젓 예시 \u0026gt; 개인용 용량 대신 대용량 도매용, 업소용 제품 검색 시 국내산이 없는 경우 이미 온라인 커머스에 진출한 대상의 경우 운영이 잘 안되는 업체를 찾아서 다른 채널을 개통\n방법론을 물어볼 경우 정보를 감춰서 지속적으로 협력할 수 있도록 제품 뒷면의 제조사에 직접 연락해 위탁 요청 셀러 오션 플랫폼 위탁 제안서 (경쟁 제한 업체) Copy text 한평생 유튜브와 인터넷에 관심도 없는 사장님이 집 주변에 있다면 먼저 제안해보세요. 갖고 있는 제품의 뒷면 라벨에 제조사나 도매처의 정보가 적혀 있습니다. 전화해서 위탁 제안을 해보세요. 위탁 판매 (해외) # alibaba, taobao, tmall, 1688 구글 크롬, 알리왕왕(톡톡상담), 위챗, 파파고 알리바바: 프리미엄이 붙기 때문에 1688에 이미지 검색 후 가격 확인 타오바오: 낱개 구매 가능, 도매 대비 가격은 높은편 1688: Origin으로 발송지 확인, 최소주문량(MOQ)가 존재 1688은 결제대행 업체 필요 (또는 판매자 통해 마스터 카드 가능한 타오바오 스토어 결제를 요청) 판매자의 신상품을 탐색 "},{"id":11,"href":"/blog/rich-dad/","title":"부자 아빠 가난한 아빠","section":"Posts","content":" 새해의 목표로 독서를 삼으면서 그동안 경제 전문가로부터 많이 들었던 이 책을 읽게 되었습니다.\nStudy Session 1 # 부자들은 돈을 위해 일하지 않는다.\n책의 초입에서는 저자의 어린 시절 두 명의 아버지로부터 보고 배운 것을 전달하면서\n부자인 아버지로부터 돈의 작용 방식을, 가난한 아버지로부터 반면교사로 삼아야할 마음가짐을 보여주었습니다.\n저자와 그의 친구는 부자 아버지로부터 가르침을 받기 위해\n3주 동안 무급으로 일하는 경험을 했는데 이를 통해 임금 제안의 욕심에 흔들리지 않고\n자신만의 사업을 열어 적지 않은 수입을 만들어낼 수 있었습니다.\n저도 사회초년생으로서 다소 적은 금액의 연봉으로 업무에 임하면서 연봉 인상을 꾀하고 있었지만,\n저자의 사례를 보면서 사소한 임금의 증가를 목적으로 남을 위해 더욱 열심히 일하기 보다는\n자신이 향후 사업으로 활용할 수 있는 지식을 업무로부터 배우는 것이 효과적임이라 생각했습니다.\n\u0026ldquo;돈이 자신을 위해 일하게 하라\u0026rdquo;\n미디어의 부자들로부터 언제나 들어왔던 말이었지만,\n이를 직접 실천한 저자를 보면서 주변에 대한 관심을 가질 필요성에 대해 느꼈습니다.\nStudy Session 2 # 왜 금융 지식을 배워야 하는가\n책에서 제시된 자산의 현금흐름 패턴에 대한 그림은 단순명료하면서도\n자산과 부채에 대한 이해를 확실하게 와닿게 했습니다.\n특히나 인상깊었던 것은 자산은 부를 만들어내고, 부채는 돈을 빼가는 것이라는 정의입니다.\n그 중에서도 주택에 대한 저자의 시선은 다소 신선했습니다.\n지금까지 주택 그 자체는 물가 상승에 따라 증가하는 자산이라고 생각했지만,\n세금과 대출 이자로부터 발생하는 지출과 목돈을 묶이면서 다양한 기회를 놓치게되는 문제를 지적하면서\n투자의 우선순위를 일깨워주었습니다.\n더욱이 2023년, 집값 하락과 금리 상승으로 고통받는 영끌족들의 사례를 보면서\n전재산과 부채를 동원해 잘못된 타이밍에 투자하는 것의 문제점을 인식했습니다.\n본문에서 엠파이어스테이트 빌딩을 건축할 때 가장 먼저 깊은 구덩이를 파고 튼튼한 토대를 쌓아야 한다고 전했습니다.\n하지만, 많은 사람들은 빨리 부자가 되고 싶은 마음에 얉은 콘크리트 기반 위에 엠파이어스테이트 빌딩을 지으려고 합니다.\n저역시 1년전 아무런 투자 지식도 없이 주식 하락장에 뛰어들어 적지 않은 마이너스 수익률을 기록한 경험이 있습니다. 돈에 대해 제대로 이해하는 것만큼 리스크를 회피할 수 있는 좋은 수단은 없다고 생각합니다.\nStudy Session 3 # 부자들은 자신을 위해 사업을 한다\n맥도날드 창업자 레이 크록은 자신이 햄버거 사업이 아닌 부동산 사업에 종사한다고 말했습니다.\n생각해보면 맥도날드는 전 세계에서 체인점을 운영하면서 좋은 입지의 건물을 차지하고 있었습니다.\n지금까지 사업이란 대단한 것이라고 여겼고, 직장을 다니면서 사업을 하는 것인 말이 안된다고 생각했습니다.\n하지만, 굳이 창업이 아니더라도 자산을 매입하고 관리하는 것 자체가 사업이라는 것을 인식했습니다.\n저자는 자신이 좋아하는 자산을 획득하라고 강조합니다.\n저 역시 과거 큰 손실을 입었지만 여전히 주식이 좋아 수익을 낼 수 있는 방법을 탐색했고,\n장기 투자 대신 단기 매매로 방향을 돌리면서 소소하게 수익을 내고 있습니다.\n업무 상으로도 스마트스토어의 많은 부분을 탐색하면서 익히고 있는데,\n이것이 향후 내가 없어도 되는 사업으로 발전할 수 있을지 기대하고 있습니다.\nStudy Session 4 # 부자들의 가장 큰 비밀, 세금과 기업\n과거 부자들은 욕심이 많고 국가는 이들로부터 세금을 착취해 가난한 이들에게 나눠줘야 한다고 생각했습니다.\n이것은 전통적인 교육과정과 부모님으로부터 들어왔던 것인데\n돈에 대한 공부를 하게 되면서 단지 누군가로부터 세금을 걷어들이길 기다리는 것이 얼마나 어리석은지 깨달았습니다.\n저자는 정부가 세금을 걷어들일 때 실질적으로 징수당하는 대상을 고소득 중산층이고\n합법적인 기업 구조를 이해하는 부자들은 리스크 상쇄의 수단으로 세금 회피를 이용하고 있다고 전했습니다.\n한국의 상황과는 맞지 않을 수 있지만, 미국 세법 1031조항을 통해\n부동산 거래에 대한 세금을 유예할 수 있는 방법은 세금을 알아야할 필요성을 일깨웠습니다.\n돈을 움직이는데 있어서 \u0026ldquo;아는 것이 힘\u0026quot;이라는 사실은 부정할 수 없습니다.\n마치며 # 이후의 내용도 완독하면서 돈을 만드는 자산을 쌓는 것의 중요성에 대해 느꼈습니다.\n비록 미국의 사례이고 개인적으로는 생소한 부동산 투자에 집중하기 때문에\n모든 사례가 공감되지는 않았지만,\n리스크 회피를 위해 너무 많은 바구니에 매물을 담기 보다는 특정 몇개에 집중할 필요가 있다는\n저자의 주장은 그동안 전문가들로부터 들어온 분산투자의 개념과 상반되어 새로웠습니다.\n향후에는 주식 투자를 지속하면서 세법에 대해 공부하고 부동산 투자에도 관심을 가져볼 생각입니다.\n자산과 부채의 차이를 인식하게 된데서 이 책을 읽게된 가치가 있었다고 판단합니다.\n"},{"id":12,"href":"/blog/2023-01-23/","title":"2022년 01월 23일 회고","section":"Posts","content":"지금으로부터 약 한 달 간 자동화 프로그램 안정화 작업으로 다소 바쁜 일정을 보냈지만,\n다행히 해당 작업이 마무리되어 이렇게 글로 정리해볼 여유가 생겼습니다.\n구글 빅쿼리 # 이전 회고에서 사내 빅쿼리 도입을 고려하고 있다고 남긴 바 있었는데,\n이제는 빅쿼리에 익숙해지며 SQL도 어느정도 원하는대로 다룰 수 있는 수준에 이르렀습니다.\n원래는 구글 시트 API를 통한 데이터 적재 자동화에 익숙해진 후 빅쿼리로 차차 넘어갈 예정이었지만,\n생각보다 구글 API의 사용법이 간단하여 얼마안가 빅쿼리 사용을 시도해볼 수 있었습니다.\nMySQL 같이 잘 알려진 DB 조차 직접 다뤄본 경험이 없었기에\n스키마 구성 등의 행위가 낯설었고, 몇 번이나 테이블의 구조를 갈아 엎었습니다.\n빅쿼리는 적은 비용 대비 관리가 편하다는 장점도 있지만,\n초급 개발자 입장에서 무엇보다 좋았던 것은 쿼리 시 예상되는 비용을 산정해주어\n더욱 효율적인 쿼리문을 작성할 수 있게 도와준다는 점입니다.\n이에 대한 것을 몰랐을 도입기 때는 어떠한 쿼리 최적화 기법도 적용하지 않아\n당시 90MB의 테이블의 모든 데이터를 매 시간마다 대시보드에서 호출하게 했습니다.\n해당 테이블은 매 시간마다 데이터가 쌓이는 구조였기 때문에 용량의 증가가 적지 않았는데,\n장기적으로 빅쿼리에서 제공하는 월 1TB의 무료 용량을 초과할 수도 있겠다는 걱정이 있었습니다.\n이러한 문제를 해결하기 위해 DB 최적화에 대해 고려하게 되었고,\n파티셔닝과 정규화 기법을 적용하게 되었습니다.\n파티셔닝 # 빅쿼리는 테이블 생성 시 하나의 수치형 열을 기준으로 파티션을 적용할 수 있습니다.\n저는 보통 데이터 수집일시에 해당하는 날짜 열을 파티션의 기준으로 삼는데,\n파티셔닝 시의 장점은 WHERE 문으로 파티션을 특정할 경우 해당 파티션의 데이터만을 호출하는 것입니다.\n이 덕분에 당일의 데이터만이 필요한 대시보드에서 빅쿼리로 데이터를 요청할 경우\n기존의 90MB에서 1MB 미만으로 쿼리 비용을 감소시킬 수 있었습니다.\n지금은 일자별로 데이터가 누적되는 모든 테이블에 파티셔닝을 적용해\n데이터가 축적되면서 증가할 수 있었던 쿼리 비용에 대한 걱정을 덜게 되었습니다.\nDB 정규화 # 데이터가 누적되면서 발생할 수 있는 비용은 쿼리에서뿐 아니라 데이터 자체의 저장 비용에서도 발생합니다.\n제가 업무에서 활용하는 상품 순위 데이터를 쌓기 시작할 당시에는\n상품코드와 순위 뿐 아니라 해당 상품의 상품명, 판매처명 등의 문자열을 같이 기록했습니다.\n상품명의 경우 문자열의 길이가 작지 않았기에 전체적인 데이터 용량의 증가를 가속시켰고\nDB 최적화를 고려할 때쯤에 해당 상품명과 판매처명 등의 문자열 컬럼이 전체 테이블에서\n절반 이상에 해당하는 용량을 차지하고 있었다는 것을 알게 되었습니다.\n이러한 문제를 해결하기 위한 기법을 찾던 중 발견한 것이 DB 정규화 기법이었습니다.\nDB 정규화 기법은 반복되는 데이터를 별도의 테이블로 분리하는 기법인데,\n분리된 데이터는 필요할 때만 JOIN을 통해 불러오면 되기 때문에\n더욱 효율적으로 데이터 공간을 구성할 수 있을 것이라 생각했습니다.\n저는 별도의 인덱스 DB를 생성하고 공통된 코드로 묶을 수 있는 문자열 컬럼들을\n여러 테이블로부터 하나의 인덱스 DB 아래 테이블로 분리시켰습니다.\n그 결과 300MB에 달하는 테이블로부터 분리된 210MB의 데이터를 3MB로 압축시키면서\n테이블의 용량을 2/3 가량 감소시켰습니다.\nDB 정규화를 거치지 않았다면 지금쯤 500MB 정도의 데이터가 쌓였겠지만,\n최적화를 통해 아직까지 140MB 정도의 용량이 기록되어 있습니다.\npandas-gbq # DB 설계도 중요하지만, 제 업무는 기본적으로 데이터 수집 및 분석이었기 때문에\n수집된 데이터를 빅쿼리로 옮기는 과정을 구현해야 했습니다.\n구글 시트의 경우 매번 JSON 인증 파일을 갖고 다니며 인증 요청을 수행해야 했기에\n업로드를 위해 별도의 함수를 작성하고 호출해야하는 불편함이 있었습니다.\n하지만, 빅쿼리는 평소에 사용하는 pandas 모듈에서 업로드용 모듈을 지원한 덕분에 엑셀을 저장하듯이 내장된 to_gbq 함수로 간단하게 업로드를 수행할 수 있었습니다.\n로컬에서 빅쿼리 업로드를 위한 인증도 초기에 한번만 구글 로그인을 수행하면 되었기에\n지금도 굉장히 간편하게 활용하고 있습니다.\n구글 클라우드 펑션 # 윈도우 스케줄러와 파이썬을 통한 빅쿼리 업로드를 통해 어느정도 자동화를 이루었다고 생각했을 때쯤\n예상치 못한 사건이 발생했습니다.\n평소에 데이터를 가져오고 있던 네이버의 경우 하나의 IP 주소로 반복 요청 시\n크롤링이 감지되어 차단당하는 문제 때문에 별도의 공인 IP를 제공하는 아이피팝 프로그램의 환경 아래서\n자동화 프로그램이 돌아가고 있었는데, 해당 아이피팝이 공격을 당해 먹통이 되는 경우가 발생했습니다.\n이 문제가 일주일 이상 지속되면서 결국 로컬에서 프로그램을 돌리는 것이 안전하지 않다는 것을 인식했고,\n굳이 이것이 아니더라도 자동화 프로그램 실행 중 발생하는 문제를\n해당 PC에 접근하지 않고는 해결할 수 없었던데서 불만을 갖고 있기도 했습니다.\n대안을 찾기 위해 우선적으로 생각해본 것은 기존에 생각하고 있었던 Airflow 였지만,\n단기간에 파이썬 코드를 Airflow에 맞게 수정하는 것은 어려움이 있다고 판단했습니다.\n이때, 어디선가 들었던 AWS 람다가 떠올랐고 관련된 서버리스 서비스에 대해 찾아보다가\n마침 현재 사용하고 있는 GCP 안에서 구글 펑션이라는 서버리스 기능을 제공한다는 것을 알게되었습니다.\n구글 펑션을 활용하면 HTTP 요청 트리거로서 현재의 파이썬 함수를 실행할 수 있었습니다.\n더욱이 매일 일정 시간에만 잠깐 수행되는 자동화 프로그램은\n실시간으로 돌아가는 Compute Engine에서 돌리는 것보다 구글 펑션이 비용적으로 더욱 효율적임이 계산되었고,\n무엇보다 HTTP 트리거가 매일보던 크롤링 작업에서 다뤘던 것과 크게 다르지 않았다는데서 적응하기 쉬웠습니다.\n다만, 항상 클라이언트의 입장에서 요청만 하다가 이러한 요청을 받아서 처리하는 로직을 구현하게 되면서,\n항상 보내던 헤더와 데이터가 서버에서 어떻게 보여지는지를 알게되는 등의 새로운 관점의 전환을 느꼈습니다.\n구글 펑션의 사용법은 간단하여, 기존에 엑셀 기반의 설정을 읽어서 파이썬 함수를 호출하던 동작을\nJSON 형식의 설정으로 대체하여 같은 함수에 동일한 파라미터를 전달해 호출하게 하면 되었습니다.\n리다이렉트 # 구글 펑션을 도입하게 되면서 크롤링 기능을 비약적으로 개선할 수 있었습니다.\n저는 이 기법을 리다이렉트라고 부르는데, 구글 펑션의 스케일 아웃 기능을 통해\n여러 개의 쿼리를 나눠서 구글 펑션에 동시에 요청하면 요청 차단의 우려 없이\n비동기적으로 크롤링을 수행할 수 있습니다.\n기존 로컬에서도 비동기적으로 크롤링을 수행하긴 했지만,\n네이버 등에서 요청이 차단당하는 문제 때문에 최대 동시 요청 횟수를 3회로 제한하고\n요청 간 딜레이를 주었습니다.\n물론, 이러한 방식이 상대방 서버 입장에서 안전하고\n동시다발적으로 요청을 보내는 기법은 서버에게 공격으로 간주될 수 있다는 것을 인지하기 때문에,\n리다이렉트의 동시 제한 횟수를 정해놓았습니다.\n이 기법을 도입하면서 기존에 2시간마다 40분씩 걸리면서 돌렸던 자동화 프로그램을\n1분 조금 넘는 실행 시간으로 단축시킬 수 있었습니다.\n구글 클라우드 스케줄러 # 구글 펑션도 결국 누군가가 트리거를 걸어줘야 했기 때문에 아직까지 로컬에서 실행되는 문제를 벗어나지 못했습니다.\n이러한 문제를 해결하기 위해 마찬가지로 GCP 안에서 활용할 수 있는 서비스를 탐색했고,\n클라우드 스케줄러라는 것을 발견했습니다.\n클라우드 스케줄러는 크론탭으로 일정 주기마다 HTTP 요청을 보낼 수 있게 설정할 수 있게 지원해주는데,\n마침 HTTP 요청에 같이 담겨야되는 인증 정보를 자동으로 첨부할 수 있어서\n로컬에서 요청을 보내는 것보다 더욱 간편하게 전송할 수 있는 장점이 있습니다.\n사용법도 주기, 대상 주소, JSON 본문만 지정하면 매일 특정 시간마다 구글 펑션을 수행하게 설정할 수 있습니다.\n이를 통해 로컬 환경을 탈피해 GCP 안에서 모든 자동화를 수행할 수 있게 되었습니다.\n마무리 # 사실 GCP 내에서의 작업은 많은 시간이 걸리지 않았고\n실질적으로 GCP에 맞게 기존 파이썬 코드를 리팩토링하는데서 더욱 깔끔하게 고치고 싶은 욕심이 들어\n한 달 동안 전체 코드를 수정하게 되었습니다.\n별도의 프레임워크 없이 제작한 제 크롤러는 scrapy 모듈에서 착안해 Spider, Parser, Pipeline의 구조로 이루어져 있는데\n기존에 비동기와 동기식 Spider의 사용여부로 구분된 별도의 프로젝트를 하나로 합친다거나\ncrawl \u0026gt; gather/redirect \u0026gt; fetch의 단계로 추상 메소드를 정의하는 등의 전반적인 리팩터링 작업이 있었습니다.\n마침 명절을 맞아 모든 작업이 안정화되었고 여유를 가지며 작업을 되돌아 볼 수 있었습니다.\n지금까지 데이터 엔지니어링 관점에서 비약적으로 성장했다고 느끼면서도,\n혼자만의 노력으로 이 이상 성장할 수 있을지에 대해서는 다소 불확실한 부분이 있습니다.\n아마 당분간은 그동안 방치했었던 데이터 분석 쪽에 눈길을 돌려\n그동안 쌓아둔 데이터들을 보면서 새로운 발견을 하게 될지도 모르겠습니다.\n"},{"id":13,"href":"/blog/smartstore-login-3/","title":"[Python] requests로 네이버 스마트스토어센터 로그인 구현하기 (3)","section":"Posts","content":"앞선 네이버 로그인 구현 과정을 통해 네이버 로그인에 대해 이해하고\n스마트스토어센터 로그인 결과로 얻을 수 있는 쿠키 값의 일부를 획득했습니다.\nCopy python cookies = { \u0026#34;NNB\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;nid_inf\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_AUT\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_JKL\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_CHK\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NSI\u0026#34;: \u0026#34;...\u0026#34;, } 하지만, 스마트스토어센터에서 데이터를 가져오기 위해 필요한 쿠키 값은\nCBI_SES, CBI_CHK, NSI 세 가지 값이기 때문에\n지금까지는 준비 과정에 불과했다고 할 수 있습니다.\n이번 게시글에서는 스마트스토어센터 로그인 과정을 이해하고\n직접 구현해보면서 SmartstoreLogin 클래스를 완성해보겠습니다.\n스마트스토어센터 로그인 이해 # 지금까지 스마트스토어센터의 두 가지 로그인 방식 중\n네이버 로그인 방식으로 로그인을 수행하기 위해,\n실제 네이버 로그인에 대한 이해 및 구현을 진행했습니다.\n요청 내역 탐색 시 주의사항 # 새 창에서 띄워지는 네이버 로그인 페이지는\n로그인이 완료되면 닫혀버리기 때문에 네트워크 요청 내역을 확인하기 어렵습니다.\n이 경우 개발자 도구 Sources 탭에서 Event Listener Breakpoints 메뉴 아래\nWindow \u0026gt; window.close 부분을 선택하면 창이 닫히는 순간에 중단시킬 수 있습니다.\n네이버 로그인과의 차이점 # 스마트스토어센터 로그인에서의 네이버 로그인은 기존 방식과 다소의 차이점이 존재합니다.\n아래는 스마트스토어센터 로그인 POST 요청에서 확인할 수 있는 데이터입니다.\nCopy json { \u0026#34;localechange\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;dynamicKey\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;logintp\u0026#34;: \u0026#34;oauth2\u0026#34;, \u0026#34;encpw\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;enctp\u0026#34;: 1, \u0026#34;svctype\u0026#34;: 64, \u0026#34;smart_LEVEL\u0026#34;: 1, \u0026#34;bvsd\u0026#34;: { \u0026#34;uuid\u0026#34;:\u0026#34;...\u0026#34;, \u0026#34;encData\u0026#34;:\u0026#34;...\u0026#34; }, \u0026#34;encnm\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;locale\u0026#34;: \u0026#34;ko_KR\u0026#34;, \u0026#34;client_id\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://nid.naver.com/oauth2.0/authorize?response_type=code\u0026amp;state=...\u0026amp;client_id=...\u0026amp;redirect_uri=https%3A%2F%2Faccounts.commerce.naver.com%2Foauth%2Fcallback\u0026amp;locale=ko_KR\u0026amp;inapp_view=\u0026amp;oauth_os=\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;pw\u0026#34;: \u0026#34;\u0026#34; } 기존의 네이버 로그인 데이터와 비교했을 때 3개의 값이 추가되었음을 알 수 있습니다.\nlogintp의 경우 \u0026quot;oauth2\u0026quot;로 고정된 값으로 보이지만,\nurl 내 state와 client_id는 지금까지의 과정에서는 얻을 수 없었던\n새로운 값으로 로그인을 위해 추가적인 동작이 필요해 보입니다.\nOAuth URL 가져오기 # state와 client_id의 경우 네이버 로그인 페이지를 불러오는 과정에서\n이미 전달되는 값이기 때문에 해당 페이지 안에서는 출처를 찾을 수 없었습니다.\n따라서 네이버 로그인 페이지로 이동하기 위해 거치는 스마트스토어센터 로그인 페이지에서\n네이버 로그인 페이지를 띄우는 과정에 집중하여 두 값이 발생하는 지점을 찾아보았고,\ngraphql 주소로 보낸 POST 요청에 대한 응답으로 url에 해당하는 authUrl 값을 받는 것을 확인했습니다.\n이렇게 구한 client_id 및 url 값을 로그인 데이터에 담아 요청을 보낼 경우\n일반적인 네이버 로그인 결과로 얻을 수 있는 NID_AUT 등의 쿠키 값을 획득할 수 있습니다.\nGraphQL 로그인 분석 # 스마트스토어센터 로그인은 네이버 로그인에서 그치지 않고\nCBI_SES, CBI_CHK, NSI 쿠키 값을 추가로 얻어야 합니다.\n이 중에서 CBI_SES를 응답 파일 내에서 검색했을 때 graphql 주소에 대한 응답으로\nCBI_SES와 CBI_CHK 값을 반환하는 것을 알 수 있었습니다.\n해당 주소는 앞서 인증 주소를 가져오는 과정에서 보았던 것인데\n당시 snsLoginBegin라는 명칭의 쿼리와는 다른 snsLoginCallback 쿼리를 사용하여\n추가적인 로그인을 수행하는 것임을 짐작할 수 있습니다.\n변수로 전달되는 state의 경우 앞에서 구한 것과 동일한 값이지만,\ncode는 아직까지 본 적 없는 값입니다.\n하지만, code는 어떠한 응답 파일 내에서도 출처를 찾아볼 수 없고,\ncode의 값 자체를 검색했을 때 oauth_token이라는 키와 동일한 값을 사용한다는 것 말고는\n별다른 단서를 찾을 수 없었습니다.\n이 경우 네이버 로그인 후에 연속적으로 진행되는 다른 요청 내역을 직접 들여다봐야 했고,\n다행히 바로 아래의 주소에 대한 응답 내역에서 oauth_token 값을 받아볼 수 있었습니다.\nCopy html \u0026lt;html\u0026gt; \u0026lt;script language=javascript nonce=\u0026#34;4SzeR1mCGzDbnzr3s5rjQ1Li\u0026#34;\u0026gt; location.replace(\u0026#34;https://nid.naver.com/login/noauth/allow_oauth.nhn?oauth_token=...\u0026amp;with_pin\u0026amp;step=agree_term\u0026amp;inapp_view=\u0026amp;oauth_os=\u0026#34;); \u0026lt;/script\u0026gt; \u0026lt;/html\u0026gt; oauth_token의 값을 code에 넣어서 state와 함께 graphql 주소에 요청할 경우\n응답 헤더의 Set-Cookie에서 볼 수 있는 CBI_SES와 CBI_CHK를 받게 됩니다.\n2단계 인증 분석 # 스마트스토어센터는 최초 로그인 시 반드시 2단계 인증을 거쳐야 합니다.\n마지막 남은 NSI 값 또한 해당 2단계 인증을 거쳐야 얻을 수 있을 것이라 걱정했지만,\n다행히 2단계 인증을 거치지 않아도 네트워크 응답 내역에서 NSI를 확인할 수 있었습니다.\nPOST 요청이지만 전달되는 데이터는 아래와 같이 단순했기에\n추가적인 분석 없이 마지막 NSI 값을 획득했습니다.\nCopy json {\u0026#34;url\u0026#34;: \u0026#34;https://sell.smartstore.naver.com/#/home/dashboard\u0026#34;} 스마트스토어센터 로그인 구현 # 지금까지의 과정을 통해 스마트스토어센터에서 데이터를 가져오기 위해 필요한 CBI_SES, CBI_CHK, NSI 값을 획득하는 방법을 파악했습니다.\n이를 SmartstoreLogin 클래스의 메소드로 구현해보겠습니다.\n네이버 로그인 구현 # 기존의 네이버 로그인 기능에 OAuth URL을 가져오는 부분을 추가시킨\nnid_login() 및 fetch_oauth_url() 메소드를 정의합니다.\nCopy python SMARTSTORE_URL = \u0026#34;https://sell.smartstore.naver.com/\u0026#34; SLOGIN_URL = \u0026#34;https://accounts.commerce.naver.com\u0026#34; GRAPHQL_DATA = str({ \u0026#34;operationName\u0026#34;: \u0026#34;snsLoginBegin\u0026#34;, \u0026#34;variables\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;login\u0026#34;, \u0026#34;snsCd\u0026#34;: \u0026#34;naver\u0026#34;, \u0026#34;svcUrl\u0026#34;: \u0026#34;https://sell.smartstore.naver.com/#/login-callback\u0026#34;}, \u0026#34;query\u0026#34;: \u0026#34;mutation snsLoginBegin($mode: String!, $snsCd: String!, $svcUrl: String!, \\ $oneTimeLoginSessionKey: String, $userInfos: [UserInfoEntry!]) {\\n snsBegin(\\n \\ snsLoginBeginRequest: {mode: $mode, snsCd: $snsCd, svcUrl: $svcUrl, oneTimeLoginSessionKey: \\ $oneTimeLoginSessionKey, userInfos: $userInfos}\\n ) {\\n authUrl\\n __typename\\n }\\n}\\n\u0026#34; }).replace(\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;) class SmartstoreLogin(NaverLogin): def fetch_oauth_url(self): referer = f\u0026#34;{SLOGIN_URL}/login?url={SMARTSTORE_URL}#/login-callback\u0026#34; headers = self.get_headers(host=SLOGIN_URL, referer=referer) response = self.post(urljoin(SLOGIN_URL, \u0026#34;graphql\u0026#34;), data=GRAPHQL_DATA, headers=headers) self.oauth_url = json.loads(response.text)[\u0026#34;data\u0026#34;][\u0026#34;snsBegin\u0026#34;][\u0026#34;authUrl\u0026#34;] self.oauth_params = {k:v.pop() for k,v in parse_qs(urlparse(self.oauth_url).query).items()} if \u0026#34;auth_type\u0026#34; in self.oauth_params: self.oauth_params.pop(\u0026#34;auth_type\u0026#34;) self.oauth_params = dict(self.oauth_params, **{\u0026#34;locale\u0026#34;:\u0026#34;ko_KR\u0026#34;,\u0026#34;inapp_view\u0026#34;:\u0026#39;\u0026#39;,\u0026#34;oauth_os\u0026#34;:\u0026#39;\u0026#39;}) graphql 주소에 대한 요청 데이터를 그대로 구현한 것이 GRAPHQL_DATA이며,\n그 결과로 OAuth URL을 얻을 수 있습니다.\nOAuth URL의 파라미터는 향후 GraphQL 인증 과정에서 재활용되기 때문에\noauth_params 변수에 저장해둡니다.\nCopy python LOGIN_URL = \u0026#34;https://nid.naver.com/nidlogin.login\u0026#34; SLOGIN_DATA = lambda dynamicKey, encpw, bvsd, encnm, client_id: \\ dict(LOGIN_DATA(dynamicKey, encpw, bvsd, encnm), **{\u0026#34;logintp\u0026#34;:\u0026#34;oauth2\u0026#34;,\u0026#34;svctype\u0026#34;:\u0026#34;64\u0026#34;,\u0026#34;client_id\u0026#34;:client_id}) class SmartstoreLogin(NaverLogin): def nid_login(self): self.fetch_keys() self.set_encpw() self.set_bvsd() self.fetch_oauth_url() data = SLOGIN_DATA(self.dynamicKey, self.encpw, self.bvsd, self.encnm, self.oauth_params.get(\u0026#34;client_id\u0026#34;)) headers = self.get_headers(LOGIN_URL, referer=self.oauth_url) headers[\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/x-www-form-urlencoded\u0026#34; headers[\u0026#34;Upgrade-Insecure-Requests\u0026#34;] = \u0026#34;1\u0026#34; response = self.post(LOGIN_URL, data=data, headers=headers) 네이버 로그인 과정에서는 bvsd를 생성한 후 OAuth URL을 추가로 가져오고\nclient_id를 기존의 로그인 데이터 내에 포함시켜 POST 요청을 보냅니다.\n해당 메소드의 결과로 NID_AUT, NID_JKL, NID_SES를 부여받을 수 있습니다.\nOAuth 로그인 구현 # OAuth 로그인은 네이버 로그인과 GraphQL 인증으로 구성됩니다.\n현시점에서 GraphQL 인증에 필요한 것은 oauth_token 뿐이기 때문에\n앞선 네이버 로그인 과정에서 획득한 주소로부터 oauth_token을 가져오는 메소드 fetch_oauth_token()과\n전체적인 OAuth 로그인 과정을 구현한 oauth_login() 메소드를 정의합니다.\nCopy python OAUTH_URL = \u0026#34;https://nid.naver.com/oauth2.0/authorize\u0026#34; class SmartstoreLogin(NaverLogin): def fetch_oauth_token(self): headers = self.get_headers(LOGIN_URL, referer=LOGIN_URL, cookies=self.get_cookies()) response = self.get(OAUTH_URL, headers=headers, params=self.oauth_params) if re.search(\u0026#34;(?\u0026lt;=oauth_token\\=)(.*?)(?=\u0026amp;)\u0026#34;, response.text): self.oauth_token = re.search(\u0026#34;(?\u0026lt;=oauth_token\\=)(.*?)(?=\u0026amp;)\u0026#34;, response.text).group() Copy python OAUTH_DATA = lambda code, state: str({ \u0026#34;operationName\u0026#34;:\u0026#34;snsLoginCallback\u0026#34;, \u0026#34;variables\u0026#34;: { \u0026#34;code\u0026#34;: code, \u0026#34;state\u0026#34;: state}, \u0026#34;query\u0026#34;:\u0026#34;mutation snsLoginCallback($code: String!, $state: String!) \\ {\\n snsCallback(snsLoginCallbackRequest: {code: $code, state: $state}) \\ {\\n statCd\\n loginStatus\\n nextUrl\\n sessionKey\\n snsCd\\n \\ idNo\\n realnm\\n age\\n email\\n __typename\\n }\\n}\\n\u0026#34; }).replace(\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;) class SmartstoreLogin(NaverLogin): def oauth_login(self): self.nid_login() self.fetch_oauth_token() code, state = self.oauth_token, self.oauth_params.get(\u0026#34;state\u0026#34;) referer = SLOGIN_URL+f\u0026#34;/oauth/callback?code={code}\u0026amp;state={state}\u0026#34; headers = self.get_headers(host=SLOGIN_URL, referer=referer, cookies=self.get_cookies()) response = self.post(urljoin(SLOGIN_URL, \u0026#34;graphql\u0026#34;), data=OAUTH_DATA(code, state), headers=headers) 2단계 인증 구현 # 2단계 인증을 직접 수행할 필요는 없습니다.\nNSI 쿠키 값을 할당받을 수 있는 주소로 POST 요청을 보내는\ntwo_factor_login() 메소드를 정의합니다.\nCopy python TWOLOGIN_URL = SMARTSTORE_URL+\u0026#34;api/login?url=https%3A%2F%2Fsell.smartstore.naver.com%2F%23%2Fhome%2Fdashboard\u0026#34; TWOLOGIN_DATA = {\u0026#34;url\u0026#34;: \u0026#34;https://sell.smartstore.naver.com/#/home/dashboard\u0026#34;} class SmartstoreLogin(NaverLogin): def two_factor_login(self): headers = self.get_headers(SMARTSTORE_URL, referer=SMARTSTORE_URL, cookies=self.get_cookies()) headers[\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/json;charset=UTF-8\u0026#34; headers[\u0026#34;x-current-state\u0026#34;] = \u0026#34;https://sell.smartstore.naver.com/#/login-callback\u0026#34; headers[\u0026#34;x-current-statename\u0026#34;] = \u0026#34;login-callback\u0026#34; headers[\u0026#34;x-to-statename\u0026#34;] = \u0026#34;login-callback\u0026#34; response = self.post(TWOLOGIN_URL, data=TWOLOGIN_DATA, headers=headers) 로그인 메소드 구현 # SmartstoreLogin 객체를 사용할 때는 login() 메소드를 활용합니다.\nCopy python class SmartstoreLogin(NaverLogin): def login(self): email_pattern = re.compile(\u0026#34;[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\u0026#34;) self.seller_login() if email_pattern.search(self.userid) else self.oauth_login() self.two_factor_login() 향후 판매자 계정으로 로그인 하는 경우를 고려해\nuserid가 이메일인 경우 seller_login() 이라는 미구현된 메소드를 실행하도록 정의했습니다.\n일반적인 네이버 아이디를 사용할 경우엔 OAuth 로그인과 2단계 인증을 거쳐\n처음 목적으로 했던 아래의 모든 쿠키 값을 획득하게 됩니다.\nCopy python cookies = { \u0026#34;NNB\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;nid_inf\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_AUT\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_JKL\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_CHK\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NSI\u0026#34;: \u0026#34;...\u0026#34;, } 해당 쿠키를 가진 SmartstoreLogin 객체를 세션 객체로 활용한다면\n스마트스토어센터 내 어떤 데이터라도 파이썬 requests 모듈로 가져올 수 있게 됩니다.\n"},{"id":14,"href":"/blog/smartstore-login-2/","title":"[Python] requests로 네이버 스마트스토어센터 로그인 구현하기 (2)","section":"Posts","content":"이번 게시글에서는 스마트스토어센터 페이지에서 데이터를 수집하는 자동화 프로그램을 제작하기 위한\n첫 번째 과정으로 네이버 로그인을 구현할 것입니다.\n앞선 게시글에서 데이터를 수집하는 방식에 대해 알아보면서\n로그인이 필요한 페이지에 접근하기 다음과 같은 쿠키 값이 필요함을 확인했습니다.\nCopy python cookies = { \u0026#34;NNB\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;nid_inf\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_AUT\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_JKL\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_CHK\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NSI\u0026#34;: \u0026#34;...\u0026#34;, } 위 키값들은 앞으로 로그인 프로세스를 파악하는 과정에서 중요하게 활용됩니다.\n네이버 로그인 이해 # 네이버 스마트스토어센터 로그인 과정에서 진행되는 네이버 로그인은\n일반적인 네이버 로그인과는 다른 과정으로 진행됩니다.\n따라서 우선 일반적인 네이버 로그인 과정을 알아보겠습니다.\n해당 파트는 아래 게시글을 참고해 작성되었습니다.\n파이썬#76 - 파이썬 크롤링 requests 로 네이버 로그인 하기\n네이버 로그인 요청 분석 # 네이버 로그인 과정을 분석하기 위해서는 우선 네이버 로그인을 요청을 시도하여\n전달되는 값을 확인해야 합니다.\n네이버 로그인 페이지에서 로그인을 수행하는 과정에서\n발견할 수 있는 POST 요청을 살펴보면 다음과 같은 데이터가 전달됨을 발견할 수 있습니다.\n암호화된 값을 생략하고 키로 전달되는 내용을 확인하면 다음과 같습니다.\nCopy json { \u0026#34;localechange\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;dynamicKey\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;encpw\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;enctp\u0026#34;: 1, \u0026#34;svctype\u0026#34;: 1, \u0026#34;smart_LEVEL\u0026#34;: 1, \u0026#34;bvsd\u0026#34;: { \u0026#34;uuid\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;encData\u0026#34;: \u0026#34;...\u0026#34; }, \u0026#34;encnm\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;locale\u0026#34;: \u0026#34;ko_KR\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://www.naver.com\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;pw\u0026#34;: \u0026#34;\u0026#34; } 공백이나 고정된 값을 가진 키를 제외하면 결과적으로\ndynamicKey, encpw, bvsd, encnm를 밝혀내는 것이 중요할 것이라 판단됩니다.\n네이버 로그인 폼 분석 # 키의 명칭만으로는 무엇을 의미하는지 알 수 없기 때문에\n로그인 페이지 소스에서 키명칭을 검색하였고 네이버 로그인 폼에서 하나의 단서를 찾을 수 있었습니다.\ndynamicKey의 경우 로그인 폼에 동적으로 부여되는 값임을 알 수 있습니다.\n하지만 나머지 encpw, bvsd, encnm의 값은 비어있기 때문에\n다른 자바스크립트 응답을 분석해야 합니다.\n네이버 로그인 RSA 암호화 # encpw 값에 대한 단서를 찾기 위해 전체 검색을 수행했을 때\ncommon_202201.js 내부에서 RSA 암호화 처리를 통해 값을 생성함을 알 수 있습니다.\n그 중에서 가장 처음 단계로 실행될 것이라 추측되는 것이 아래 confirmSubmit() 함수입니다.\n해당 함수는 아이디와 비밀번호의 여부를 체크하고 encryptIdPw() 함수의 결과를 반환합니다.\n바로 밑에서 확인할 수 있는 encryptIdPw() 함수의 내용은 다음과 같습니다.\nCopy js function encryptIdPw() { var id = $(\u0026#34;id\u0026#34;); var pw = $(\u0026#34;pw\u0026#34;); var encpw = $(\u0026#34;encpw\u0026#34;); var rsa = new RSAKey; if (keySplit(session_keys)) { rsa.setPublic(evalue, nvalue); try{ encpw.value = rsa.encrypt( getLenChar(sessionkey) + sessionkey + getLenChar(id.value) + id.value + getLenChar(pw.value) + pw.value); } catch(e) { return false; } $(\u0026#39;enctp\u0026#39;).value = 1; id.value = \u0026#34;\u0026#34;; pw.value = \u0026#34;\u0026#34;; return true; } else { getKeyByRuntimeInclude(); return false; } return false; } 해당 함수는 session_keys라는 값을 처리하고 RSA 암호화한 결과를\nencpw의 값으로 대체하는 것을 알 수 있습니다.\n마찬가지로 해당 명칭을 검색했을 때\nsession_keys는 Ajax 통신의 응답 결과를 받아오는 것을 확인할 수 있습니다.\n하지만 네이버 로그인 페이지에서 svctype=262144를 추가적인 파라미터로 입력할 경우\n접근할 수 있는 모바일 로그인 페이지에서 해당 값을 확인할 수 있었습니다.\n다시 encryptIdPw() 함수로 돌아가서 session_keys를 처리하기 위해\nkeySplit() 함수를 찾아보았습니다.\nCopy js function keySplit(a) { keys = a.split(\u0026#34;,\u0026#34;); if (!a || !keys[0] || !keys[1] || !keys[2] || !keys[3]) { return false; } sessionkey = keys[0]; keyname = keys[1]; evalue = keys[2]; nvalue = keys[3]; $(\u0026#34;encnm\u0026#34;).value = keyname; return true } 모바일 페이지에서 볼 수 있는 session_keys 값은 콤마를 기준으로\n4개의 값으로 구분되어 있었는데 해당 함수에서는 각각을\nsessionKey, encnm, evalue, nvalue으로 분리했습니다.\n여기서 encnm 값을 우선적으로 가져올 수 있었고,\n다음으로 encpw 값을 찾기 위해 RSA 암호화 부분을 탐색해봅니다.\nCopy js rsa.setPublic(evalue, nvalue); encpw.value = rsa.encrypt( getLenChar(sessionkey) + sessionkey + getLenChar(id.value) + id.value + getLenChar(pw.value) + pw.value); session_keys에서 분리된 evalue와 nvalue로 RSA 공개키를 생성하고\n마찬가지로 session_keys에 포함된 sessionKey 및 아이디, 비밀번호의 조합을\n암호화한 결과가 encpw임을 확인할 수 있습니다.\n파이썬에서는 공개키 생성을 rsa.PublicKey() 함수로 수행할 수 있으며\nrsa.encrypt() 함수로 RSA 암호화를 진행할 수 있습니다.\n해당 과정은 아래와 같이 구현됩니다.\nCopy python publicKey = rsa.PublicKey(int(nvalue,16), int(evalue,16)) value = \u0026#39;\u0026#39;.join([chr(len(key))+key for key in [sessionKey, id, pw]]) encpw = rsa.encrypt(value.encode(), publicKey).hex() 여기까지의 과정으로 dynamicKey, encpw, encnm의 값을 얻을 수 있습니다.\nbvsd 값 생성하기 # 마지막으로 필요한 bvsd 값에 대한 단서는 응답 문서 내에서\nbvsd.1.3.8.min.js란 명칭으로 알기 쉽게 확인할 수 있지만\n그 내용은 가독성 면에서 쉽게 해석하기 어려웠습니다.\n다른 자료를 참고했을 때 bvsd는 브라우저가 정상적인지 여부를 파악하기 위한 값으로\n해당 값이 없을 경우 로그인 과정에서 캡차를 발생시킨다는 것을 알 수 있었습니다.\nbvsd.1.3.8.min.js에서 주목할 부분은 uuid 및 encData를 생성하는 부분인데\n아래 코드에서 encData는 o라는 값을 인코딩하는 것으로 추측됩니다.\no 값을 코드 내에서 찾아보니 아래와 같이 디바이스의 마우스 상태 등을\n기록한 값임을 확인할 수 있었습니다.\nCopy js o = { a: n, b: \u0026#34;1.3.8\u0026#34;, c: (0, m[\u0026#34;default\u0026#34;])(), d: r, e: this._deviceOrientation.get(), f: this._deviceMotion.get(), g: this._mouse.get(), j: this._fpDuration || y.NOT_YET, h: this._fpHash || \u0026#34;\u0026#34;, i: this._fpComponent || [] }; 하지만 각각의 값을 해석하고 생성하는 것은 쉽지 않았기에\n이미 완성된 코드를 참고하여 set_bvsd() 메소드를 정의했습니다.\nencData의 인코딩에는 lzstring 모듈의\nLZString.compressToEncodedURIComponent() 함수를 활용했습니다.\nCopy python from lzstring import LZString import uuid ENC_DATA = lambda uuid, userid, passwd: str({ \u0026#34;a\u0026#34;: f\u0026#34;{uuid}-4\u0026#34;, \u0026#34;b\u0026#34;: \u0026#34;1.3.4\u0026#34;, \u0026#34;d\u0026#34;: [{ \u0026#34;i\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;b\u0026#34;: {\u0026#34;a\u0026#34;: [\u0026#34;0\u0026#34;, userid]}, \u0026#34;d\u0026#34;: userid, \u0026#34;e\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;f\u0026#34;: \u0026#34;false\u0026#34; }, { \u0026#34;i\u0026#34;: passwd, \u0026#34;e\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;f\u0026#34;: \u0026#34;false\u0026#34; }], \u0026#34;h\u0026#34;: \u0026#34;1f\u0026#34;, \u0026#34;i\u0026#34;: {\u0026#34;a\u0026#34;: \u0026#34;Mozilla/5.0\u0026#34;} }).replace(\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;) class NaverLogin(LoginSpider): def set_bvsd(self): uuid4 = str(uuid.uuid4()) encData = LZString.compressToEncodedURIComponent(ENC_DATA(uuid4, self.userid, self.passwd)) self.bvsd = str({\u0026#34;uuid\u0026#34;:uuid4, \u0026#34;encData\u0026#34;:encData}).replace(\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;) 네이버 로그인 구현 # 지금까지의 과정을 통해 네이버 로그인에 필요한\ndynamicKey, encpw, bvsd, encnm 값을 생성하는 법을 파악했습니다.\n이를 NaverLogin 클래스의 메소드로 구현해보겠습니다.\nRSA 암호화 구현 # 먼저 dynamicKey와 함께 encpw, encmn 생성에 필요한\nsession_keys를 가져오기 위한 메소드 fetch_keys()와,\nRSA 암호화를 통해 encpw 값을 구하는 set_encpw() 메소드를 정의합니다.\nCopy python from bs4 import BeautifulSoup import rsa LOGIN_URL = \u0026#34;https://nid.naver.com/nidlogin.login\u0026#34; class NaverLogin(LoginSpider): def fetch_keys(self): response = self.get(LOGIN_URL, headers=self.get_headers(host=LOGIN_URL), params={\u0026#34;svctype\u0026#34;:\u0026#34;262144\u0026#34;}) source = BeautifulSoup(response.text, \u0026#39;lxml\u0026#39;) keys = source.find(\u0026#34;input\u0026#34;, {\u0026#34;id\u0026#34;:\u0026#34;session_keys\u0026#34;}).attrs.get(\u0026#34;value\u0026#34;) self.sessionKey, self.encnm, n, e = keys.split(\u0026#34;,\u0026#34;) self.dynamicKey = source.find(\u0026#34;input\u0026#34;, {\u0026#34;id\u0026#34;:\u0026#34;dynamicKey\u0026#34;}).attrs.get(\u0026#34;value\u0026#34;) self.publicKey = rsa.PublicKey(int(n,16), int(e,16)) session_keys의 경우 모바일 로그인 페이지에서만 가져올 수 있기 때문에\nsvctype=262144를 GET 요청의 파라미터로 전달해 모바일 로그인 페이지를 가져옵니다.\nnvalue와 evalue는 별도의 변수로 저장하지 않고\npublicKey를 생성해 클래스 변수로 저장합니다.\nCopy python class NaverLogin(LoginSpider): def set_encpw(self): value = \u0026#34;\u0026#34;.join([chr(len(key))+key for key in [self.sessionKey, self.userid, self.passwd]]) self.encpw = rsa.encrypt(value.encode(), self.publicKey).hex() 앞에서 가져온 sessionKey와 함께 미리 초기화된 네이버 아이디 및 비밀번호를\n조합 및 암호화하여 encpw를 생성합니다.\nPOST 요청 구현 # 미리 정의한 set_bvsd() 메소드를 포함해 모든 준비 과정이 마무리되었습니다.\n클래스 변수로 저장된 암호화된 값들을 데이터에 담아 POST 로그인 요청을 보내는\nlogin() 메소드는 다음과 같이 정의할 수 있습니다.\nCopy python NAVER_URL = \u0026#34;https://www.naver.com\u0026#34; LOGIN_DATA = lambda dynamicKey, encpw, bvsd, encnm: { \u0026#34;localechange\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;dynamicKey\u0026#34;: dynamicKey, \u0026#34;encpw\u0026#34;: encpw, \u0026#34;enctp\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;svctype\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;smart_LEVEL\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;bvsd\u0026#34;: bvsd, \u0026#34;encnm\u0026#34;: encnm, \u0026#34;locale\u0026#34;: \u0026#34;ko_KR\u0026#34;, \u0026#34;url\u0026#34;: quote_plus(NAVER_URL), \u0026#34;id\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;pw\u0026#34;: \u0026#34;\u0026#34;, } class NaverLogin(LoginSpider): def login(self): self.fetch_keys() self.set_encpw() self.set_bvsd() data = LOGIN_DATA(self.dynamicKey, self.encpw, self.bvsd, self.encnm) headers = self.get_headers(LOGIN_URL, referer=LOGIN_URL) headers[\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/x-www-form-urlencoded\u0026#34; headers[\u0026#34;Upgrade-Insecure-Requests\u0026#34;] = \u0026#34;1\u0026#34; self.post(LOGIN_URL, data=data, headers=headers) POST 요청 시 전달되었던 데이터와 동일한 값을 반환하는 LOGIN_DATA 함수를 생성하고\n암호화된 값을 전달해 최종적인 POST 데이터를 만들었습니다.\n해당 데이터로 요청을 보낼 경우 정상적인 응답을 받게 되고\nNaverLogin 세션 객체의 쿠키 값을 확인하면 아래와 같은 결과를 확인할 수 있습니다.\nCopy python naver = NaverLogin(\u0026#34;userid\u0026#34;, \u0026#34;passwd\u0026#34;) naver.login() naver.get_cookies() ====================================== \u0026#39;NID_AUT=...; NID_JKL=...; NID_SES=...; nid_inf=1228467713\u0026#39; 또한 해당 결과는 개발자 도구에서도 응답 헤더의 set-cookie 값에서 찾아볼 수 있습니다.\n지금까지의 과정으로 네이버 로그인 과정을 거쳤을 때,\n게시글의 서두에서 언급한 쿠키 값의 목록 중에서 일부 값을 획득할 수 있습니다.\nCopy python cookies = { \u0026#34;NNB\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;nid_inf\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_AUT\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_JKL\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_CHK\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NSI\u0026#34;: \u0026#34;...\u0026#34;, } 이 중에서 NNB의 경우 네이버 페이지 접속 시 기본적으로 부여되는 값이기 때문에 무시하고\nNID_AUT, NID_JKL, NID_SES가 채워졌습니다.\n나머지 값들은 스마트스토어센터 로그인 과정에서 얻을 수 있기 때문에\n다음 게시글에서 다뤄보도록 하겠습니다.\n"},{"id":15,"href":"/blog/smartstore-login-1/","title":"[Python] requests로 네이버 스마트스토어센터 로그인 구현하기 (1)","section":"Posts","content":"네이버 스마트스토어센터에서는 매출 향상에 도움을 주는 유용한 통계 데이터를 제공해줍니다.\n쇼핑몰 데이터를 분석하는 입장에서 무료로 제공되는 이런 데이터는 큰 도움이 되지만,\n대부분이 엑셀 파일 다운로드를 지원하지 않고 빈번하게 수치가 바뀌는 데이터를 각각의 메뉴에서 매번 확인하기도 어렵습니다.\n이런 데이터를 자동화 프로그램으로 수집 및 적재할 수 있다면 업무 효율을 크게 향상시킬 수 있을 것입니다.\n이번 게시글에서는 실제 네이버 스마트스토어 로그인 구현에 앞서\n데이터 수집에 대한 간단한 설명을 진행하고 네이버 로그인 구현의 바탕이 되는 클래스와 메소드를 정의합니다.\n데이터 수집 개요 # 네이버 웹사이트에서 데이터를 수집할 때 활용할 수 있는 방안은 2가지가 있습니다.\n첫 번째는 CSS Selector 또는 XPath를 활용해 웹사이트 특정 위치의 값을 가져오는 것,\n두 번째는 API에 요청을 보내 JSON 형태의 데이터를 가져오는 것입니다.\n특정 위치의 값을 가져오는 첫 번째 방식은 UI에 의존적이어서 코드의 지속성을 보장하기 어렵고\n원하는 데이터와 관련없는 웹 소스 전체를 불러오기 때문에 속도 면에서도 단점이 있습니다.\n따라서, API를 제공하는 경우 두 번째 방식을 이용하는 것이 효율적입니다.\n데이터 수집 시나리오 # 네이버 쇼핑에서 표시되는 상품의 순위는 검색인기도를 기준으로 결정됩니다.\n키워드별 상위권 상품의 검색인기도를 가져오는 것을 예시로 데이터 수집을 진행해보겠습니다.\n위 이미지에서 왼쪽 부분은 실제 UI, 오른쪽 부분은 HTML 소스 입니다.\n해당 소스에서 데이터를 가져온다면 div.popularity-product \u0026gt; div.box-border 위치에서\ndd 태그를 순서대로 지정해서 각각의 종합, 적합도, 인기도 값을 가져올 수 있습니다.\n해당 데이터를 분석에 활용하기 위해서는 인기도 수치를 구성하는 클릭수, 판매실적 등도 필요하기 때문에\n상세보기 페이지를 확인해야하고 결과적으로 하나의 상품에 대한 데이터를 보기 위해 두 개의 페이지를 방문해야 합니다.\n하지만 네이버의 대부분의 웹페이지는 API를 기반으로 가져온 데이터로 구성되기 때문에\n해당 API를 활용할 수 있다면 더욱 효율적인 데이터 수집이 가능합니다.\n서버에서 가져오는 데이터를 확인할 때는 주로 개발자 도구의 네트워크 탭을 활용합니다.\n웹페이지 로드 시 가져오는 문서를 확인하다보면 위 이미지와 같이 목표로 하는 데이터를 보내주는 API를 발견할 수 있습니다.\n새 탭에서 해당 API 주소를 요청하면 위 이미지 내 오른쪽 부분과 같은 JSON 형식의 데이터를 받을 수 있습니다.\n실제 UI에서 가져오고자 하는 종합, 적합도, 인기도 수치도 해당 데이터에서 확인할 수 있습니다.\n여기에는 추가로 클릭수, 판매실적 등에 대한 수치 데이터도 포함되어 있기 때문에\n해당 API를 활용하면 다수의 페이지에 요청을 보낼 수고도 줄어들게 됩니다.\n로그인이 필요한 페이지의 데이터 가져오기 # 여기까지는 간단해보이지만 네이버 스마트스토어센터 데이터를 requests 모듈로 가져오는데는\n하나의 추가적인 문제가 존재합니다.\n단순한 GET 요청일지라도 로그인 정보를 갖고 있지 않다면 데이터를 받을 수 없습니다.\n스마트스토어센터에 로그인하지 않은 상태에서 위 API 주소로 요청을 보내게 된다면\n아래와 같은 에러 메시지를 받아볼 수 있습니다.\nCopy json { \u0026#34;error\u0026#34;: \u0026#34;Full authentication is required to access this resource\u0026#34; } 이 문제에 대한 해결방법은 헤더에 있습니다.\n개발자 도구 네트워크 탭에서 하나의 문서를 클릭하고 Headers 탭에서 스크롤을 내리면\n아래와 같은 Request Headers 정보를 확인할 수 있습니다.\n서버와 클라이언트 간 네트워크 요청 시 서버는 클라이언트의 정보를 확인할 목적으로\n클라이언트에 쿠키라는 암호화된 인증 정보를 남깁니다.\n클라이언트가 해당 정보를 헤더에 담아 요청을 보내는 경우에만 서버가 올바른 응답을 전달합니다.\nrequests 모듈에서는 이러한 과정을 다음과 같이 구현할 수 있습니다.\nCopy python headers = {\u0026#34;cookie\u0026#34;: \u0026#34;...\u0026#34;} response = requests.get(url, headers=headers) 하지만 일반적인 쿠키 값은 30분의 유통기한이 있기 때문에, 매번 쿠키 값을 갱신해야 하는데\n자동화 프로그램을 돌리기 전에 직접 로그인해서 쿠키 값을 갱신하는 것은 바람직하지 못합니다.\n결과적으로 로그인이 필요한 스마트스토어 페이지의 데이터를 가져오기 위해서는\n자동화된 로그인 과정을 거쳐서 쿠키 값을 갱신할 필요가 있습니다.\n쿠키 확인하기 # 클라이언트에서 요청하는 헤더 내역에서 확인할 수 있는 정보는 표현하면 다음과 같습니다.\nCopy json { \u0026#34;NNB\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;nid_inf\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_AUT\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_JKL\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_CHK\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NSI\u0026#34;: \u0026#34;...\u0026#34; } 이는 앞으로 스마트스토어센터 로그인을 구현하는데서 반드시 확인해야할 목록입니다.\n지금은 이 값들이 어떤 의미를 가지고 어디서 발생하는 값인지 알 수는 없지만,\n서버로부터 해당 값들을 받아오는 것에 집중하여 로그인 프로세스를 파악하고\n로그인 진행 과정을 쿠키 값을 통해 시각적으로 점검할 것입니다.\n스마트스토어센터 로그인 개요 # 스마트스토어센터 로그인을 구현하기 위해 로그인 페이지를 탐색할 필요가 있습니다.\n메인 페이지에서 로그인하기 버튼을 클릭했을 때 이동하는 로그인 페이지에서 실제 로그인이 이루어집니다.\n스마트스토어센터 로그인에는 판매자 아이디로 로그인하는 방식과\n네이버 아이디로 로그인하는 방식이 있습니다.\n우선적으로 네이버 아이디로 로그인하는 방식을 알아보겠습니다.\n네이버 로그인을 구현하는 것에 관해선 좋은 선례가 있어 많은 부분을 참고했습니다.\n해당 내용은 아래 링크를 참고할 수 있습니다.\n파이썬#76 - 파이썬 크롤링 requests 로 네이버 로그인 하기\n클래스 정의 # 네이버 로그인 기능은 자동화 프로그램에서 지속적으로 활용될 것이기 때문에\n별도의 클래스에서 메소드로 구현할 필요가 있습니다.\n먼저 requests 모듈의 Session 클래스를 상속받는 NaverLogin 클래스를 정의합니다.\nNaverLogin은 네이버 ID와 비밀번호를 초기화하는 단순한 기능만을 구현했지만\nrequests.Session 클래스를 상속받았기 때문에\n웹페이지 요청과 관련된 다양한 기능을 가지고 있습니다.\nCopy python class NaverLogin(requests.Session): def __init__(self, userid: str, passwd: str, **kwargs): super().__init__(**kwargs) self.userid = userid self.passwd = passwd 그리고 NaverLogin을 상속받는 SmartstoreLogin 클래스를 정의합니다.\n일반적인 네이버 로그인과 스마트스토어센터에서 진행되는 네이버 로그인이 다르기 때문에\nNaverLogin 메소드의 일부를 변경할 필요가 있을 것입니다.\nCopy python class SmartstoreLogin(NaverLogin): def __init__(self, userid=str(), passwd=str(), **kwargs): super().__init__(userid, passwd, **kwargs) 추가적으로 로그인 페이지 요청 과정에서 빈번하게 정의해야 하는 매개변수 생성을\n간단하게 할 수 있는 메소드를 정의하겠습니다.\n헤더 생성 메소드 정의 # requests 모듈은 기본적으로 헤더를 갖고 있지 않는데\n이 상태로 다수의 웹페이지에 요청을 보낸다면 로봇으로 간주당해 차단당할 것입니다.\n임의의 웹페이지에 요청을 보낼 때 확인할 수 있는 요청 헤더 HEADERS를 기본 바탕으로,\n웹페이지 별로 최적화된 헤더를 생성하는 get_headers() 메소드를 정의합니다.\nCopy python HEADERS = { \u0026#34;Accept\u0026#34;: \u0026#34;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\u0026#34;, \u0026#34;Accept-Encoding\u0026#34;: \u0026#34;gzip, deflate, br\u0026#34;, \u0026#34;Accept-Language\u0026#34;: \u0026#34;ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\u0026#34;, \u0026#34;Connection\u0026#34;: \u0026#34;keep-alive\u0026#34;, \u0026#34;sec-ch-ua\u0026#34;: \u0026#39;\u0026#34;Chromium\u0026#34;;v=\u0026#34;106\u0026#34;, \u0026#34;Google Chrome\u0026#34;;v=\u0026#34;106\u0026#34;, \u0026#34;Not;A=Brand\u0026#34;;v=\u0026#34;99\u0026#34;\u0026#39;, \u0026#34;sec-ch-ua-mobile\u0026#34;: \u0026#34;?0\u0026#34;, \u0026#34;sec-ch-ua-platform\u0026#34;: \u0026#39;\u0026#34;Windows\u0026#34;\u0026#39;, \u0026#34;Sec-Fetch-Dest\u0026#34;: \u0026#34;empty\u0026#34;, \u0026#34;Sec-Fetch-Mode\u0026#34;: \u0026#34;cors\u0026#34;, \u0026#34;Sec-Fetch-Site\u0026#34;: \u0026#34;same-origin\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\u0026#34;, } Copy python from urllib.parse import urlparse class NaverLogin(requests.Session): def get_headers(self, authority=str(), referer=str(), cookies=str(), host=str(), **kwargs) -\u0026gt; Dict[str,Any]: headers = HEADERS.copy() if authority: headers[\u0026#34;Authority\u0026#34;] = urlparse(authority).hostname if host: headers[\u0026#34;Host\u0026#34;] = urlparse(host).hostname if referer: headers[\u0026#34;Referer\u0026#34;] = referer if cookies: headers[\u0026#34;Cookie\u0026#34;] = cookies return dict(headers, **kwargs) 호스트명을 의미하는 Authority 또는 Host, 리다이렉트 전 경로를 의미하는 Referer,\n그리고 쿠키를 의미하는 Cookie 등의 값은 수시로 변하기 때문에 별도의 입력으로 지정합니다.\n쿠키 생성 메소드 정의 # 헤더와 함께 활용되는 쿠키는 헤더와 마찬가지로 웹페이지 요청 시 빈번히 활용되는데\nrequests 모듈의 쿠키 자료형인 RequestsCookieJar를 헤더에 직접 포함시킬 수 없기 때문에,\n쿠키를 적절한 형태의 문자열로 변환하는 get_cookies() 메소드를 정의합니다.\nCopy python from requests.cookies import RequestsCookieJar class NaverLogin(requests.Session): def get_cookies(self, **kwargs) -\u0026gt; str: return self.parse_cookies(dict(self.cookies, **kwargs)) def parse_cookies(self, cookies: RequestsCookieJar) -\u0026gt; str: return \u0026#34;; \u0026#34;.join([str(key)+\u0026#34;=\u0026#34;+str(value) for key,value in cookies.items()]) 마치며 # 이번 게시글에서는 두 가지 데이터 수집 방식을 예시를 통해 알아보았고\n스마트스토어센터 로그인의 바탕이 되는 클래스와 메소드를 정의했습니다.\n다음 게시글에서는 네이버 로그인을 본격적으로 구현해보겠습니다.\n"},{"id":16,"href":"/blog/2022-11-13/","title":"2022년 11월 13일 회고","section":"Posts","content":"데이터 분석가로 업무를 수행한지 두 달이 되었습니다.\n신입으로서 사수 없이 다양한 과제를 수행하다보니 다사다난했던 2개월이었습니다.\nUI # 초기엔 Airflow와 태블로를 위주로 데이터 수집 및 분석을 계획했지만,\n네이버 등 대상 사이트 크롤링을 구현하는데 소비하는 시간이 크다보니\n서버를 통한 체계적인 자동화를 구현할 여유가 없었습니다.\n대신, Streamlit과 PyInstaller를 통해 UI를 구성하고\n각각의 메뉴와 엑셀 설정 파일을 정의하여 자동화 서비스를 구현했습니다.\n개발 환경이 짜여짐을 전제로 프로그램을 제작했던 이전과는 정반대로\n아무것도 없는 윈도우 환경에서도 돌아가는 프로그램을 만들려다보니 적잖은 고민을 했습니다.\n밑바닥부터 UI를 개발하려 했으면 그 자체로 많은 시간이 걸렸겠지만,\n다행히 이전 교육 과정에서 활용해본 Streamlit을 도입하면서\nUI에 대한 걱정을 일체하지 않고 크롤링 기능만을 구현할 수 있었습니다.\n하지만, Streamlit도 마냥 편하게 사용할 수는 없었던 것이,\n무료 배포 서비스의 제한된 자원 때문에 크롤링이 안정적이지 못했고\n여러 사람이 하나의 서버에 붙다보어 크롤링이 진행되다 보니\n대상 사이트로부터 차단당하는 경우도 있었습니다.\n더욱이 Streamlit의 배포 서버가 해외 소재였기 때문에\n쿠팡 크롤링 또는 그외 사이트의 로그인 등이 해외 IP 차단의 이유로 원활하게 이루어지지 않았습니다.\n결국 로컬 환경에서 돌아가는 실행 프로그램이 필요했고,\n초기에는 개인적으로 활용하던 Bittorrent Web 프로그램처럼\nStreamlit을 로컬 브라우저에서 실행시키는 방안을 구상했지만,\n생각만큼 쉽지 않아 단순히 크롤링 기능을 실행 프로그램으로 변환하는 것으로 타협했습니다.\n사용자 편의성을 고려했을 때 GUI가 포함된 실행 프로그램이 좋을 수 있지만,\n윈도우 작업 스케줄러를 통해 자동으로 돌아가는 방식을 고려 중에 있었기 때문에\n최대한 사람 손을 거치지 않게 모든 설정을 엑셀 파일로 제어하게 하고\n터미널에서 진행상황만 표시하도록 개발했습니다.\n향후 Airflow와 같은 웹 서버 설계를 기대하지만,\n현재로서는 위와 같은 실행 방식이 가장 개발 효율적이라 생각하여 진행 중입니다.\n크롤링 # 이전까지 배운 크롤링은 HTML을 파싱하는 굉장히 단순한 수준이었고\n파이썬 자체를 깊이 다뤄볼 일도 없었기에 requests 라이브러리로 크롤링을 시도했습니다.\n하지만, 1주 정도가 크롤링 기능을 개발하면서 유지보수 대비 및 시간 개선의 필요성을 느끼고\n다른 방안을 탐색했습니다.\n당시부터 지금까지 집중하고 있던 것이 네이버 크롤링이었는데,\n우연히 네이버 내 데이터는 Open API와는 별개의 내부 API로 전달되는 것을 파악했습니다.\n이때부터 크롬 개발자 도구의 네트워크 탭에서 모든 데이터의 소스를 파악했고\n네이버 쇼핑 한정해서는 보이는 모든 데이터를 가져올 수 있는 기반을 다졌습니다.\n또한, 시간 개선을 위해 멀티 쓰레딩 방식을 알아보았고,\n이와 유사한 비동기 방식이 파이썬에서 가장 효율적임을 인식하고\nrequests로 이루어진 코드를 asyncio 및 aiohttp로 변경했습니다.\n이 과정에서 Scrapy의 Spider 및 Pipeline 구조를 참고해\n기존의 함수 위주의 코드도 Spider, Parser, Pipeline으로 구분된 클래스 위주로 변환했습니다.\n초기보다 훨씬 다양한 크롤링 기능이 생긴 지금도 위와 같은 구조 덕분에\n코드 관리 및 재활용을 편리하게 수행하고 있습니다.\n한편, 네이버 외에 이지어드민이라는 종합 쇼핑몰 관리 솔루션 내에서\n데이터를 가져오는 과정에서 로그인을 requests로 처리하기 위해 많은 자료를 참고했습니다.\n그 중에서 가장 도움이 되었던 것은 네이버 로그인을 requests로 구현한 네이버 블로그 자료였는데,\n해당 내용을 통해 POST 요청 및 RSA 암호화에 대해 이해할 수 있었습니다.\n덕분에 개발자 도구 네트워크 탭에서 로그인 과정을 역추적하는 식으로\n이지어드민과 11번가 셀러오피스의 requests 로그인을 구현할 수 있게 되었고,\n네이버 스마트스토어센터를 위한 2차 인증 또한 다소의 시간이 걸렸지만 해결할 수 있었습니다.\n특히 네이버 2차 인증을 구현하면서 세션과 쿠키에 대한 개념을 잡을 수 있었습니다.\n업무와는 별개로 개인적으로 크롤링에 고전하고 있는 jQuery 기반의 웹사이트가 있는데,\n이런 특수한 경우를 제외하고는 대부분의 웹사이트 크롤링에 자신이 들었습니다.\n또한, 여러 사이트의 소스를 뜯어보면서 자바스크립트에 대한 흥미가 생겼습니다.\n대시보드 # 한달 전쯤, 외부 업체를 통해 외주를 맡기면서 구글 데이터 스튜디오 교육을 들었는데,\n최근들어서는 이를 활용한 대시보드 제작에 개발하는 것보다 많은 시간을 쓰고 있습니다.\n대시보드 제작 자체는 이미 예상된 일이었기 때문에 태블로를 공부한 이력이 있는데,\n예상과 다른 BI 툴을 사용하게 되었습니다.\n태블로와 비교했을 때 구글 데이터 스튜디오는 무료라는 장점과 함께\n구글 플랫폼 내에서 쉽게 온라인으로 접근할 수 있다는 편의성이 있지만,\n태블로 대비 많이 부족한 기능과 투박한 그래프 디자인의 단점이 있습니다.\n태블로의 Fixed LOD나 툴팁 커스터마이징과 같은 기능이 없는 것이 다소 답답하지만,\n최대 장점인 구글 플랫폼 내 연동성의 장점을 살려 순위 비교와 같은 설정을\n구글 시트 내에서 구현하고 구글 데이터 스튜디오 내에 필드로 연동시키는 방식을 활용했습니다.\n덕분에 구글 시트의 ARRAYFORMULA 등의 함수에 익숙해지게 되었고,\n구글 시트의 매크로라고 할 수 있는 앱 스크립트도 조작해보았습니다.\n앱 스크립트의 경우 자바스크립트 기반이기 때문에 크롤링 과정에서 느낀\n자바스크립트에 대한 흥미를 가지고 재밌게 활용했습니다.\n구글 데이터 스튜디오도 1주 이상 사용해보면서 업무현황을 관리하거나\n매출 등을 참고할 수 있는 다양한 대시보드를 제작할 수 있게 되었습니다.\n간단한 것은 테이블 계산식으로, 어려운 것은 구글 시트로 떠넘기다보니\n태블로 대비 부족한 기능의 단점을 어느정도 보완할 수 있었습니다.\n회사 차원에서는 향후 대시보드와 연계할 데이터 적재를 위해 빅쿼리 도입을 고려하고 있는데,\n당장의 저는 구글 시트 API를 통한 업로드를 통해 다음 발전에 대비하고 있습니다.\n앞으로 # 이 글을 작성하고 있는 시점의 저는 한 주 가까이 감기에 걸린 것과 함께\n현재 진행 중인 업무가 대부분 마무리되면서 무료한 상태를 보내고 있습니다.\n밤낮없이 하루종일 일만 했던 이전과 대비해서 편해진 것은 있지만,\n갑작기 할일이 줄어든 것과 함께 감기로 인한 심경의 변화로 다소 의욕이 줄어들었습니다.\n슬럼프보다는 약한 수준의 일시적인 현상이라고 생각하지만,\n향후 흥미를 돋굴 새로운 일거리를 찾지 못한다면 좋지 못한 방향으로 갈 것이라 생각합니다.\n개인적인 차원에서 평소 기획하고만 있던 크롤링을 개발하고는 있지만,\n이보다 더욱 발전해서 팀 단위의 사이드 프로젝트를 기대하고 있습니다.\n야근의 불확실성도 많이 개선되었기에 이제는 개발자끼리의 스터디를 통해\n기술적 발전과 네트워킹을 같이 챙겨보려 합니다.\n"},{"id":17,"href":"/blog/datacamp-tableau/","title":"[DataCamp] Tableau Courses","section":"Posts","content":"가짜연구소에서 지원하는 Data Science Fellowship(DSF) 프로그램을 통해\n개인 스터디 멤버의 권한으로 1년 동안 이용가능한 DataCamp 라이센스를 획득하였습니다.\nDataCamp는 데이터 직군과 관련된 300개 이상의 강좌를 제공하는 플랫폼으로,\n저는 초기에 데이터 엔지니어링 기술을 목적으로 해당 프로그램에 지원했지만\n갑작스럽게 태블로를 활용해야할 필요가 생겨 관련된 트랙 과정을 수강하고 있습니다.\n제가 현재 수강하고 있는 트랙은 Tableau Fundamentals이며,\n각각의 과정을 수료하고 학습 내용, 후기 등을 정리할 예정입니다.\n진도가 진행됨에 따라 해당 게시글을 업데이트할 것이며,\n향후 SQL, 데이터 사이언스, 데이터 엔지니어링 등의 트랙도 계획하고 있습니다.\n아래 제목과 연결되는 링크에 접속하기 위해선 DataCamp 회원가입 및 로그인이 필요합니다.\nIntroduction to Tableau # Introduction to Tableau 과정은 태블로 시각화 및 대시보드 구성에 이르는 전반적인 기능을\n간단하게 이해하고 다뤄보기 위한 과정으로, 아래 4개의 챕터로 구성되어 있습니다.\nGetting Started with Tableau Building and Customizing Visualizations Digging Deeper Presenting Your Data 윈도우 가상 환경 상에서 미리 준비된 태블로 파일을 활용해 필터, 테이블 계산 등을 따라해보고\n지도, 시계열 등 다양한 종류의 데이터를 다뤄볼 수 있었습니다.\n마지막 챕터에서는 특히 게임 플랫폼 별 매출 데이터를 활용해 대시보드 및 액션을 구현해보고\n시트, 대시보드, 스토리의 차이에 대해 알아볼 수 있었습니다.\n짧은 강의를 수강한 후 실제 태블로 프로그램 상에서 바로 실습을 수행해볼 수 있기 때문에\n지루하지 않으면서 쉽게 태블로의 기능을 익힐 수 있었습니다.\n불특정 다수에게 제공되는 가상 환경이라는 특성 상 조작에 다소 딜레이를 느꼈지만,\n강의를 듣고 즉각적으로 활용해볼 수 있게 하는 경험은 매우 새로웠습니다.\nAnalyzing Data in Tableau # Analyzing Data in Tableau 과정은 전문적인 수준에서\n데이터 분석 및 시각화 기술을 익히기 위한 과정으로, 아래 4개의 챕터로 구성되어 있습니다.\nPreparing for Analysis Exploring Visualizations Mapping Analysis Groups, Sets, and Parameters 학습 중\n"},{"id":18,"href":"/blog/hugo-blog-3/","title":"Hugo 블로그 만들기 (3) - 테마 커스터마이징","section":"Posts","content":"Hugo 블로그 만들기 (3) - 테마 커스터마이징 # 블로그를 구성할 때 기술적, 시간적 한계 때문에 이미 만들어진 테마를 사용하게 됩니다.\n제가 Hugo 블로그를 만들 때도 이러한 문제 때문에 PaperMod 테마를 사용했지만,\n블로그를 보다보면 만족스럽지 못한 부분이 발견됩니다.\n이번 포스트에서는 제가 PaperMod 테마를 커스터마이징한 과정을 안내해드리겠습니다.\nArchive, Search 추가하기 # PaperMod 테마를 가져오면서 가장 신경쓰였던 부분은\n메인 메뉴가 Categories, Tags 두 개 뿐이었단 점입니다.\nArchive는 그렇다쳐도 Search 기능은 빼먹을 수 없는 부분이라 생각하기 때문에,\nHugo 및 PaperMod 내 이슈를 참고하여 관련된 내용을 탐색했습니다.\n다행히 PaperMod 테마에서 해당 기능을 연결하지 않았을 뿐,\n기능에 대한 레이아웃은 존재하기 때문에 content/ 디렉토리 아래 다음과 같은 파일을 추가했습니다.\nCopy yaml # content/archive.md --- title: \u0026#34;Archive\u0026#34; layout: \u0026#34;archives\u0026#34; url: \u0026#34;/archive\u0026#34; summary: \u0026#34;archive\u0026#34; --- Copy yaml # content/search.md --- title: \u0026#34;Search\u0026#34; layout: \u0026#34;search\u0026#34; url: \u0026#34;/search\u0026#34; summary: \u0026#34;search\u0026#34; --- 추가로, 설정에서도 해당 파일을 인식해야되기 때문에 다음과 같은 설정을 추가했습니다.\npost/ 외에 다른 디렉토리를 등록하고 싶은 경우에도 해당 키값을 활용할 수 있습니다.\nCopy yaml params: mainsections: [\u0026#34;page\u0026#34;, \u0026#34;post\u0026#34;, \u0026#34;archive\u0026#34;, \u0026#34;search\u0026#34;] 마지막으로, 메인 메뉴에서 해당 링크로 이동하기 위한 바로가기를 추가했습니다.\n여기에는 카테고리, 태그 등이 있을건데 weight 값을 통해 적절하게 위치를 조정할 수 있습니다.\nCopy yaml menu: main: - identifier: archive name: Archive url: /archive/ weight: 10 - identifier: search name: Search url: /search/ weight: 20 위와 같은 과정을 통해 Archive, Search 기능을 추가했습니다.\n검색 엔진 등록하기 # 검색 엔진에 등록하기 위한 과정은 해당 영상을 참고해주시기 바랍니다.\n저는 위 과정에서 블로그 내에 추가해야 할 Site Verification Tag를 추가하는 법을 전달드리겠습니다.\nPaperMod 테마에서는 아래처럼 해당 부분이 만들어져 있기 때문에 크게 걱정할 필요는 없습니다.\n아래는 layouts/partials/ 내에 head.html 파일에서 가져왔습니다.\nCopy html {{- if site.Params.analytics.google.SiteVerificationTag }} \u0026lt;meta name=\u0026#34;google-site-verification\u0026#34; content=\u0026#34;{{ site.Params.analytics.google.SiteVerificationTag }}\u0026#34;\u0026gt; {{- end }} {{- if site.Params.analytics.yandex.SiteVerificationTag }} \u0026lt;meta name=\u0026#34;yandex-verification\u0026#34; content=\u0026#34;{{ site.Params.analytics.yandex.SiteVerificationTag }}\u0026#34;\u0026gt; {{- end }} {{- if site.Params.analytics.bing.SiteVerificationTag }} \u0026lt;meta name=\u0026#34;msvalidate.01\u0026#34; content=\u0026#34;{{ site.Params.analytics.bing.SiteVerificationTag }}\u0026#34;\u0026gt; {{- end }} {{- if site.Params.analytics.naver.SiteVerificationTag }} \u0026lt;meta name=\u0026#34;naver-site-verification\u0026#34; content=\u0026#34;{{ site.Params.analytics.naver.SiteVerificationTag }}\u0026#34;\u0026gt; {{- end }} 구글, 네이버 외에 Bing, Yandex를 지원하며 저는 다음과 같이 구글과 네이버만 설정했습니다.\nCopy yaml params: analytics: google: SiteVerificationTag: \u0026lt;YOUR-VERIFICATION-TAG\u0026gt; naver: SiteVerificationTag: \u0026lt;YOUR-VERIFICATION-TAG\u0026gt; 번외로 Google Tag 등 head에 추가로 입력할 부분이 있다면,\n동일한 위치에 extend_head.html을 사용할 수 있습니다.\n아래는 제가 extend_head.html 내에 Google Tag를 위한 스크립트를 추가한 부분입니다.\nCopy html {{- if site.GoogleAnalytics }} {{- /* Google tag (gtag.js) */}} \u0026lt;script async src=\u0026#34;https://www.googletagmanager.com/gtag/js?id={{ site.GoogleAnalytics }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\u0026#39;js\u0026#39;, new Date()); gtag(\u0026#39;config\u0026#39;, \u0026#39;{{ site.GoogleAnalytics }}\u0026#39;); \u0026lt;/script\u0026gt; {{- end }} KaTex 추가하기 # KaTex는 웹에서 수식을 표현하기 위한 방식입니다.\n제 과거 게시글엔 KaTex 표기법을 사용한 것이 존재하는데 이것이 제대로 표시되지 않는 문제를 발견했습니다.\n저는 공식 문서 대신 Stack Overflow 등을 참고해 아래 코드를 extend_head.html에 추가했는데,\n아쉽게도 출처는 남겨두지 못했습니다.\nCopy html \u0026lt;script\u0026gt; MathJax = { tex: { inlineMath: [[\u0026#39;$\u0026#39;, \u0026#39;$\u0026#39;], [\u0026#39;\\\\(\u0026#39;, \u0026#39;\\\\)\u0026#39;]], displayMath: [[\u0026#39;$$\u0026#39;,\u0026#39;$$\u0026#39;], [\u0026#39;\\\\[\u0026#39;, \u0026#39;\\\\]\u0026#39;]], processEscapes: true, processEnvironments: true }, options: { skipHtmlTags: [\u0026#39;script\u0026#39;, \u0026#39;noscript\u0026#39;, \u0026#39;style\u0026#39;, \u0026#39;textarea\u0026#39;, \u0026#39;pre\u0026#39;] } }; window.addEventListener(\u0026#39;load\u0026#39;, (event) =\u0026gt; { document.querySelectorAll(\u0026#34;mjx-container\u0026#34;).forEach(function(x){ x.parentElement.classList += \u0026#39;has-jax\u0026#39;}) }); \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://polyfill.io/v3/polyfill.min.js?features=es6\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; id=\u0026#34;MathJax-script\u0026#34; async src=\u0026#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Cover 간편하게 지정하기 # 저는 Github 저장소 내에 업로드한 이미지 주소를 속성값에 연결해 블로그 이미지를 표시하는데,\n게시글을 작성할 때마다 지정하게 되는 Cover 이미지의 경우 매번 전체 링크를 지정하는게 불편했습니다.\n대표적으로 해당 게시글의 Cover 이미지 주소는 다음과 같습니다.\nCopy html https://github.com/minyeamer/til/blob/main/.media/covers/hugo-logo.png?raw=true 저는 여기서 hugo-logo.png를 제외한 앞뒤의 요소가 불필요하다는 것을 인식했고\n설정 파일에 다음과 같이 prefix, suffix라는 키값으로 지정하게 처리했습니다.\nCopy yaml params: cover: prefix: \u0026#34;https://github.com/minyeamer/til/blob/main/.media/covers/\u0026#34; suffix: \u0026#34;?raw=true\u0026#34; 그리고 해당 설정을 적용시키기 위해 실질적으로 Cover 이미지를 표시하는 layouts/partials/ 아래 cover.html 파일을 수정했습니다.\n주석으로 지정된 부분이 원본이며, image 키값의 앞뒤로 prefix와 suffix를 덧붙였습니다.\nCopy html \u0026lt;!-- {{- if $addLink }}\u0026lt;a href=\u0026#34;{{ (.Params.cover.image) | absURL }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34;\u0026gt;{{ end -}} \u0026lt;img loading=\u0026#34;lazy\u0026#34; src=\u0026#34;{{ (.Params.cover.image) | absURL }}\u0026#34; alt=\u0026#34;{{ $alt }}\u0026#34;\u0026gt; --\u0026gt; {{- if $addLink }}\u0026lt;a href=\u0026#34;{{ if site.Params.cover.prefix }}{{ site.Params.cover.prefix }}{{ end }}{{ .Params.cover.image }}{{ if site.Params.cover.suffix }}{{ site.Params.cover.suffix }}{{ end }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34;\u0026gt;{{ end -}} \u0026lt;img loading=\u0026#34;lazy\u0026#34; src=\u0026#34;{{ if site.Params.cover.prefix }}{{ site.Params.cover.prefix }}{{ end }}{{ .Params.cover.image }}{{ if site.Params.cover.suffix }}{{ site.Params.cover.suffix }}{{ end }}\u0026#34; alt=\u0026#34;{{ $alt }}\u0026#34;\u0026gt; 기타 설정 # 너비 설정 # 초기에 PaperMod 테마를 사용할 때 너비가 좁아 불편한 느낌이 있었습니다.\n해당 설정은 css 파일로 지정할 것이라 생각했고,\nassets/css/core/ 경로에 있는 theme-vars.css 파일을 발견해 다음과 같이 수정했습니다.\n기존 720px에서 900px로 늘어나 쾌적하게 블로그를 볼 수 있게 되었습니다.\nCopy css :root { --main-width: 900px; 새 탭에서 링크 열기 # 다음으로 관심을 가진 건 깃허브에서 매번 불편하게 생각했던 링크 오픈 방식인데,\n개인적으로는 현재 탭이 아닌 새 탭에서 열리는 방식을 선호하기 때문에 해당 부분의 수정이 필요했습니다.\n다행히 Hugo 이슈 내용 중 다음과 같은 답변을 참고해 파일을 추가했습니다.\n아래는 layouts/_default/_markup/ 경로에 추가한 render-link.html 파일입니다.\nCopy html \u0026lt;a href=\u0026#34;{{ .Destination | safeURL }}\u0026#34;{{ with .Title}} title=\u0026#34;{{ . }}\u0026#34;{{ end }}{{ if strings.HasPrefix .Destination \u0026#34;http\u0026#34; }} target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34;{{ end }}\u0026gt;{{ .Text | safeHTML }}\u0026lt;/a\u0026gt; 포스트 수정 # 마지막으로 포스트 수정 버튼에 문제를 인식했습니다.\n물론, 모든 포스트는 로컬에서 작성하고 수정하지만, 오류가 발생하는 버튼을 그냥 놔둘 수는 없습니다.\nGo에 대해 잘 알지 못해 최선의 기능이라고 생각하지는 않지만,\n검색을 통해 발견한 replace 함수를 사용해 기존 경로에서 오류를 일으키는 부분을 제거했습니다.\nCopy html {{- if or .Params.editPost.URL site.Params.editPost.URL -}} {{- $fileUrlPath := path.Join .File.Path }} {{- if or .Params.author site.Params.author (.Param \u0026#34;ShowReadingTime\u0026#34;) (not .Date.IsZero) .IsTranslated }}\u0026amp;nbsp;|\u0026amp;nbsp;{{- end -}} \u0026lt;a href=\u0026#39;{{ .Params.editPost.URL | default site.Params.editPost.URL }}{{ if .Params.editPost.appendFilePath | default ( site.Params.editPost.appendFilePath | default false ) }}/{{ replace $fileUrlPath site.Params.editPost.ignoreFilePath \u0026#34;\u0026#34; 1 }}{{ end }}\u0026#39; rel=\u0026#34;noopener noreferrer\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt; {{- .Params.editPost.Text | default (site.Params.editPost.Text | default (i18n \u0026#34;edit_post\u0026#34; | default \u0026#34;Edit\u0026#34;)) -}} \u0026lt;/a\u0026gt; {{- end }} 개선사항 # 현재 PaperMod 테마의 카테고리는 아래 그림처럼 태그와 동일한 리스트 템플릿을 사용하는데,\n개인적으로는 트리 형태의 계층식 카테고리를 선호합니다.\n언제나처럼 PaperMod 이슈를 탐색하던 중 해당 이슈를 발견했는데,\n아래 그림처럼 제가 머릿속에 그리던 방식을 그대로 표현하여 큰 관심을 가졌습니다.\n해당 기능을 구현한 분께 메일을 보내 참고 자료를 얻었지만,\n아직까진 시간적 여유가 부족해 해당 작업을 처리하지 못한 상태입니다.\n향후 개선되기를 희망하는 부분입니다.\n마치며 # Hugo 블로그 만들기 시리즈의 마지막으로 커스터마이징 과정을 소개했습니다.\n커스터마이징은 그때그때 필요하다고 생각하는 부분을 수정하는 것이기 때문에\n본인의 입맛에 맛는 블로그를 만들기 위해서는 테마의 구조를 이해해야 합니다.\n아직 Go에 대해서도 잘 몰라 검색을 통해 요령껏 찾아내는 수준이지만,\nGo에 익숙해지게 된다면 동적 TOC 등 기능의 개선을 기대해 볼 수 있을 것입니다.\n해당 게시글을 통해 Hugo 블로그 만들기에 도움이 되었으면 좋겠습니다.\n참고 자료 # EP09. 구글, 네이버 검색엔진 등록하기 KaTex Simple way to open in a new tab [Feature][Discussion] Tree-style category list page "},{"id":19,"href":"/blog/hugo-blog-2/","title":"Hugo 블로그 만들기 (2) - Utterances 댓글 적용","section":"Posts","content":"Hugo 블로그는 기본적으로 댓글 기능을 제공하지는 않습니다.\n제가 사용하는 PaperMod 테마에서는 서드파티 서비스인 Disqus를 위한 레이아웃이 존재하지만,\n저는 기본적인 블로그 운영을 Github 플랫폼 내에서 구성하고 싶기 때문에 다른 기능을 사용해보려 합니다.\n이번 포스트에서는 Utterances 댓글 기능을 추가하는 방법을 안내해드리겠습니다.\nUtterances 설치하기 # Utterances는 Github issues 기반으로 댓글을 관리하는 기능입니다.\n무료 플랜에서 광고가 붙는 Disqus와 다르게 별도의 유료 플랜이 없어 간편하게 사용할 수 있습니다.\nUtterances 설치는 단순히 레이아웃 상에서 댓글이 위치할 곳에 자바스크립트 코드를 삽입하면 됩니다.\n하지만, 선행적으로 해당 링크를 통해 Utterances와 연동시킬 저장소를 등록해야 합니다.\n무료 플랜 선택 후 Utterances를 적용할 저장소를 선택하게 되는데\n모든 저장소를 지정해도 되지만, 저는 댓글을 관리할 저장소만 지정하겠습니다.\n간단하게 Utterances 적용이 완료되면 아래 공식 문서 페이지로 이동합니다.\nhttps://utteranc.es/ 공식 문서에서 저장소 이름, 이슈 맵핑 방식 등을 지정하면 해당하는 스크립트가 생성됩니다.\n저는 포스트 제목이 변경될 수 있기 때문에 pathname을 기준으로 이슈를 생성하고,\n사용자 시스템 설정에 호환되는 Preferred Color Scheme 테마를 사용합니다.\nCopy html \u0026lt;script src=\u0026#34;https://utteranc.es/client.js\u0026#34; repo=\u0026#34;[ENTER REPO HERE]\u0026#34; issue-term=\u0026#34;pathname\u0026#34; theme=\u0026#34;github-light\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; 스크립트 삽입하기 # PaperMod 테마에는 layouts/partials/ 위치에 comments.html이라는 레이아웃이 존재합니다.\n테마 별로 레이아웃이 다르기 때문에 다른 테마의 경우 이슈 등을 참고하여 구조를 파악할 필요가 있습니다.\nCopy html {{- /* Comments area start */ -}} {{- /* to add comments read =\u0026gt; https://gohugo.io/content-management/comments/ */ -}} {{- if $.Site.Params.utteranc.enable -}} \u0026lt;script src=\u0026#34;https://utteranc.es/client.js\u0026#34; repo=\u0026#34;{{ .Site.Params.utteranc.repo }}\u0026#34; issue-term=\u0026#34;{{ .Site.Params.utteranc.issueTerm }}\u0026#34; {{- if $.Site.Params.utteranc.label -}}label=\u0026#34;{{ .Site.Params.utteranc.label }}\u0026#34;{{- end }} theme=\u0026#34;{{ .Site.Params.utteranc.theme }}\u0026#34; crossorigin=\u0026#34;{{ .Site.Params.utteranc.crossorigin }}\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; {{- end }} {{- /* Comments area end */ -}} 단순하게 레이아웃에 스크립트를 붙여넣어도 되지만,\n향후 속성값을 변경하기 위해 불필요하게 테마를 수정하는 경우를 방지하기 위해\n설정 파일을 통해 동적으로 속성값을 집어넣도록 설정했습니다.\nHugo HTML 코드 내에 이중 중괄호({{ }})는 Go 템플릿을 코딩하는 부분으로,\n아래와 같은 설정 파일을 읽어서 각각의 키에 해당하는 값을 할당합니다.\n이에 대한 자세한 사용법은 Hugo 공식 문서를 참조할 수 있습니다.\nCopy yaml params: utteranc: enable: true repo: \u0026#34;minyeamer/minyeamer.github.io\u0026#34; issueTerm: \u0026#34;pathname\u0026#34; label: \u0026#34;comments\u0026#34; theme: \u0026#34;preferred-color-scheme\u0026#34; crossorigin: \u0026#34;anonymous\u0026#34; 정상적으로 스크립트가 삽입되었다면 아래와 같이 댓글을 입력하는 부분이 표시됩니다.\n댓글 기능이 정상적으로 적용되는지 확인하기 위해 실험적으로 댓글을 작성해봅니다.\n저도 과거 게시글에 댓글을 작성하여 아래와 같이 올라온 이슈를 확인했습니다.\n마치며 # Hugo 블로그를 통한 소통을 기대하여 댓글 기능을 추가해보았습니다.\n생각보다 간단하기 때문에 깃허브 블로그를 꾸미면서 댓글 기능을 희망하시는 분들이라면\nUtterances를 적극 활용해보시기를 추천드립니다.\n마지막 포스트로는 PaperMod 테마를 수정한 과정을 안내해드리겠습니다.\nHugo 테마끼리 공통적인 부분이 있기 때문에 다른 테마를 사용하시더라도 도움이 될 것입니다.\n참고 자료 # Utterances Documents Introduction to Hugo Templating "},{"id":20,"href":"/blog/hugo-blog-1/","title":"Hugo 블로그 만들기 (1) - Hugo 기본 구성","section":"Posts","content":"얼마 전, 티스토리 블로그에서 Jekyll 블로그로 이동했는데,\n처음 기대했던 submodule을 활용한 효율적인 저장소 연동에서 어려움을 겪고 다른 대안을 탐색하게 되었습니다.\nJekyll 블로그를 사용함에 있어서, Ruby 언어로 구성된 블로그 구조에 대해 이해하기 어려운데다가\n로컬 환경에서 Jekyll 블로그를 실행하면서 발생하는 에러를 처리하는데도 난항을 겪었는데,\n웹상에서 자동 배포가 이루어지는 과정에서 submodule인 TIL 저장소를 포스트로 인식하지 못하는 문제가 있었습니다.\nJekyll 블로그의 대안으로 Hexo 및 Hugo 프레임워크에 주목했고,\n두 제품의 장단점을 비교하여 상대적으로 배포가 빠르고 현재까지도 업데이트가 이루어지는 Hugo를 선택했습니다.\n이번 포스트에서는 제가 Hugo 블로그를 구성한 과정을 간략한 설명과 함께 안내해드리겠습니다.\n테마 선택하기 # 블로그의 모든 페이지 레이아웃을 만들 계획이 아니라면 블로그 선택에 있어 테마 선정이 필요합니다.\nHugo는 아래 페이지에서 다양한 테마를 제공하며, 태그를 통해 블로그 외에도 목적에 맞는 테마를 찾아볼 수 있습니다.\n미리보기만으로 알기 어렵다면 제작자가 제공하는 데모 사이트를 방문해볼 수 있고,\n아래 안내드릴 Hugo 설치를 통해 로컬에서 exampleSite를 확인해 볼 수도 있습니다.\nhttps://themes.gohugo.io/ Jekyll 블로그를 사용했을 당시 적용했던 Chirpy 테마는 사이드 메뉴, 계층식 카테고리, 동적 TOC 등\n제가 추구하는 모든 기능을 가지고 있었는데, Hugo에는 저의 취향을 완벽히 만족시키는 테마가 없었습니다.\n그나마 괜찮았던 LoveIt 테마의 경우 설정 곳곳에 중국어가 포함되어 있어 이해하기 어렵겠다는 생각이 들었습니다.\n결국, 저는 모든 테마를 둘러본 후 다루기 쉬워보이면서 외적으로도 괜찮았던 PaperMod 테마를 선택했습니다.\nHugo 블로그 구성하기 # 이번 Hugo 블로그 구성은 Mac 환경에서 진행되었으며, 다른 환경의 구성 방식은 제공되지 않습니다.\nHugo 설치 # Mac 사용자라면 Homebrew를 통해 쉽게 Hugo를 설치하여 사용할 수 있습니다.\n터미널에 아래 명령어를 입력해 설치가 가능합니다.\nCopy bash brew install hugo 설치가 완료되면, 버전 정보를 출력해서 정상 설치 여부를 확인합니다.\nCopy bash % hugo version hugo v0.102.2+extended darwin/arm64 BuildDate=unknown Github 저장소 생성 # Hugo는 원본 데이터 및 설정 파일이 포함될 공간과, 렌더링된 페이지가 저장될 공간이 필요합니다.\n일반적으로는 분리된 저장소를 통해 구현하지만, 앞서 Jekyll 블로그를 구성해보면서\n브랜치를 통해 하나의 저장소에서 두 개의 공간을 관리할 수 있을 것이라 판단했습니다.\n하나의 저장소를 main과 gh-pages, 두 개의 브랜치로 나누어 구성할 계획이며,\n우선적으로 \u0026lt;USERNAME\u0026gt;.github.io 명칭의 저장소를 생성합니다.\nHugo 프로젝트 생성 # 일반적인 웹 프레임워크에서 프로젝트를 시작하는 것처럼, Hugo에서도 기본 템플릿을 제공합니다. 아래 명령어를 통해 프로젝트를 생성할 수 있고, 이름은 자유롭게 지정해도 됩니다.\nCopy bash % hugo new site \u0026lt;NAME\u0026gt; 만들어진 프로젝트 구조는 다음과 같습니다.\n만들어진 테마를 사용한다면 대부분의 구성요소들이 themes/ 디렉토리 내에 위치하게 되며,\n포스트를 위한 content/, 이미지 등을 위한 static/ 디렉토리 외엔 거의 사용하지 않습니다.\nCopy bash . ├── archetypes │ └── default.md ├── config.toml ├── content ├── data ├── layouts ├── public ├── static └── themes 저장소 연동 # 테마를 불러오기에 앞서 git 설정이 필요합니다.\n프로젝트 디렉토리로 이동한 후, 아래 명령어를 통해 원격 저장소와 연동합니다.\nCopy bash % git init % git add . % git commit -m \u0026#34;feat: new site\u0026#34; % git branch -M main % git remote add origin https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git % git push -u origin main 추가적으로, 렌더링된 페이지가 저장되고 실질적인 배포가 이루어지는 브랜치를 생성합니다.\nCopy bash % git branch gh-pages main % git checkout gh-pages % git push origin gh-pages % git checkout main Hugo에서 페이지를 렌더링한 결과는 public/ 디렉토리에 저장되며, 이를 gh-pages 브랜치와 연결해야 합니다.\n기존에 존재하는 빈 디렉토리를 제거하고 gh-pages 브랜치를 main 브랜치의 submodule로 연결합니다.\nsubmodule에 대한 개념은 해당 영상을 참고해주시기 바라며, 단순하게 설명하자면 동기화 기능입니다.\nCopy bash % rm -rf public % git submodule add -b gh-pages https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git public % git add public % git add .gitmodules % git commit -m \u0026#34;feat: add submodule for github pages\u0026#34; % git push 테마 불러오기 # git 설정을 완료한 후, 미리 정해두었던 테마를 themes/ 디렉토리 내에 위치시킵니다.\n마찬가지로 submodule을 활용하며, 테마의 디렉토리명은 반드시 테마 설정에 명시된 것과 동일한 이름이어야 합니다.\n커스터마이징을 고려하면 원본 저장소가 아닌 별도로 fork한 저장소를 연결시키는게 좋습니다.\nCopy bash % git submodule add https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod % git add themes/PaperMod % git add .gitmodules % git commit -m \u0026#34;feat: import hugo theme\u0026#34; 만약 fork 저장소를 사용하면서 원본 저장소의 변경사항을 업데이트하고 싶다면,\n원본 저장소를 새로운 원격 저장소로 등록해 pull 작업을 수행합니다.\nCopy python git remote add upstream https://github.com/adityatelange/hugo-PaperMod git fetch upstream git merge upstream/master git commit -m \u0026#34;update: pull upstream\u0026#34; 아래는 PaperMod 테마의 디렉토리 구조입니다.\n테마를 수정할 일이 있다면 아래 구조를 참고해 필요한 파일에 접근해 볼 수 있습니다.\nCopy bash themes/PaperMod ├── LICENSE ├── README.md ├── assets │ ├── css │ │ ├── common │ │ ├── core │ │ ├── extended │ │ ├── hljs │ │ └── includes │ └── js ├── go.mod ├── i18n ├── images ├── layouts │ ├── 404.html │ ├── _default │ │ └── _markup │ ├── partials │ ├── robots.txt │ └── shortcodes └── theme.toml Hugo 설정 # Hugo 블로그 설정은 config 파일에서 지정할 수 있고, toml, yaml, json 형식을 지원합니다.\n테마를 사용할 경우 커스텀 키가 존재할 수 있어 별도의 문서를 참조하는게 좋습니다.\nHugo 공식 설정에 관한 문서와 PaperMod 설정에 관한 문서는 아래를 참고해주시기 바랍니다.\nhttps://gohugo.io/getting-started/configuration/ https://github.com/adityatelange/hugo-PaperMod/wiki/Installation 제 설정 파일의 경우 커스터마이징을 통해 호환되지 않는 키가 존재할 수 있지만,\n동일한 테마를 사용한다면 일부분을 참고해 도움을 받을 수 있을거라 기대합니다.\nHugo 배포 # Hugo는 hugo -t \u0026lt;THEMES\u0026gt; 명령어를 통해 로컬에서 페이지 렌더링을 진행할 수 있고,\n그 결과인 public/ 디렉토리 내 내용을 gh-pages에 push하여 배포를 수행합니다.\n배포에 앞서, 깃허브에서 제공하는 Github Pages가 gh-pages 브랜치를 참고하도록\n아래 그림과 같이 저장소 설정에서 빌드 및 배포 대상 브랜치를 지정해주어야 합니다.\n위와 같이 수동으로 배포할 경우 두 번의 push 과정을 거쳐야 합니다.\n매번 이 과정을 수행하는 것은 불편하기 때문에 쉘 스크립트를 작성하여 작업을 단순화합니다.\n해당 스크립트는 다른 포스트를 참고해 작성했습니다.\nCopy bash #!/bin/bash echo -e \u0026#34;\\033[0;32mDeploying updates to GitHub...\\033[0m\u0026#34; # Build the project. # hugo -t \u0026lt;your theme\u0026gt; hugo -t PaperMod # Go to public folder, submodule commit cd public # Add changes to git. git add . # Commit changes. msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; # Push source and build repos. git push origin gh-pages # Come back up to the project root cd .. # Commit and push to main branch git add . msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; git push origin main 스크립트 파일에 실행 권한을 부여하고 실행해 볼 수 있습니다.\nCopy bash % chmod 777 deploy.sh % ./deploy.sh 배포가 완료되면, https://.github.io 주소로 접속해 블로그를 확인할 수 있습니다.\n포스트 작성하기 # Hugo 포스트는 아래 명령어를 통해 생성할 수 있고,\n별도의 markdown 파일을 content/post/ 경로 내에 추가할 수도 있습니다.\nCopy bash % hugo new post/\u0026lt;FILENAME\u0026gt;.md Front Matter # 제목, 작성일자 등을 지정하기 위해 포스트 상단에 Front Matter라고 하는 토큰을 작성해야 합니다. Front Matter는 설정 파일과 동일하게 toml, yaml, json 형식을 지원하며,\nHugo 공식 문서 또는 PaperMod에서 안내하는 아래형식을 참고할 수 있습니다.\nCopy yaml --- title: \u0026#34;My 1st post\u0026#34; date: 2020-09-15T11:30:03+00:00 # weight: 1 # aliases: [\u0026#34;/first\u0026#34;] tags: [\u0026#34;first\u0026#34;] author: \u0026#34;Me\u0026#34; # author: [\u0026#34;Me\u0026#34;, \u0026#34;You\u0026#34;] # multiple authors showToc: true TocOpen: false draft: false hidemeta: false comments: false description: \u0026#34;Desc Text.\u0026#34; canonicalURL: \u0026#34;https://canonical.url/to/page\u0026#34; disableHLJS: true # to disable highlightjs disableShare: false disableHLJS: false hideSummary: false searchHidden: true ShowReadingTime: true ShowBreadCrumbs: true ShowPostNavLinks: true ShowWordCount: true ShowRssButtonInSectionTermList: true UseHugoToc: true cover: image: \u0026#34;\u0026lt;image path/url\u0026gt;\u0026#34; # image path/url alt: \u0026#34;\u0026lt;alt text\u0026gt;\u0026#34; # alt text caption: \u0026#34;\u0026lt;text\u0026gt;\u0026#34; # display caption under cover relative: false # when using page bundles set this to true hidden: true # only hide on current single page editPost: URL: \u0026#34;https://github.com/\u0026lt;path_to_repo\u0026gt;/content\u0026#34; Text: \u0026#34;Suggest Changes\u0026#34; # edit text appendFilePath: true # to append file path to Edit link --- 게시글 저장소 연동 # 저는 기존 TIL 저장소를 게시글로 활용할 예정이었기에,\ncontent/post/ 디렉토리를 TIL 저장소의 submodule로 대체했습니다.\nCopy bash % git submodule add https://github.com/minyeamer/til.git content/post/ % git add content/post/ % git add .gitmodules % git commit -m \u0026#34;feat: add til repository as post\u0026#34; 이렇게 설정했을 때 장점은 TIL 저장소에 변경사항이 발생했을 경우,\n아래와 같은 단 한 줄의 명령어로 블로그 저장소에서 업데이트할 수 있습니다.\n해당 명령어는 물론, 테마와 같은 다른 submodule에도 적용할 수 있습니다.\nCopy bash % git submodule update --remote 마치며 # Jekyll 블로그와 며칠간 씨름하다 Hugo로 이동해 기존의 목표를 달성할 수 있었습니다.\nChirpy 테마를 활용하지 못하는 것이 아쉽지만, PaperMod의 코드는 알기 쉽게 작성되어 있어\n시간적 여유만 있다면 커스터마이징에서 어려움이 없을 것이라 판단합니다.\n이번 포스트에서는 Hugo 블로그를 구성하고 포스트를 작성하는 과정을 전달했습니다.\n다음엔 Utterances 위젯을 활용해 댓글 기능을 추가하는 방법을 안내해드리겠습니다.\n참고 자료 # Hugo Documents PaperMod Documents 블로그 구축기 (1) Hugo + Github으로 개인 블로그 만들기 저장소 안에 저장소 - git submodule "},{"id":21,"href":"/blog/2022-09-07/","title":"데이터 분석가의 첫 스텝","section":"Posts","content":"서론 # 2022년 3월 코로나19로 인한 조기전역 후,\n멋쟁이사자처럼에서 운영하는 AI SCHOOL 교육 과정에 참여했습니다.\n군대에서 나태하게 보내며 공부에는 익숙하지 않았지만, 몇달 뒤 서류상으로 전역하게 되면\n결국 일자리를 구해야 했기에 평소 목표로 했던 개발 관련 교육을 들을 필요성을 느꼈고,\n흔히 말하는 국비 지원 교육인 K-Digital Training을 수강했습니다.\n비전공자도 3개월의 과정을 거쳐 딥러닝 모델이 탑재된 웹 서비스를 만들 수 있다는\n희망에 가득찬 상태로 교육을 수료했지만,\n관심있었던 NLP 분야를 중심으로 독학하면서 스스로의 부족함을 크게 실감했습니다.\n그나마 다행인 것은 교육 과정을 통해, 그리고 이후 스터디 그룹을 통해 만난 인연이 있었습니다.\n자신의 공부 진도가 어느 정도 수준인지 감이 잡히지 않는 불확실성은 독학에 있어 가장 큰 불안요소인데,\n타인의 공부 진도 및 공유된 코드 등을 확인하면서 저의 수준을 가늠할 수 있었습니다.\n다행히 하고 싶은 것만큼은 명확했기에 방향성이 흔들릴 일은 없었지만,\n머신러닝 엔지니어를 목적으로 실무적인 부분에 집중하다보니 기초 수학, 데이터 분석 기법 등\n기반지식의 부족으로 언젠가는 무너져 버릴 수 있겠다는 생각을 했습니다.\n교육 과정 수료 후 약 3개월이 다되가는 시점에서 저에게 두 가지 선택지 중 하나를 골라야 했습니다.\n첫 번째 선택지는 현재의 방식을 고수하면서 \u0026ldquo;속아줄\u0026rdquo; 회사를 찾는 것.\n두 번째 선택지는 밑바닥부터, 선형대수와 통계학부터 기반을 쌓아가는 것.\n이때, 우연한 기회가 찾아왔습니다.\n첫 면접 # 기존에 참여하던 단톡방 커뮤니티에서 평소 관심을 두던 회사의 CSO 분께 연락을 드릴 기회를 잡았습니다.\n많지는 않지만 앞선 회사 지원 과정에서 서류광탈을 느꼈던 저는 첫 서류합격을 경험했습니다.\n해당 회사는 AI 작곡 스타트업으로 저는 NLP 개발자 직무에 지원했는데,\n일반적인 NLP 과제를 기대했던 제가 받아본 과제는 도메인 특화된 새로운 과제였습니다.\n다행히 이틀의 기간 동안 논문과 코드를 뜯어보며 성공적으로 과제를 마무리했고,\n이는 제 개인적으로도 크게 성장할 수 있는 계기였습니다.\n하지만, 처음 보는 면접이었기에 도저히 감이 잡히지 않았고,\n오직 제가 경험한 프로젝트 위주로만 준비하는 실수를 저질렀습니다.\n실제 면접에서는 과제 20, 프로젝트 40, 딥러닝 지식 40으로 질문을 받았었는데,\n전혀 예상하지 못했던 질문들이었기에 제대로 답변할 수 없었습니다.\n이후, 관련된 질문들을 찾아보니 데이터 사이언스 질문 모음집에서 나온 것들이었습니다.\n사실 과제를 받아볼 때부터 느꼈던 것이지만 해당 회사는 저의 수준에 맞지 않은 뛰어난 기업이었습니다.\n우연한 기회를 놓치지 않기 위해 자신의 부족함을 감수하고 지원한 것이었는데,\n역시 예상은 빗나가지 않았습니다.\n비록 결과는 좋지 않았지만, 당시 저는 오히려 \u0026ldquo;불확실성 해소\u0026quot;를 느끼고 만족했습니다.\n그리고, 이 경험이 뒷받침 되지는 않았지만 다음 면접에서 합격할 수 있었습니다.\n취업 # 첫 면접 이후 제 수준에서는 인턴부터 시작하는 것이 맞다고 판단하여 관련된 공고에 지원했습니다.\n그리 많은 공고가 올라와 있지는 않아서 당첨을 기대하지는 않았지만,\n다행히 스포츠 브랜드 쇼핑몰을 운영하는 기업에서 데이터 분석가 직무로 연락을 주었습니다.\n연락 후 주말을 거쳐 바로 보게된 1차 면접에서는 이전 면접에서의 과오를 되풀이하지 않기 위해\n질문 리스트를 보고 하나하나 답변을 기록해가면서 준비했는데,\n실제 면접에서는 합격을 전제로 실제 진행될 업무에 대해 말씀을 주셨습니다.\n해당 기업의 경우 개발팀이란 것이 존재하지 않는 마케팅 중심 기업이었고,\n실험적으로 데이터 수집 자동화 및 시각화를 위해 관련 전문가를 채용해보는 것이었습니다.\n제가 지금까지 준비했던 것과는 다른 업무였지만, 평소 데이터 관련한 전반적인 기술에 흥미를 가지고 있었기에\n제안된 사항들을 어떻게 구현할지에 대한 방향성은 금방 잡을 수 있었습니다.\n예상대로 1차 면접을 통과하고 대표님과 대면하여 2차 면접을 진행했습니다.\n질문을 받는 것보다는 제가 질문을 드리는 방식이었고 자리에서 합격 통보를 받았습니다.\n앞으로 # 합격 통보를 듣고 현재는 향후 주어진 과제를 어떻게 구현할 것인지에 대해 고민하고 있습니다.\n크게 데이터 분석가, 데이터 엔지니어, DB 개발자의 3가지 분야의 역량을 가지고\n3개월 동안 혼자서 성과를 보여야 하는데 머신러닝 엔지니어 직무를 준비했던 제게는 굉장히 도전적인 선택입니다.\n현재 앞선 분야 별 직무와 관련해 도입을 계획하고 있는 기술은 태블로, MongoDB, Airflow 입니다.\n사실 세 가지 서비스 모두 얕은 수준에서만 사용해본게 전부라서 기술을 적용하는데 있어서 중압감을 느끼지만,\n최소한 태블로 만큼은 첫 출근까지 남은 5일의 시간 동안 마스터할 각오로 준비해야 할 것입니다.\n"},{"id":22,"href":"/blog/desktop-settings/","title":"가벼운 딥러닝용 조립PC 후기","section":"Posts","content":"구매 계기 # 매번 딥러닝 모델을 학습하는 실험을 하면서 Colab에 의존하는 방식에 불편함을 느꼈는데,\n결국 190발을 사용해서 RTX 3060이 포함된 조립 PC를 구매했습니다.\n이번 기회에 혼자서 컴퓨터를 조립해봤으면 좋았겠지만,\n여기에 많은 시간을 쏟을만한 상황도 아니었고 이쪽이 고장날 일도 없어서 맡겼습니다.\n컴퓨터 조립 업체로 다나와, 피씨팩토리 등을 고려했는데,\n현금 결제를 권장하는 부분이 미심쩍었고 컴퓨존의 평이 좋아 믿고 맡겼습니다.\n부품 선정 # 그래픽 카드 # 처음엔 약 100만원을 얹어서 RTX 3090Ti을 구매할 생각이었지만,\n당시 소득이 없는 상황이라 저렴한 가격의 RTX 3060을 구매했습니다.\nColab에서 RoBERTa 등의 모델을 돌리면서 15GB의 VRAM이 고갈되어 배치 사이즈를 줄인 경험이 있어\n상대적으로 적은 12GB VRAM의 RTX 3060 제품이 불안하긴 했습니다.\n하지만, 당장엔 큰 모델을 돌릴 예정이 없고 그래픽 카드를 교체하는게 어렵지는 않기 때문에\n향후 확장성을 고려하는 방향으로 저사양의 제품을 구매했습니다.\n파워 # 다만, 파워의 경우 교체가 쉽지 않기 때문에 향후 그래픽 카드를 교체할 것을 고려해 850W로 선택했고,\nWiFi 보드, 강화유리가 없는 케이스를 필수 요건으로 삼아\n퀘이사존을 통해 부품에 대한 정보를 탐색했습니다.\n메인보드 # 메인보드로는 인텔용 B660 보드를 선택했는데,\n구매하고 보니 썬더볼트 독과 연결할만한 고속 포트가 존재하지 않아 아쉬웠습니다.\n이점은 충분히 고려하지 못하고 결정한 문제입니다.\n케이스 # 케이스는 프랙탈 디자인 사의 Torrent Compact 제품을 구매했는데,\n강화유리가 안달린 제품들의 가격이 대체로 비싼 편이어서 이왕이면 취향에 맞는 제품으로 선택했습니다.\n기타 부품 # 게임이 목적은 아니었기 때문에 CPU는 적당한 인텔 사 제품을 골랐고,\n관리의 불편함 때문에 CPU용 쿨러로 DEEPCOOL 사의 공랭 쿨러를 선택했습니다.\n램, SSD, HDD는 각각 마이크론, 하이닉스, WD 사의 제품을 선택했습니다.\n마찬가지로 크게 중요한 부품은 아니었기 때문에 유명한 것으로 골랐습니다.\n컴퓨존 조립 # 컴퓨존에 견적 신청을 한 후 당일날 조립이 완료되었다는 답변을 받았습니다.\n용산에서 택배가 발송되고 다음날 아래와 같은 사진을 전달받았습니다.\n제품 배송도 주문 후 하루 내지 이틀 사이에 도착한 것으로 기억합니다.\n컴퓨터 자체도 스티로폼이랑 완충재로 단단히 감싸져 있어 배송 상태에 만족했습니다.\n제품 수령 및 확인 # 각 제품의 박스는 찌그러짐 없이 잘 배송되었는데, 겉박스가 살짝 과대포장이 아닌가 생각했습니다.\n컴퓨터는 아래 사진과 같이 내부까지 완충재로 보호된 상태로 배송되었습니다.\n완충재를 걷어내니 그래픽 카드의 아름다운 자태가 눈에 들어왔습니다.\n개인적으로 RGB를 선호하지는 않기 때문에 제품 그 자체를 보는 것만으로 행복감을 느꼈습니다.\n케이스도 개인 취향에 살짝 비싼 제품을 구매했는데 이렇게 다시 보니 잘 샀다고 생각합니다.\n특히 케이스 정면의 Y자 형태의 디자인이 매력적입니다.\n별도의 수작업 없이 바로 컴퓨터를 실행시켰는데 한가지 아쉬웠던 점은 쿨러 소리가 너무 시끄러웠던 것입니다.\n특별히 불량이 있는 것은 아니고 단순히 쿨러의 RPM이 높게 느껴졌던 건데\n메인보드에서 CPU 쿨러의 속도를 조절해도 변화가 없어 각각의 쿨러를 확인해보니\n케이스 쿨러에서 제어가 이루어지지 않는 문제가 있었습니다.\n제가 조립한 제품이 아니어서 다 뜯어보기도 난감했는데\n다행히 가장 의심쩍었던 팬 허브와 관련해 찾아보면서 메인보드와 연결 문제가 있을 것이라 짐작하게 되었고,\n팬 허브의 PWM 포트가 메인보드가 아닌, 동일 허브의 FAN4 포트에 연결되어 있었습니다.\n황당하긴 하지만 정상적으로 연결 후 조용히 돌아가는 모습을 보니 만족했습니다.\n현재는 우분투를 탑재해 장남감으로서 재밌게 가지고 놀고 있습니다.\nM1 맥북을 유일한 PC로 사용하면서 가끔씩 호환성에 불편함을 느꼈었는데\n리눅스 시스템과 함께 VMware 상에서 윈도우를 운영하면서 더이상 그러한 걱정을 할 필요가 없어졌습니다.\n무엇보다 가장 만족스러운건 터미널에서 nvidia-smi 명령어를 입력했을 때\n표시되는 그래픽 카드의 상태를 확인하는 것입니다.\n적지않은 지출이었지만 그 이상으로 만족한 경험이었습니다.\n"},{"id":23,"href":"/blog/leetcode-problems-236/","title":"[LeetCode 236] Lowest Common Ancestor of a Binary Tree (Python)","section":"Posts","content":"문제 링크 # https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-tree/ 문제 해설 # Idea # root부터 조건을 만족하는 깊이까지 재귀적으로 자식 노드를 탐색하면서\np 또는 q 노드를 발견한 경우 해당 노드를 호출한 함수에게 반환 최종적으로 좌,우에 각각 p와 q가 있을 경우 깊이가 가장 깊은 부모 노드를 반환하고,\n한쪽 방향에 p와 q가 몰려있을 경우 둘 중 부모 관계에 있는 노드를 반환 Time Complexity # O(N) = 10^5 Data Size # nodes: [2, 10^5] val: -10^9 \u0026lt;= int \u0026lt;= 10^9 해설 코드 # Copy python # Definition for a binary tree node. # class TreeNode(object): # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution(object): def lowestCommonAncestor(self, root, p, q): \u0026#34;\u0026#34;\u0026#34; :type root: TreeNode :type p: TreeNode :type q: TreeNode :rtype: TreeNode \u0026#34;\u0026#34;\u0026#34; if root: if root == p or root == q: return root left = self.lowestCommonAncestor(root.left, p, q) right = self.lowestCommonAncestor(root.right, p, q) if left and right: return root return left if left else right "},{"id":24,"href":"/blog/boj-problems-10026/","title":"[백준 10026] 적록색약 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/10026 문제 해설 # Idea # 모든 방문하지 않은 칸에 대해 BFS 탐색하면서 같은 구역을 방문 적록색약의 경우 R과 G를 같은 구역으로 판단하고 탐색 각각의 경우에 대한 BFS 호출 횟수를 서로 다른 구역의 수로 판단하여 출력 Time Complexity # BFS: O(N^2) = 10,000 Data Size # N: 1 \u0026lt;= int \u0026lt;= 100 해설 코드 # Copy python from collections import deque import sys input = sys.stdin.readline N = int(input()) grid = [list(input().strip()) for _ in range(N)] visited = [[[False] * N for _ in range(N)] for _ in range(2)] answer = [0, 0] def bfs(start, visited, st): queue = deque([start]) dy = [0,1,0,-1] dx = [-1,0,1,0] while queue: y,x = queue.popleft() for i in range(4): ny, nx = y+dy[i], x+dx[i] if 0\u0026lt;=ny\u0026lt;N and 0\u0026lt;=nx\u0026lt;N and not visited[ny][nx]: if st[grid[y][x]] == st[grid[ny][nx]]: queue.append((ny,nx)) visited[ny][nx] = True return 1 for i in range(N): for j in range(N): if not visited[0][i][j]: answer[0] += bfs((i,j), visited[0], {\u0026#39;R\u0026#39;:0,\u0026#39;G\u0026#39;:1,\u0026#39;B\u0026#39;:2}) if not visited[1][i][j]: answer[1] += bfs((i,j), visited[1], {\u0026#39;R\u0026#39;:0,\u0026#39;G\u0026#39;:0,\u0026#39;B\u0026#39;:2}) print(answer[0], answer[1]) "},{"id":25,"href":"/blog/boj-problems-21758/","title":"[백준 21758] 꿀 따기 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/21758 문제 해설 # Idea # 벌이 같은 방향을 향하는 경우 상자까지의 총합에서 두 벌의 시작 위치에 있는 값을 제외 벌이 다른 방향을 향하는 경우 상자까지의 총합에 절댓값을 취해서 더함 Data Size # N: 3 \u0026lt;= int \u0026lt;= 100,000 arr[i]: 1 \u0026lt;= int \u0026lt;= 10,000 해설 코드 # Copy python N = int(input()) arr = list(map(int, input().split())) forward, backward = [arr[0]]+[0]*(N-1), [0]*(N-1)+[arr[-1]] for i in range(1,N): forward[i] = forward[i-1] + arr[i] backward[N-i-1] = backward[N-i] + arr[N-i-1] answer = 0 for i in range(1,N-1): answer = max(answer, forward[N-1]*2-forward[0]-forward[i-1]-arr[i]*2) answer = max(answer, backward[0]*2-backward[N-1]-backward[N-i]-arr[N-i-1]*2) answer = max(answer, forward[i]-arr[0]+backward[i]-arr[-1]) print(answer) "},{"id":26,"href":"/blog/boj-problems-5547/","title":"[백준 5547] 일루미네이션 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/5547 문제 해설 # Idea # 전체 좌표 평면의 외곽에 1만큼의 여백을 추가하고 x,y 좌표가 0부터 시작한다고 판단 y가 홀수 일 때, 인접한 좌표는 상하좌우와 함께 우상단,우하단을 포함 y가 짝수 일 때, 인접한 좌표는 상하좌우와 함께 좌상단, 좌하단을 포함 건물이 없는 좌표를 BFS 탐색하면서 건물과 만나는 지점을 카운트 Time Complexity # BFS: O(N^2) = 10,000 Data Size # W, H: 1 \u0026lt;= int \u0026lt;= 100 해설 코드 # Copy python from collections import deque import sys input = sys.stdin.readline W, H = map(lambda x: int(x)+2, input().split()) maps = [[0] * W for _ in range(H)] for i in range(1,H-1): maps[i] = [0]+list(map(int, input().split()))+[0] visited = [[False] * W for _ in range(H)] def bfs(start): queue = deque([start]) dy = [0,1,0,-1,1,-1] cnt = 0 while queue: y,x = queue.popleft() dx = [-1,0,1,0,-1,-1] if y%2==0 else [-1,0,1,0,1,1] for i in range(6): ny, nx = y+dy[i], x+dx[i] if 0\u0026lt;=ny\u0026lt;H and 0\u0026lt;=nx\u0026lt;W: if maps[ny][nx] == 0 and not visited[ny][nx]: queue.append((ny,nx)) visited[ny][nx] = True elif maps[ny][nx] == 1: cnt += 1 return cnt visited[0][0] = True print(bfs((0,0))) "},{"id":27,"href":"/blog/jekyll-blog/","title":"깃허브 블로그 시작하기","section":"Posts","content":"블로그를 처음 시작함에 있어서 모든 것이 준비된 호스팅 서비스의 편의성은 무시할 수 없습니다.\n저도 처음엔 코드를 직접 건드리는 자유도 높은 방식의 블로그에 진입 장벽을 느끼고\n가볍게 시작할 수 있는 티스토리를 통해 블로그에 입문했습니다.\n하지만, 개발적 지식을 학습하면서 깃허브에 마크다운 문서를 올리는 빈도가 늘어났고,\n깃허브에 올린 문서를 굳이 티스토리로 다시 옮겨 담는 것에 불편함을 느끼게 되었습니다.\n마크다운 문서를 자주 작성하고 깃허브 저장소를 학습 노트로 활용한다면,\n깃허브 블로그를 구성해보는 것이 문서를 통합적으로 관리할 수 있다는 점에서 매력적이라 생각합니다.\n현재는 막 깃허브 블로그를 꾸려서 적응해가는 단계에 불과하지만,\n웹에서 정적 파일을 수집하는 기술을 적용할 수 있다면 중복된 자료를 생성할 필요 없이\nTIL 저장소 자체를 블로그 포스트 저장소로도 활용할 수 있을 것이라 기대합니다.\n블로그를 개설하고 처음 작성하는 이번 포스트에서는 깃허브 블로그를 만든 과정을 소개해드리겠습니다.\n테마 선택 및 가져오기 # 깃허브 블로그를 생성하는데 있어 주로 사용되는 기술이 Jekyll이라는 사이트 생성 엔진 입니다.\nJekyll을 구성하는 Ruby와 쉘 스크립트 작성에 대한 이해가 있다면 더욱 자유도 높은 작업을 할 수 있지만,\n다행히 이를 모를지라도 다른 사용자들이 만든 테마를 가져와 블로그를 구성해 볼 수 있습니다. Jekyll 테마는 아래와 같은 사이트를 참조하여 마음에 드는 UI를 확인할 수 있습니다.\nhttps://jekyllthemes.io http://jekyllthemes.org 무료로 가져다 사용할 수 있는 여러 테마 중 개인적으로 마음에 드는 Chirpy 테마를 활용해 보겠습니다.\n테마 별로 적용 및 활용하는 방식에 다소 차이가 있지만,\nChirpy 같은 경우 아래 튜토리얼 사이트가 만들어져 있어 비교적 쉽게 블로그를 구성할 수 있습니다.\nhttps://chirpy.cotes.page 블로그 배포하기 # Chirpy 테마를 설치하고 배포하는 방법엔 두 가지 방식이 있습니다.\nChirpy Starter를 통해 간단하게 설치하기\n튜토리얼에서는 Jekyll을 전혀 모르는 사용자도 쉽게 테마를 활용할 수 있는 프로젝트 파일이 마련되어 있습니다.\n깃허브 저장소를 생성하는 것과 같은 단순한 버튼 클릭만으로 완성된 사이트를 배포할 수 있습니다.\nGithub에서 소스코드를 fork 받아 직접 설치하기\n스크립트를 실행하는 등 다소의 작업이 추가되지만, 블로그 커스터마이징에 유리한 방식입니다.\nJekyll을 다뤄볼 줄 안다면 직접 설치를 진행하는 것이 취향에 맞는 방식일 수 있습니다.\n저 같은 경우 Jekyll에 친숙한 편이 아니기 때문에 1번째 방법을 통해 설치를 진행했습니다.\n이때, 저장소 이름은 \u0026lt;GH_USERNAME\u0026gt;.github.io 형식으로 지정해야 하며,\n\u0026lt;GH_USERNAME\u0026gt;에는 깃허브 아이디를 입력하면 됩니다.\n위 방식으로 저장소를 생성하면 자동으로 배포가 수행되는데, Actions 탭을 통해 아래처럼 진행사항을 확인할 수 있습니다.\n빌드 및 배포가 완료되면 https://\u0026lt;저장소 이름\u0026gt; 주소를 통해 블로그 페이지에 접근할 수 있는데,\n2022년 8월 기준에서 해당 테마를 가져온 직후엔\n--- layout: home # Index page --- 텍스트만 존재하는 화면을 마주하게 됩니다.\n이것은 현재 Github Pages가 스타일이 적용되지 않는 main 브랜치를 대상으로 하고 있는 것이 원인으로,\nSettings 탭 아래 Pages 메뉴를 클릭했을 때 보이는 Branch 부분을 gh-pages로 수정하면 됩니다.\n블로그 설정하기 # 향후 블로그 호스팅 및 사이트 제목을 수정하는 등의 설정을 위해 _config.yml 파일을 수정할 필요가 있습니다.\n제가 블로그 세팅에 도움을 받은 게시글로부터 일부 항목에 대한 설명을 가져왔습니다.\n항목 값 설명 timezone Asia/Seoul 시간대를 설정하는 부분으로 서울 표준시로 설정합니다. title 블로그 제목 프로필 사진 아래 큰 글씨로 제목이 표시됩니다. tagline 프로필 설명 블로그 제목 아래에 작은 글씨로 부연설명을 넣을 수 있습니다. description SEO 구글 검색에 어떤 키워드로 내 블로그를 검색하게 할 것인가를 정의하는 부분입니다. url https://*.github.io 블로그와 연결된 url을 입력합니다. github Github ID 본인의 github 아이디를 입력합니다. twitter.username Twitter ID 트위터를 사용한다면 아이디를 입력합니다. social.name 이름 포스트 등에 작성자로 표시할 나의 이름을 입력합니다. social.email 이메일 나의 이메일 계정을 입력합니다. social.links 소셜 링크들 트위터, 페이스북 등 내가 사용하고 있는 소셜 서비스의 나의 홈 url을 입력합니다. avatar 프로필 사진 블로그 왼쪽 상단에 표시될 프로필 사진의 경로를 설정합니다. toc true 포스트 오른쪽에 목차를 표시합니다. paginate 10 한 목록에 몇 개의 글을 표시할 것인지 지정합니다. 이 부분은 저의 설정 파일 _config.yml 또는 github 내 검색을 통해 접근할 수 있는\n다른 사용자 분들의 설정 파일을 참고하면 원하는 부분을 수정하는데 도움이 될 것입니다.\n_config.yml 파일이 수정 등 저장소에 변화가 발생하면 자동으로 빌드 및 배포 과정이 수행되며,\n변경사항이 적용되는데 약간 시간이 걸릴 수 있습니다.\n포스트 작성하기 # Jekyll은 마크다운 문법으로 글을 작성할 수 있습니다.\n마크다운 문법에 익숙하지 않다면 해당 게시글을 참고해 주시기 바랍니다.\nVS Code 또는 기타 웹 편집기를 활용하면 마크다운 작성 내용을 실시간으로 렌더링해 확인할 수 있습니다.\n게시글에 대한 마크다운 파일은 _posts 디렉토리 내에 위치시키고,\nyyyy-mm-dd-제목.md의 형식으로 파일 이름을 지정해야 합니다.\n제목에 해당하는 부분은 실제 포스트 제목이 아닌, url로 활용되는 부분이기 때문에\n게시글의 내용을 짐작하게 하는 간단한 단어나 문장을 활용하는게 좋습니다.\n마크다운 파일의 상단엔 Front Matter라고 하는 Jekyll 게시글에서 허용하는 규칙을 통해\n게시글 제목, 작성일자, 카테고리, 태그 등을 지정할 수 있습니다.\n자세한 내용은 튜토리얼을 참조할 수도 있고,\n해당 게시글에 대한 raw 파일을 확인해보셔도 좋습니다.\n마치며 # 과거 깃허브 블로그를 만들려고 했을 때는 Jekyll을 직접 다뤄야 해서 쉽게 접근하지 못했는데,\n이제는 그럴 필요 없이 완성된 패키지를 가져다 쓸 수 있게 되어서 많이 편해졌다고 생각합니다.\n참고 자료 # Chirpy Documents 깃헙(GitHub) 블로그 10분안에 완성하기 Jekyll Chirpy 테마 사용하여 블로그 만들기 Github 블로그 테마적용하기(Chirpy) "},{"id":28,"href":"/blog/2022-08-26/","title":"2022-08-26 Log","section":"Posts","content":"NNLM # Word Embedding: 의미적으로 유사한 단어를 가까운 벡터 공간에 표현 one-hot vector: 단어의 개수만큼의 공간을 표현, 모든 단어간 유사도가 0에 가까움 어떤 feature vector 표현이 좋고, sequence들의 probability가 높음을 학습 chain rule에 기반해 이전 단어에 대한 다음 단어의 확률을 계산,\n단어의 개수가 많아질수록 과거로 봐야할 단어의 수가 많아지기 때문에,\nMarkov assumption을 적용해 n개의 단어만 참조 (n-grams) input이 hidden node를 거치지 않고 output으로 연결되는 skip-connection이 존재할 수 있음 (optional) NNLM의 목적은 윈도우 내에서 t번째 단어에 대해 t-1번째 단어의 확률이 극대화되는 것을 학습 Word2Vec # CBOW: 주변 단어를 가지고 중심 단어를 예측, Skip-gram: 중심 단어를 가지고 주변 단어 예측 CBOW는 gradient가 각각의 주변 단어에 분산되지만,\nSkip-gram은 주변 단어의 gradient를 합산하는 방향으로 학습하여 성능이 좋음 activation function이 존재하지 않는 linear 구조 Skip-gram의 목적 함수는 t번째 단어에 대해 좌우로 n개의 단어에 대한 확률 합의 최대화 대상 단어 벡터와 특정 단어 벡터 간 내적을 하고 총합에 softmax를 취함 빈번한 단어는 한쌍으로 묶고, 빈도가 높은 단어의 학습을 제거해 계산을 간소화 negative sampling: output 단어에 대한 softmax 값을 계산하기 위해 나머지 모든 것들에 대한 내적값을 계산해야 하는데,\nk 개의 예시에 대해서만 sampling하여 계산의 효율성을 추구 FastText # 형태론적 feature 정보를 한 단어의 subwork unit, 문자 단위에서 추출하는 기법 기존 방법론은 모든 단어를 각각의 vector로 표현하는 것에 한계가 있고 (OOV 문제),\n단어의 내부적 구조를 무시하여 유사한 형태의 단어를 표현하지 못함 skip-gram 기반에 문자 단위 character n-gram을 활용 기존 모델의 경우 corpus 대비 context word에 대해서만 학습해 비효율적인 연산이 많은데,\nnegative sampling을 적용해 일정한 확률값으로 뽑인 word를 참고 공통된느 형태소들에 대해 parameter sharing을 하여 embedding에 반영 (의미 공유) 단어의 시작과 끝 표현 \u0026lsquo;\u0026lt;\u0026rsquo;, \u0026lsquo;\u0026gt;\u0026lsquo;을 추가하고 n개 문자 단위의 벡터를 사용해 계산하며,\nn-gram이 커지는 문제를 방지하기 위해 hasing function으로 hash값 계산 LSTM # RNN에서 새로운 가중치는 기존 가중치 W - lr * W에 대한 미분값의 chain rule인데,\nhidden state가 많아질수록 새로운 weight가 기존 weight와 거의 차이가 없어짐 memory cell을 추가해 새로운 input에 대해 과거의 정보에 대해 몇 퍼센트를 기억할지 결정하고 나머지 정보를 제거 일부가 제거된 cell state는 새로운 input에 대한 hidden state와 더해지고,\n이것이 다시 hidden state와 곱해져 다음 hidden state를 생성 Seq2Seq # LSTM을 활용한 효율적인 기계 번역 아키텍쳐 전통적인 RNN 기반의 기계 번역은 입력과 출력의 크기가 같다고 가정 (입력 토큰과 출력 토큰이 대응) 위 문제를 해결하기 위해 인코더에서 고정된 크기의 context vector를 추출,\n디코더가 문맥 벡터로부터 번역 결과를 추론 (인코더와 디코더는 서로 다른 파라미터를 가짐) 시작 시점에 대한 토큰 \u0026lt;sos\u0026gt;와 종료 시점에 대한 토큰 \u0026lt;eos\u0026gt;를 추가하여,\n종료 토큰을 생성할때까지 반복문을 반복 입력 문장의 순서를 거꾸로 했을 때 더 높은 정확도를 보임 (앞쪽에 위치한 단어끼리 연관성이 높음) Attention을 적용한 Seq2Seq의 경우 모든 hidden states를 디코더로 전달하여,\n필요한 hidden state를 선택 ELMo # ELMo에서의 embedding vector는 bi-directional LSTM에서 가져옴 forward 및 backward의 각 단계별 hidden state를 concatenate하고,\n각각의 벡터에 대한 가중합을 통해 embedding을 생성 특정 task에 대한 가중합 embedding과 task에 대한 scale vector를 곱해 계산 forward LM의 경우 k번째 토큰의 j번째 hidden state를 사용하며,\n마지막 hidden state가 t+1 단어를 예측하는데 사용 backward LM의 경우 역방향으로 t-1 단어를 예측 bi-directional LSTM은 forward와 backward에서의 정확도를 함께 최대화 2L+1개의 표현 R에 가중합을 취해 하나의 벡터로 만드는데, 태스크에 맞는 j번째 layer에 가중칠르 두고 전반적으로 스케일을 취함 가중합을 취하는 방식에 대해선 task별로 각각 취하는 것이 가장 좋고,\n1/(L+1)로 통합하는 것이 다음, 마지막 hidden state의 가중치를 사용하는 것이 다음으로 좋음 ELMo의 조합은 downstream task 모델의 input과 output 양쪽에 붙이는게 가장 좋고,\ninput에 붙이는 것, output에 붙이는 것 순으로 좋음 GPT # unlabeled text 데이터가 labeled text 데이터보다 훨씬 많기 때문에,\n사전학습을 우선 수행하고 labeled 데이터에 대해 fine-tuning을 수행하면 더욱 도움이 될 것 transformer의 decoder block을 사용하며,\nencoder-decoder 간의 masked multi-head attention 없이 단순히 쌓아올림 ELMo는 bi-directional LSTM을 사용하는 반면, GPT는 마스크된 forward를 사용해 다음 단어를 예측 일반적인 LM은 전 단계까지의 시퀀스에 대해 i번째 토큰의 likelihood를 최대화하는 것이 목적 토큰 임베딩과 포지션 임베딩을 더해 첫번째 hidden state를 만들고,\nl-1 번째 hidden state를 decoder block을 통과시켜 l번째 hidden state 생성 GPT는 입력 단어에 대해 정방향으로 예측을 수행하기 때문에 BERT처럼 masking할 필요가 없음 BERT와 달리 각각의 토큰이 생성되면, 그 다음 토큰을 생성하기 위해 해당 토큰이 사용되는 auto-regressive 방식을 가짐 GPT-3 # transformer 모델에서 log loss는 scale이 커질수록 개선되고, context도 마찬가지로 scale이 커질수록 정보가 향상됨 example이 많을수록 성능이 향상되는데 scale이 클수록 차이가 도드라짐 기존엔 각각의 example에 대한 결과를 보면서 gradient를 update하면서 fine-tuning 하는데,\nGPT-3는 zero-shot일 경우 task description과 prompt를 주고,\none-shot일 경우 하나의 example, few-shot일 경우 여러 개의 example을 줌 (fine-tuning을 다시 하지 않음) fine-tuning은 새로운 데이터셋이 필요하면서, 일반화 성능이 떨어지고 과적합 문제가 발생,\nfew-shot learning은 task-specific 데이터를 사용하지 않지만 SOTA보다는 성능이 떨어짐 기존 transformer는 이전 토큰에 대해 모두 attention이 걸리는데,\nsparse transformer를 사용해 개선 학습 데이터와 테스트 데이터 간 overlap이 존재하는데, 비용이 많이 들어 해결하지 못함 문서 레벨에서 특정 표현을 반복하는 경우, 매우 긴 문장의 경우 등 특정 task에 대해 성능이 떨어짐 bidirectional이 아닌 auto-regressive한 구조적 단점 "},{"id":29,"href":"/blog/boj-problems-1308/","title":"[백준 1308] D-Day (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/1308 문제 해설 # Idea # 각각의 날짜에 대한 문자열을 date 타입으로 변환하고, today 기준 1000년 후 날짜와 dday를 비교 조건이 맞을 경우 \u0026lsquo;gg\u0026rsquo;를 출력하고, 아니면 두 날짜의 차이를 출력 Data Size # y,m,d: 1,1,1 \u0026lt;= int*3 \u0026lt;= 9999,12,31 해설 코드 # Copy python from datetime import date strptime = lambda: date(**{k:int(v) for k,v in zip([\u0026#39;year\u0026#39;,\u0026#39;month\u0026#39;,\u0026#39;day\u0026#39;],input().split())}) today, dday = strptime(), strptime() if dday \u0026gt;= today.replace(today.year+1000): print(\u0026#39;gg\u0026#39;) else: print(\u0026#39;D-\u0026#39;+str((dday-today).days)) "},{"id":30,"href":"/blog/boj-problems-7569/","title":"[백준 7569] 토마토 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/7569 문제 해설 # Idea # BFS 7569번 토마토 문제에서 하나의 차원이 추가된 버전입니다. 차원이 늘어난만큼 N의 최대 길이가 감소했기 때문에 여전히 BFS로 해결할 수 있습니다. 익은 토마토의 기준에서 전체 상자를 BFS로 완전탐색하면서 안익은 토마토까지의 최소 거리를 기록합니다. 최소 거리의 최댓값이 곧 토마토들이 모두 익는 최소 일수이며,\n모든 토마토가 다 익었을 경우에 최소 일수를 출력하고, 그렇지 않은 경우엔 -1을 출력합니다. Time Complexity # O(N^3) = 1,000,000 Data Size # M,N: 2 \u0026lt;= int \u0026lt;= 100 H: 1 \u0026lt;= int \u0026lt;= 100 t in [1,0,-1] 해설 코드 # Copy python from collections import deque import sys input = sys.stdin.readline M, N, H = map(int, input().split()) box = [[list(map(int, input().split())) for _ in range(N)] for _ in range(H)] queue = deque([(h,r,c) for h in range(H) for r in range(N) for c in range(M) if box[h][r][c]==1]) days = 0 dz = [0,0,0,0,1,-1] dy = [0,1,0,-1,0,0] dx = [-1,0,1,0,0,0] while queue: z,y,x = queue.popleft() days = max(days, box[z][y][x]) for i in range(6): nz,ny,nx = z+dz[i],y+dy[i],x+dx[i] if 0\u0026lt;=nz\u0026lt;H and 0\u0026lt;=ny\u0026lt;N and 0\u0026lt;=nx\u0026lt;M and box[nz][ny][nx]==0: box[nz][ny][nx] = box[z][y][x] + 1 queue.append((nz,ny,nx)) if 0 in {cell for layer in box for row in layer for cell in row}: print(-1) else: print(days-1) "},{"id":31,"href":"/blog/boj-problems-7576/","title":"[백준 7576] 토마토 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/7576 문제 해설 # Idea # BFS를 활용한 시뮬레이션을 통해 모든 토마토가 익을 떄까지 걸리는 최소 기간을 계산 초기엔 안익은 토마토의 기준에서 매번 익은 토마토까지의 최단거리를 탐색하여,\nO(N^4)의 시간 복잡도로 시간 초과가 발생 이후 익은 토마토의 기준에서 시뮬레이션을 단 한번만 수행하여 각각의 칸에 도달하는데 걸리는 거리값을 갱신 모두 익지 못하는 상황에 대해 1안에선 에러를 발생시켜 처리했고, 2안에선 종료 코드를 실행해 처리 Time Complexity # O(N^2) = 1,000,000 Data Size # M,N: 2 \u0026lt;= int \u0026lt;= 1,000 t in [1,0,-1] 해설 코드 # Copy python from collections import deque import sys input = sys.stdin.readline M, N = map(int, input().split()) box = [list(map(int, input().split())) for _ in range(N)] days = 0 # ============== 1안 (시간초과) ============= def bfs(graph, start, n, m, vistied): queue = deque([start]) dist = 0 dy = [0,1,0,-1] dx = [-1,0,1,0] while queue: for _ in range(len(queue)): y,x = queue.popleft() if graph[y][x] == 1: return dist for i in range(4): ny,nx = y+dy[i],x+dx[i] if 0\u0026lt;=ny\u0026lt;n and 0\u0026lt;=nx\u0026lt;m and not vistied[ny][nx] and graph[ny][nx]!=-1: queue.append((ny,nx)) vistied[ny][nx] = True dist += 1 raise Exception() try: for i in range(N): for j in range(M): if box[i][j] == 0: vistied = [[False] * M for _ in range(N)] vistied[i][j] = True days = max(days, bfs(box, (i,j), N, M, vistied)) print(days) except Exception: print(-1) # =============== 2안 (통과) =============== queue = deque() for i in range(N): for j in range(M): if box[i][j] == 1: queue.append((i,j)) def bfs(graph, queue, n, m): dy = [0,1,0,-1] dx = [-1,0,1,0] while queue: y,x = queue.popleft() for i in range(4): ny,nx = y+dy[i],x+dx[i] if 0\u0026lt;=ny\u0026lt;n and 0\u0026lt;=nx\u0026lt;m and graph[ny][nx]==0: graph[ny][nx] = graph[y][x] + 1 queue.append((ny,nx)) bfs(box, queue, N, M) for i in range(N): for j in range(M): if box[i][j] == 0: print(-1) exit(0) days = max(days, box[i][j]) print(days-1) "},{"id":32,"href":"/blog/boj-problems-18870/","title":"[백준 18870] 좌표 압축 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/18870 문제 해설 # Idea # Sort 집합을 통해 압축한 unique한 좌표 목록을 정렬시키고,\n정렬된 리스트 내에서 좌표와 인덱스를 딕셔너리로 맵핑 Time Complexity # O(N Log N) = 13,000,000 Data Size # N: 1 \u0026lt;= int \u0026lt;= 1,000,000 X: -10^9 \u0026lt;= int \u0026lt;= 10^9 해설 코드 # Copy python N = int(input()) X = list(map(int, input().split())) xtoi = {x:i for i,x in enumerate(sorted(set(X)))} print(\u0026#39; \u0026#39;.join(map(lambda x: str(xtoi[x]), X))) "},{"id":33,"href":"/blog/boj-problems-1931/","title":"[백준 1931] 회의실 배정 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/1931 문제 해설 # Idea # Sliding Window 슬라이딩 윈도우의 전형적인 문제로, 끝 시간을 기준으로 시간을 정렬해서 겹치지 않는 수를 계산 Time Complexity # O(N) = 100,000 Data Size # N: 1 \u0026lt;= int \u0026lt;= 100,000 t1,t2: 0 \u0026lt;= int \u0026lt;= 2^31-1 해설 코드 # Copy python import sys input = sys.stdin.readline N = int(input()) times = sorted([tuple(map(int, input().split())) for _ in range(N)], key=lambda x: [x[1],x[0]]) count, end_time = 0, 0 for t1,t2 in times: if t1 \u0026gt;= end_time: count += 1 end_time = t2 print(count) "},{"id":34,"href":"/blog/boj-problems-15686/","title":"[백준 15686] 치킨 배달 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/15686 문제 해설 # Idea # Combinations 최대 집의 개수가 100, 최대 치킨집의 개수가 13으로 매우 적은 경우의 수를 가지고 있기 때문에,\n모든 조합에 대한 완전탐색을 통해 최소 거리를 계산 초기에는 집에 대한 치킨 거리가 작은 치킨집을 우선적으로 선발해서,\n폐업하지 않은 치킨집에 대한 치킨 거리의 최소 합을 계산했지만 틀림 이후 combinations 모듈을 활용한 완전탐색을 통해 통과 Time Complexity # O(N * nCr) ~ 100,000 Data Size # N: 2 \u0026lt;= int \u0026lt;= 50 M: 1 \u0026lt;= int \u0026lt;= 13 cell in (0, 1, 2) count(house): 1 \u0026lt;= int \u0026lt; 2N count(chicken): M \u0026lt;= int \u0026lt;= 13 해설 코드 # Copy python from itertools import combinations import sys input = sys.stdin.readline N, M = map(int, input().split()) city = {\u0026#39;0\u0026#39;:list(),\u0026#39;1\u0026#39;:list(),\u0026#39;2\u0026#39;:list()} for r in range(N): for c,i in enumerate(input().split()): city[i].append((r,c)) houses, chickens = city[\u0026#39;1\u0026#39;], city[\u0026#39;2\u0026#39;] diff = lambda c1,c2: abs(c1[0]-c2[0])+abs(c1[1]-c2[1]) # =============== 1안 (틀림) =============== house_cost = {chicken:0 for chicken in chickens} chicken_cost = {house:house_cost.copy() for house in houses} for house in houses: for chicken in chickens: cost = diff(house,chicken) chicken_cost[house][chicken] = cost house_cost[chicken] += cost city_cost = 0 open = [chicken for chicken,cost in sorted(house_cost.items(), key=lambda x: x[1])[:M]] for house, costs in chicken_cost.items(): city_cost += min([cost for chicken,cost in costs.items() if chicken in open]) # =============== 2안 (통과) =============== city_cost = sys.maxsize for comb in combinations(chickens, M): cost = 0 for house in houses: cost += min([diff(house,chicken) for chicken in comb]) city_cost = min(city_cost,cost) print(city_cost) "},{"id":35,"href":"/blog/boj-problems-1927/","title":"[백준 1927] 최소 힙 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/1927 문제 해설 # Idea # Heapq 파이썬 heapq 모듈 자체가 최소힙이기 때문에 해당하는 기능을 활용하여 구현 Time Complexity # O(Log N) = 16 Data Size # N: 1 \u0026lt;= int \u0026lt;= 100,000 x: 0 \u0026lt;= int \u0026lt; 2^31 해설 코드 # Copy python import heapq import sys input = sys.stdin.readline N = int(input()) arr = list() for _ in range(N): x = int(input()) if x \u0026gt; 0: heapq.heappush(arr, x) else: if arr: print(heapq.heappop(arr)) else: print(0) "},{"id":36,"href":"/blog/boj-problems-1780/","title":"[백준 1780] 종이의 개수 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/1780 문제 해설 # Idea # Divide and Conquer 2차원 배열의 요소를 완전탐색하면서 동일한 값으로 구성되지 않을 경우,\n행렬을 9등분하여 재귀적 호출 수행 처음 시도에서는 행렬을 매번 슬라이싱하면서 전달하여 시간 초과가 발생 행렬의 시작 인덱스 번호를 전달하고 길이만큼 참조하는 방식으로 시간 복잡도 개선 Data Size # N: 1 \u0026lt;= int \u0026lt;= 3^7 해설 코드 # Copy python import sys input = sys.stdin.readline value2id = {-1:0,0:1,1:2} # ============== 1안 (시간초과) ============= from itertools import chain mat_slice = lambda mat,r1,r2,c1,c2: list(map(lambda x: x[c1:c2], mat[r1:r2])) def nona_comp(n, arr, answer): values = set(chain.from_iterable(arr)) if len(values) == 1: answer[value2id[values.pop()]] += 1 return div = n//3 for i in range(3): for j in range(3): nona_comp(div, mat_slice(arr,div*i,div*(i+1),div*j,div*(j+1)), answer) # =============== 2안 (통과) =============== def nona_comp(n, arr, r, c, answer): start = arr[r][c] for row in range(r,r+n): for col in range(c,c+n): if arr[row][col] != start: div = n//3 for i in range(3): for j in range(3): nona_comp(div, arr, r+div*i, c+div*j, answer) return answer[value2id[start]] += 1 N = int(input()) arr = [list(map(int, input().split())) for _ in range(N)] answer = [0, 0, 0] nona_comp(len(arr), arr, 0, 0, answer) for num in answer: print(num) "},{"id":37,"href":"/blog/programmers-problems-77486/","title":"[프로그래머스 77486] 다단계 칫솔 판매 (Python)","section":"Posts","content":"문제 링크 # https://school.programmers.co.kr/learn/courses/30/lessons/77486 문제 해설 # Idea # Union-Find 알고리즘의 Find() 함수를 사용하여 부모 노드에 대해 재귀적으로 접근 최악의 경우 O(NM)=10^10으로 시간 초과가 발생하지만, 매 탐색마다 최대 10,000원을 10씩 나눠 0이 되는 순간에 재귀가 종료되기 때문에 최대 깊이가 5로 좁혀짐 Time Complexity # O(N) = 100,000 Data Size # enroll, referral: str(10) * 10,000 seller: str(10) * 100,000 amount: int(100) * 100,000 해설 코드 # Copy python def find(parents, cur, income, answer): alloc = income//10 if parents[cur] == cur or alloc == 0: answer[cur] += income-alloc return answer[cur] += income-alloc find(parents, parents[cur], alloc, answer) return def solution(enroll, referral, seller, amount): N = len(enroll) answer = [0] * N name2id = {name:i for i,name in enumerate(enroll)} parents = [i if referral[i]==\u0026#39;-\u0026#39; else name2id[referral[i]] for i in range(N)] for i in range(len(seller)): find(parents, name2id[seller[i]], amount[i]*100, answer) return answer "},{"id":38,"href":"/blog/boj-problems-1182/","title":"[백준 1182] 부분수열의 합 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/1182 문제 해설 # Idea # Brute Force 전체 배열에서 1부터 N개의 부분 조합을 완전탐색하면서 합이 S와 같은 경우를 카운트하고 출력 Data Size # N: 1 \u0026lt;= int \u0026lt;= 20 S: abs(int) \u0026lt;= 1,000,000 arr: int(100,000) * N 해설 코드 # Copy python from itertools import combinations N, S = map(int, input().split()) arr = list(map(int, input().split())) count = 0 for i in range(1,N+1): comb = combinations(arr, i) count += sum(map(lambda x: sum(x)==S, comb)) print(count) "},{"id":39,"href":"/blog/boj-problems-11725/","title":"[백준 11725] 트리의 부모 찾기 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/11725 문제 해설 # Idea # BFS 1번 노드부터 BFS를 수행하면서 다음 노드에 순차적으로 접근 다음 노드가 이미 방문한 노드의 경우 부모 노드라 판단하여 배열에 저장 부모 노드가 저장된 배열에 대해 2번 노드부터 순차적으로 부모 노드를 출력 Time Complexity # O(N+E) = 200,000 Data Size # N: 2 \u0026lt;= int \u0026lt;= 100,000 해설 코드 # Copy python from collections import deque import sys input = sys.stdin.readline N = int(input()) graph = [[] for _ in range(N+1)] visited = [False] * (N+1) parents = [1] * (N+1) for _ in range(N-1): u, v = map(int, input().split()) graph[u].append(v) graph[v].append(u) queue = deque([1]) visited[1] = True while queue: node = queue.popleft() for next in graph[node]: if not visited[next]: queue.append(next) visited[next] = True else: parents[node] = next for parent in parents[2:]: print(parent) "},{"id":40,"href":"/blog/programmers-problems-68936/","title":"[프로그래머스 68936] 쿼드압축 후 개수 세기 (Python)","section":"Posts","content":"문제 링크 # https://school.programmers.co.kr/learn/courses/30/lessons/68936 문제 해설 # Idea # Divide and Conquer 2차원 배열을 4등분씩 나눠 재귀함수를 호출하고 동일한 값으로 채워져 있는지 판단하여 값의 개수 증가 2^n 형태의 정수에 대해 NumPy를 활용해 행렬 인덱싱을 간단히 구현 Time Complexity # O(N^2 Log N^2) = 20,000,000 Data Size # arr: [[int(1)]], shape(2^n, 2^n) 1 \u0026lt;= 2^n \u0026lt;= 1024 해설 코드 # Copy python import numpy as np def quad_comp(n, arr, answer): values = np.unique(arr) if len(values) == 1: answer[values[0]] += 1 return div = n//2 quad_comp(div, arr[:div, :div], answer) quad_comp(div, arr[:div, div:], answer) quad_comp(div, arr[div:, :div], answer) quad_comp(div, arr[div:, div:], answer) def solution(arr): arr = np.array(arr) answer = [0,0] quad_comp(len(arr), arr, answer) return answer "},{"id":41,"href":"/blog/programmers-problems-87390/","title":"[프로그래머스 87390] n^2 배열 자르기 (Python)","section":"Posts","content":"문제 링크 # https://school.programmers.co.kr/learn/courses/30/lessons/87390 문제 해설 # Idea # Greedy n의 크기가 굉장히 크기 때문에 2차원 배열을 만드는 것만으로 시간 초과가 발생할 것을 예상 r행 c열의 값은 max(r,c)+1과 같고 1차원 배열의 인덱스 i에 대해 r은 i//n, c는 i%n와 동일 left부터 right까지의 인덱스를 규칙에 맞는 값으로 변환하여 반환 Time Complexity # O(N) = 10^5 Data Size # n: 1 \u0026lt;= int \u0026lt;= 10^7 left, right: 0 \u0026lt;= long \u0026lt;= n^2 right - left \u0026lt; 10^5 해설 코드 # Copy python def solution(n, left, right): return [max(divmod(i,n))+1 for i in range(left,right+1)] "},{"id":42,"href":"/blog/programmers-problems-17687/","title":"[프로그래머스/카카오 17687] n진수 게임 (Python)","section":"Posts","content":"문제 링크 # https://school.programmers.co.kr/learn/courses/30/lessons/17687 문제 해설 # Idea # Math 0부터 시작해 t*m의 길이를 만족하는 N진법 배열을 생성 매 순서마다 p 위치에 해당하는 값을 추출해 문자열로 반환 Data Size # n: 2 \u0026lt;= int \u0026lt;= 16 t: 0 \u0026lt; int \u0026lt;= 1,000 m: 2 \u0026lt;= int \u0026lt;= 100 p: 1 \u0026lt;= int \u0026lt;= m 해설 코드 # Copy python alpha = {10:\u0026#39;A\u0026#39;,11:\u0026#39;B\u0026#39;,12:\u0026#39;C\u0026#39;,13:\u0026#39;D\u0026#39;,14:\u0026#39;E\u0026#39;,15:\u0026#39;F\u0026#39;} def n_base(num, base): result = str() while num \u0026gt; 0: num, mod = divmod(num, base) result += str(mod) if mod \u0026lt; 10 else alpha[mod] return result[::-1] def solution(n, t, m, p): arr = \u0026#39;01\u0026#39; total = t*m p = p%m i = 2 while len(arr) \u0026lt; total: arr += n_base(i, n) i += 1 answer = [t for i,t in enumerate(arr[:total]) if (i+1)%m==p] return \u0026#39;\u0026#39;.join(answer) "},{"id":43,"href":"/blog/boj-problems-1676/","title":"[백준 1676] 팩토리얼 0의 개수 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/1676 문제 해설 # Idea # Math 팩토리얼 수를 구하고 문자열로 변환해 연속되는 0의 개수를 출력 Data Size # N: 0 \u0026lt;= int \u0026lt;= 500 해설 코드 # Copy python from math import factorial import re N = int(input()) zeros = re.findall(\u0026#39;0+\u0026#39;, str(factorial(N))) if zeros: print(len(zeros[-1])) else: print(0) "},{"id":44,"href":"/blog/boj-problems-1541/","title":"[백준 1541] 잃어버린 괄호 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/1541 문제 해설 # Idea # Greedy 최솟값을 만들기 위해서는 \u0026lsquo;-\u0026lsquo;를 기준으로 괄호를 치는 것이 최선 \u0026lsquo;-\u0026lsquo;를 기준으로 식을 나누고 구분된 식을 계산하여 결과를 출력 Data Size # arr: str(50) 해설 코드 # Copy python arr = input().split(\u0026#39;-\u0026#39;) answer = sum(map(int,arr[0].split(\u0026#39;+\u0026#39;))) for i in arr[1:]: answer -= sum(map(int,i.split(\u0026#39;+\u0026#39;))) print(answer) "},{"id":45,"href":"/blog/boj-problems-1389/","title":"[백준 1389] 케빈 베이컨의 6단계 법칙 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/1389 문제 해설 # Idea # BFS 1부터 N까지의 번호에 대해 매번 BFS를 수행하면서 다른 모든 노드와의 거리를 파악 가장 작은 거리의 합을 가진 노드의 인덱스 번호를 출력 Time Complexity # O(N^2+NM) = 510,000 Data Size # N: 2 \u0026lt;= int \u0026lt;= 100 M: 1 \u0026lt;= int \u0026lt;= 5,000 해설 코드 # Copy python from collections import deque import sys input = sys.stdin.readline def bfs(target, nodes): num = [0] * (N+1) queue = deque([target]) visited = [False] * (N+1) visited[target] = True while queue: node = queue.popleft() for next in nodes[node]: if not visited[next]: num[next] = num[node]+1 visited[next] = True queue.append(next) return sum(num) N, M = map(int, input().split()) rels = [list() for _ in range(N+1)] kevin = [0] * (N+1) for _ in range(M): A, B = map(int, input().split()) rels[A].append(B) rels[B].append(A) for i in range(1,N+1): kevin[i] = bfs(i, rels) print(kevin.index(min(kevin[1:]))) "},{"id":46,"href":"/blog/boj-problems-5430/","title":"[백준 5430] AC (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/5430 문제 해설 # Idea # Implementation, Deque 문제에서 주어진대로 매번 배열을 뒤집으면 O(N^2)의 시간 복잡도로 시간 초과가 발생 배열에 영향을 주지 않으면서 R 함수를 처리하기 위해 상태 변수를 정의하고,\nD 함수가 호출될 경우 배열의 상태에 따라 첫 번째 수를 버릴지 마지막 수를 버릴지 결정 마지막에 배열의 상태를 업데이트하고 정해진 형태로 결과를 출력 Time Complexity # O(N) = 100,000 Data Size # T: 1 \u0026lt;= int \u0026lt;= 100 p: 1 \u0026lt;= int \u0026lt;= 100,000 n: 1 \u0026lt;= int \u0026lt;= 100,000 arr: int(100) * n (like [x_1,\u0026hellip;,x_n]) 해설 코드 # Copy python from collections import deque for _ in range(int(input())): p = input() n = int(input()) arr = deque(eval(input())) forward = True try: for cmd in p: if cmd == \u0026#39;R\u0026#39;: forward = not forward elif cmd == \u0026#39;D\u0026#39;: if forward: arr.popleft() else: arr.pop() arr = map(str,arr) if forward else map(str,reversed(arr)) print(f\u0026#39;[{\u0026#34;,\u0026#34;.join(map(str,arr))}]\u0026#39;) except IndexError: print(\u0026#39;error\u0026#39;) "},{"id":47,"href":"/blog/boj-problems-1463/","title":"[백준 1463] 1로 만들기 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/1463 문제 해설 # Idea # Dynamic Programming N에 대해 조건을 만족하는 경우에서 3으로 나누기, 2로 나누기, 1을 빼는 연산을 반복 수행하고\n각각의 연산횟수 별로 도출할 수 있는 값을 모두 저장 앞선 결과를 모두 활용해 다음 결과에 대한 모든 경우를 탐색하고 결과 집합에 1이 있을 시 탐색을 종료 1이 포함된 마지막 집합의 인덱스 번호를 최소 연산횟수로 출력 Data Size # N: 1 \u0026lt;= int \u0026lt;= 10^6 해설 코드 # Copy python N = int(input()) dp = [{N,}] while 1 not in dp[-1]: dp.append(set()) for n in dp[-2]: if n % 3 == 0: dp[-1].add(n//3) if n % 2 == 0: dp[-1].add(n//2) dp[-1].add(n-1) print(len(dp)-1) "},{"id":48,"href":"/blog/boj-problems-1697/","title":"[백준 1697] 숨바꼭질 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/1697 문제 해설 # Idea # BFS N에서 시작해 K에 도달할 때까지 x-1, x+1, x*2에 대한 최단거리를 탐색 두 점이 위치할 수 있는 범위 내에서 가까운 거리의 점부터 탐색을 수행 K에 대한 거리를 출력 N이 K보다 클 경우 x-1 외에는 이동수단이 없기 때문에 시간 단축을 위해 예외로 처리 Time Complexity # O(N) = 100,000 Data Size # N: 0 \u0026lt;= int \u0026lt;= 100,000 K: 0 \u0026lt;= int \u0026lt;= 100,000 해설 코드 # Copy python from collections import deque def bfs(start, target): MAX = 10**5 count = [0] * (MAX+1) queue = deque([start]) while queue: x = queue.popleft() if x == target: return count[x] for nx in (x-1,x+1,x*2): if 0 \u0026lt;= nx \u0026lt;= MAX and not count[nx]: count[nx] = count[x] + 1 queue.append(nx) N, K = map(int, input().split()) if N \u0026gt;= K: print(N - K) else: print(bfs(N, K)) "},{"id":49,"href":"/blog/boj-problems-20922/","title":"[백준 20922] 겹치는 건 싫어 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/20922 문제 해설 # Idea # Two Pointer 수열의 시작과 끝 지점에 대한 두 개의 포인터 지정 끝 지점에 대한 포인터를 확장하면서 탐색되는 원소의 수를 카운트 원소의 수가 K개와 같아지는 시점부터 시작 지점에 대한 포인터를 확장하여 범위 축소 최종적으로 두 포인터 간 거리의 최대치를 출력 Time Complexity # O(N) = 200,000 Data Size # N: 1 \u0026lt;= int \u0026lt;= 200,000 K: 1 \u0026lt;= int \u0026lt;= 100 a: int(100,000) * N 해설 코드 # Copy python N, K = map(int, input().split()) a = list(map(int, input().split())) answer = 0 start, end = 0, 0 counter = [0] * (max(a)+1) while end \u0026lt; N: if counter[a[end]] \u0026lt; K: counter[a[end]] += 1 end += 1 else: counter[a[start]] -= 1 start += 1 answer = max(end-start, answer) print(answer) "},{"id":50,"href":"/blog/boj-problems-22859/","title":"[백준 22859] HTML 파싱 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/22859 문제 해설 # Idea # Implementation, String , , 태그 등을 구분 의 attribute인 title을 우선 출력하고 안에 있는 를 한 줄씩 출력 안에 있는 태그와 시작과 끝에 있는 공백을 지우고 2개 이상의 공백을 하나로 변경 제목은 무조건 존재하고 태그 사이에는 공백이 없으며 태그는 올바른 쌍으로만 주어짐을 보장 정규 표현식을 활용해 조건에 맞는 문장을 파싱하고 불필요한 문자를 제거해 출력 Data Size # source: str(1,000,000) 해설 코드 # Copy python import re source = input() main = re.findall(\u0026#39;\u0026lt;main\u0026gt;(.*)\u0026lt;/main\u0026gt;\u0026#39;, source)[0] div_list = re.findall(\u0026#39;\u0026lt;div title=\u0026#34;(.*?)\u0026#34;\u0026gt;(.*?)\u0026lt;/div\u0026gt;\u0026#39;, main) for title, paragraph in div_list: print(\u0026#39;title :\u0026#39;, title) p_list = re.findall(\u0026#39;\u0026lt;p\u0026gt;(.*?)\u0026lt;/p\u0026gt;\u0026#39;, paragraph) for p in p_list: p = re.sub(\u0026#39;(\u0026lt;.*?\u0026gt;)\u0026#39;, \u0026#39;\u0026#39;, p) p = re.sub(\u0026#39;\\s+\u0026#39;, \u0026#39; \u0026#39;, p.strip()) print(p) "},{"id":51,"href":"/blog/programmers-problems-43238/","title":"[프로그래머스 43238] 입국심사 (Python)","section":"Posts","content":"문제 링크 # https://school.programmers.co.kr/learn/courses/30/lessons/43238 문제 해설 # Idea # Binary Search answer에 대한 이진탐색 수행 (1 \u0026lt;= answer \u0026lt;= max(times)*n) 매 탐색마다 answer 시간 동안 심사관들이 심사할 수 있는 사람의 수를 계산 심사한 사람의 수가 n명 이상일 경우 최대 범위를 조정하고 재탐색 심사한 사람의 수가 n명 미만일 경우 최소 범위를 조정하고 재탐색 n명 이상의 사람을 심사할 수 있는 가장 작은 answer를 반환 Time Complexity # Binary Search: O(M Log N^N) = 6,000,000 Data Size # n: 1 \u0026lt;= int \u0026lt;= 1,000,000,000 times: int(1,000,000,000) * 100,000 해설 코드 # Copy python def solution(n, times): answer = 0 start, end = 1, max(times)*n while start \u0026lt;= end: mid = (start+end)//2 passed = 0 for time in times: passed += mid // time if passed \u0026gt;= n: break if passed \u0026gt;= n: answer = mid end = mid-1 elif passed \u0026lt; n: start = mid+1 return answer "},{"id":52,"href":"/blog/programmers-problems-42895/","title":"[프로그래머스 42895] N으로 표현 (Python)","section":"Posts","content":"문제 링크 # https://school.programmers.co.kr/learn/courses/30/lessons/42895 문제 해설 # Idea # Dynamic Programming S[1] = {N} S[2] = {NN, N+N, N-N, N*N, N/N} S[3] = {NNN, S[2][x] (+,-,*,/) S[1][y], \u0026hellip;} 2부터 8까지의 범위를 가진 i와 1부터 i-1까지의 범위를 가진 j에 대해,\nS[j]와 S[i-j]의 사칙연산 결과를 S[i]에 추가하고 해당 집합이 number를 포함하는지 검증 Time Complexity # DP: O(1) Data Size # N: 1 \u0026lt;= int \u0026lt;= 9 number: 1 \u0026lt;= int \u0026lt;= 32,000 answer: int \u0026lt;= 8 해설 코드 # Copy python from itertools import product def solution(N, number): S = [set() for _ in range(9)] if N == number: return 1 else: S[1].add(N) for i in range(2,9): S[i].add(int(str(N)*i)) for j in range(1,i): for x,y in product(S[j],S[i-j]): S[i].update({x+y,x-y,x*y}) if y != 0: S[i].add(x//y) if number in S[i]: return i return -1 "},{"id":53,"href":"/blog/boj-problems-21318/","title":"[백준 21318] 피아노 체조 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/21318 문제 해설 # Idea # Prefix Sum 실수한 곡에 대한 누적합을 구하고 인덱싱을 통해 특정 구간에 대한 누적합 출력 마지막 곡은 항상 성공하기 때문에 y에 대한 누적합과 y-1에 대한 누적합이 다르면 1 감소 Time Complexity # Prefix Sum: O(N) = 100,000 Data Size # N: 1 \u0026lt;= int \u0026lt;= 100,000 scores: int(10^9) * N Q: 1 \u0026lt;= int \u0026lt;= 100,000 해설 코드 # Copy python import sys input = sys.stdin.readline N = int(input()) scores = list(map(int, input().split())) Q = int(input()) fails = [0]*(N+1) for i in range(1,N+1): fails[i] = fails[i-1] + int(scores[i-1] \u0026gt; scores[min(i,N-1)]) for _ in range(Q): x, y = map(int, input().split()) answer = fails[y] - fails[x-1] if fails[y] != fails[y-1]: answer -= 1 print(answer) "},{"id":54,"href":"/blog/boj-problems-16987/","title":"[백준 16987] 계란으로 계란치기 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/16987 문제 해설 # Idea # Backtracking 0번째 계란부터 마지막 계란까지의 모든 경우의 수를 탐색 시간 단축을 위해 현재 계란이 깨진 경우 또는 나머지 모든 계란이 깨진 경우를 예외로 처리 한 번에 두 개 이상의 계란을 치는 경우를 방지하기 위해 계란을 친 후 원상복구 수행 Time Complexity # Backtracking: O(N^N) = 16,777,216 Data Size # N: 1 \u0026lt;= int \u0026lt;= 8 S, W: 1 \u0026lt;= int \u0026lt;= 300 해설 코드 # Copy python N = int(input()) eggs = list() for _ in range(N): eggs.append(list(map(int, input().split()))) answer = 0 def dfs(eggs, idx): global answer if idx == N: answer = max(answer, len([s for s,w in eggs if s \u0026lt; 1])) return if eggs[idx][0] \u0026lt; 1: dfs(eggs, idx+1) return if len([s for s,w in eggs if s \u0026lt; 1]) \u0026gt;= N-1: answer = max(answer, N-1) return for target in range(N): if target != idx and eggs[target][0] \u0026gt; 0: eggs[target][0] -= eggs[idx][1] eggs[idx][0] -= eggs[target][1] dfs(eggs, idx+1) eggs[target][0] += eggs[idx][1] eggs[idx][0] += eggs[target][1] dfs(eggs, 0) print(answer) "},{"id":55,"href":"/blog/boj-problems-16918/","title":"[백준 16918] 봄버맨 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/16918 문제 해설 # Idea # Simulation (or BFS) 초기에 빈 칸(.)을 0, 폭탄이 있는 칸(O)을 1로 설정 처음에 폭탄이 있는 칸의 상태를 우선 1 증가시키고, 이후 모든 칸의 상태를 1씩 증가시키는 과정 반복 매번 각 칸의 상태를 점검하면서 3을 초과할 경우 해당 위치 및 이웃 위치를 폭발 대상에 추가 폭발 대상이 존재할 경우 격자의 범위를 벗어나지 않는 범위 내에서 상태를 0으로 변환 위 과정을 N초 동안 반복하고, 0은 빈칸으로, 나머지는 O로 표시하여 출력 Time Complexity # Simulation: O(N^3) = 8,000,000 Data Size # R, C, N: 1 \u0026lt;= int \u0026lt;= 200 해설 코드 # Copy python import sys input = sys.stdin.readline R, C, N = map(int, input().split()) board = list() char2id = lambda x: 0 if x == \u0026#39;.\u0026#39; else 1 id2char = lambda x: \u0026#39;.\u0026#39; if x == 0 else \u0026#39;O\u0026#39; for _ in range(R): board.append(list(map(char2id, input().strip()))) board = [[state+1 if state \u0026gt; 0 else 0 for state in row] for row in board] for s in range(2,N+1): board = [[state+1 for state in row] for row in board] bomb = set() for i in range(R): for j in range(C): if board[i][j] \u0026gt; 3: neighbor = [(0,0),(1,0),(-1,0),(0,1),(0,-1)] [bomb.add((i+dy,j+dx)) for dy,dx in neighbor] for i,j in bomb: if 0 \u0026lt;= i \u0026lt; R and 0 \u0026lt;= j \u0026lt; C: board[i][j] = 0 for row in board: print(\u0026#39;\u0026#39;.join(list(map(id2char, row)))) "},{"id":56,"href":"/blog/boj-problems-2302/","title":"[백준 2302] 극장 좌석 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/2302 문제 해설 # Idea # Dynamic Programming 자리를 옮길 수 있는 연속되는 좌석의 수는 피보나치 수열을 따름 (S[i] = F[i+1]) VIP 좌석 번호를 기준으로 연속되는 좌석의 수를 리스트로 저장 모든 연속되는 좌석 수에 대한 피보나치 수를 곱하고 출력 Sequence # S2 (1,2) -\u0026gt; (1,2), (2,1) = 2(F3)\nS3 (1,2,3) -\u0026gt; (1,2,3), (2,1,3), (1,3,2) = 3(F4)\nS4 (1,2,3,4) -\u0026gt; (1,2,3,4), (2,1,3,4), (1,2,4,3), (1,3,2,4), (2,1,4,3) = 5(F5)\nS5 (1,2,3,4,5) -\u0026gt; (1,2,3,4,5), (1,2,4,3,5), (1,2,3,5,4), (2,1,3,4,5), (2,1,4,3,5), (2,1,3,5,4), (1,3,2,4,5), (1,3,2,5,4) = 8(F6)\nS6 (1,2,3,4,5,6) -\u0026gt; (1,2,3,4,5,6), (1,2,4,3,5,6), (1,2,3,5,4,6), (1,2,3,4,6,5), (1,2,4,3,6,5), \u0026hellip;\nTime Complexity # DP: O(N) = 40 Data Size # N: 1 \u0026lt;= int \u0026lt;= 40 M: 0 \u0026lt;= int \u0026lt;= N answer: int \u0026lt; 2^31-1 해설 코드 # Copy python import sys input = sys.stdin.readline N = int(input()) M = int(input()) seats, idx = list(), 0 for _ in range(M): vip = int(input()) seats.append(vip-idx-1) idx = vip seats.append(N-idx) F = {1:1, 2:1} def fibonacci(n): if n in F: return F[n] F[n] = fibonacci(n-1) + fibonacci(n-2) return F[n] answer = 1 for seat in seats: if seat \u0026gt; 1: answer *= fibonacci(seat+1) print(answer) "},{"id":57,"href":"/blog/boj-problems-18352/","title":"[백준 18352] 특정 거리의 도시 찾기 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/18352 문제 해설 # Idea # BFS 시작 노드 X부터 연결된 노드를 순차적으로 방문하면서 X로부터 떨어진 거리를 기록 거리가 K와 같은 노드를 출력하고, 해당하는 노드가 없을 경우 -1을 출력 거리가 K를 넘어가지 않는 노드에 대해서만 탐색하여 시간 단축 Time Complexity # BFS: O(N+M) = 1,300,000 Data Size # N: 2 \u0026lt;= int \u0026lt;= 300,000 M: 1 \u0026lt;= int \u0026lt;= 1,000,000 K: 1 \u0026lt;= int \u0026lt;= 300,000 X: 1 \u0026lt;= int \u0026lt;= N A, B: 1 \u0026lt;= int \u0026lt;= N 해설 코드 # Copy python from collections import deque import sys input = sys.stdin.readline N, M, K, X = map(int, input().split()) nodes = [[] for _ in range(N+1)] visited = [False] * (N+1) dists = [0] * (N+1) for _ in range(M): A, B = map(int, input().split()) nodes[A].append(B) queue = deque() queue.append(X) visited[X] = True while queue: city = queue.popleft() for next in nodes[city]: if not visited[next] and dists[city] \u0026lt; K: queue.append(next) visited[next] = True dists[next] = dists[city]+1 targets = [i for i,d in enumerate(dists) if d==K] if targets: for target in targets: print(target) else: print(-1) "},{"id":58,"href":"/blog/boj-problems-1495/","title":"[백준 1495] 기타리스트 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/1495 문제 해설 # Idea # Dynamic Programming P[i] = max(P[i-1]+V[i-1],P[i-1]-V[i-1]), 0 \u0026lt;= P[i] \u0026lt;= M 모든 P[i-1]가 P[i+1]에 영향을 줄 수 있기 때문에 범위 내 모든 값을 집합에 저장 마지막 곡에 대한 P가 존재할 경우 최댓값을 출력하고, 없을 경우 -1을 출력 Time Complexity # DP: O(NM) = 50,000 Data Size # N: 1 \u0026lt;= int \u0026lt;= 50 M: 1 \u0026lt;= int \u0026lt;= 1,000 S: 0 \u0026lt;= int \u0026lt;= M V: int * N 해설 코드 # Copy python N, S, M = map(int, input().split()) V = list(map(int, input().split())) P = [set() for _ in range(N+1)] P[0] = {S,} for i in range(1,N+1): for j in P[i-1]: if j+V[i-1] \u0026lt;= M: P[i].add(j+V[i-1]) if j-V[i-1] \u0026gt;= 0: P[i].add(j-V[i-1]) if P[-1]: print(max(P[-1])) else: print(-1) "},{"id":59,"href":"/blog/programmers-problems-17686/","title":"[프로그래머스/카카오 17686] 파일명 정렬 (Python)","section":"Posts","content":"문제 링크 # https://school.programmers.co.kr/learn/courses/30/lessons/17686 문제 해설 # Idea # 정규표현식을 활용해 HEAD, NUMBER, TAIL을 분리 전체 파일명을 완전탐색하면서 리스트에 분리된 파일명을 저장 HEAD와 NUMBER를 기준으로 파일명을 정렬하고 정렬된 원본 파일명을 반환 Time Complexity # Brute-Force + Sort: O(NM+NlogN)) = 110000 Data Size # files: str(100) * 1000 해설 코드 # Copy python import re def solution(files): answer = [] for file in files: head, number, tail = re.findall(\u0026#39;([^0-9]+)([0-9]+)(.*)\u0026#39;, file)[0] answer.append((head.lower(), int(number), tail, file)) answer.sort(key=lambda x: [x[0],x[1]]) return [file for _,_,_,file in answer] "},{"id":60,"href":"/blog/programmers-problems-17684/","title":"[프로그래머스/카카오 17684] 압축 (Python)","section":"Posts","content":"문제 링크 # https://school.programmers.co.kr/learn/courses/30/lessons/17684 문제 해설 # Idea # LZW 알고리즘 (List로 구현) 단어를 문자 단위로 탐색하면서 캐시에 추가 캐시가 문자 사전에 없을 경우 이전 문자까지의 인덱스를 반환하고 캐시를 문자 사전에 추가 Time Complexity # Brute-Force: O(N^2) = 1000000 Data Size # msg: str(1000) 해설 코드 # Copy python def solution(msg): answer = [] chars = [chr(x) for x in range(64,91)] cache = str() for c in msg: cache += c if cache not in chars: answer.append(chars.index(cache[:-1])) chars.append(cache) cache = c answer.append(chars.index(cache)) return answer "},{"id":61,"href":"/blog/programmers-problems-17683/","title":"[프로그래머스/카카오 17683] 방금그곡 (Python)","section":"Posts","content":"문제 링크 # https://programmers.co.kr/learn/courses/30/lessons/17683 문제 해설 # Idea # 악보 정보에서 #이 포함된 음을 소문자로 대체하고 완전탐색 시간 계산은 timedelta 활용 (재생시간,제목)으로 구성된 리스트를 정렬 Time Complexity # Brute-Force: O(NM) = 143,900 Data Size # m: 1 \u0026lt;= int \u0026lt;= 1439 musicinfos: list \u0026lt;= 100 musicinfos[0,1]: HH:MM (00:00 - 23:59) musicinfos[2]: str(64) musicinfos[4]: 1 \u0026lt;= int \u0026lt;= 1439 해설 코드 # Copy python import datetime as dt import re import math def solution(m, musicinfos): answer = list() lower_repl = lambda match: match.group(1)[0].lower() sharp_repl = lambda s: re.sub(\u0026#39;([A-G]#)\u0026#39;, lower_repl, s) m = sharp_repl(m) strptime = lambda x: dt.timedelta(hours=int(x[0]),minutes=int(x[1])) for info in musicinfos: start, end, title, chord = info.split(\u0026#39;,\u0026#39;) plays = (strptime(end.split(\u0026#39;:\u0026#39;))-strptime(start.split(\u0026#39;:\u0026#39;))).seconds//60 chord = sharp_repl(chord) chord = (chord * math.ceil(plays/len(chord)))[:plays] if m in chord: answer.append((plays,title)) return sorted(answer, key=lambda x: x[0], reverse=True)[0][1] if len(answer) else \u0026#39;(None)\u0026#39; "},{"id":62,"href":"/blog/programmers-problems-17680/","title":"[프로그래머스/카카오 17680] 캐시 (Python)","section":"Posts","content":"문제 링크 # https://school.programmers.co.kr/learn/courses/30/lessons/17680 문제 풀이 # Idea # LRU 알고리즘 (Deque로 구현) 도시이름이 캐시에 존재할 경우 시간에서 1 추가, 아닐 경우 5 추가 캐시에서 참고한 도시는 deque 최상단으로 재배치 캐시 사이즈를 초과할 경우 가장 오래된 도시를 제거 Time Complexity # Deque: O(N) = 100,000 Data Size # cacheSize: 0 \u0026lt;= int \u0026lt;= 30 cities: str(20) * 100,000 해설 코드 # Copy python from collections import deque def solution(cacheSize, cities): answer = 0 cache = deque(maxlen=cacheSize) for city in cities: city = city.lower() if city in cache: answer += 1 cache.remove(city) cache.append(city) else: answer += 5 cache.append(city) return answer "},{"id":63,"href":"/blog/2022-07-21/","title":"2022-07-21 Log","section":"Posts","content":"Task # 통계 모델: ARIMA, ARIMA 확장 모델 딥러닝 모델: DeepAR, MQRNN, N-BEATS, Informer, TFT Data # Stationary: 시계열의 특징이 관측된 시간과 무관 (백색잡음 등) Non-stationary: Trend 혹은 seasonality가 존재하는 데이터 Analysis # log 차분 ACF(자기 상관 그래프): 무작위 신호가 두 시점에서 갖는 상관계수를 표현하는 함수 stationary: 예측값이 무한대로 발산하지 않고, 일정한 범위 내에서 안정적으로 예측하기 위한 목적 ARIAM: stationary 데이터 예측 성능 우수 Deep-Learning # non-stationary: training-validation, validation-test 간 mismatching 알고리즘 거래가 데이터의 분포를 변환 domain adaptation \u0026gt; transfer learning Self-Supervised Learning # unlabeld 데이터 활용 \u0026gt; labeling 지정 (pretext task) 모델이 pretext task를 학습하여 데이터 자체에 대한 이해 (pre-training) main task에 대하여 transfer learning 수행 (downstream task) "},{"id":64,"href":"/blog/2022-07-19/","title":"2022-07-19 Log","section":"Posts","content":"Time Series # 시계열 데이터 분석은 시간을 독립변수로 활용 시계열 분석은 시계열 데이터의 확률적 특성이 시간이 지나도 그대로 유지될 것(정상성)을 가정 AR Model: 자기 회귀 모델, 과거 시점의 자신의 데이터가 현 시점의 자신에게 영향을 미침 MA Model: 이동 평균 모델, 이전 항의 에러를 현 시점에 반영해 변화율에 맞춰 추정 ARMA Model: AR과 MA 모델을 합친 모델, 과거 시점의 자신과 추세까지 전부 반영 ARIMA Model: 현재 상태에서 바로 이전 상태를 빼주는 차분을 적용 ARIMA Model # 미래를 예측하기 위해 자신의 시차로 이루어진 선형 조합과 지연된 예측 오차의 선형조합에 기반 $AR(1):Y_t=\\alpha+\\beta_1Y_{t-1}+\\epsilon_t,where|\\beta_1|\u0026lt;1$ $MA(2):Y_t=\\alpha+\\phi_1\\epsilon_{t-1}+\\phi_2\\epsilon_{t-2}+\\epsilon_t,where|\\phi_1|,|\\phi_2|\u0026lt;1$ AR(1)은 Y_t가 알려진 Y_t-1 값으로부터 얻어졌다는 것을 의미 자기상관함수(ACF)와 부분자기상관함수(PACF)를 사용해 AR과 MA항의 차수를 결정 ACF: Lag에 따른 관측치들 사이의 관련성을 측정하는 함수 PACF: k 이외의 모든 다른 시점 관측치의 영향력을 배제하고 y_t와 y_t-k 두 관측치의 관련성을 측정하는 함수 "},{"id":65,"href":"/blog/2022-07-17/","title":"2022-07-17 Log","section":"Posts","content":"기계번역 과제 # 인코더: 입력 문장의 표현 방법 학습 디코더: 인코더에서 학습한 표현 결과를 입력받아 사용자가 원하는 문장 생성 Encoder # Self Attention # 임베딩: 각각의 단어를 표현하는 벡터값, [문장 길이 x 임베딩 차원] 쿼리(Q), 키(K), 밸류(V) \u0026gt; 각각의 가중치 행렬을 입력 행렬에 곱해 Q, K, V 행렬 생성 Q, K, V의 차원 [문장 길이 x 벡터의 차원] 1단계: Q와 K^T 행렬의 내적 연산, 쿼리 벡터(I)와 키 벡터(I, am, good) 사이의 유사도 계산 2단계: QK^T 행렬을 키 벡터 차원의 제곱근값($\\sqrt{d_k}$)으로 나눈 것, 안정적인 gradient 얻음 3단계: 소프트맥스 함수를 사용해 비정규화된 형태의 유사도 값을 정규화 (score 행렬) 4단계: 스코어 행렬에 V 행렬을 곱해 어텐션(Z) 행렬 계산, 어텐션 행렬은 문장의 각 단어와 벡터값 가짐\n(단어 I의 셀프 어텐션은 각 밸류 벡터값의 가중치 합으로 계산, 단어가 문장 내에 있는 다른 단어와의 연관성) Multi-Head Attention # 문장 내에서 모호한 의미를 가진 단어(it)가 있을 경우,\n문장의 의미가 잘못 해석될 수 있기 때문에 멀티 헤드 어텐션을 사용한 후 그 결괏값을 더함 다수의 어텐션 행렬을 구하기 위해 서로 다른 가중치 행렬을 입력 행렬에 곱해 Q, K, V 생성 다수의 어텐션 행렬을 concatenate하고 새로운 가중치 행렬을 곱해 멀티 헤드 어텐션 결과 도출\n(concatenate 시 [어텐션 헤드 x h] 크기가 되기 때문에 원래 크기로 만들기 위해 가중치 행렬 곱함) Positional Encoding # 트랜스포머는 문장 안에 있는 모든 단어를 병렬 형태로 입력 단어의 순서 정보를 제공하기 위해 문장에서 단어의 위치를 나타내는 인코딩 제공 위치 인코딩은 사인파 함수를 사용 입력 임베딩 결과에 위치 인코딩을 합한 후 멀티 헤드 어텐션에 입력 Feed Forward Network # 2개의 전결합층(Dense)과 ReLU 활성화 함수로 구성 add와 norm을 추가해 서브레이어에서 멀티 헤드 어텐션의 입력값과 출력값을 서로 연결 add와 norm은 레이어 정규화(각 레이어 값이 크게 변화하는 것을 방지해 모델 학습 빠르게)와 잔차 연결 인코더 순서 # 입력값은 입력 임베딩으로 변환한 다음 위치 인코딩 추가, 가장 아래 있는 인코더 1의 입력값으로 공급 인코더 1은 입력값을 받아 멀티 헤드 어텐션의 서브레이어에 값을 보냄, 어텐션 행렬을 결괏값으로 출력 어텐션 행렬의 값을 다음 서브레이어인 피드포워드 네트워크에 입력, 결괏값 출력 인코더 1의 출력값을 그 위에 있는 인코더 2에 입력값으로 제공 인코더 2에서 이전과 동일한 방법 수행, 주어진 문장에 대한 인코더 표현 결과를 출력으로 제공 Decoder # 이전 디코더의 입력값과 인코더의 표현(인코더의 출력값), 2개를 입력 데이터로 받음 t=1에서 디코더의 입력값은 문장의 시작을 알리는 를 입력 \u0026gt; 타깃 문장의 첫 번째 단어(Je) 생성 t=2에서 t-1 디코더에서 생성한 단어(, Je)를 추가해 문장의 다음 단어 생성 t=3에서도 동일하게 (, Je, vais)를 입력받아 다음 단어 생성 디코더에서 토큰을 생성할 때 타깃 문장의 생성이 완료 디코더도 입력값을 바로 입력하지 않고 위치 인코딩을 추가한 값을 출력 임베딩에 더해 입력값으로 사용 Masked Multi-Head Attention # 디코더에서 문장을 생성할 때 이전 단계에서 생성한 단어만 입력으로 넣기 때문에,\n아직 예측하지 않은 오른쪽의 모든 단어를 마스킹해 학습을 진행 소프트맥스 함수를 적용한 정규화 작업 전에 오른쪽의 모든 단어를 $-{\\infty}$로 마스킹 수행\n($-{\\infty}$는 학습 도중 발산하는 경우가 있기 때문에 실제로는 작은 값 $e^{-9}$으로 지정) Encoder-Decoder Attention Layer # 디코더의 멀티 헤드 어텐션의 입력으로 인코더의 표현값 R과 마스크된 멀티 헤드 어텐션의 결과 M을 받을 때 상호작용 발생 쿼리, 키, 밸류 행렬을 생성할 때, M을 사용해 Q를 생성, R을 활용해 K, V를 생성\n(쿼리 행렬은 타깃 문장의 표현을 포함하기 때문에 M을 참조, 키와 밸류 행렬은 입력 문장의 표현을 참조) 쿼리, 키 행렬 간의 내적 시 타깃 단어 가 입력 문장의 모든 단어(I, am, good)와 얼마나 유사한지 계산\n(두 번째 행에서 Je에 대해, 나머지 행에서도 동일한 방법을 적용해 유사도 계산) Linear and Sofmax Layer # 최상위 디코더에서 얻은 출력 값을 선형 및 소프트맥스 레이어에 전달 선형 레이어는 vocab 크기와 같은 logit 형태 logit 값을 확률값으로 변환하고, 디코더에서 가장 높은 확률값을 갖는 인덱스의 단어로 출력 디코더 순서 # 디코더에 대한 입력 문장을 임베딩 행렬로 변환하고 위치 인코딩 정보를 추가해 디코더 1에 입력 입력을 가져와서 마스크된 멀티 헤드 어텐션 레이어에 보내고, 출력으로 어텐션 행렬 M 반환 어텐션 행렬 M, 인코딩 표현 R을 입력받아 멀티 헤드 어텐션 레이어에 값을 입력, 새로운 어텐션 행렬 생성 인코더-디코더 어텐션 레이어에서 출력한 어텐션 행렬을 피드포워드 네트워크에 입력, 디코더의 표현으로 값 출력 디코더 1의 출력값을 다음 디코더 2의 입력값으로 사용 디코더 2는 이전과 동일한 방법 수행, 타깃 문장에 대한 디코더 표현 반환 타깃 문장의 디코더 표현을 선형 및 소프트맥스 레이어에 입력해 최종으로 예측된 단어 얻음 학습 # 손실 함수로 cross-entropy를 사용해 분포의 차이를 확인 옵티마이저로 Adam 사용 과적합을 방지하기 위해 각 서브레이어 출력에 dropout 적용 (임베딩 및 위치 인코딩 합을 구할 때도 포함) BERT # Word2Vec: 문맥 독립 임베딩, BERT: 문맥 기반 임베딩 BERT는 인코더-디코더가 있는 트랜스포머 모델에서 인코더만 사용 BERT-base: L(인코더 레이어)=12, A(어텐션 헤드)=12, H(은닉 유닛)=768 BERT-large: L=24, A=16, H=1024 BERT-tiny(L=2, A=2, H=128), BERT-mini(L=4, A=4, H=256) 등 사전 학습 # 대규모 데이터셋으로 학습된 가중치를 활용해 새로운 태스크에 적용 (find-tuning) BERT는 MLM과 NSP 태스크를 이용해 거대한 말뭉치를 기반으로 사전 학습 Token Embedding # 첫 번째 문장의 시작 부분에 [CLS] 토큰 추가 모든 문장 끝에 [SEP] 토큰 추가 Segment Embedding # 두 문장을 구별하는데 사용 [SEP] 토큰과 별도로 두 문장을 구분하기 위해 입력 토큰($E_A,E_B$) 제공 Position Embedding # 단어(토큰)의 위치에 대한 정보 제공 입력 데이터 # 토큰 임베딩 + 세그먼트 임베딩 + 위치 임베딩 으로 표현 WordPiece Tokenizer # 하위 단어 토큰화 알고리즘 기반 Let us start pretraining the model \u0026gt; [let, us, start, pre, ##train, ##ing, the, model] 단어가 어휘 사전에 있으면 토큰으로 사용, 없으면 하위 단어로 분할해 하위 단어가 어휘 사전에 있는지 확인 (OOV 처리에 효과적) Language Modeling # 임의의 문장이 주어지고 단어를 순서대로 보면서 다음 단어를 예측하도록 모델 학습 자동 회귀 언어 모델링: 전방(좌\u0026gt;우) 예측, 후방(좌\u0026lt;우) 예측, 각 방향(단방향)으로 공백까지 모든 단어를 읽음 자동 인코딩 언어 모델링: 예측하면서 양방향으로 문장을 읽음 Masked Language Modeling (MLM) # 주어진 입력 문장에서 전체 단어의 15%를 무작위로 마스킹, 마스크된 단어를 예측 (빈칸 채우기 태스크) [MASK] 토큰을 사전 학습시킬 경우 파인 튜닝 시 입력에 [MASK] 토큰이 없어 불일치가 발생 15% 토큰에 대해 80%만 [MASK] 토큰으로 교체, 10%는 임의의 토큰(단어)로 교체, 10%는 변경하지 않음\n(사전 학습과 파인 튜닝 태스크의 차이를 줄이기 위한 일종의 정규화 작업) 역전파를 통한 반복 학습을 거치며 최적의 가중치 학습 Whole Word Masking (WWM) # WWM 방법에서는 하위 단어가 마스킹되면 해당 하위 단어와 관련된 모든 단어를 마스킹 하위 단어와 관련된 모든 단어의 마스크 비율이 15%를 초과하면 다른 단어의 마스킹을 무시 Next Sentence Prediction (NSP) # BERT에 두 문장을 입력하고 두 번째 문장이 첫 번째 문장의 다음 문장인지 예측 B 문장이 A 문장에 이어지만 isNext를 반환하고, 그렇지 않으면 notNext를 반환 두 문장 사이의 관계를 파악해 질문-응답 및 유사문장탐지와 같은 downstream 태스크에서 유용 한 문서에서 연속된 두 문장을 isNext로 표시하고, 두 문서에서 각각 문장을 가져와 notNext로 표시 [CLS] 토큰 표현에 소프트맥스 함수를 사용하고 피드포워드 네트워크에 입력해 두 클래스에 대한 확률값 반환 [CLS] 토큰은 모든 토큰의 집계 표현을 보유하고 있기 때문에 문장 전체에 대한 표현을 담고 있음 사전 학습 절차 # lr = 1e-4, b1 = 0.9, b2 = 0.999\n초기 모델의 큰 변화를 유도하기 위해 웜업으로 1만 스텝 학습\n(0에서 1e-4로 선형적으로 학습률 증가, 1만 스탭 후 수렴에 가까워짐에 따라 학습률을 선형적으로 감소) dropout 0.1, GELU(가우시안 오차 선형 유닛) 활성화 함수 사용 하위 단어 토큰화 알고리즘 # Byte Pair Encoding (BPE) # 모든 단어를 문자로 나누고 문자 시퀀스로 만듦 우선 문자 시퀀스에 있는 고유 문자를 어휘 사전에 추가 어휘 사전 크기에 도달할 때까지 가장 빈도수가 큰 기호 쌍을 반복적으로 병합해 어휘 사전에 추가 토큰화 시 어휘 사전에 존재하지 않는 단어는 하위 단어로 나눔, 사전에 없는 개별 문자는 토큰으로 교체 Byte-Level Byte Pair Encoding (BBPE) # 문자 수준 시퀀스 대신 바이트 수준 시퀀스를 사용 유니코드 문자가 바이트로 변환되어 단일 문자 크기는 1~4 바이트가 됨 바이트 수준에서 빈번한 쌍을 구분해 어휘 사전을 구축 다국어 설정에서 유용, OOV 단어 처리에 효과적 WordPiece # BPE랑 다르게 빈도수 대신 likelihood를 기준으로 기호 쌍을 병합 모든 기호 쌍에 대해 언어 모델의 가능도를 확인, 가능도가 가장 높은 기호 쌍을 병합 "},{"id":66,"href":"/blog/2022-07-13/","title":"2022-07-13 Log","section":"Posts","content":"PyTorch Basic # PyTorch Packages # torch: 메인 네임스페이스 torch.autograd: 자동 미분을 위한 함수 torch.nn: 데이터 구조나 레이어 등 정의 torch.optim: 경사 하강법 등 파라미터 최적화 알고리즘 구현 torch.utils.data: 미니 배치용 유틸리티 함수 torch.onnx: ONNX의 포맷으로 모델을 저장할 때 사용 Tensor # 2D Tensor: (batch size, dim) 3D Tensor: (batch size, width, height) 3D Tensor(NLP): (batch size, length, dim) torch.FloatTensor(): 텐서 생성 행렬 곱셈(.matmul()), 원소 별 곱셈(.mul(), *) dim=0: 첫번째 차원(행) 제거 = 같은 열끼리 연산, dim=1: 두번째 차원(열) 제거 = 같은 행끼리 연산 view(): reshape, squeeze(): 1인 차원 제거, unsequeeze(): 특정 위치에 1인 차원 추가 cat(), stack(), ones_like(), zeros_like() Linear Regression # 선형 회귀 구현 # 기본 셋팅 및 변수 선언: seed 설정, x_train 및 y_train 선언 가중치와 편향의 초기화: W = torch.zeros(1, requires_grad=True) 가설 세우기: hypothesis = x_train * W + b 비용 함수 선언: cost = torch.mean((hypothesis - y_train) ** 2) 경사 하강법 구현: Copy python optimizer = optim.SGD([W, b], lr=0.01) optimizer.zero_grad() # gradient를 0으로 초기화 # 파이토치가 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값을 누적시키기 때문에 미분값을 초기화 cost.backword() # 비용 함수를 미분하여 gradient 계산 optimizer.step() # W와 b를 업데이트 Autograd # requires_grad=True, backward() 등 다중 선형 회귀 # x의 계수를 행렬로 변환해 W와 내적 5x3 크기의 x_train과 3x1 크기의 W를 내적해 5x1 크기의 y_train 계산 nn.Module # Copy python from torch import nn import torch.nn.functional as F model = nn.Linear(input_dim, output_dim) cost = F.mse_loss(prediction, y_train) optimizer = torch.optim.SGD(model.parameters(), lr=0.01) Class # Copy python class LinearRegressionModel(nn.Module): def __init__(self): # super().__init__() self.linear = nn.Linear(1, 1) def forward(self, x): return self.linear(x) Custom Dataset # Copy python class CustomDataset(torch.utils.data.Dataset): def __init__(self): # 데이터셋의 전처리를 해주는 부분 def __len__(self): # 데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분 def __getitem__(self, idx): # 데이터셋에서 특정 1개의 샘플을 가져오는 함수 Logistic Regression # hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b))) cost = F.binary_cross_entropy(hypothesis, y_train) Copy python class BinaryClassifier(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(2, 1) self.sigmoid = nn.Sigmoid() def forward(self, x): return self.sigmoid(self.linear(x)) Softmax Regression # F.softmax(z, dim=1) F.softmax() + torch.log() = F.log_softmax() F.log_softmax() + F.nll_loss() = F.cross_entropy() Copy python # One-Hot Encoding y_one_hot = torch.zeros_like(hypothesis) y_one_hot.scatter_(1, y_train.unsqueeze(1), 1) Artificial Neural Network # Copy python model = nn.Sequential( nn.Linear(64, 32), nn.ReLU(), nn.Linear(32, 16), nn.ReLU(), nn.Linear(16, 10) ) Convolutional Neural Network # Copy python class CNN(torch.nn.Module): def __init__(self): super(CNN, self).__init__() # 첫번째층 # ImgIn shape=(?, 28, 28, 1) # Conv -\u0026gt; (?, 28, 28, 32) # Pool -\u0026gt; (?, 14, 14, 32) self.layer1 = torch.nn.Sequential( torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(), torch.nn.MaxPool2d(kernel_size=2, stride=2)) # 두번째층 # ImgIn shape=(?, 14, 14, 32) # Conv -\u0026gt;(?, 14, 14, 64) # Pool -\u0026gt;(?, 7, 7, 64) self.layer2 = torch.nn.Sequential( torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(), torch.nn.MaxPool2d(kernel_size=2, stride=2)) # 전결합층 7x7x64 inputs -\u0026gt; 10 outputs self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True) # 전결합층 한정으로 가중치 초기화 torch.nn.init.xavier_uniform_(self.fc.weight) def forward(self, x): out = self.layer1(x) out = self.layer2(out) out = out.view(out.size(0), -1) # 전결합층을 위해서 Flatten out = self.fc(out) return out NLP # Copy python import torch.nn as nn embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=3, padding_idx=1) torchtext Error # cannot import name \u0026rsquo;load_state_dict_from_url'\napply this change in _download_hooks.py Copy python try: from torch.hub import load_state_dict_from_url except ImportError: from torch.utils.model_zoo import load_url as load_state_dict_from_url Make Vocabulary # Copy python from torchtext.vocab import build_vocab_from_iterator vocab = build_vocab_from_iterator(sequences) vocab.get_stoi() # 각 단어의 정수 인덱스가 저장된 딕셔너리 Recurrent Neural Network # Copy python nn.RNN(input_dim, hidden_size, num_layers, batch_fisrt=True) Copy python nn.LSTM(input_dim, hidden_size, num_layers, batch_fisrt=True) Char RNN # Copy python class Net(nn.Module): def __init__(self, vocab_size, input_size, hidden_size, batch_first=True): super(Net, self).__init__() self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, # 워드 임베딩 embedding_dim=input_size) self.rnn_layer = nn.RNN(input_size, hidden_size, # 입력 차원, 은닉 상태의 크기 정의 batch_first=batch_first) self.linear = nn.Linear(hidden_size, vocab_size) # 출력은 원-핫 벡터의 크기를 가져야함. 또는 단어 집합의 크기만큼 가져야함. def forward(self, x): # 1. 임베딩 층 # 크기변화: (배치 크기, 시퀀스 길이) =\u0026gt; (배치 크기, 시퀀스 길이, 임베딩 차원) output = self.embedding_layer(x) # 2. RNN 층 # 크기변화: (배치 크기, 시퀀스 길이, 임베딩 차원) # =\u0026gt; output (배치 크기, 시퀀스 길이, 은닉층 크기), hidden (1, 배치 크기, 은닉층 크기) output, hidden = self.rnn_layer(output) # 3. 최종 출력층 # 크기변화: (배치 크기, 시퀀스 길이, 은닉층 크기) =\u0026gt; (배치 크기, 시퀀스 길이, 단어장 크기) output = self.linear(output) # 4. view를 통해서 배치 차원 제거 # 크기변화: (배치 크기, 시퀀스 길이, 단어장 크기) =\u0026gt; (배치 크기*시퀀스 길이, 단어장 크기) return output.view(-1, output.size(2)) # 모델 생성 model = Net(vocab_size, input_size, hidden_size, batch_first=True) # 손실함수 정의 loss_function = nn.CrossEntropyLoss() # 소프트맥스 함수 포함이며 실제값은 원-핫 인코딩 안 해도 됨. # 옵티마이저 정의 optimizer = optim.Adam(params=model.parameters()) Classification with GRU # Copy python class GRU(nn.Module): def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2): super(GRU, self).__init__() self.n_layers = n_layers self.hidden_dim = hidden_dim self.embed = nn.Embedding(n_vocab, embed_dim) self.dropout = nn.Dropout(dropout_p) self.gru = nn.GRU(embed_dim, self.hidden_dim, num_layers=self.n_layers, batch_first=True) self.out = nn.Linear(self.hidden_dim, n_classes) def forward(self, x): x = self.embed(x) h_0 = self._init_state(batch_size=x.size(0)) # 첫번째 히든 스테이트를 0벡터로 초기화 x, _ = self.gru(x, h_0) # GRU의 리턴값은 (배치 크기, 시퀀스 길이, 은닉 상태의 크기) h_t = x[:,-1,:] # (배치 크기, 은닉 상태의 크기)의 텐서로 크기가 변경됨. 즉, 마지막 time-step의 은닉 상태만 가져온다. self.dropout(h_t) logit = self.out(h_t) # (배치 크기, 은닉 상태의 크기) -\u0026gt; (배치 크기, 출력층의 크기) return logit def _init_state(self, batch_size=1): weight = next(self.parameters()).data return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_() "},{"id":67,"href":"/blog/2022-07-08/","title":"2022-07-08 Log","section":"Posts","content":"Optimization # minimize(최소화) f(x) subject to(제약조건) h(x)=0 glaobal minimum, local minimum, local maximum 목적함수: 주어진 점과 임의의 선의 간격(error)를 모두 더함 Gradient # 경사도 벡터: n개의 변수에 대한 함수 f(x)의 $x^*$에서의 편미분 계수 (열 벡터) 경사도 벡터는 $f(x^*)=c$인 표면의 초접평면에 수직, 함수의 최대 증가 방향 Gradient Descent Method # $$x^{(t+1)}=x^{(t)}-\\eta\\frac{dy(x^{(t)})}{dx}$$\nminimize 문제를 풀기 위해 gradient 벡터의 반대 방향으로 이동 (최대 하강 방향) 강하방향(descent direction): gradient 벡터와의 내적이 0보다 작은 경우 ($c^{(k)}ㆍd^{(k)}\u0026lt;0$) $f(x)$ 정식화 가능할 경우 최적성 기준법, 간접법 등, 정식화 할 수 없는 경우 탐색법, 직접법(경사하강법) 등 경사도 벡터 계산 2) 강하 방향 선택 3) 이동거리($\\alpha$) 결정 step size($\\alpha$)를 결정하기 위해 선탐색(황금분할 탐색) 사용 최속 강하법: 경사도 벡터의 반대 반향을 강하 방향으로 선택 켤레 경사법: 경사도 벡터의 반대 방향에 이전의 강하 방향을 더함 목적 함수로 booth 함수, rosenbrock 함수 사용 "},{"id":68,"href":"/blog/2022-07-07/","title":"2022-07-07 Log","section":"Posts","content":"Function # scalar: 양만으로 표시할 수 있는 물리량 vector: 양과 방향으로 표현할 수 있는 물리량 일변수-스칼라함수 # 다항함수, 분수함수, 지수함수, 로그함수, 삼각함수 등 입력: 스칼라, 출력: 스칼라 독립변수: 함수의 출력 결정, 매개변수: 함수의 모양 결정 시그모이드 함수: 모든 실수를 0~1 사이로 찌그러트림, 출력을 확률로 해석 다변수-스칼라함수 # 곡면과 등고선 그래프 입력: 벡터, 출력: 스칼라 평면: 이변수 스칼라 함수, 직선: 우변이 0으로 고정 퍼셉트론: $\\ f(x,y)=ax+by+c$ 일변수-벡터함수 # 평면 또는 공간에 존재하는 곡선 입력: 스칼라, 출력: 벡터 다변수-벡터함수 # 입력: 벡터, 출력: 벡터 인공신경망과 유사 Composite Function: 함수의 합성, $\\ g∘f=g(f(x))$ 신경망은 매우 많은 함수가 겹겹이 합성된 것 (목적함수, 비용함수, 손실함수) 머신러닝 분류 문제 평가 함수로 지수,로그함수 활용 Matrix # 행렬의 차원(크기): 행 개수 x 열 개수 (${m}\\times{n}$) 행렬의 전치: $\\ A^T$ 행렬의 덧셈: 두 행렬의 크기가 같을 때 같은 위치 요소 덧셈 행렬의 곱셈: 행과 열의 요소를 곱해서 더함 단위행렬: 대각 성분이 모두 1, 나머지는 0인 정사각 행렬 대각행렬: 대각 성분이 아닌 모든 성분이 0인 정사각 행렬 대칭행렬: 정사각 행렬에 대해서 $\\ S^T=S$ 직교행렬: 전치된 것이 자신인 역행렬 $\\ A^T=A^{-1}$ 이미지 표현: (C, H, W)=PyTorch 또는 (H, W, C)=TensorFlow 열 결합: 뒤에서 곱하는 벡터의 요소를 계수로 한 모든 열의 선형결합 행 결합: 앞에서 곱하는 행렬의 행 요소를 계수로 뒷 행렬의 행을 조합 (열 결합 전치) 외적: 행렬 반환, 내적: 스칼라 반환 인공신경망 # $a^{(1)}=f(W^{(1)}x+b^{(1)})$ (W=weight, b=bias) 활성 함수: Sigmoid, ReLU, Tanh MNIST 데이터: (28,28)인 손글씨 이미지가 (1,781)인 어레이로 저장 로지스틱 함수: 각각의 y에 대한 확률 계산 (다중 분류에서 정확도가 낮음) softmax: 실수 k개가 0에서 1사이의 숫자 k개로 맵핑 회귀: 입력 x에 대응하는 실수 y값을 출력 Differentiation # 에러를 줄이 위한 목적 (미분 정보를 이용해 탐색 방향 결정) 평균변화율: 함수의 변화를 보는 구간이 중요, 자세한 정보를 위해 간격을 좁힐 필요 순간변화율: 평균변화율의 분모를 순간에 이를 정도로 작게 만듦, 극한값이 미분계수 순간변화율은 그 위치에서 접선의 기울기 sympy 패키지를 사용해 파이썬에서 기호 연산 미분법 # 상수의 미분, 덧셈/뺄셈의 미분, 곱셈의 미분법, 나눗셈의 미분법 함성함수 미분: 라이프니츠 미분법, $\\ \\sqrt{ax^2+bx+c}=\\sqrt{y}$ $z=\\sqrt{x^2+3x},\\quad y=x^2+3x,\\quad z=\\sqrt{y}$ 연쇄법칙(Chain Rule): $\\ \\frac{dz}{dx}=\\frac{dz}{dy}ㆍ\\frac{dy}{dx}$ 연쇄법칙은 인공신경망(합성함수) 역전파 알고리즘의 기본 아이디어 편미분 # 다변수 함수에 구할 수 있는 변화율은 무수히 많음 다변수 함수에서 변수 고정, 한번에 변수 하나만 변화 (윤곽선, 1차원 함수) 편도함수: 다변수 함수일 때 하나의 변수에 대해서만 미분한 도함수, 기호: $\\partial$ Saliency Map: 이미지 분류에서 y에 대한 다변수-스칼라함수의 편미분 계수를 이미지화 Guided Back Propagation: saliency map보다 더 확실한 특징을 보임 다변수 함수의 연쇄법칙은 모든 노드의 미분계수를 더함 Jacobian # 야코비안: $\\ w=f_1(x,y),\\ z=f_2(x,y)$일 때 편도함수 4개를 블록 형태로 표시 로지스틱 함수의 미분: 나눗셈의 미분과 함성함수의 미분을 사용, $\\ \\frac{d}{dz}\\sigma(z)=\\sigma(z)(1-\\sigma(z))$ 소프트맥스 함수의 미분: 벡터함수의 편미분 (인데스 별로 나눠서), ${K}\\times{K}$개의 미분계수 인덱스가 같은 경우 $\\ \\frac{\\partial}{\\partial{z_j}}s_i(z)=s_j(z)(1-s_j(z))$ 인덱스가 다른 경우 $\\ \\frac{\\partial}{\\partial{z_j}}s_i(z)=-s_i(z)s_j(z)$ 직접 미분 # 직접 미분하여 결과를 코딩 정확한 결과, 빠른 속도 장점, 미분을 직접 해야하나는 단점 수치 미분 # 수치적 계산으로 미분 계수를 근사 구현 간단, 변수가 많으면 매우 느리고 수치적으로 불안정 자동 미분 계산이 정확한지 확인할 목적으로 사용 전방 차분법 diff = (f(x+h) - f(x)) / h 중앙 차분법 diff = (f(x+(h/2)) - f(x-(h/2))) / h 식을 모를 경우 brute force \u0026gt; 2변수 이상일 때 이동할 수 있는 방향이 무한대가 되는 단점 gradient descent 방식을 사용해 반복 횟수를 줄임 "},{"id":69,"href":"/blog/2022-07-06/","title":"2022-07-06 Log","section":"Posts","content":"Regular Expression # regexpal.com [wW]oodchuck = Woodchuck, woodchuck ranges: [A-z], [a-z], [0-9] negations: [^Ss], [^A-Z], a^b disjunction: pipe(|), yours|mine ?,,+,.: colou?r(= color, colour), ooh!=o+h!, beg.n(= begin, began) ^(start), $(end): ^[A-Z], \\.$(\\: escape) subset: ([0-9]+), \\1er (= [fast]er) non-capturing groups: /(?:some|a few) (people|cats) ?= (exact match), ?! (does not match) Tokenization # Type vs Token N = number of tokens V = vocabulary (set of types) NLP task: 1)tokenizing words 2)normalizing word formats 3)segmenting sentences simpe way to tokenize: use space chracters issue: punctuation (like Ph.D), clitic (like we\u0026rsquo;re), multiword (like New York) chinese tokenization: don\u0026rsquo;t use spaces, count as a word is complex, so treat each character as token Byte Pair Encoding # subword tokenization (tokens can be parts of words) BPE, Unigram language modeling tokenization, WordPiece add most frequent pair of adjancent tokens in characters er -\u0026gt; er_ -\u0026gt; ne -\u0026gt; new -\u0026gt; lo -\u0026gt; low -\u0026gt; \u0026hellip; frequent subwords are most morphemes like -est or -er Word Normalization # put tokens in a standard format all letters to lower case (but, US vs us) all words to lemma (he is reading -\u0026gt; he be read) morphemes: the smallest meaningful units (stems, affixes) porter stemmer: ational -\u0026gt; ate, sses -\u0026gt; ss abbreviation dictionary can help "},{"id":70,"href":"/blog/2022-07-04/","title":"2022-07-04 Log","section":"Posts","content":"4. 랭크, 차원 # 4-4. 고유 벡터 # 고윳값, 고유 벡터 = 특성 값, 특성 벡터 = 행렬의 특성 고유 벡터(eigenvector): 벡터에 선형 변환을 취했을 때, 방향은 변하지 않고 크기만 변하는 벡터 고윳값(eigenvalue): 선형 변환 이후 변한 크기, 고유 벡터가 변환되는 크기의 정도 4-5. 특이값 분해 # 닮음(similar): $P^{-1}AP=B$를 만족하는 가역 행렬 $P$가 존재 시, 정사각 행렬 $A, B$는 서로 닮음 직교 닮음(orthogonally similar): $B=P^{-1}AP$를 만족하는 직교 행렬 $P$가 존재 시, $B$는 $A$에 직교 닮음 직교 대각화(orthogonal diagonalization): 직교 닮음의 경우에서 정사각 행렬 $B$가 대각 행렬 $D$일 경우 직교 대각화가 가능하기 위해 $A$는 반드시 대칭 행렬 ($A^T=A$) 이어야 함 (공분산 행렬 등) 4-6. 고윳값 분해 # 행렬을 고유 벡터, 고윳값의 곱으로 분해하는 것 직교 벡터 $P$를 고유 벡터를 이용해 만들고 대각 행렬의 원소에 해당하는 것이 고윳값 $A=PDP^T$ 4-7. 특이값 분해 # 정사각 행렬을 대상으로 하는 고윳값 분해와 달리 대상 행렬을 ${m}\\times{n}$ 행렬로 일반화 인수 분해처럼 행렬의 차원 축소를 위한 도구로 사용 차원 축소를 $n$개의 점을 표현할 수 있는 기존 $p$보다 작은 차원인 $d$ 차원인 부분 공간(subspace)을 찾는 문제 데이터와 부분 공간으로부터의 수직 거리를 최소화(제곱합 $A^TA,AA^T$ 사용)하여 부분 공간을 찾음 특이값(singular value): 행렬 $A$를 제곱한 행렬의 고윳값에 루트를 씌운 값, $\\sigma_1=\\sqrt{\\lambda_1}$ $A=U\\Sigma{V^T}$ 행렬 U의 열벡터는 $AA^T$의 고유 벡터로 구성되는 left singular vector 행렬 V의 열벡터는 $A^TA$의 고유 벡터로 구성되는 right singular vector $\\Sigma$의 대각 원소는 행렬 A의 특이값 4-8. 이차식 표현 # 다항식을 벡터 형태로 나타낼 때 사용하는 방법 대칭 행렬 $W$에 대해 $x^TWx$ 형태로 표현한 식 양정치(positive definite): $x^TWx\u0026gt;0, \\text{ for all }x\\neq{0}$ (행렬 W의 고윳값이 모두 0보다 큼) 음정치(negative definite): $x^TWx\u0026lt;0, \\text{ for all }x\\neq{0}$ (행렬 W의 고윳값이 모두 0보다 작음) 4-9. 벡터의 미분 # 타깃 $y=w^Tx=x^Tw$를 데이터 벡터 x에 대해 미분하면 w가 나옴 5. 확률 변수와 확률 분포 # 5-1. 확률 변수 # 확률(probability): 어떤 사건이 일어날 가능성을 수치화시킨 것 모든 확률은 0에서 1 사이에 있으며, 모든 경우인 표본 공간(sample space)의 $P(S)=1$ 동시에 발생할 수 없는 사건들에 대해 각 사건의 합의 확률은 개별 확률이 일어날 확률의 합과 같음 확률 변수(random variable): 확률적으로 정해지는 변수, 동전 던지기에서 확률 변수 $X$는 0 또는 1의 값을 가짐 상수(constant): 변수와 다르게 항상 값이 고정된 수, $\\pi=3.14$ 등 함수(function): 한 집합의 임의의 한 원소를 다른 집합의 한 원소에 대응시키는 관계 5-2. 확률 분포 # 확률 변수가 특정값을 가질 확률의 함수 이산 확률 변수: 확률 변수가 가질 수 있는 값을 셀 수 있음 확률 질량 함수: 이산 확률 변수에서 특정값에 대한 확률을 나타내는 함수, $p_X(x)=P(X=x)$ 연속 확률 변수: 확률 변수가 가질 수 있는 값의 개수를 셀 수 없음 확률 밀도 함수: 연속 확률 변수의 분포를 나타내는 함수, $P(a\\lt{X}\\lt{b})=\\int_a^bf_X(x)dx$ 누적 분포 함수: 주어진 확률 변수가 특정값보다 작거나 같은 확률, $F_X(x)=P(X\\in{-\\infty,x}$ 결합 확률 밀도 함수: 확률 변수 여러 개를 함께 고려하는 확률 분포, $P_{X,Y}(x,y)=P(X=x,Y=y)$ 독립 항등 분포: 두 개 이상의 확률 변수를 고려할 때, 각 확률 변수가 통계적으로 독립이고 동일한 확률 분포(iid)를 따름 5-3. 모집단과 표본 # 모집단(population)은 관심이 있는 대상 전체, 표본(sample)은 모집단의 일부 모집단의 특성을 나타내는 대푯값을 모수(population parameter), 표본의 대푯값(sample statistic)을 표본 통계량 5-4. 평균과 분산 # 산술 평균: 모든 데이터값을 덧셈한 후 데이터 개수로 나누는 것 모평균: 모집단의 평균, $E(X)=\\mu$ 표본 평균: 모평균의 추정량, $\\bar{X}=\\frac{1}{n}\\Sigma^n_{i=1}{x_i}$ Location parameter: 평균의 변화로, 그래프의 위치 변화를 나타냄 분산: 데이터가 얼마나 퍼져 있는지를 수치화, 평균에 대한 편차 제곱의 평균 모분산: $Var(X)=E[(X-\\mu)^2]=\\sigma^2=E(X^2)-\\mu^2$ 표본 분산: $\\sigma^2=s^2=\\frac{1}{n-1}\\Sigma^n_{i=1}(x_i-\\bar{x})^2$ $x_i-\\bar{x}$는 평균에 대한 편차를 의미하며, 편차 제곱의 합을 n-1로 나누는 것은 자유도와 관련 자유도는 변수가 얼마나 자유로운지 나타내는 것으로,\n분산을 구하는 시점에서 이미 표본 평균이 정해져 있어 자유롭게 정할 수 있는 데이터가 n-1개인 것을 의미 Scale parameter: 분산과 같이 데이터의 흩어짐 정도를 결정하는 파라미터 표준 편차: 분산의 양의 제곱근으로 정의, 분산 계산 중 제곱으로 커진 결과를 다시 원래 단위로 조정하는 과정 $$E(\\Sigma^n_{i=1}X_i)=n\\mu_X\\text{, }Var(\\Sigma^n_{i=1}X_i)=n\\sigma^2$$\n5-5. 상관관계 # 공분산(covariance): 두 확률 변수의 상관관계를 나타내는 값, 같은 방향으로 움직이면 양수, 반대의 경우 음수 공분산은 변수 X의 편차와 변수 Y의 편차를 곱한 값의 평균, $Cov(X,Y)=E[(X-\\mu_X)(Y-\\mu_Y)]$ 공분산 행렬: 확률 변수 간 분산, 공분산을 행렬로 표현한 것, 차원 축소 등에서 자주 사용 상관 계수: 공분산을 각 변수의 표준 편차로 나누어 계산 $$Corr(X,Y)=\\frac{Cov(X,Y)}{\\sqrt{Var(X)}\\sqrt{Var(Y)}}=\\frac{Cov(X,Y)}{\\sigma_X\\sigma_Y}$$\n5-6. 균일 분포 # 특정 범위 내에서 확률 분포가 균일한 분포 이산형 균일 분포라면 모든 확률 변수의 확률값이 동일, $X~U(1,N)$ 연속형 균일 분포는 확률 변수의 범위가 연속형, $X~U(a,b)$ 5-7. 정규 분포 # 정규 분포 또는 가우시안 분포는 평균을 중심으로 대칭 형태를 띠는 종 모양 분포 $$f_X(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2} \\text{, } E(X)=\\mu \\text{, } Var(X)=\\sigma^2$$\n$\\frac{x-\\mu}{\\sigma}$는 머신러닝에서 쓰이는 데이터 표준화와 일치 표준 정규 분포: 평균이 0, 분산이 1인 정규 분포 5-8. 이항 분포 # 베르누이 분포, 베르누이 시행: 한 가지 실험에서 결과가 오직 2개인 시행 베르누이 시행의 성공 확률이 p일 때, 실패 확률은 1-p 이항 분포: 성공 확률이 p인 독립적인 베르누이 시행을 n회 했을 때, 성공 횟수 X가 따르는 이산형 확률 분포 다항 분포: 이항 분포를 일반화한 분포, 각 시행에서 나올 수 있는 결과가 m개로 확장 5-9. 최대 가능도 추정 # 가능도, 우도(likelihood): 파라미터가 주어질 때 해당 표본이 수집될 확률 가능도가 높다는 것은 해당 파라미터가 실젯값일 확률이 높다는 뜻 가능도 함수 $L(\\theta|x)=\\Pi^n_{i=1}{f(x_i|\\theta)}$ 로그 함수가 1대1 함수이기 때문에 가능도 함수에 로그 함수를 취할 수 있음 (log-likelihood function) 많은 확률을 곱할 경우 0에 가까워지기 때문에 계산상의 오류를 해결하기 위해 로그를 취함 최대 가능도 추정량(MLE): 파라미터별 가능도를 구해 가장 높은 가능도를 파라미터 추정값으로 사용 5-10. 최대 사후 추정 # 조건부 확률: 조건이 주어질 때의 확률, $P(A|B)=\\frac{P({A}\\bigcap{B})}{P(B)}$ 두 사건이 독립일 경우, 두 사건이 동시에 발생할 확률($P({A}\\bigcap{B}$)은 각 사건이 일어날 확률의 곱과 같음 Bayesian: 확률 분포의 파라미터를 상수로 보는 일반적인 빈도주의(Frequentist)와 달리 파라미터를 확률 변수로 보는 방법 베이즈 추정: 파라미터 $\\theta$가 확률 변수이므로 사전 확률 밀도 함수 $P(\\theta)$를 구할 수 있음 $P(\\theta,x)=P(x|\\theta)P(\\theta)$ 사후 확률 밀도 함수 $P(\\theta|x)\\propto{P(x|\\theta)P(\\theta)}$ 최대 사후 추정(MAP): 사후 확률 밀도 함수 $P(\\theta|x)$를 최대화하는 파라미터 $\\theta$ 6. 최적화 # 6-1. 컨벡스 셋 # 직선은 시작과 끝이 존재하지 않지만, 선분은 시작과 끝 지점이 존재 아핀 셋(affine set): $wx_1+(1-w)x_2\\in{C}$를 만족하는 집합 C 함수 $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$가 존재할 때,\n선형 함수 $f(x)=Wx$,\n아핀 함수 $f(x)=Wx+b$ 컨벡스 셋(convex set): 두 점 $x_1,x_2\\in{C}$에 대해 아래 조건을 만족하는 집합 C $$wx_1+(1-w)x_2\\in{C}\\text{ }(0\\le{w}\\le{1})$$\n컨벡스 셋은 두 점을 잇는 직선을 포함하는 아핀 셋과 달리 두 점 사이의 선분을 포함 (집합의 경계가 존재, 컨벡스 셋 $\\subset$ 아핀 셋) 컨벡스 헐(convex hull): 선분이 아닌, 주어진 점들을 포함하는 컨벡스 셋 초평면(hyperplane): 서포트 벡터 머신 알고리즘의 핵심 개념, ${x|w^Tx=b}$ 내적값 b가 0일 경우 벡터 w와 벡터 $x-x_0$는 수직 반공간(halfspace): 초평면으로 나뉜 공간의 일부, ${w^Tx\\le{b}}$ 6-2. 컨벡스 함수 # 컨벡스 함수: $$f(wx_1+(1-w)x_2 \\le wf(x_1)+(1-w)f(x_2)$$ 컨벡스 함수에서 등호가 없고 $0 \\le w \\le 1$이면 strictly 컨벡스라고 말함 콘케이브(concave): 컨벡스의 반대되는 개념 (-f가 컨벡스할 경우의 f) 컨벡스 함수의 예로 지수 함수, 절댓값 함수, 멱함수, 지시 함수, 최대 함수 등이 있음 미분이 가능하다는 말은 그래디언트(gradient) $\\nabla f$가 존재한다는 뜻 1차 미분 조건: 최적값 탐색에 사용, $f(x_2) \\ge f(x_1)+\\nabla{f(x_1)^T}(x_2-x_1)$ 그래디언트 값이 0일 때, $x_1$은 함수 f에 대한 전역 최솟값(global minimizer) 2차 미분 조건: 함수 f가 두 번 미분 가능할 경우, $\\nabla^2f(x) \\ge 0$ 얀센의 부등식: $f(wx_1+(1-w)x_2) \\le wf(x_1)+(1-w)f(x_2)$ "},{"id":71,"href":"/blog/2022-06-30/","title":"2022-06-30 Log","section":"Posts","content":"15-01. Attention Mechanism # seq2seq 모델은 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 해서 정보 손실이 발생하며,\nRNN의 고질적인 문제인 기울기 소실 문제도 존재 기계 번역 분야에서 입력 문장이 길면 번역 품질이 떨어지는 것을 보정하기 위해 어텐션 기법 활용 어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시접마다,\n인코더에서 전체 입력 문장을 다시 한 번 참고한다는 점 Attention Function # Attention(Q, K, V) = Attention Value 어텐션 함수는 주어진 Query에 대해서 모든 Key와의 유사도를 각각 구하고,\n유사도를 키와 맵핑되어있는 각각의 Value에 반영, 이후 유사도가 반영된 Value를 모두 더해 리턴 Q(Query): t 시점의 디코더 셀에서의 은닉 상태\nK(Keys): 모든 시점의 인코더 셀의 은닉 상태들\nV(Values): 모든 시점의 인코더 셀의 은닉 상태들\nDot-Product Attention # 어텐션 메커니즘에서 출력 단어를 예측하기 위해 디코더 셀은\nt-1의 hidden state, t-1에 나온 출력 단어, Attention Value $a_t$를 필요 제안자의 이름을 따서 루옹(Luong) 어텐션이라고도 함 1. Attention Score # $a_t$를 구하기 위해서는 Attention Score를 구해야 함\n(인코더의 모든 은닉 상태 각각의 디코더의 현 시점의 은닉 상태 $s_t$와 얼마나 유사한지 판단하는 스코어) Dot-Product Attention에서는 스코어 값을 구하기 위해 $s_t$를 전치하고 각 은닉 상태와 내적을 수행 스코어 함수 $score(s_t,h_i)={s^T_t}{h_i}$ $s_t$와 인코더의 모든 은닉 상태의 어텐션 스코어 모음값 $e^t=[{s^T_t}{h_1},\u0026hellip;,{s^T_t}{h_N}]$ 스코어 함수에 따라 scaled dot, general, concat, location-base 어텐션 등이 존재 2. Attention Distribution # $e^t$에 softmax 함수를 적용하여 모든 값을 합하면 1이 되는 확률 분포,\nAttention Distribution을 얻으며, 분포 각각의 값을 Attention Weight라 함 어텐션 분포 $\\alpha^t=softmax(e^t)$ 3. Attention Value # 어텐션의 최종 결과값을 얻기 위해서 각 인코더의 은닉 상태와 어텐션 가중치들을 곱하고,\n최종적으로 모두 더하는 Weighted Sum을 진행 어텐션 값이 구해지면 어텐션 메커니즘은 $a_t$를 $s_t$와 결합(concatenate)하여 하나의 벡터 $v_t$를 생성 $v_t\\text{를 }\\hat{y}$ 예측 연산의 입력으로 사용해 인코더로부터 얻은 정보를 활용하여 $\\hat{y}$를 예측 $$a_t=\\Sigma^N_{i=1}{\\alpha^t_i}{h_i}$$\n15-02. Bahdanau Attention # 바다나우 어텐션 함수의 Query는 t 시점의 은닉 상태가 아니라 t-1 시점의 은닉 상태를 사용 $score(s_{t-1},h_i)={W^T_\\alpha}\\tanh{({W_b}{s_{t-1}}+{W_c}{h_i})}$ $W_a,W_b,W_c$는 학습 가능한 가중치 행렬을 의미하며, $s_{t-1}$와 $h_1,h_2,h_3,h_4$의 어텐션 스코어를\n각각 구하는 병렬 연산을 위해 $h_1,h_2,h_3,h_4$를 하나의 행렬 $H$로 변환 $e^t={W^T_\\alpha}\\tanh{({W_b}{s_{t-1}}+{W_c}H)}$ 컨텍스트 벡터를 구하면, 현재 시점의 입력인 단어의 임베딩 벡터와 연결(concatenate)하고,\n현재 시점의 새로운 입력으로 사용 16-01. Transformer # 어텐션을 RNN의 보정을 위한 용도로 사용하지 않고, 어텐션만으로 인코더와 디코더를 생성 Transformer Hyperparameter # $d_{model}=152$\n트랜스 포머의 인코더와 디코더에서의 정해진 입력과 출력의 크기, 임베딩 벡터의 차원\n$num_{layers}=6$\n트랜스포머 모델에서 인코더와 디코더가 총 몇 층으로 구성되었는지\n$num_{heads}=8$\n어텐션을 여러 개로 분할해서 병렬로 수행하고 결과값을 다시 하나로 합치는데, 이때 이 병렬의 개수\n$d_{ff}=2048$\n피드 포워드 신경망의 은닉층의 크기, 입력층과 출력층의 크기는 $d_{model}$\nPositional Encoding # RNN은 단어의 위치에 따라 단어를 순차적으로 입력받아 처리하는 특성으로 인해 각 단어의 위치 정보를 가짐 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니기 때문에\n단어의 위치 정보를 얻기 위해 각 단어의 임베딩 벡터에 위치 정보들을 더해 모델의 입력으로 사용(포지셔널 인코딩) 트랜스포머는 위치 정보를 가진 값을 만들기 위해 아래 함수를 사용 $$PE_{(pos,2i)}=\\sin{(pos/10000^{2i/d_{model}})}$$ $$PE_{(pos,2i+1)}=\\cos{(pos/10000^{2i/d_{model}})}$$\n사인 함수와 코사인 함수의 값을 임베딩 벡터에 더해 단어에 순서 정보를 부여 $pos$는 입력 문장에서의 임베딩 벡터의 위치, $i$는 임데빙 벡터 내의 차원의 인덱스 의미 $d_{model}$은 트랜스포머의 모든 층의 출력 차원을 의미하는 트랜스포머의 하이퍼파라미터 Self-Attention # 어텐션을 자기 자신에게 수행하는 것 Q, K, V는 모두 입력 문장의 모든 단어 벡터들을 의미 셀프 어텐션은 입력 문장 내의 단어들끼리 유사도를 구해 it이 어떤 단어와 연관되었는지 확률을 찾아냄 셀프 어텐션은 인코더의 초기 입력인 $d_{model}$의 차원을 가지는 단어 벡터들을 사용하여 수행하지 않고,\n각 단어 벡터들로부터 Q벡터, K벡터, V벡터를 얻는 작업을 거침 $d_{model}=512$의 차원을 가졌던 각 단어 벡터들은 Q벡터, K벡터, V벡터로 변환되면서\n$d_{model}$을 $num_{heads}$로 나눈 값 64를 차원으로 갖게 됨 Scaled dot-product Attention # 트랜스포머에서는 스케일드 닷-프로적트 어텐션을 사용 벡터마다 일일히 컨텍스트 벡터를 구하는 벡터 연산을 하지 않고,\n문장 행렬에 가중치 행렬을 곱하여 구한 Q행렬, K행렬, V행렬에 행렬 연산을 수행 행렬 연산에서 어텐션 스코어는 행렬의 값에 전체적으로 $\\sqrt{d_k}$를 나누어 스코어 값을 가지는 행렬을 구함 $$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\nMulti-head Attention # 한 번의 어텐션을 하는 것보다 여러번의 어텐션을 병렬로 사용하는 것이 더 효과적이기 때문에\n$d_{model}$의 차원을 $num_{heads}$개로 나누어 Q, K, V에 대해서 $num_{heads}$개의 병렬 어텐션을 수행 각각의 어텐션 값 행렬을 어텐션 헤드라고 불는데, 이때 가중치 행렬 $W^Q, W^K, W^V$의 값은 어텐션 헤드마다 전부 다름 멀티 헤드 어텐션은 어텐션을 병렬로 수행하여 다른 시각으로 정보들을 수집 벙렬 어텐션 수행 후 모든 어텐션 헤드를 연결(concatenate)하여 $(seq_{len}, d_{model})$ 크기의 행렬 생성 연결한 행렬에 가중치 행렬 $W^O$를 곱한 것이 멀티 헤드 어텐션의 최종 결과물이며, 인코더의 입력이었던 문장 행렬과 동일\n(인코더에서의 입력의 킉가 출력에서도 동일 크기로 계쏙 유지되어야만 다음 인코더에서도 다시 입력이 될 수 있음) Padding Mask # 입력 문장에 토큰이 있을 경우 어텐션에서 제외하기 위해 -1e9라는 아주 작은 음수 값을 곱함 Masking: 어텐션에서 제외하기 위해 값을 가리는 것 어텐션 스코어 행렬에서 행에 해당하는 문장은 Query, 열에 해당하는 문장은 Key이며,\nKey에 가 있는 경우 해당 열 전체를 마스킹 Position-wise FFNN # 포지션 와이즈 FFNN은 인코더와 디코더에서 공통적으로 가지고 있는 서브층으로, 완전 연결 FFNN을 의미 $FFNN(x)=MAX(0,x{W_1}+b_1){W_2}+{b_2}$ $x$는 멀티 헤드 어텐션의 결과로 나온 $(seq_{len}, d_{model})$ 크기의 행렬을 의미,\n가중치 행렬 $W_1\\text{은 }(d_{model},d_{ff})\\text{, }W_2\\text{은 }(d_{ff},d_{model})$의 크기를 가짐 서브층을 지난 인코더의 최종 출력은 여전히 인코더의 입력의 크기였던 $(seq_{len}, d_{model})$의 크기가 보존 Residual Connection # 트랜스포머에서 두 개의 서브층을 가진 인코더에 추가적으로 사용하는 Add \u0026amp; Norm 기법 중 Add에 해당 잔차 연결은 서브층의 입력과 출력을 더하는 것으로, 컴퓨터 비전 분야에서 주로 사용되는 모델의 학습을 돕는 기법 $x+Sublayer(x)$로 표현할 수 있으며, 멀티 헤드 어텐션이라면 $H(x)=x+Multi\\text{-}head\\ Attention(x)$과 같음 Layer Normalization # 잔차 연결의 입력을 $x$, 잔차 연결과 층 정규화 두 가지 연산을 모두 수행한 후의 결과 행렬을 $LN$이라 할 때,\n잔차 연결 후 정규화 연산을 수식으로 표현하면 $LN=LayerNorm(x+Sublayer(x))$와 같음 총 정규화는 텐서의 마지막 차원에 대해서 평균과 분산을 구하고, 이를 가지고 어떤 수식을 통해 값을 정규화하여 학습을 도움 총 정규화를 평균과 분산을 통한 정규화, 감마와 베타를 도입하는 것으로 나누었을 때,\n우선, 평균과 분산을 통해 벡터 $x_i$를 정규화 $x_i$는 벡터인 반면, 평균 $\\mu_i\\text{과 분산 }\\sigma^2_i$은 스칼라이기 때문에,\n벡터 $x_i$의 각 $k$차원의 값은 다음과 같이 정규화 ($\\epsilon$은 분모가 0이 되는 것을 방지하는 값) $$\\hat{x_{i,k}}=\\frac{x_{i,k}-\\mu_i}{\\sqrt{\\sigma^2_i+\\epsilon}}$$\nLook-ahead Mask # 입력 단어를 매 시점마다 순차적으로 입력받는 RNN 계열의 신경망에 반해,\n트랜스포머는 문장 행렬을 한 번에 받기 때문에 미래 시점의 단어까지 참고할 수 있는 현상 발생 룩-어헤드 마스크는 디코더의 첫번째 서브층에서 이루어지며,\n자기 자신보다 미래에 있는 단어들은 참고하지 못하도록 마스킹함 룩-어헤드 마스크를 한다고해서 패딩 마스크가 불필요한 것이 아니므로 패딩 마스크를 포함하도록 구현 Endocer-Decoder Attention # 디코더의 두번째 서브층은 멀티 헤드 어텐션을 수행한다는 점에서 이전의 어텐션들과 공통점이 있지만,\nQuery와 Key, Value가 달라 셀프 어텐션이 아님 인코더의 첫번째 서브층 Query = Key = Value\n디코더의 첫번째 서브층 Query = Key = Value\n디코더의 두번째 서브층 Query: 디코더 행렬 / Key = Value: 인코더 행렬\n16-02. Transformer Chatbot # 트랜스포머를 이용한 한국어 챗봇 참고 챗본 데이터 사용 17-01. Pre-training # 사전 훈련된 워드 임베딩 # Word2Vec나 GloVe 등의 워드 임베딩은 하나의 단어가 하나의 벡터값으로 맵핑되므로,\n문맥을 고려하지 못하여 다의어나 동음이의어를 구분하지 못하는 문제 사전 훈련된 언어 모델 # 언어 모델은 주어진 텍스트로부터 다음 단어를 예측하도록 학습하여 별도의 라벨 없이 학습 가능 다의어를 구분할 수 없었던 문제점을 해결하고, RNN 계열의 신경망에서 탈피하기 위해 트랜스포머로 학습 시도 트랜스포머로 데이터를 학습시킨 언어 모델 GPT-1 등은 다양한 태스크에서 높은 성능을 얻음 이전 단어들로부터 다음 단어를 예측하는 언어 모델의 특성으로는 양방향 언어 모델을 사용할 수 없으므로,\nELMo에서는 두 개의 단방향 언어 모델을 따로 준비해 학습하는 방법을 사용했는데 여기서 발전된 마스크드 언어 모델이 등장 Masked Language Model # 마스크드 언어 모델은 입력 텍스트의 단어 집합의 15%의 단어를 랜덤으로 Masking 빈칸 채우기 형식으로 마스킹된 단어들을 예측하게 함 17-02. BERT # 구글이 공개한 사전 훈련된 모델로 수많은 NLP 태스크에서 최고 성능을 보임 트랜스포머를 이용하여 구현되었으며, 위키피디아와 BooksCorpus 같이 라벨이 없는 텍스트 데이터로 사전 훈련된 언어 모델 사전 훈련된 모델을 라벨이 있는 다른 작업에서 추가 훈련과 함께 하이퍼파라미터를 재조정,\n다른 작업에 대해서 파라미터 재조정을 위한 추가 훈련 과정을 Fine-tuning이라고 함 BERT-Base: L=12, D=768, A=12: 110M개의 파라미터 BERT-Large: L=24, D=1024, A=16: 340M개의 파라미터 Contextual Embedding # BERT의 입력은 기존 모델들과 마찬가지로 임베딩 층을 지난 임베딩 벡터 BERT의 연산을 거친 후의 출력 임베딩은 문장의 문맥을 모두 참고한 문맥을 반영한 임베딩이 됨 BERT는 트랜스포머 인코더를 12번 쌓은 구조로, 셀프 어텐션을 통해 문맥을 반영 Subword Tokenizer # BERT는 서브워드 토크나이저로 WordPiece 토크나이저를 사용 자주 등장하는 단어는 그대로 단어 집합에 추가하지만, 자주 등장하지 않는 단어는 서브워드로 분리해 집합에 추가하며,\n집합이 만들어지고 나면 단어 집합을 기반으로 토큰화 수행 BERT에서 토큰이 단어 집합에 존재할 경우 해당 토큰을 분리하지 않지만,\n존재하지 않으면 토큰을 서브워드로 분리하고, 첫번째 서브워드를 제외한 나머지 서브워드들은 앞에 #를 붙인 것을 토큰으로 함 #은 서브워드들이 단어의 중간부터 등장하는 것을 알려주기 위해 표시해둔 기호 Copy python from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained(\u0026#34;bert-base-uncased\u0026#34;) # BERT-Base의 토크나이저 Position Embedding # 포지셔널 인코딩과 유사하지만, 위치 정보를 사인 함수와 코사인 함수가 아닌 학습을 통해서 얻는 방법 위치 정보를 위한 임베딩 층을 하나 더 사용하고, 입력마다 포지션 임베딩 벡터를 더해줌 MLM (Pre-training) # BERT는 사전 훈련을 위해서 입력 텍스트의 15%의 단어를 랜덤으로 마스킹 [MASK] 토큰이 파인 튜닝 단계에서 나타나지 않아 사전 학습 단계와 파인 튜닝 단계에서의 불일치 문제가 생기는데,\n이를 완화하기 위해 마스킹 단어 중 80%는 [MASK]로 변경, 10%는 랜덤으로 단어가 변경, 10%는 동일하게 둠 NSP (Pre-training) # BERT는 두 개의 문장을 준 후에 이어지는 문장인지 아닌지를 맞추는 방식으로 훈련 BERT의 입력에서 [SEP]라는 특별 토큰을 사용해서 문장을 구분 두 문장이 실제 이어지는 문장인지 아닌지에 대한 이진 분류 문제를 [CLS] 토큰의 위치로 결정 Segment Embedding # WordPiece Embedding, Position Embedding과 함께 두 개의 문장을 구분하기 위한 목적으로 사용되는 임베딩 층 세그먼트 임베딩으로 구분되는 BERT의 입력에서 두 개의 문장은 두 종류의 텍스트, 두 개의 문서일 수 있음 Find-tuning # 영화 리뷰 감성 분류, 로이터 뉴스 분류 등 Single Text Classification을 위해,\n문서의 시작에 [CLS] 토큰을 입력해 분류에 대해 예측 태깅 작업을 위해 각 토큰의 위치에 밀집층을 사용하여 분류에 대해 예측 자연어 추론 등의 Text Pair Classification을 위해,\n텍스트 사이에 [SEP] 토큰을 집어넣고 두 종류의 세그먼트 임베딩을 모두 사용하여 문서를 구분 QA(Question Answering)를 풀기 위해 질문과 본문이라는 두 개의 텍스트의 쌍을 입력 (SQuAD v1.1) Attention Mask # BERT가 어텐션 연산을 할 때, 불필요하게 패딩 토큰에 대해서 어텐션을 하지 않도록 실제 단어와 패딩 토큰을 구분하는 입력 숫자 1은 실제 단어로 마스킹을 하지 않고, 숫자 0은 패딩 토큰으로 마스킹을 함 17-03. Pre-training 실습 # 구글 BERT의 마스크드 언어 모델 실습 참고 한국어 BERT의 마스크드 언어 모델 실습 참고 구글 BERT의 다음 문장 예측 참고 한국어 BERT의 다음 문장 예측 참고 17-07. Sentence BERT(SBERT) # BERT의 문장 임베딩의 성능을 우수하게 개선시킨 모델 문장 쌍 분류 태스크 또는 문장 쌍 회귀 태스크로 파인 튜닝 Sentence Embedding # [CLS] 토큰은 입력된 문장에 대한 총체적 표현으로, [CLS] 토큰 자체를 입력 문장의 벡터로 간주 문장 벡터를 얻기 위해 [CLS] 토큰뿐 아니라, BERT의 모든 출력 벡터들을 평균냄 출력 벡터들의 평균을 pooling이라 하며, mean pooling, max pooling 등이 있음 18. BERT 실습 # Colab에서 TPU 사용하기 Transformers의 모델 클래스 불러오기 KoBERT를 이용한 네이버 영화 리뷰 분류하기 TFBertForSequenceClassification KoBERT를 이용한 KorNLI 풀어보기 (다중 클래스 분류) KoBERT를 이용한 개체명 인식 KoBERT를 이용한 기계 독해 BERT의 문장 임베딩(SBERT)을 이용한 한국어 챗봇 Faiss와 SBERT를 이용한 시맨틱 검색기 19. Topic Modelling # 토픽이라는 문서 집합의 추상적인 주제를 발견하기 위한 통계적 모델 중 하나 텍스트 본문의 숨겨진 의미 구조를 발견하기 위해 사용되는 텍스트 마이닝 기법 19-01. LSA # SVD # 특이값 분해(Singular Value Decomposition)는 $A$가 ${m}\\times{m}$ 행렬일 때,\n다음과 같이 3개의 행렬의 곱으로 분해(decomposition)하는 것 $$A={U}\\Sigma{V^T}$$\n각 3개 행렬은 다음과 같은 조건을 만족\n${U}\\text{: }{m}\\times{m}\\text{ 직교행렬 }(AA^T=U(\\Sigma\\Sigma^T)U^T)$\n$V\\text{: }{n}\\times{n}\\text{ 직교행렬 }(A^TA=U(\\Sigma^T\\Sigma)V^T)$\n$\\Sigma\\text{: }{m}\\times{n}\\text{ 직사각 대각행렬}$ Truncated SVD # Full SVD에서 나온 3개의 행렬에서 일부 벡터들을 삭제시킨 절단된 SVD 대각 행렬 $\\Sigma$의 대각 원소의 값 중에서 상위값 t개만 남기고,\nU행렬과 V행렬의 t열까지만 남김 일부 벡터들을 삭제해 데이터의 차원을 줄이는 것으로 계산 비용이 낮아지는 효과를 얻음 또한 상대적으로 중요하지 않은 정보(노이즈)를 삭제해 기존의 행렬에서 드러나지 않았던 심층적인 의미 확인 Latent Semantic Analysis(LSA) # BoW에 기반한 DTM이나 TF-IDF는 단어의 빈도 수를 이용한 수치화 방법으로 단어의 의미를 고려하지 못함 LSA는 DTM이나 TF-IDF 행렬에 절단된 SVD를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌어냄 문서의 유사도 계산 등에서 좋은 성능을 보여줌 SVD의 특성상 이미 계산된 LSA에 새로운 데이터를 추가하려면 처음부터 다시 계산해야해 업데이트가 어려움 19-02. LDA # Latent Dirichlet Allocation(LDA) # 문서의 집합으로부터 어떤 토픽이 존재하는지를 알아내기 위한 알고리즘 문서들은 토픽들의 혼합으로 구성되어져 있으며, 토픽들은 확률 분포에 기반하여 단어들을 생성한다 가정 문서가 작성되었다는 가정 하에 토픽을 뽑아내기 위해 아래 과정을 역으로 추적하는 역공학을 수행 LDA의 가정 # 문서에 사용할 단어의 개수 N을 정합 문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정 문서에 사용할 각 단어를 (아래와 같이) 정함\n3-1. 토픽 분포에서 토픽 T를 확률적으로 고름\n3-2. 선택한 토픽 T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어를 고름 LDA 수행 # 사용자는 알고리즘에게 토픽의 개수 k를 알려줌 모든 단어를 k개 중 하나의 토픽에 할당 모든 문서의 모든 단어에 대해서 아래의 사항을 반복 진행\n3-1. 어떤 문서의 단어 w는 자신의 잘못된 토픽에 할당되어 있지만,\n다른 단어들은 올바른 토픽에 할당되어 있다는 가정 하에 단어 w의 토픽을 재할당 LSA DTM을 차원 축소하여 축소 차원에서 근접 단어들을 토픽으로 묶음 LDA 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽 추출 19-08. BERTopic # BERT embeddings과 클래스 기반 TF-IDF를 활용하여\n주제 설명에서 중요한 단어를 유지하면서도 쉽게 해석할 수 있는 조밀한 클러스터를 만드는 토픽 모델링 기술 텍스트 데이터를 SBERT로 임베딩 문서를 군집화 (UMAP을 사용해 임베딩의 차원을 줄이고 HDBSCAN 기술을 사용해 클러스터링) 토픽 표현을 생성 (클래스 기반 TF-IDF 토픽 추출) 20. Text Summarization # Extractive Summarization # 원문에서 중요한 핵심 문장 또는 단어구 몇 개를 뽑아서 이들로 구성된 요약문을 만드는 방법 추출적 요약의 결과로 나온 요약문의 문장이나 단어구들은 전부 원문에 있는 문장들 대표적인 알고리즘으로 머신 러닝 알고리즘 TextRank가 있음 Abstractive Summarization # 원문에 없던 문장이라도 핵심 문맥을 반영한 새로운 문장을 생성해서 원문을 요약하는 방법 주로 seq2seq 같은 인공 신경망을 이용하지만, 지도 학습이기 때문에 라벨 데이터가 있어야함 추상적 요약 구현 # 아마존 리뷰 데이터 사용 TextRank # 페이지랭크를 기반으로 한 텍스트 요약 알고리즘으로,\n텍스트랭크에서 그래프의 노드들은 문장들이며 각 간선의 가중치는 문장들 간의 유사도를 의미 사전 훈련된 GloVe 및 테니스 관련 기사 데이터 사용 21. Question Answering(QA) # Babi 데이터셋 # ID는 각 문장의 번호를 의미, 스토리가 시작될 때는 1번으로 시작 라벨의 supporting fact는 실제 정답이 주어진 스토리에서 몇 번 ID 문장에 있었는지를 알려줌 메모리 네트워크 구조 # 두 개의 문장, 스토리 문장과 질문 문장이 입력으로 들어오며, 두 문장은 각각 임베딩 과정을 거침 Embedding C를 통해서 임베딩 된 스토리 문장과 Embedding B를 통해서 임베딩 된 질문 문장은\n내적을 통해 각 단어 간 유사도를 구하고, 그 결과가 softmax 함수를 지나서\nEmbedding A로 임베딩이 된 스토리 문장에 더해짐\n(Embedding A, B, C는 각각 별개의 임베딩 층) Query(질문 문장)와 Key(스토리 문장)의 유사도를 구하고 softmax 함수를 통해 정규화해\nValue(스토리 문장)에 더하는 어텐션 메커니즘과 유사 어텐션 메커니즘을 통해서 질문 문장과의 유사도를 반영한 스토리 문장 표현을 얻고,\n이를 질문 표현과 연결(concatenate)하여 LSTM과 밀집층의 입력으로 사용 QA 태스크 풀기 # Babi 데이터셋 사용 MeaN으로 한국어 QA # 한국어 Babi 데이터셋 사용 (훈련 데이터, 테스트 데이터) 22. GPT # Generative Pre-trained Transformer(GPT) # GPT 설명 참고 GPT 실습 # KoGPT-2를 이용한 문장 생성 KoGPT-2 텍스트 생성을 이용한 한국어 챗봇 KoGPT-2를 이용한 네이버 영화 리뷰 분류 KoGPT-2를 이용한 KorNLI 분류 "},{"id":72,"href":"/blog/2022-06-29/","title":"2022-06-29 Log","section":"Posts","content":"09-01. Word Embedding # Sparse Representation # 벡터 또는 행렬의 값이 대부분 0으로 표현되는 방법, one-hot vector 등 단어의 개수가 늘어나면 벡터의 차원이 한없이 커지는 문제, 공간적 낭비 발생 Dense Representation # 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤 (0과 1뿐 아니라 실수 포함) Word Embedding # 단어를 밀집 벡터의 형태로 표현하는 방법 Embedding Vector: 워드 임베딩 과정을 통해 나온 결과 LSA, Word2Vec, FastText, Glove 등 09-02. Word2Vec # Distributed Representation # 희소 표현을 다차원 공간에 벡터화하는 방법 분산 표현 방법은 분포 가설이라는 가정 하에 만들어진 표현 방법 분포 가설: 비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다 (귀엽다, 예쁘다, 애교 등의 단어가 주로 함께 등장할 경우 벡터화 시 유사한 벡터값을 가짐) 저차원에 단어의 의미를 여러 차원에다가 분산하여 표현하기 대문에 단어 벡터 간 유의미한 유사도 계산 가능 Word2Vec의 학습 방식으로 CBOW(Continuous Bag of Words)와 Skip-Gram이 존재 CBOW # 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법 중심 단어를 기준으로 window size만큼 앞뒤로 단어를 확인 (2n개의 단어 확인) Sliding Window: window를 옆으로 움직여서 주변 단어와 중심 단어의 선택을 변경해가며 학습 데이터셋 생성 CBOW의 인공 신경망은 주변 단어들의 one-hot vector를 입력으로 중간 단어의 one-hot vector를 예측 Word2Vec은 은닉층이 1개이며, 활성화 함수 없이 룩업 테이블이라는 연산을 담당하는 projection layer로 불림 입력층과 투사층 사이의 가중치 W는 ${V}\\times{M}$ 행렬, 투사층에서 출력층 사이의 가중치 W\u0026rsquo;는 ${M}\\times{V}$ 행렬 W와 W\u0026rsquo;는 동일한 행렬의 전치가 아니라 서로 다른 행렬이기 대문에 CBOW는 W와 W\u0026rsquo;를 학습해가는 구조를 가짐 입력 벡터와 가중치 W 행렬의 곱은 W 행렬의 i번째 행을 그대로 읽어오는 것(lookup)과 동일 주변 단어의 one-hot vector와 가중치 W를 곱한 결과 벡터들은 투사층에서 만나 평균인 벡터를 구함 평균 벡터는 두 번째 가중치 행렬 W\u0026rsquo;와 곱해져서 one-hot vector들과 차원이 V로 동일한 벡터가 나옴 해당 벡터에 softmax 함수를 거쳐 다중 클래스 분류 문제를 위한 score vector를 생성 score vector의 j번째 인덱스가 가진 값은 j번째 단어가 중심 단어일 확률로,\nscore vector $\\hat{y}$와 중심 단어의 one-hot vector $y$의 오차를 줄이기 위해 cross-entropy 함수 사용 Skip-gram # CBOW와 반대로 중심 단어에서 주변 단어를 예측 중심 단어만을 입력으로 받기 때문에 투사층에서 벡터들의 평균을 구하는 과정은 없음 전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려짐 NNLM vs Word2Vec # NNLM은 다음 단어를 예측하는 목적이지만, Word2Vec(CBOW)은 중심 단어를 예측하는 목적, 때문에 NNLM이 이전 단어들만 참고한다면, Word2Vec은 예측 단어의 전후 단어들을 모두 참고 Word2Vec은 NNLM에 존재하던 활성화 함수가 있는 은닉층을 제거하여 학습 속도에서 강점을 가짐 09-03. Word2Vec 실습 # 위키피디아 실습 Copy python from gensim.models import Word2Vec from gensim.models import KeyedVectors model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0) size: 워드 벡터의 특징 값, 임베딩된 벡터의 차원 window: context window size min_count: 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 무시) workers: 학습을 위한 프로세스 수 sg: 0은 CBOW, 1은 Skip-gram Copy python # 입력한 단어에 대해서 가장 유사한 단어들을 출력 model_result = model.wv.most_similar(\u0026#34;man\u0026#34;) print(model_result) [(\u0026#39;woman\u0026#39;, 0.842622697353363), (\u0026#39;guy\u0026#39;, 0.8178728818893433), (\u0026#39;boy\u0026#39;, 0.7774451375007629), (\u0026#39;lady\u0026#39;, 0.7767927646636963), (\u0026#39;girl\u0026#39;, 0.7583760023117065), (\u0026#39;gentleman\u0026#39;, 0.7437191009521484), (\u0026#39;soldier\u0026#39;, 0.7413754463195801), (\u0026#39;poet\u0026#39;, 0.7060446739196777), (\u0026#39;kid\u0026#39;, 0.6925194263458252), (\u0026#39;friend\u0026#39;, 0.6572611331939697)] Copy python model.wv.save_word2vec_format(\u0026#39;eng_w2v\u0026#39;) # 모델 저장 loaded_model = KeyedVectors.load_word2vec_format(\u0026#34;eng_w2v\u0026#34;) # 모델 로드 Copy python # 사전 훈련된 Word2Vec 임베딩 urllib.request.urlretrieve(\u0026#34;https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\u0026#34;, filename=\u0026#34;GoogleNews-vectors-negative300.bin.gz\u0026#34;) word2vec_model = KeyedVectors.load_word2vec_format(\u0026#39;GoogleNews-vectors-negative300.bin.gz\u0026#39;, binary=True) # shape(3000000, 300) Copy python # 두 단어의 유사도 계산 print(word2vec_model.similarity(\u0026#39;this\u0026#39;, \u0026#39;is\u0026#39;)) # 0.407970363878 09-04. Negative Sampling # Word2Vec이 학습 과정에서 전체 단어 집합이 아니라 일부 단어 집합에만 집중할 수 있도록 하는 방법 중심 단어에 대해 무작위로 선택된 일부 단어 집합에 대해 긍정 또는 부정을 예측하는 이진 분류 수행 전체 단어 집합의 크기만큼 선택지를 두고 다중 클래스 분류 문제를 푸는 Word2Vec보다 효율적인 연산 SGNS # 네거티브 샘플링을 사용하는 Skip-gram은 중심 단어와 주변 단어가 모두 입력이 되고,\n두 단어가 실제로 윈도우 크개 내에 존재하는 이웃 관계인지 확률을 예측 중심 단어에 대한 라벨로 주변 단어를 사용하지 않고,\n중심 단어와 주변 단어에 대한 이웃 관계를 표시하기 위한 라벨로 1 또는 0을 사용 SGNS 구현 # 20newsgroups 데이터 사용 Copy python # 네거티브 샘플링 데이터셋 생성 from tensorflow.keras.preprocessing.sequence import skipgrams skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded] # (commited (7837), badar (34572)) -\u0026gt; 0 # (whole (217), realize (1036)) -\u0026gt; 1 # (reason (149), commited (7837)) -\u0026gt; 1 Copy python from tensorflow.keras.models import Sequential, Model from tensorflow.keras.layers import Embedding, Reshape, Activation, Input from tensorflow.keras.layers import Dot Copy python embedding_dim = 100 # 중심 단어를 위한 임베딩 테이블 w_inputs = Input(shape=(1, ), dtype=\u0026#39;int32\u0026#39;) word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs) # 주변 단어를 위한 임베딩 테이블 c_inputs = Input(shape=(1, ), dtype=\u0026#39;int32\u0026#39;) context_embedding = Embedding(vocab_size, embedding_dim)(c_inputs) Copy python # 두 임베딩 테이블에 대한 내적의 결과로 1 또는 0을 예측하기 위해 시그모이드 함수 사용 dot_product = Dot(axes=2)([word_embedding, context_embedding]) dot_product = Reshape((1,), input_shape=(1, 1))(dot_product) output = Activation(\u0026#39;sigmoid\u0026#39;)(dot_product) model = Model(inputs=[w_inputs, c_inputs], outputs=output) model.summary() model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;) Copy python # 5epochs 학습 for epoch in range(1, 6): loss = 0 for _, elem in enumerate(skip_grams): first_elem = np.array(list(zip(*elem[0]))[0], dtype=\u0026#39;int32\u0026#39;) second_elem = np.array(list(zip(*elem[0]))[1], dtype=\u0026#39;int32\u0026#39;) labels = np.array(elem[1], dtype=\u0026#39;int32\u0026#39;) X = [first_elem, second_elem] Y = labels loss += model.train_on_batch(X,Y) print(\u0026#39;Epoch :\u0026#39;,epoch, \u0026#39;Loss :\u0026#39;,loss) 09-05. GloVe # 카운트 기반과 예측 기반을 모두 사용하는 방법론으로, LSA와 Word2Vec의 단점 보완 LSA는 단어의 빈도수를 차원 축소하여 잠재된 의미를 끌어내지만, 같은 단어 의미의 유추 작업 성능은 떨어짐 Word2Vec는 예측 기반으로 단어 간 유추 작업에는 LSA보다 뛰어나지만,\nwindow size 내 주변 단어만을 고려하여 전체적인 통계 정보를 반영하지 못함 임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것이 목적 Window based Co-occurrence Matrix # 행과 열을 전체 단어 집합의 단어들로 구성하고,\ni 단어의 window size 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬 Co-occurence Probability # 동시 등장 확률 $P(k|i)$는 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고,\n특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률 Copy python # pip install glove_python_binary from glove import Corpus, Glove corpus = Corpus() # 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성 corpus.fit(result, window=5) glove = Glove(no_components=100, learning_rate=0.05) glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True) glove.add_dictionary(corpus.dictionary) print(glove.most_similar(\u0026#34;man\u0026#34;)) [(\u0026#39;woman\u0026#39;, 0.9621753707315267), (\u0026#39;guy\u0026#39;, 0.8860281455579162), (\u0026#39;girl\u0026#39;, 0.8609057388487154), (\u0026#39;kid\u0026#39;, 0.8383640509911114)] 09-06. FastText # Word2Vec가 단어를 쪼개질 수 없는 단위로 생각한다면,\nFastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주 각 단어를 글자 단위 n-gram의 구성으로 취급하여 n에 따라 단어들이 얼마나 분리되는지 결정 tri-gram의 경우 apple에 대해서 [\u0026lt;ap, app, ppl, ple, le\u0026gt;]로 분리된 벡터 생성\n(\u0026lt;, \u0026gt;는 시작과 끝을 의미) 내부 단어들을 Word2Vec로 벡터화하고 apple의 벡터값은 내부 단어의 벡터값들의 총 합으로 구성 Out Of Vocabulary # FastText는 데이터셋만 충분하다면 내부 단어를 통해 모르는 단어에 대해서도 유사도 계산 가능 birthplace를 학습하지 않은 상태라도, birth와 place라는 내부 단어가 있다면 벡터를 얻을 수 있음 Rare Word # Word2Vec는 등장 빈도 수가 적은 단어에 대해서 임베딩의 정확도가 높지 않은 단점 FastText는 희귀 단어라도 n-gram이 다른 단어의 n-gram과 겹치는 경우라면,\nWord2Vec보다 비교적 높은 임베딩 벡터값을 얻음 오타와 같은 노이즈가 많은 코퍼스에서도 일정 수준의 성능을 보임 (apple, appple) Copy python from gensim.models import FastText model = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1) 09-08. Pre-trained Word Embedding # 사전 훈련된 워드 임베딩 참고 09-09. ELMo # 언어 모델로 하는 임베딩이라는 뜻으로, 사전 훈련된 언어 모델을 사용 Word2Vec는 Bank Account와 River Bank에서 Bank의 차이를 구분하지 못하지만,\nELMo는 문맥을 반영한 워드 임베딩을 수행 ELMo 표현을 기존 임베딩 벡터와 연결(concatenate)해서 입력으로 사용 가능 biLM # RNN 언어 모델에서 $h_t$는 시점이 지날수록 업데이트되기 때문에,\n문장의 문맥 정보를 점차적으로 반영함 ELMo는 양쪽 방향의 언어 모델(biLM)을 학습하여 활용 biLM은 은닉층이 최소 2개 이상인 다층 구조를 전제로 함 양방향 RNN은 순방향 RNN의 hidden state와 역방향 RNN의 hidden state를 연결하는 것이지만,\nbiLM은 순방향 언어 모델과 역방향 언어 모델이라는 두 개의 언어 모델을 별개의 모델로 보고 학습 각 층(embedding, hidden state)의 출력값이 가진 정보가 서로 다른 것이므로,\n이를 모두 활용하여 순방향 언어 모델과 역방향 언어 모델의 각 층의 출력값을 연결 ELMo Representation # 각 층의 출력값을 연결(concatenate) 각 층의 출력값 별로 가중치($s_1, s_2, s_3$) 부여 각 층의 출력값을 모두 더함 (2번과 3번을 요약하여 가중합이라 표현) 벡터의 크기를 결정하는 스칼라 매개변수($\\gamma$)를 곱함 ELMo 활용 # 스팸 메일 분류하기 데이터 사용 Copy python # 텐서플로우 1버전에서 사용 가능 %tensorflow_version 1.x pip install tensorflow-hub import tensorflow_hub as hub Copy python # 텐서플로우 허브로부터 ELMo를 다운로드 elmo = hub.Module(\u0026#34;https://tfhub.dev/google/elmo/1\u0026#34;, trainable=True) sess = tf.Session() K.set_session(sess) sess.run(tf.global_variables_initializer()) sess.run(tf.tables_initializer()) Copy python # 데이터의 이동이 케라스 → 텐서플로우 → 케라스가 되도록 하는 함수 def ELMoEmbedding(x): return elmo(tf.squeeze(tf.cast(x, tf.string)), as_dict=True, signature=\u0026#34;default\u0026#34;) Copy python from keras.models import Model from keras.layers import Dense, Lambda, Input input_text = Input(shape=(1,), dtype=tf.string) embedding_layer = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text) hidden_layer = Dense(256, activation=\u0026#39;relu\u0026#39;)(embedding_layer) output_layer = Dense(1, activation=\u0026#39;sigmoid\u0026#39;)(hidden_layer) model = Model(inputs=[input_text], outputs=output_layer) model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) 09-10. Embedding Visualization # 구글 embedding projector 시각화 도구 (논문 참고) Copy python # !python -m gensim.scripts.word2vec2tensor --input 모델이름 --output 모델이름 !python -m gensim.scripts.word2vec2tensor --input eng_w2v --output eng_w2v # 임베딩 프로젝트에 사용할 metadata.tsv와 tensor.tsv 파일 생성 09-11. Document Embedding # 문서 벡터를 이용한 추천 시스템 참고 문서 임베딩 : 워드 임베딩의 평균 참고 Doc2Vec으로 공시 사업보고서 유사도 계산하기 참고 10. RNN Text Classification # 케라스를 이용한 텍스트 분류 개요 스팸 메일 분류하기 (RNN) 로이터 뉴스 분류하기 (LSTM) IMDB 리뷰 감성 분류하기 (GRU) 나이브 베이즈 분류기 네이버 영화 리뷰 감성 분류하기(LSTM) 네이버 쇼핑 리뷰 감성 분류하기(GRU) BiLSTM으로 한국어 스팀 리뷰 감성 분류하기 Bayes\u0026rsquo; Theorem # $$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$$\nNaive Bayes Classifier # 베이즈 정리를 이용한 스팸 메일 확률 표현\nP(정상 메일 | 입력 텍스트) = (P(입력 텍스트 | 정상 메일) x P(정상 메일)) / P(입력 텍스트)\nP(스팸 메일 | 입력 텍스트) = (P(입력 텍스트 | 스팸 메일) x P(스팸 메일)) / P(입력 텍스트) 나이브 베이즈 분류기에서 토큰화 이전의 단어의 순서는 중요하지 않음\n(BoW와 같이 단어의 순서를 무시하고 빈도수만 고려) 정상 메일에 입력 텍스트가 없어 확률이 0%가 되는 것을 방지하기 위해\n각 단어에 대한 확률의 분모, 분자에 전부 숫자를 더해서 분자가 0이 되는 것을 방지하는 라플라스 스무딩 사용 Copy python from sklearn.feature_extraction.text import TfidfTransformer from sklearn.naive_bayes import MultinomialNB # 다항분포 나이브 베이즈 모델 # alpha=1.0: 라플라스 스무딩 적용 model = MultinomialNB() # MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) model.fit(tfidfv, newsdata.target) 11-01. Convolutional Neural Network # 이미지 처리에 탁월한 성능을 보이는 신경망 합성곱 신경망은 convolutional layer와 pooling layer로 구성 합성곱 연산(CONV)의 결과가 ReLU를 거쳐서 POOL 구간을 지나는 과정 Channel: 이미지는 (높이, 너비, 채널)이라는 3차원 텐서로 구성, 채널은 색 성분을 의미 합성곱 신경망은 이미지의 모든 픽셀이 아닌, 커널과 맵핑되는 픽셀만을 입력으로 사용하여\n다층 퍼셉트론보다 훨씬 적은 수의 가중치를 사용하여 공간적 구조 정보를 보존 편향을 추가할 경우 커널을 적용한 뒤에 더해지며, 단 하나의 편향이 커널이 적용된 결과의 모든 원소에 더해짐 다수의 채널을 가진 입력 데이터일 경우 커널의 채널 수도 입력의 채널 수만큼 존재,\n각 채널 간 합성곱 연산을 마치고 그 결과를 모두 더해서 하나의 채널을 가지는 특성 맵 생성 Convolution Operation # 이미지의 특징을 추출 Kernel(filter)라는 ${n}\\times{m}$ 크기의 행렬로 각 이미지를 순차적으로 훑음 Feature map: 합성곱 연산을 통해 나온 결과 Stride: 커널의 이동 범위, 특성 맵의 크기 Padding: 합성곱 연산 이후에도 특성 맵의 크기가 입력과 동일하도록 행과 열 추가 11-02. 1D CNN # 1D Convolutions # LSTM과 동일하게 각 단어가 벡터로 변환된 문장 행렬을 입력으로 받음 커널의 너비는 임베딩 벡터의 차원과 동일, 커널의 높이만으로 해당 커널의 크기라 간주 커널의 너비가 임베딩 벡터의 차원이기 때문에 너비 방향으로 움직이지 못하고 높이 방향으로만 움직임 Max-pooling # 1D CNN에서의 폴링 층 각 합성곱 연산으로부터 얻은 결과 벡터에서 가장 큰 값을 가진 스칼라 값을 빼내는 연산을 수행 1D CNN 구현 # Copy python from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint embedding_dim = 256 # 임베딩 벡터의 차원 dropout_ratio = 0.3 # 드롭아웃 비율 num_filters = 256 # 커널의 수 kernel_size = 3 # 커널의 크기 hidden_units = 128 # 뉴런의 수 model = Sequential() model.add(Embedding(vocab_size, embedding_dim)) model.add(Dropout(dropout_ratio)) model.add(Conv1D(num_filters, kernel_size, padding=\u0026#39;valid\u0026#39;, activation=\u0026#39;relu\u0026#39;)) model.add(GlobalMaxPooling1D()) model.add(Dense(hidden_units, activation=\u0026#39;relu\u0026#39;)) model.add(Dropout(dropout_ratio)) model.add(Dense(1, activation=\u0026#39;sigmoid\u0026#39;)) es = EarlyStopping(monitor=\u0026#39;val_loss\u0026#39;, mode=\u0026#39;min\u0026#39;, verbose=1, patience=3) mc = ModelCheckpoint(\u0026#39;best_model.h5\u0026#39;, monitor=\u0026#39;val_acc\u0026#39;, mode=\u0026#39;max\u0026#39;, verbose=1, save_best_only=True) model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[es, mc]) 11-06. Intent Classification # 사전 훈련된 워드 임베딩을 이용한 의도 분류 참고 11-07. Character Embedding # \u0026lsquo;misunderstand\u0026rsquo;의 의미를 \u0026lsquo;mis-\u0026lsquo;라는 접두사와 \u0026lsquo;understand\u0026rsquo;를 통해 추측하는 것과 같이,\n사람의 이해 능력을 흉내내는 알고리즘 1D CNN에서는 단어를 문자 단위로 쪼개기만하면 되기 때문에 OOV라도 벡터를 얻을 수 있음 BiLSTM에서도 문자에 대한 임베딩을 통해 얻은 벡터를 단어에 대한 벡터로 사용 12. Tagging Task # 양방향 LSTM를 이용한 품사 태깅 개체명 인식 개체명 인식의 BIO 표현 이해하기 BiLSTM을 이용한 개체명 인식 BiLSTM-CRF를 이용한 개체명 인식 문자 임베딩 활용하기 BIO 표현 # 개체명이 시작되는 부분에 B(Begin), 개체명의 내부에 I(Inside), 나머지로 O(Outside) 태깅 개체명 태깅엔 LOC(location), ORG(organization), PER(person), MISC(miscellaneous)\n태그가 추가로 붙음 (B-ORG 등) CRF(Conditional Random Field) # LSTM 위에 CRF 층을 추가하면 모델은 예측 개체명(레이블 간 의존성)을 고려 기존 양방향 LSTM 모델은 활성화 함수를 지난 시점에서 개체명을 결정했지만,\nCRF 층을 추가한 모델에서는 활성화 함수의 결과들이 CRF 층의 입력으로 전달 CRF 층은 [문장의 첫번쨰 단어에서는 I가 나오지 않는다, O-I 패턴은 나오지 않는다] 등의 제약사항을 학습 양방향 LSTM은 입력 단어에 대한 양방향 문맥을 반영하며, CRF는 출력 레이블에 대한 양방향 문맥을 반영 CRF 층은 one-hot encoding된 라벨을 지원하지 않음 13-01. Byte Pair Encoding # UNK(Unknown Token) 등의 OOV 문제를 해결하기 위해 서브워드 분리 작업을 수행 BPE 알고리즘은 연속적으로 가장 많이 등장한 글자의 쌍을 찾아서 하나의 글자로 병합 BPE는 글자 단위에서 점차적으로 단어 집합을 만들어내는 Bottom up 방식의 접근 사용 WordPiece Tokenizer # BPE의 변형 알고리즘으로, 코퍼스의 likelihood를 가장 높이는 쌍을 병합 모든 단어의 맨 앞에 _를 붙이고, 단어는 subword로 통계에 기반하여 띄어쓰기로 분리 WordPiece TOkenizer 겨로가를 되돌리기 위해서는 모든 띄어쓰기를 제거하고 언더바를 띄어쓰기로 바꿈 Unigram Language Model Tokenizer # 각각의 서브워드들에 대해서 손실(loss)을 계산 서브 단어의 손실은 해당 서브워드가 단어 집합에서 제거되었을 경우, 코퍼스의 likelihood가 감소하는 정도 서브워드들의 손실의 정도를 정렬하여, 최악의 영향을 주는 10~20%의 토큰을 제거 13-02. SentencePiece # 내부 단어 분리를 위한 구글의 패키지 사전 토큰화 작업없이 단어 분리 토큰화를 수행하여 언어에 종속적이지 않음 Copy python import sentencepiece as spm # IMDB 리뷰 데이터 사용 spm.SentencePieceTrainer.Train(\u0026#39;--input=imdb_review.txt --model_prefix=imdb --vocab_size=5000 --model_type=bpe --max_sentence_length=9999\u0026#39;) input: 학습시킬 파일 model_prefix: 만들어질 모델 이름 vocab_size: 단어 집합의 크기 model_type: 사용할 모델 (unigram(default), bpe, char, word) max_sentence_length: 문장의 최대 길이 13-03. SubwordTextEncoder # Wordpiece 모델을 채택한 텐서플로우의 서브워드 토크나이저 Copy python import tensorflow_datasets as tfds tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus( train_df[\u0026#39;review\u0026#39;], target_vocab_size=2**13) 13-04. Huggingface Tokenizer # BertWordPieceTokenizer: BERT에서 사용된 워드피스 토크나이저(WordPiece Tokenizer) CharBPETokenizer: 오리지널 BPE ByteLevelBPETokenizer: BPE의 바이트 레벨 버전 SentencePieceBPETokenizer: 앞서 본 패키지 센텐스피스(SentencePiece)와 호환되는 BPE 구현체 BertWordPieceTokenizer # Copy python from tokenizers import BertWordPieceTokenizer tokenizer = BertWordPieceTokenizer(lowercase=False, trip_accents=False) Copy python data_file = \u0026#39;naver_review.txt\u0026#39; vocab_size = 30000 limit_alphabet = 6000 min_frequency = 5 tokenizer.train(files=data_file, vocab_size=vocab_size, limit_alphabet=limit_alphabet, min_frequency=min_frequency) 14-01. Sequence-to-Sequence(seq2seq) # seq2seq는 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 다양한 분야에서 사용되는 모델 챗봇, 기계 번역, 내용 요약, STT(Speech to Text) 등에서 주로 사용 seq2seq는 인코더와 디코더로 나눠지며, 둘 다 LSTM 셀 또는 GRU 셀을 사용하는 RNN 아키텍처로 구성 softmax 함수를 통해 출력 시퀀스의 각 단어별 확률값을 반환하고, 디코더는 출력 단어를 결정 Encoder # 입력 문장의 모든 단어들을 순차적으로 입력받고 모든 단어 정보들을 압축해서 하나의 벡터 생성 인코더 RNN 셀의 마지막 hidden state를 context vector로 디코더에 넘겨줌 Decoder # 압축된 컨텍스트 벡터를 받아서 번역된 단어를 한 개씩 순차적으로 출력 기본적으로 RNNLM으로, 초기 입력으로 문장의 시작을 의미하는 심볼 가 들어감 첫번째 시점의 디코더 RNN 셀은 예측된 단어를 다음 시점의 RNN 셀 입력으로 넣으며,\n문장의 끝을 의미하는 심볼인 가 다음 단어로 예측될 때까지 반복해서 예측 훈련 과정에서는 실제 정답 상황에서 가 나와야 된다고 정답을 알려줌\n(교사 강요: 이전 시점의 디코더 셀의 예측이 틀릴 경우 연쇄 작용을 방지) seq2seq 구현 # 프랑스-영어 병렬 코퍼스 데이터 사용 병렬 코퍼스 데이터에서 쌍이 되는 데이터의 길이가 같지 않음에 주의 Copy python # Encoder encoder_inputs = Input(shape=(None, src_vocab_size)) encoder_lstm = LSTM(units=256, return_state=True) # encoder_outputs은 여기서는 불필요 encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs) # LSTM은 바닐라 RNN과는 달리 상태가 두 개, 은닉 상태와 셀 상태 encoder_states = [state_h, state_c] Copy python # Decoder decoder_inputs = Input(shape=(None, tar_vocab_size)) decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True) # 디코더에게 인코더의 은닉 상태, 셀 상태를 전달 decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states) decoder_softmax_layer = Dense(tar_vocab_size, activation=\u0026#39;softmax\u0026#39;) decoder_outputs = decoder_softmax_layer(decoder_outputs) model = Model([encoder_inputs, decoder_inputs], decoder_outputs) model.compile(optimizer=\u0026#34;rmsprop\u0026#34;, loss=\u0026#34;categorical_crossentropy\u0026#34;) Copy python model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=40, validation_split=0.2) seq2seq 동작 # 번역하고자 하는 입력 문장이 인코더에 들어가서 은닉 상태와 셀 상태를 얻음 상태와 에 해당하는 \u0026lsquo;\\t\u0026rsquo;를 디코더로 보냄 디코더가 에 해당하는 \u0026lsquo;\\n\u0026rsquo;이 나올 때까지 다음 문자를 예측하는 행동을 반복 14-02. BLEU Score # Bilingual Evaluation Understudy(BLEU) # 기계 번역 결과와 사람이 직접 번역한 결과가 얼마나 유사한지 비교하여 번역에 대한 성능을 측정하는 방법 측정 기준은 n-gram에 기반 언어에 구애받지 않고 사용할 수 있으며, 계산 속도가 빠른 이점 PPL과 달리 높을 수록 성능이 더 좋음을 의미 Unigram Precision # 사람이 번역한 문장 중 어느 한 문장이라도 등장한 단어의 개수를 카운트하는 측정 방법 기계 번역기가 번역한 문장을 Ca, 사람이 번역한 문장을 Ref라 표현 $$\\text{Unigram Precision}=\\frac{\\text{Ref들 중에서 존재하는 Ca의 단어의 수}}{\\text{Ca의 총 단어 수}}$$\nModified Unigram Precision # 하나의 단어가 여러번 반복되는 경우에서 정밀도가 1이 나오는 문제를 개선하기 위해\n유니그램이 이미 매칭된 적이 있는지를 고려 $Max_Ref_Count$: 유니그램이 하나의 Ref에서 최대 몇 번 등장했는지 카운트 $Count_{dip}=min(Count,Max_Ref_Count)$ $$\\text{Modified Unigram Precision}=\\frac{\\text{Ca의 각 유니그램에 대해 }Count_{dip}\\text{을 수행한 값의 총 합}}{\\text{Ca의 총 유니그램 수}}$$\nBLEU Score # 유니그램 정밀도는 단어의 빈도수로 접근하기 때문에 단어의 순서를 고려하기 위해 n-gram 이용 BLEU 최종 식은 보정된 정밀도 $p_1,p_2,\u0026hellip;,p_n$을 모두 조합 해당 BLEU 식의 경우 문장의 길이가 짧을 때 높은 점수를 받는 문제가 있기 때문에,\n길이가 짧은 문장에게 Brevity Penalty를 줄 필요가 있음 $BP$는 Ca와 가장 길이 차이가 작은 Ref의 길이 $r$을 기준으로 $e^{(1-r/c)}$ 값을 곱하며,\n문장이 $r$보다 길어 패널티를 줄 필요가 없는 경우 1이어야 함 $$\\text{보정된 정밀도 } p_1=\\frac{\\Sigma_{{unigram}\\in{Candidate}}Count_{dip}(unigram)}{\\Sigma_{{unigram}\\in{Candidate}}Count(unigram)}$$ $$\\text{n-gram 일반화 } p_n=\\frac{\\Sigma_{{n\\text{-}gram}\\in{Candidate}}Count_{dip}(n\\text{-}gram)}{\\Sigma_{{n\\text{-}gram}\\in{Candidate}}Count(n\\text{-}gram)}$$ $$BLEU={BP}\\times{exp(\\Sigma^N_{n=1}{w_n}{\\log{p_n}})}$$\nCopy python import nltk.translate.bleu_score bleu_score(candidate.split(),list(map(lambda ref: ref.split(), references))) "},{"id":73,"href":"/blog/2022-06-28/","title":"2022-06-28 Log","section":"Posts","content":"02-01. Tokenization # Corpus에서 token이라 불리는 단위로 나누는 작업 단어 토큰화에서 단순히 구두점이나 특수문자를 제거하는 것은 의미의 손실을 발생시킬 수 있기 때문에,\n사용자의 목적과 일치하는 토큰화 도구를 사용할 필요가 있음 구두점이나 특수 문자가 필요한 경우: Ph.D, AT\u0026amp;T, $45.55, 01/02/06 등 줄임말과 단어 내에 띄어쓰기가 있는 경우: what\u0026rsquo;re/what are, New York, rock \u0026rsquo;n\u0026rsquo; roll 등 문장 토큰화에서 단순히 마침표를 기준으로 문장을 잘라내는 것은 192.168.56.31, gmail.com과 같은 경우를 고려했을 때 올바르지 않음 한국어 토큰화 # 한국어의 경우 띄어쓰기가 가능한 단위가 어절인데,\n\u0026lsquo;그가\u0026rsquo;, \u0026lsquo;그에게\u0026rsquo;, \u0026lsquo;그를\u0026rsquo;과 같이 어절이 독립적인 단어로 구성되는 것이 아니라\n조사 등의 무언가가 붙어있는 경우가 많기 때문에 이를 전부 형태소 단위로 분리해줘야 함 자립 형태소: 접사, 어미, 조사와 상관업싱 자립하여 사용할 수 있는 형태소, [체언, 수식언, 감탄사] 등 의존 형태소: 다른 형태소와 결합하여 사용되는 형태소, [접사, 어미, 조사, 어간] 한국어의 경우 띄어쓰기가 지켜지지 않아도 글을 쉽게 이해할 수 있어 띄어쓰기가 잘 지켜지지 않음 품사 태깅: 단어의 의미를 제대로 파악하기 위해서는 해당 단어가 어떤 품사로 쓰였는지 구분할 필요 NLTK, KoNLPy # Copy python from nltk.tokenize import word_tokenize # 단어 토큰화 from nltk.tag import pos_tag # 품사 태깅 Copy python from konlpy.tag import Okt okt = Okt() okt.porphs(sentence) # 형태소 추출 okt.pos(sentence) # 품사 태깅 02-02. Cleaning and Normalization # Cleaning(정제): 갖고 있는 corpus로부터 노이즈 데이터를 제거 Normalization(정규화): 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만듦 영어권 언어에서 단어의 개수를 줄이는 정규화 방법으로 대,소문자 통합을 활용 노이즈 데이터: 아무 의미 없는 특수 문자 등, 분석하고자 하는 목적에 맞지 않는 불필요한 단어들 불필요한 단어를 제거하기 위해 불용어, 등장 빈도가 적은 단어, 길이가 짧은 단어 등을 제거 노이즈 데이터의 특징을 잡아낼 수 있다면, 정규표현식을 사용해서 제거 02-03. Stemming and Lemmatization # Lemmatization # Lemma(표제어): 기본 사전형 단어, [am, are, is]의 뿌리 단어 be 등 Stem(어간): 단어의 의미를 담고 있는 단어의 핵심 부분, \u0026lsquo;cats\u0026rsquo;에서 \u0026lsquo;cat\u0026rsquo; Affix(접사): 단어에 추가적인 의미를 주는 부분, \u0026lsquo;cats\u0026rsquo;에서 \u0026rsquo;s\u0026rsquo; Copy python from nltk.stem import WordNetLemmatizer lemmatizer = WordNetLemmatizer() lemmatizer.lemmatize(word) # am -\u0026gt; be, having -\u0026gt; have Stemming # Copy python from nltk.stem import PorterStemmer stemmer = PorterStemmer() stemmer.stem(word) # am -\u0026gt; am, having -\u0026gt; hav 한국어에서의 어간 추출 # 5언 9품사의 구조에서 용언에 해당되는 동사와 형용사는 어간과 어미의 결합으로 구성 활용: 용언의 어간이 어미를 가지는 일 규칙 활용: 어간이 어미를 취할 때 어간의 모습이 일정, 잡/어간 + 다/어미 불규칙 활용: 어간이 어미를 취할 때 어간의 모습이 바뀌거나 특수한 어미일 경우, \u0026lsquo;오르+아/어-\u0026gt;올라\u0026rsquo; 등 02-04. Stopword # Stopword(불용어): 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 기여하지 않는 단어, [조사, 접미사] 등 Copy python from nltk.corpus import stopwords stop_words_list = stopwords.words(\u0026#39;english\u0026#39;) Copy python okt = Okt() okt.morphs(sentence) # 조사, 접속사 등 제거 # 또는 불용어 사전을 만들어서 제거 02-05. Regular Expression # 정규 표현식 참고 02-06. Integer Encoding # 컴퓨터는 텍스트보다 숫자를 더 잘 처리할 수 있기 때문에 텍스트를 숫자로 변경 단어를 빈도수 순으로 정렬하고 순서대로 낮은 숫자부터 정수를 부여 dictionary, Counter, nltk.FreqDist, keras.Tokenizer 등 활용 Copy python from nltk import FreqDist FreqDist(np.hstack(preprocessed_sentences)) # np.hastack으로 문장 구분을 제거 Copy python from tensorflow.keras.preprocessing.text import Tokenizer tokenizer = Tokenizer() # num_words 파라미터로 사용할 단어 개수 지정 tokenizer.fit_on_texts(preprocessed_sentences) # 빈도수 기분으로 단어 집합 생성 print(tokenizer.word_intex) # 정수 인덱스 확인 print(tokenizer.word_counts) # 단어 빈도수 확인 print(tokenizer.texts_to_sequences(preprocessed_sentences)) # corpus를 인덱스로 변환 02-07. Padding # 병렬 연산을 위해 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요 Copy python from tensorflow.keras.preprocessing.sequence import pad_sequences encoded = tokenizer.texts_to_sequences(preprocessed_sentences) padded = pad_seqences(encoded) # padding=\u0026#39;post\u0026#39;를 입력해야 뒤에서 부터 0을 채움 # maxlen으로 문장 길이 조절 # truncating=\u0026#39;post\u0026#39;를 통해 문장 길이 초과 시 뒤의 단어가 삭제되도록 설정 02-08. One-Hot Encoding # Vocabulary(단어 집합): 서로 다른 단어들의 집합, book과 books과 같은 변형 형태도 다른 단어로 간주 One-Hot Encoding: 단어 집합의 크기를 벡터의 차원으로 하고,\n표현하고 싶은 단어에 1, 다른 인텍스에 0을 부여하는 단어의 벡터 표현 방식 단어의 개수가 늘어날 수록 벡터를 저장하기 위해 필요한 공간이 계속 늘어나는 단점 단어의 유사도를 표현하지 못하는 단점 (강아지, 개, 냉장고 등) Copy python from tensorflow.keras.utils import to_categorical encoded = tokenizer.texts_to_sequences(preprocessed_sentences)[0] one_hot = to_categorical(encoded) 03-01. Language Model # 단어 시퀀스(문장)에 확률을 할당하는 모델, 이전 단어들이 주어졌을 때 다음 단어를 예측 단어 시퀀스 W의 확률 $P(W)=P(w_1,w_2,w_3,w_4,w_5,\u0026hellip;,w_n)$ 다음 단어 등장 확률 $P(w_n|w_1,\u0026hellip;,w_{n-1})$ 03-02. Statistical Language Model # 조건부 확률 # 남학생(A) 여학생(B) 계 중학생(C) 100 60 160 고등학생(D) 80 120 200 계 180 180 360 학생을 뽑았을 때, 고등학생이면서 남학생일 확률 $P(A \\bigcap B)=80/360$ 고등학생 중 한명을 뽑았을 때, 남학생일 확률 $P(A|D)=P(A \\bigcap D)/P(D)=(80/360)/(200/360)$ 문장에 대한 확률 # \u0026lsquo;An adorable little boy is spreading smiles\u0026rsquo;의 확률\n$P(\\text{An adorable little boy is spreading smiles})=\\ P(\\text{An}) \\times P(\\text{adorable}|\\text{An}) \\times \u0026hellip; \\times P(\\text{smiles}|\\text{An adorable little boy is spreading})$ 카운트 기반 접근 # An adorable little boy가 100번 등장했을 때 그 다음에 is가 등장한 경우가 30번이라면,\n$P(\\text{is}|\\text{An adorable little boy})$는 30% 카운트 기반으로 훈련할 경우 단어 시퀀스가 없어 확률이 0이 되는 경우를 방지하기 위해 방대한 양의 훈련 데이터가 필요 회소 문제: 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제 $$P(\\text{is}|\\text{An adorable little boy})= \\frac{count(\\text{An adorable little boy is})}{count(\\text{An adorable little boy})}$$\n03-03. N-gram Language Model # 통계적 언어 모델의 일종이지만, 모든 단어가 아닌 일부 단어만 고려하는 접근 방법 사용 An adorable little boy에서 is가 나올 확률을 boy가 나왔을 때 is가 나올 확률로 대체\n$P(\\text{is}|\\text{An adorable little boy}) \\approx P(\\text{is}|\\text{boy})$ 뒤의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 발생 전체 문장을 고려한 언어 모델보다는 정확도가 떨어짐 몇 개의 단어를 볼지 n을 정하는 것은 trade-off 문제를 발생시킴, n은 최대 5를 넘게 잡아서는 안된다고 권장 N-gram # n개의 연속적인 단어 나열 An adorable little boy에 대해\nunigrams: an, adorable, little, boy\nbigrams: an adorable, adorable little, little boy 03-05. Perplexity # Perplexity(PPL): 헷갈리는 정도, 낮을수록 언어 모델의 성능이 좋음 $$PPL(W)=P(w_1,w_2,w_3,\u0026hellip;,w_N)^{-\\frac{1}{N}}=\\sqrt[N]{\\frac{1}{P(w_1,w_2,w_3,\u0026hellip;,w_N)}}$$\nBranching Factor # Branching factor(분기계수): PPL이 선택할 수 있는 가능한 경우의 수 대해 PPL이 10이 나왔을 때, 언어 모델은 테스트 데이터에 대해 다음 단어를 예측할 때 평균 10개의 단어를 고려 PPL의 값이 낮다는 것은 테스트 데이터 상에서 높은 정확도를 보이는 것일뿐, 반드시 사람이 직접 느끼기에 좋은 모델인 것은 아님 $$PPL(W)=P(w_1,w_2,w_3,\u0026hellip;,w_N)^{-\\frac{1}{N}}=(\\frac{1}{10}^N)^{-\\frac{1}{N}}=\\frac{1}{10}^{-1}=10$$\n04-01. 단어의 표현 방법 # 국소 표현(이산 표현): 해당 단어 그 자체만 보고, 특정값을 맵핑하여 단어를 표현하는 방법 분산 표현(연속 표현): 그 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법 04-02. Bag of Words(BoW) # 단어들의 순서는 고려하지 않고, 단어들의 출현 빈도에만 집중하는 텍스트 데이터 수치화 표현 방법 Copy python doc1 = \u0026#39;정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\u0026#39; vocabulary : {\u0026#39;정부\u0026#39;: 0, \u0026#39;가\u0026#39;: 1, \u0026#39;발표\u0026#39;: 2, \u0026#39;하는\u0026#39;: 3, \u0026#39;물가상승률\u0026#39;: 4, \u0026#39;과\u0026#39;: 5, \u0026#39;소비자\u0026#39;: 6, \u0026#39;느끼는\u0026#39;: 7, \u0026#39;은\u0026#39;: 8, \u0026#39;다르다\u0026#39;: 9} bag of words vector : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1] Copy python from sklearn.feature_extraction.text import CounteVectorizer vector = CounterVectorizer() # stop_words 파라미터로 불용어 제거(\u0026#39;english\u0026#39; 또는 리스트 등) print(\u0026#39;bag of words vector:\u0026#39;, vector.fit_transform(corpus).toarray()) print(\u0026#39;vocabulary:\u0026#39;, vector.vocabulary_) 04-03. Document-Term Matrix(DTM) # 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것 One-hot vector와 마찬가지로 대부분의 값이 0인 희소 표현의 문제 발생 불용어와 중요한 단어에 대해서 가중치를 주기 위해 TF-IDF를 사용 04-04. TF-IDF # 단어의 빈도와 역 문서 빈도를 사용하여 DTM 내의 각 단어들마다 중요도를 가중치로 부여하는 방법 $tf(d,t)$: 특정 문서 $d$에서의 특정 단어 $t$의 등장 횟수, DTM에서의 각 단어들의 가진 값 $df(t)$: 특정 단어 $t$가 등장한 문서의 수, 특정 단어가 각 문서에서 등장한 횟수는 무시 $idf(d,t)$: $df(t)$에 반비례하는 수,\n총 문서의 수 n이 커질수록 기하급수적으로 증가하는 것을 방지하기 위해 $log$(일반적으로 자연 로그) 적용 $$idf(d,t)=log(\\frac{n}{1+df(t)})$$\nCopy python from sklearn.feature_extraction.text import TfidfVectorizer print(vector.fit_transform(corpus).toarray()) print(ve) 05. Vector Similarity # Cosine Similarity # 두 벡터 간의 코사인 각도를 이용하여 구할 수 있는 두 벡터의 유사도 두 벡터의 방향이 동일하면 1의 값을 가지며, 값이 1에 가까울수록 유사도가 높음 문서의 길이가 다른 상황에서 비교적 공정한 비교를 할 수 있음 Euclidean Distance # 다차원 공간에서 두 개의 점 $p$와 $q$가 각각 $p=(p_1,p_2,p_3,\u0026hellip;,p_n)$과 $q=(q_1,q_2,q_3,\u0026hellip;,q_n)$의 좌표를 가질 때\n두 점 사이의 거리를 계산하는 유클리드 거리 공식 $$\\sqrt{(q_1-p_1)^2+(q_2-p_2)^2+\u0026hellip;+(q_n-p_n)^2}=\\sqrt{\\Sigma^n_{i=1}(q_i-p_i)^2}$$\nJaccard Similarity # 합집합에서 교집합의 비율을 구한다면 두 집합 A와 B의 유사도를 구할 수 있음 자카드 유사도 J는 0과 1사이의 값을 가지며, 두 집합이 동일하면 1, 공통 원소가 없으면 0의 값을 가짐 $$J(A,B)=\\frac{|A \\bigcap B|}{|A \\bigcup B|}=\\frac{|A \\bigcap B|}{|A|+|B|-|A \\bigcap B|}$$\n06. Machine Learning # Classification and Regression # Bianry Classification: 두 개의 선택지 중 하나의 답을 선택 Multi-class Classification: 세 개 이상의 선택지 중에서 답을 선택 Regression: 연속적인 값의 범위 내에서 예측값을 도출 Learning # Supervised Learning: 정답 레이블과 함께 함습 Unsupervised Learning: 데이터에 별도의 레이블이 없이 학습 Self-Supervised Learning: 레이블이 없는 데이터가 주어지면, 모델이 학습을 위해 스스로 레이블을 생성 Confusion Matrix # 예측 참 예측 거짓 실제 참 TP(정답) FN(오답) 실제 거짓 FP(오답) TN(정답) Precision(정밀도): True라고 분류한 것 중 실제 True의 비율, $Precision=\\frac{TP}{TP+FP}$ Recall(재현율): 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율, $Recall=\\frac{TP}{TP+FN}$ Accuracy(정확도): 전체 예측한 데이터 중 정답을 맞춘 것에 대한 비율, $Accuracy=\\frac{TP+TN}{TP+FN+FP+TN}$ Overfitting and Underfitting # Overfitting: 훈련 데이터를 과하게 학습, 훈련 데이터에 비해 테스트 데이터의 오차가 커짐 Underfitting: 테스트 데이터의 성능이 올라갈 여지가 있음에도 훈련을 덜 한 상태, 훈련 데이터에서도 정확도가 낮음 07. Deep Learning # Perceptron # 입력값 $x$, 가중치 $w$, 출력값 $y$ 가중치의 값이 크면 클수록 해당 입력 값이 중요하다는 것을 의미 단층 퍼셉트론: 값을 보내는 input layer와 값을 받아서 출력하는 output layer로 구성 다층 퍼셉트론(MLP): 입력층과 출력층 사이에 hidden layer를 추가 FFNN # FFNN(피드 포워드 신경망): 오직 입력층에서 출력층 방향으로 연산이 전개되는 신경망 RNN(순환 신경망): 은닉층의 출력값이 다시 은닉층으로 입력되는 신경망 Activision Function # 은닉층과 출력층의 뉴런에서 출력값을 결정하는 함수 Step function, Sigmoid function, ReLU 등 비선형 함수의 특성 Loss Function # MSE: 연속형 변수 예측 Binary Cross-Entropy: 시그모이드 함수 출력 Categorical Cross-Entropy: 소프트맥스 함수 출력 Optimizer # Momentum: 경사 하강법에 모멘텀을 더해 Local Minimum에 빠지더라도 빠져나갈 수 있게 함 Adagrad: 각 매개변수에 서로 다른 학습률을 적용 RMSprop: Adagrad가 학습을 진행할수록 학습률이 지나치게 떨어지는 단점을 개선 Adam: RMSprop과 Momentum을 합친 듯한 방법, 방향과 학습률 두 가지를 모두 잡기 위한 방법 Overfitting 방지 # 데이터의 양을 늘리기\n데이터의 양이 적으면 데이터의 특정 패턴이나 노이즈까지 쉽게 암기해버림, Data Augmentation 활용 모델의 복잡도 줄이기\n인공 신경망의 복잡도는 은닉층의 수나 매개변수의 수 등으로 결정 가충치 규제 적용하기\nL1 규제(가중치의 절댓값 합계를 비용 함수에 추가), L2 규제(모든 가중치들의 제곱합을 비용 함수에 추가) Dropout\n학습 시에 인공 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지 기울기 소실 # 기울기 소실: 역전파 과정에서 입력층으로 갈 수록 기울기가 점차적으로 작아지는 현상 기울기 폭주: 기울기가 점차 커지다가 가중치들이 비정상적으로 큰 값이 되면서 발산되는 경우 Gradient Clipping: 기울기 폭주를 막기 위해 임계값을 넘지 않도록 값의 크기를 감소 Weight Initialization # Xavier Initialization: 균등 분포 또는 정규 분포로 초기화 할 때 두 가지 경우로 나뉨 He Initialization: Xavier 초기화와 다르게 다음 층의 뉴런의 수를 반영하지 않음 Batch Normalization # 내부 공변량 변화: 학습 과정에서 층 별로 입력 데이터 분포가 달라지는 현상 배치 정규화: 한 번에 들어오는 배치 단위로 정규화하는 것 배치 정규화는 추가 계산을 발생시켜 모델을 복잡하게 하기 때문에 예측 시 실행 시간이 느려지는 단점 너무 작은 배치 크기에서는 잘 동작하지 않을 수 있기 때문에 미니 배치 크기에 의존적임 RNN은 각 시점마다 다른 통계치를 가지기 때문에 RNN에 적용하기 어려움 Keras API # Sequential API: 단순하게 층을 쌓는 방식, 다수의 입출력 및 층 간 연산을 구현하기 어려움 Functional API: 입력의 크기(shape)를 명시한 입력층을 모델의 앞단에 정의 Subclassing API: Functional API로도 구현할 수 없는 모델들도 구현 가능 texts_to_matrix() # tokenizer.texts_to_matrix(texts, mode='count'): DTM 생성 tokenizer.texts_to_matrix(texts, mode='binary'): DTM과 유사하지만 단어의 개수는 무시 tokenizer.texts_to_matrix(texts, mode='tfidf'): TF-IDF 행렬 생성 tokenizer.texts_to_matrix(texts, mode='freq'):\n각 문서에서의 단어 등장 횟수를 분자로, 문서의 크기를 분모로 하는 표현하는 방법 NNLM # 피드 포워드 신경망 언어 모델, 신경망 언어 모델의 시초로, RNNLM, BiLM 등으로 발전 기존 N-gram 언어 모델은 충분한 데이터를 관측하지 못하면 언어를 정확히 모델링하지 못하는 희소 문제를 가짐 NNLM은 N-gram 언어 모델처럼 정해진 개수(window size)의 단어만을 참고 NNLM은 N개의 input layer와 projection layer, hidden layer, output layer로 구성 Projection layer의 크기가 M일 때, 각 입력 단어들은 V x M 크기의 가중치 행렬과 곱해짐 충분한 양의 훈련 코퍼스를 학습한다면 단어 간 유사도를 구할 수 있는 임베딩 벡터값을 얻을 수 있음 08-01. RNN # 입력과 출력을 시퀀스 단위로 처리하는 시퀀스 모델 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 다시 은닉층 노드의 다음 계산 입력으로 보내는 특징 셀(메모리 셀, RNN 셀): 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드, 이전의 값을 기억하는 역할 Hidden state: 현재 시점을 t라 할 때, 메모리 셀이 다음 시점인 t+1의 자신에게 보내는 값 입력과 출력의 길이를 다르게 설계할 수 있어 다양한 용도로 사용 가능 one-to-many: 하나의 이미지 입력에 대해서 사진의 제목인 시퀀스를 출력하는 이미지 캡셔닝 작업에 사용 many-to-one: 단어 시퀀스에 대해서 하나의 출력을 하는 감성 분류, 스팸 메일 분류 등에 사용 many-to-many: 사용자가 문장을 입력하면 대답 문장을 출력하는 챗봇이나 번역기에 사용 RNN Parameter # 현재 시점 $t$에서의 hidden state가 $h_t$라 할 때, 두 개의 가중치 $W_x$, $W_h$가 필요 $W_x$는 입력층을 위한 가중치, $W_h$는 $t-1$의 hidden state인 $h_{t-1}$을 위한 가중치 은닉층 $h_t=tanh({W_x}{x_t}+{W_h}{h_{t-1}}+b)$, 출력층 $y_t=f({W_y}{h_t}+b)$ 출력층의 활성화 함수 $f$는 이진 분류에서 시그모이드 함수, 다중 클래스 분류에서 소프트맥스 함수 등 사용 RNN의 입력 $x_t$는 단어 벤터로 간주, 단어 벡터의 차원을 $d$, hidden state의 크기를 $D_h$라 할 때,\n메모리 셀 $h_t$ = $tanh({W_h}\\times{h_{t-1}}\\times{W_x}\\times{x_t}+{b})$ $x_t$ $W_x$ $W_h$ $h_{t-1}$ $b$ $({d}\\times{1})$ $({D_h}\\times{d})$ $({D_h}\\times{D_h})$ $({D_h}\\times{1})$ $({D_h}\\times{1})$ Keras RNN # hidden_units: hidden state의 크기 (output_dim) timesteps: 입력 시퀀스(문장)의 길이 (input_length) input_dim: 입력의 크기, 단어 벡터의 차원 RNN 층은 (batch_size, timesteps, input_dim) 크기의 3D 텐서를 입력으로 받음 메모리 셀의 최종 시점의 hidden state만 리턴할 경우 (batch_size, output_dim) 크기의 2D 텐서 반환 메모리 셀의 각 시점(time step)의 hidden state 값들을 모아 전체 시퀀스를 리턴할 경우 3D 텐서를 반환 return_sequences=True 옵션으로 반환값 설정 Copy python from tensorflow.keras.layers import SimpleRNN model = Sequential() model.add(SimpleRNN(hidden_units, input_shape=(timesteps, input_dim))) 메모리 셀에서 hidden state 계산은 다음과 같은 코드로 동작\nCopy python hidden_state_t = 0 # 초기 은닉 상태를 0(벡터)로 초기화 for input_t in input_length: # 각 시점마다 입력을 받는다. output_t = tanh(input_t, hidden_state_t) # 각 시점에 대해서 입력과 은닉 상태를 가지고 연산 hidden_state_t = output_t # 계산 결과는 현재 시점의 은닉 상태가 된다. Deep RNN # 순환 신경망에서 은닉층이 1개 더 추가되어 은닉층이 2개인 깊은 구조 첫번째 은닉층은 다음 은닉층이 존재하기 때문에 return_sequences=True를 설정하여 모든 시점을 전달 Copy python from tensorflow.keras.layers import SimpleRNN model = Sequential() model.add(SimpleRNN(hidden_units, input_shape=(timesteps, input_dim), return_sequences=True)) model.add(SimpleRNN(hidden_units, return_sequences=True)) Bidirectional RNN # t에서의 출력값을 예측할 때 이전 시점의 입력 뿐 아니라, 이후 시점의 입력 또한 예측 빈칸 채우기 등의 문제에서 미래 시점의 입력에 힌트가 있기 때문에 이전과 이후의 시점을 모두 고려 하나의 출력값을 예측하기 위해 두 개의 메모리 셀을 사용 첫 번째 메모리 셀은 앞 시점의 hidden state를 전달받아 현재의 hidden state를 계산 (forward) 두 번째 메모리 셀은 뒤 시점의 hidden state를 전달받아 현재의 hidden state를 계산 (backward) 은닉층을 추가하면 학습할 수 있는 양이 많아지지만, 훈련 데이터 또한 많은 양이 필요 Copy python from tensorflow.keras.layers import Bidirectional model = Sequential() model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True), input_shape=(timesteps, input_dim))) 08-02. LSTM # Valina RNN(Simple RNN)은 출력 결과가 이전의 계산 결과에 의존하여 비교적 짧은 시퀀스에서면 효과를 봄 장기 의존성 문제: time step이 길어질수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상 LSTM(장단기 메모리)은 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여\n불필요한 기억을 지우고, 기억해야할 것들을 결정 전통적인 RNN에서 cell state $C_t$를 추가하여 긴 시퀀스의 입력을 처리하는데 탁월한 성능 cell state 또한 이전 시점의 cell state가 다음 시점의 cell state를 구하기 위한 입력으로서 사용 입력 게이트: 현재 정보를 기억하기 위한 게이트 삭제 게이트: 기억을 삭제하기 위한 게이트, 시그모이드 함수의 반환값이 0에 가까울수록 많은 정보가 삭제 셀 상태: 삭제 게이트에서 일부 기억을 잃은 상태, 입력 게이트에서 선택된 기억을 삭제 게이트의 결과값과 더함 삭제 게이트는 이전 시점의 입력을 얼마나 반영할지 의미, 입력 게이트는 현재 시점의 입력을 얼마나 반영할지 결정 출력 게이트: 현재 시점 $t$의 hidden state를 결정하는 일에 사용 08-03. GRU # LSTM의 장기 의존성 문제에 대한 해결책을 유지하면서, hidden state를 업데이트하는 계산을 감소 GRU는 업데이트 게이트와 리셋 게이트로 구성 데이터의 양이 적을 때는 매개 변수의 양이 적은 GRU가 유리, 반대의 경우엔 LSTM이 유리 08-04. Keras RNN and LSTM # 입력 생성 # train_X가 2D 텐서의 형태일 경우 batch_size 1을 추가해 3D 텐서로 변경 Copy python train_X = [[[0.1, 4.2, 1.5, 1.1, 2.8], [1.0, 3.1, 2.5, 0.7, 1.1], [0.3, 2.1, 1.5, 2.1, 0.1], [2.2, 1.4, 0.5, 0.9, 1.1]]] train_X = np.array(train_X, dtype=np.float32) print(train_X.shape) # (1, 4, 5) SimpeRNN # return_sequences=False일 경우 2D 텐서 반환 return_sequences=True일 경우 timesteps를 포함하는 3D 텐서 반환 return_state=True일 경우 return_sequences에 관계없이 마지막 시점의 hidden state 출력 Copy python rnn = SimpleRNN(3) hidden_state = rnn(train_X) # shape(1, 3) Copy python rnn = SimpleRNN(3, return_sequences=True) hidden_states = rnn(train_X) # shape(1, 4, 3) Copy python rnn = SimpleRNN(3, return_sequences=True, return_state=True) hidden_states, last_state = rnn(train_X) # shape(1, 4, 3), shape(1, 3) LSTM # return_state=True일 경우 마지막 cell state를 포함한 세 개의 결과를 반환 Copy python lstm = LSTM(3, return_sequences=False, return_state=True) hidden_state, last_state, last_cell_state = lstm(train_X) # each shape(1, 3) Bidirectional LSTM # 정방향과 역방향에 대한 hidden state와 cell state를 반환 return_sequences=False일 경우 정방향 LSTM의 마지막 시점 hidden state와\n역방향 LSTM의 첫번째 시점 hidden state가 연결된 채 반환 return_sequences=True일 경우 각각의 순서 맞게 연결된 hidden state 반환 Copy python bilstm = Bidirectional(LSTM(3, return_sequences=False, return_state=True, \\ kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init)) hidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X) # shape(1, 6), shape(1, 3), shape(1, 3) 08-05. RNNLM # NNLM과 달리 time step을 도입하여 입력의 길이가 고정되지 않는 언어 모델 Teaching Forcing: 테스트 과정에서 RNN 모델을 훈련시킬 때 사용하는 훈련 기법,\n모델이 t 시점에서 예측한 값을 t+1 시점에 입력으로 사용하지 않고 실제 알고 있는 정답(t 시점의 레이블)을 사용 한 번 잘못 예측하면 뒤에서의 예측가지 영향을 미쳐 훈련 시간이 느려지기 때문에 교사 강요를 사용 훈련 과정 동안 활성화 함수로는 softmax 함수, 손실 함수로는 cross-entropy 함수를 사용 one-hot vector $x_t$를 입력받으면 NNLM에서와 동일한 embedding layer를 거쳐\n${V}\\times{M}$ 크기의 embedding vector로 변환, $e_t=lookup(x_t)$ 이후 RNN과 동일한 과정을 거쳐 $\\hat{y_t}$를 반환, $h_t=\\tanh({W_x}{e_t}+{W_h}{h_{t-1}}+b)$ $\\hat{y}_t$의 각 차원 안에서의 값은 $\\hat{y}_t$의 j번째 인덱스가 가진 0과 1사이의 값이 j번째 단어가 다음 단어일 확률 08-06. Text Generation using RNN # 데이터 전처리 # Copy python # 원본 한국어 문장 text = \u0026#34;\u0026#34;\u0026#34;경마장에 있는 말이 뛰고 있다\\n 그의 말이 법이다\\n 가는 말이 고와야 오는 말이 곱다\\n\u0026#34;\u0026#34;\u0026#34; Copy python # 단어 집합 생성 tokenizer = Tokenizer() tokenizer.fit_on_texts([text]) vocab_size = len(tokenizer.word_index) + 1 print(\u0026#39;단어 집합의 크기 : %d\u0026#39; % vocab_size) # 12 Copy python # 각 라인마다 texts_to_sequences() 함수를 적용해서 훈련 데이터 생성 print(sequences) [[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]] Copy python # 패딩 후 라벨 분리 (가장 우측에 있는 단어, [경마장에, 있는]에서 \u0026#39;있는\u0026#39; 등을 라벨로 지정) sequences = pad_sequences(sequences, maxlen=max_len, padding=\u0026#39;pre\u0026#39;) sequences = np.array(sequences) X = sequences[:,:-1] y = sequences[:,-1] Copy python # 라벨에 대해서 one-hot encoding 수행 y = to_categorical(y, num_classes=vocab_size) RNN 모델 설계 # many-to-one 구조의 RNN을 사용 모든 가능한 단어 중 마지막 시점에서 하나의 단어를 예측하는 다중 클래스 분류 문제 수행 활성화 함수로는 softmax 함수, 손실 함수로는 cross-entropy 함수 사용 첫 단어가 주어졌을 때, n번 동안 예측을 반복하면서 현재 단어와 문장에 예측 단어를 저장 Copy python embedding_dim = 10 hidden_units = 32 model = Sequential() model.add(Embedding(vocab_size, embedding_dim)) model.add(SimpleRNN(hidden_units)) model.add(Dense(vocab_size, activation=\u0026#39;softmax\u0026#39;)) model.compile(loss=\u0026#39;categorical_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) model.fit(X, y, epochs=200, verbose=2) LSTM 모델 설계 # 뉴욕 타임즈 기사 제목 데이터 전처리 (단어 집합 크기 3494, 샘플 최대 길이 24) RNN과 동일한 작업을 수행할 LSTM 모델 설계, 예측 과정 또한 동일 Copy python embedding_dim = 10 hidden_units = 128 model = Sequential() model.add(Embedding(vocab_size, embedding_dim)) model.add(LSTM(hidden_units)) model.add(Dense(vocab_size, activation=\u0026#39;softmax\u0026#39;)) model.compile(loss=\u0026#39;categorical_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) model.fit(X, y, epochs=200, verbose=2) 08-07. Char RNN # 입출력의 단위를 word-level에서 character-level로 변경한 RNN 문자 단위를 입출력으로 사용하기 때문에 embedding layer를 사용하지 않음 이상한 나라의 앨리스 데이터 사용 (문자열 길이 159484, 문자 집합 크기 56) 훈련 데이터에 apple이라는 시퀀스가 있고 입력의 길이가 4일 때, \u0026lsquo;appl\u0026rsquo;을 입력하면 \u0026lsquo;pple\u0026rsquo;을 예측할 것으로 기대 train_X.shape(2658, 60, 56), train_y.shape(2658, 60, 56) Char RNN 모델 설계 # Copy python hidden_units = 256 model = Sequential() model.add(LSTM(hidden_units, input_shape=(None, train_X.shape[2]), return_sequences=True)) model.add(LSTM(hidden_units, return_sequences=True)) model.add(TimeDistributed(Dense(vocab_size, activation=\u0026#39;softmax\u0026#39;))) model.compile(loss=\u0026#39;categorical_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) model.fit(train_X, train_y, epochs=80, verbose=2) "},{"id":74,"href":"/blog/2022-06-19/","title":"2022-06-19 Log","section":"Posts","content":"1. 행렬 # feature 1 feature 2 1 5 3 4 5 2 1-1. 행렬의 요소 # 행(row)과 열(column)로 구성 스칼라(scalar): 행렬을 구성하는 요소인 각 숫자, $a_{ij}$ ($i$: 행 번호, $j$: 열 번호) 벡터(vector): 스칼라의 집합, 크기와 방향을 모두 가짐, 행벡터 또는 열벡터로 표시 행렬(matrix): 행벡터의 집합 (또는 열벡터의 집합) 텐서(tensor): 2차원으로 구성된 행렬이 아닌 n차원으로 일반화한 행렬 1-2. 행렬의 종류 # 대각 행렬(diagonal matrix): 대각 원소 이외의 모든 성분이 0인 행렬 단위 행렬(identity matrix): 주 대각선의 원소가 모두 1이며 나머지는 0인 정사각 행렬 행렬을 대각 행렬($D$)이나 단위 행렬($I$)로 변환 시 연산량을 크게 줄이는 효과 전치 행렬(transposed matrix): 기존 행렬의 행과 열을 바꾸는 행렬, $a_{ij} \\rightarrow a_{ji}$ 1-3. 행렬의 연산 # 행렬의 덧셈, 뺄셈: 연산 대상이 되는 행렬의 행 번호와 열 번호가 일치하는 원소끼리 계산 행렬의 스칼라곱: 행렬을 구성하는 모든 원소에 스칼라를 곱함 행렬곱: $AB_{ij}=a_{i1}b_{1j}+a_{i2}b_{2j}+\u0026hellip;+a_{ir}b_{rj}$, 행렬 $A$의 열 크기와 행렬 $B$의 행 크기가 같아야 가능 행렬의 원소곱: 차원이 동일한 두 행렬의 동일 위치 원소를 서로 곱함, 딥러닝 최적화 관련 알고리즘에 자주 사용 행렬식(determinant): 행렬의 특성을 하나의 숫자로 표현, 행렬을 구성하고 있는 벡터로 만들 수 있는 도형의 부피를 계산 역행렬(inverse matrix): 행렬 $A$에 대해서 $AB=I$를 만족하는 행렬 $B$ ($A^{-1}$로 표기) 2. 내적 # $\u0026lt;u,v\u0026gt;=uㆍv=u_1v_1+u_2v_2+\u0026hellip;+u_nv_n$ 각 벡터의 요소를 서로 곱한 후 더함 벡터의 길이(norm)를 구하거나 벡터 사이의 관계를 파악할 수 있음 내적을 통해 두 벡터 사이의 각도를 추정 가능 (내적이 양수면 예각, 내적이 음수면 둔각) 2-1. 벡터의 길이 # 벡터 $u$의 길이는 $||u||$로 표기 $u=(u_1,u_2,⋯,u_n)$일 때, $||u||=\\sqrt{u_1^2+u_2^2+⋯+u_n^2}$ 위 식은 피타고라스의 정리를 일반화한 것 벡터의 길이를 통해 $x$ 좌표를 $||u||\\cos(\\theta)$, $y$ 좌표를 $||u||\\sin(\\theta)$로 표시 내적값 $uㆍv=||u||||v||\\cos(\\theta)$ 벡터 $u$를 벡터 $v$에 정사영한 벡터의 길이 $||proj_vu||=||u||\\cos(\\theta)$ $uㆍv=||u||||v||\\cos(\\theta)=(||v||)\\times(||u||\\cos(\\theta))$ 3. 선형 변환 # 두 벡터 공간 사이의 함수 좌표 평면 상 벡터를 확대, 축소, 회전, 반사하는 것은 모두 변환 4. 랭크, 차원 # 4-1. 벡터 공간, 기저 # 벡터 공간(vector space): 벡터 집합이 존재할 때, 해당 벡터들로 구성할 수 있는 공간 기저(basis): 벡터 공간을 생성하는 선형 독립인 벡터들 부분 공간(subspace): 벡터 공간의 일부분, 전체 벡터 집합의 부분 집합 2개의 기저 벡터 집합 $S$에 속하는 부분 공간을 $W$라 할 때, $W=span(S)$ 의 관계를 가짐 4-2. 랭크와 차원 # 차원(dimension): 기저 벡터의 개수, 벡터 공간을 구성하는데 필요한 최소한의 벡터 개수 랭크(rank): 열벡터에 의해 span된 벡터 공간의 차원 영공간(numm space): 행렬 $A$가 주어질 때 $Ax=0$을 만족하는 모든 벡터 $x$의 집합 4-3. 직교 행렬 # 직교 행렬(orthogonal matrix): 어떤 행렬의 행벡터와 열벡터가 유클리드 공간의 정규 직교 기저를 이루는 행렬 $AA^T=A^TA=I$ 직교 행렬 $A$의 역행렬은 자신의 전치 행렬, $A^T=A^{-1}$ "},{"id":75,"href":"/blog/programmers-problems-60059/","title":"[프로그래머스/카카오 60059] 자물쇠와 열쇠 (Python)","section":"Posts","content":"문제 링크 # https://programmers.co.kr/learn/courses/30/lessons/60059 개요 # numpy 라이브러리와 중복 순열을 사용해 해결할 수 있는 문제다. 문제 조건 # 2차원 배열인 열쇠(M)를 회전하거나 이동해 2차원 배열인 자물쇠(N)에 맞는지 여부를 반환하는 문제다. 문제 해설 # 2차원 배열을 numpy.ndarray 형식으로 변환하면 회전 및 이동 연산을 쉽게 처리할 수 있다. 90도 단위로 4번 회전된 각각의 목록을 구하고 상하좌우 이동을 위해 바깥쪽에 0으로 채워진 padding을 추가한다. padding이 채워진 N+M-1크기의 2차원 배열에 대해 자물쇠 크기만큼의 부분만 잘라내어 자물쇠의 구멍과 비교한다. OR 연산 시 자물쇠가 1로 채워지고 열쇠와 자물쇠 사이에 겹치는 부분이 없다면 열쇠가 자물쇠에 맞다고 판단한다. 시행착오 # 열쇠의 크기가 자물쇠의 크기보다 작을 경우를 고려하지 못하고 둘의 사이즈를 맞추는 과정을 무시해 에러가 생겼다. 처음엔 0으로 채워진 단일 행 또는 열과 concatenate 연산을 진행해 열쇠를 아래쪽과 오른쪽으로만 이동했는데,\n그 반대의 경우를 고려하지 못해서 에러가 생겼다. 이후 padding을 사용하는 코드로 변경했다. 통계학적 지식이 부족해 인덱스 목록에 대해 중복 조합 연산을 했었는데 잘못됨을 인지하고 중복 순열로 변경했다. 열쇠와 자물쇠의 돌기가 만나선 안된다는 부분을 처리하지 않아 특정 케이스에 대해 실패가 발생하는 이유를 인지하지 못했다.\nXOR 연산도 하나의 방법일 수 있지만 대신에 두 배열의 합을 사용해 중복되는 부분을 판단했다. 프로그래머스가 백준처럼 NumPy 라이브러리를 지원하지 않았다면 매우 난해한 문제였을테지만,\n다행히 NumPy 라이브러리를 활용해 비교적 단순한 방법으로 풀 수 있었다. 해설 코드 # Copy python import numpy as np from itertools import product def solution(key, lock): key, lock = np.array(key), np.array(lock) rotated_keys = [np.pad(np.rot90(key, i),len(lock)-1) for i in range(4)] index_list = list(range(len(rotated_keys[0])-len(lock)+1)) for rotated_key in rotated_keys: for i, j in list(product(index_list, index_list)): key = rotated_key[i:i+len(lock),j:j+len(lock)] if (0 not in np.logical_or(key,lock)) and (2 not in (key + lock)): return True return False "},{"id":76,"href":"/blog/programmers-problems-81301/","title":"[프로그래머스/카카오 81301] 숫자 문자열과 영단어 (Python)","section":"Posts","content":"문제 링크 # https://programmers.co.kr/learn/courses/30/lessons/81301 개요 # 딕셔너리를 사용해 해결할 수 있는 문제다. 문제 조건 # 일부 숫자가 영단어로 변환된 문자열을 원래의 숫자로 되돌려 반환하는 문제다. 문제 해설 # 각각의 영단어에 대한 숫자 맵과 문자열의 replace 함수를 사용하면 쉽게 해결할 수 있다. 해설 코드 # Copy python def solution(s): answer = s word_dict = {\u0026#39;zero\u0026#39;:\u0026#39;0\u0026#39;,\u0026#39;one\u0026#39;:\u0026#39;1\u0026#39;,\u0026#39;two\u0026#39;:\u0026#39;2\u0026#39;,\u0026#39;three\u0026#39;:\u0026#39;3\u0026#39;, \u0026#39;four\u0026#39;:\u0026#39;4\u0026#39;,\u0026#39;five\u0026#39;:\u0026#39;5\u0026#39;,\u0026#39;six\u0026#39;:\u0026#39;6\u0026#39;,\u0026#39;seven\u0026#39;:\u0026#39;7\u0026#39;, \u0026#39;eight\u0026#39;:\u0026#39;8\u0026#39;,\u0026#39;nine\u0026#39;:\u0026#39;9\u0026#39;} for key, value in word_dict.items(): answer = answer.replace(key, value) return int(answer) "},{"id":77,"href":"/blog/programmers-problems-17676/","title":"[프로그래머스/카카오 17676] 추석 트래픽 (Python)","section":"Posts","content":"문제 링크 # https://programmers.co.kr/learn/courses/30/lessons/17676 개요 # datetime 라이브러리를 사용해 해결할 수 있는 문제다. 문제 조건 # 트래픽 처리 종료 시간 및 처리 시간이 짝지어진 로그 문자열을 해석하여 초당 최대 처리량을 반환하는 문제다. 문제 해설 # datetime과 timedelta 모듈을 활용하여 각각의 트래픽 로그에 대한 시작과 끝 시간을 계산한다. 트래픽의 시작 또는 끝을 1초 구간의 시작으로 정의하고 해당 구간에서 시작됐거나 진행 중인 트래픽 수를 합산한다. 합산된 트래픽 수 중에서 최댓값을 초당 최대 처리량으로 판단하여 반환한다. 한계 # 트래픽 로그를 시작 시간과 끝 시간으로 분리하지 않고 시간 범위로 변환할 수 있다면,\n굳이 2N 길이의 반복문을 사용하지 않고 교집합 연산을 사용해서 시간 복잡도를 개선할 수 있었을 것이다. 해설 코드 # Copy python from datetime import datetime, timedelta def solution(lines): answer = 0 lines = sorted(map(interpret_log, lines)) delta = timedelta(seconds=1) for start in sum(lines, []): t = sum([check_time_range(time_range, start, start+delta) for time_range in lines]) answer = max(t, answer) start += delta return answer def interpret_log(line): line = line.split() line = [word.split(s) for word, s in zip(line, [\u0026#39;-\u0026#39;,\u0026#39;:\u0026#39;,\u0026#39;s\u0026#39;])] Y,m,d,H,M,S,ms = list(map(int,line[0]+line[1][:-1]+line[1][-1].split(\u0026#39;.\u0026#39;))) end_date = datetime(Y,m,d,H,M,S,ms*1000) duration = line[2][0] if \u0026#39;.\u0026#39; in line[2][0] else line[2][0]+\u0026#39;.0\u0026#39; S,ms = list(map(int,duration.split(\u0026#39;.\u0026#39;))) start_date = end_date - timedelta(seconds=S,milliseconds=ms-1) return [start_date, end_date] def check_time_range(time_range, start, end): con1 = start \u0026lt;= time_range[0] \u0026lt; end con2 = time_range[0] \u0026lt;= start \u0026lt;= time_range[1] return con1 or con2 "},{"id":78,"href":"/blog/programmers-problems-42888/","title":"[프로그래머스/카카오 42888] 오픈채팅방 (Python)","section":"Posts","content":"문제 링크 # https://programmers.co.kr/learn/courses/30/lessons/42888 개요 # 딕셔너리를 사용해 해결할 수 있는 문제다. 문제 조건 # 채팅방 상태 메시지에 대해 닉네임 변경 사항을 적용하여\n최종적으로 UI 상에서 보여지는 메시지를 목록을 반환하는 문제다. 문제 해설 # uid에 대한 닉네임이 짝지어진 딕셔너리(name_dict)를 기반으로 최종적인 닉네임 목록을 기록한다. 메시지가 Enter와 Change로 시작하는 경우 닉네임이 설정 또는 변경된 것이라 인지하여 딕셔너리를 수정한다. name_dict에서 uid에 대한 닉네임을 참조하여 상태 메시지를 조건에 맞는 형식으로 변환한다. 해설 코드 # Copy python def solution(record): answer = [] record = [rec.split() for rec in record] name_dict = {rec[1]:rec[2] for rec in record if rec[0] in {\u0026#39;Enter\u0026#39;,\u0026#39;Change\u0026#39;}} msg_dict = {\u0026#39;Enter\u0026#39;:\u0026#39;들어왔습니다.\u0026#39;,\u0026#39;Leave\u0026#39;:\u0026#39;나갔습니다.\u0026#39;} for rec in record: if rec[0] in {\u0026#39;Enter\u0026#39;,\u0026#39;Leave\u0026#39;}: answer.append(name_dict[rec[1]]+\u0026#39;님이 \u0026#39;+msg_dict[rec[0]]) return answer "},{"id":79,"href":"/blog/programmers-problems-60057/","title":"[프로그래머스/카카오 60057] 문자열 압축 (Python)","section":"Posts","content":"문제 링크 # https://programmers.co.kr/learn/courses/30/lessons/60057 개요 # 문자열 처리 능력이 요구되는 문제다. 문제 조건 # 문자열에서 반복되는 문자 또는 단어를 압축하고 가장 짧게 압축된 길이를 반환한다. 문제 해설 # 문자열을 단일 문자부터 2등분이 될 때까지 한 단위씩 늘려가면서 분리된 문자들에 대한 압축 과정을 진행한다. 분리된 문자들을 순회하면서 반복되는 문자열을 무시하고 남은 문자열의 길이를 세는 방법도 있지만,\n여기선 문자열을 형식에 맞게 압축시키고 그 길이를 구한다. 이전 문자열이 담길 메모리와 해당 문자열이 반복된 횟수를 기록하는 변수를 각각 선언한다. 분리된 문자들을 순회하면서 현재 문자와 메모리가 다르면(반복되지 않으면)\n압축된 문자열에 메모리를 추가하고 초기화한다. 모든 과정에 대한 최소 길이 값을 기록하고 반환한다. 해설 코드 # Copy python def solution(s): answer = len(s) for unit in range(1,len(s)//2+1): s_range = list(range(0, len(s), unit))+[None] s_split = [s[s_range[i]:s_range[i+1]] for i in range(len(s_range)-1)]+[\u0026#39;\u0026#39;] new_s, memory, cnt = str(), s_split[0], 1 for s_unit in s_split[1:]: if memory != s_unit: new_s += ((str(cnt) if cnt \u0026gt; 1 else str()) + memory) memory, cnt = s_unit, 1 else: cnt += 1 answer = min(answer, len(new_s)) return answer "},{"id":80,"href":"/blog/programmers-problems-72410/","title":"[프로그래머스/카카오 72410] 신규 아이디 추천 (Python)","section":"Posts","content":"문제 링크 # https://programmers.co.kr/learn/courses/30/lessons/72410 개요 # 정규식을 사용해 해결할 수 있는 문제다. 문제 조건 # 유저가 제시한 아이디 문자열을 규칙에 맞게 변경하여 반환하는 문제다. 문제 해설 # 제시된 조건에 대해 정규식을 구현하여 문자열에 적용하면 된다. 정규식 활용 능력에 따라 더욱 간단한 코드로 구현할 수도 있다. 해설 코드 # Copy python import re def solution(new_id): answer = new_id.lower() answer = re.sub(r\u0026#34;[^a-z0-9-_\\.]\u0026#34;,\u0026#34;\u0026#34;,answer) answer = re.sub(r\u0026#34;\\.+\u0026#34;,\u0026#34;.\u0026#34;,answer) answer = re.sub(r\u0026#34;^\\.\u0026#34;,\u0026#34;\u0026#34;,answer) answer = re.sub(r\u0026#34;\\.$\u0026#34;,\u0026#34;\u0026#34;,answer) answer = \u0026#39;a\u0026#39; if not answer else answer answer = answer[:15] answer = answer[:-1] if answer[-1] == \u0026#39;.\u0026#39; else answer answer += answer[-1]*(3-len(answer)) return answer "},{"id":81,"href":"/blog/programmers-problems-92334/","title":"[프로그래머스/카카오 92334] 신고 결과 받기 (Python)","section":"Posts","content":"문제 링크 # https://programmers.co.kr/learn/courses/30/lessons/92334 개요 # 딕셔너리를 사용해 해결할 수 있는 문제다. 문제 조건 # 일정 횟수 이상 신고당한 불량 이용자를 신고한 이용자들에게 발송되는 메일의 횟수를 리스트로 반환하는 문제다. 문제 해설 # 이용자 자신이 신고당한 횟수(report_dict)와 이용자가 신고한 대상 목록(mail_dict)을 각각 기록할 필요가 있다. 각각의 신고 건수에 대해 반복하며 두 가지 딕셔너리에 기록한다. 이용자id를 key로 참고하여 각각의 이용자마다 자신이 신고한 대상 중 정지된 대상의 수를 계산한다. 해설 코드 # Copy python def solution(id_list, report, k): report_dict = {id: 0 for id in id_list} mail_dict = {id: set() for id in id_list} for rep in set(report): user, target = rep.split() report_dict[target] += 1 mail_dict[user].add(target) answer = [] for user, targets in mail_dict.items(): answer.append(sum([1 if report_dict[target] \u0026gt;= k else 0 for target in targets])) return answer "},{"id":82,"href":"/blog/aischool-06-09-pipeline/","title":"[AI SCHOOL 5기] 머신 러닝 실습 - Pipeline","section":"Posts","content":"Feature Transformer # Import Libraries # Copy python from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline ColumnTransformer # Copy python numeric_features = [\u0026#39;CRIM\u0026#39;, \u0026#39;ZN\u0026#39;, \u0026#39;INDUS\u0026#39;, \u0026#39;NOX\u0026#39;, \u0026#39;RM\u0026#39;, \u0026#39;AGE\u0026#39;, \u0026#39;DIS\u0026#39;, \u0026#39;TAX\u0026#39;, \u0026#39;PTRATIO\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;LSTAT\u0026#39;] numeric_transformer = StandardScaler() categorical_features = [\u0026#39;CHAS\u0026#39;, \u0026#39;RAD\u0026#39;] categorical_transformer = OneHotEncoder(categories=\u0026#39;auto\u0026#39;) preprocessor = ColumnTransformer( transformers=[ (\u0026#39;num\u0026#39;, numeric_transformer, numeric_features), (\u0026#39;cat\u0026#39;, categorical_transformer, categorical_features)]) OneHotEncoder()의 handle_unknown 설정\nerror: 숫자로 변환된 분류형 범주에 새로운 문자열 데이터가 들어올 경우 에러를 발생시킴 ignore: 카테고리에 해당되는 번호가 없으면 자동으로 0으로 바꿈 Preprocessing-Only # Copy python preprocessor_pipe = Pipeline(steps=[(\u0026#39;preprocessor\u0026#39;, preprocessor)]) steps: 전처리 도구를 순서대로 적용 (모델도 입력 가능) Model Fitting # Copy python preprocessor_pipe.fit(x_train) x_train_transformed = preprocessor_pipe.transform(x_train) x_test_transformed = preprocessor_pipe.transform(x_test) Numeric Variables에 대한 11개의 열,\nCategorical Variables에 대한 2개의 열,\n카테고리 별 One-Hot Encoding이 적용된 9개의 열을 함께 표시 Pipeline을 통해 전처리를 진행할 경우 데이터를 원래대로 되돌리는 inverse_trasnform 불가능 Preprocessing + Training # Copy python from sklearn.ensemble import GradientBoostingClassifier model = Pipeline(steps=[(\u0026#39;preprocessor\u0026#39;, preprocessor), (\u0026#39;classifier\u0026#39;, GradientBoostingClassifier(n_estimators=200, random_state=0))]) Preprocessing과 Training을 같이 묶을 경우 다른 모델을 끼워넣기 어려움 위 단점 떄문에 전처리만을 사용하는 것을 권장 Preprocessing + Training + HPO # Copy python model = Pipeline(steps=[(\u0026#39;preprocessor\u0026#39;, preprocessor), (\u0026#39;classifier\u0026#39;, GradientBoostingClassifier())]) param_grid = { \u0026#39;classifier__loss\u0026#39;: [\u0026#39;deviance\u0026#39;, \u0026#39;exponential\u0026#39;], \u0026#39;classifier__learning_rate\u0026#39;: [0.01, 0.001], \u0026#39;classifier__n_estimators\u0026#39;: [200, 400], \u0026#39;classifier__min_samples_split\u0026#39;: [2, 4], \u0026#39;classifier__max_depth\u0026#39;: [2, 4], \u0026#39;classifier__random_state\u0026#39;: [0] } grid_search = GridSearchCV(model, param_grid, refit=True, cv=3, n_jobs=1, verbose=1, scoring= \u0026#39;accuracy\u0026#39;) "},{"id":83,"href":"/blog/aischool-06-08-model-stacking/","title":"[AI SCHOOL 5기] 머신 러닝 실습 - Model Stacking","section":"Posts","content":"Model Stacking # 서로 다른 모델들을 모으고 Ensemble 기법을 사용해 개선된 모델을 만드는 것 기존 모델들로부터 예측 결과를 도출하는 1st Stage와\n이를 기반으로 추가적인 판단을 진행하는 2nd Stage로 나뉨 1st Stage # train_X를 가지고 1번 모델을 Training Training을 거친 1번 모델에 train_X를 넣었을 때 결과(예측값)을 저장 다른 모델에도 동일한 작업을 했을 때 나온 1열의 예측값들을 묶어 S_train을 생성 (기존 Ensemble은 S_train을 행별로 투표해서 분류함) 2nd Stage # 새로운 모델 생성 (1st Stage에서 사용한 것과 다른 모델 사용 가능) S_train_X, train_Y를 가지고 새로운 모델을 Training Test Model # test_X를 1st Stage 모델에 넣고 결과로 나온 예측값들의 묶음 S_test를 생성 (2nd Stage 모델의 학습 데이터는 원본 데이터와 다르기 때문에 test_X를 바로 넣으면 안됨) S_train_X, train_Y를 2nd Stage 모델에 넣었을 때 결과를 가지고 Accuracy 계산 Functional API # Import Library # Copy python from vecstack import stacking 1st Level Models # Copy python models = [ ExtraTreesClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3), RandomForestClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3), XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.1, n_estimators = 100, max_depth = 3)] Stacking # Copy python S_train, S_test = stacking(models, X_train, y_train, X_test, regression = False, metric = accuracy_score, n_folds = 4, stratified = True, shuffle = True, random_state = 0, verbose = 2) S_train과 S_test를 같이 생성 (y_test는 2차 모델 성능 평가에서만 사용) metric: Focus를 맞출 대상 2nd Level Model # Copy python model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.1, n_estimators = 100, max_depth = 3, eval_metric=\u0026#39;mlogloss\u0026#39;) model = model.fit(S_train, y_train) y_pred = model.predict(S_test) accuracy_score(y_test, y_pred)를 확인하여 모델의 성능 평가 Scikit-learn API # Import Library # Copy python from vecstack import StackingTransformer 1st Level Estimators # Copy python estimators = [ (\u0026#39;ExtraTrees\u0026#39;, ExtraTreesClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3)), (\u0026#39;RandomForest\u0026#39;, RandomForestClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3)), (\u0026#39;XGB\u0026#39;, XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.1, n_estimators = 100, max_depth = 3, eval_metric=\u0026#39;mlogloss\u0026#39;))] stacking과 다르게 모델 이름과 모델 객체를 같이 튜플로 묶음 StackingTransformer # Copy python stack = StackingTransformer(estimators, regression = False, metric = accuracy_score, n_folds = 4, stratified = True, shuffle = True, random_state = 0, verbose = 2) stacking과 다르게 x data와 y data를 입력하지 않고 객체 자체를 모델처럼 사용 Model Fitting # Copy python stack = stack.fit(X_train, y_train) S_train = stack.transform(X_train) S_test = stack.transform(X_test) stacking은 새로운 데이터를 넣을 때 어려움이 있지만,\nStackingTransformer는 전처리 도구처럼 사용 가능 2nd Level Estimator # Copy python model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.1,n_estimators = 100, max_depth = 3, eval_metric=\u0026#39;mlogloss\u0026#39;) model = model.fit(S_train, y_train) y_pred = model.predict(S_test) "},{"id":84,"href":"/blog/aischool-06-07-pca/","title":"[AI SCHOOL 5기] 머신 러닝 실습 - PCA","section":"Posts","content":"Principal Component Analysis # 차원 축소를 통해 최소 차원의 정보로 원래 차원의 정보를 모사하는 알고리즘 데이터의 열의 수가 많아 학습 속도가 느려질 때 열의 수를 줄이기 위해 사용 Dimension Reduction: 고차원 벡터에서 일부 차원의 값을 모두 0으로 만들어 차원을 줄임 원래의 고차원 벡터의 특성을 최대한 살리기 위해 가장 분산이 높은 방향으로 회전 변환 진행 전체 데이터를 기반으로 분산이 가장 큰 축을 찾아 PC 1으로 만들고,\nPC 1에 직교하는 축 중에서 분산이 가장 큰 축을 PC 2로 만드는 과정 반복 정보의 누락이 있기 때문에 경우에 따라 모델의 성능 하락 발생 Feature Selection: 기존에 존재하는 열 중에 n개를 선택 Feature Extraction: 기존에 있는 열들을 바탕으로 새로운 열들을 만들어냄 (차원 축소) Learning Process # Import Libraries # Copy python from sklearn import decomposition from sklearn import datasets Load Model # Copy python iris = datasets.load_iris() x = iris.data y = iris.target Create Model # Copy python model = decomposition.PCA(n_components=1) component의 개수에 상관없이 PC 1은 언제나 동일 Model Fitting # Copy python model.fit(x) x1 = model.transform(x) Plot Model # Histogram (components=1) # Copy python import seaborn as sns sns.distplot(x1[y==0], color=\u0026#34;b\u0026#34;, bins=20, kde=False) sns.distplot(x1[y==1], color=\u0026#34;g\u0026#34;, bins=20, kde=False) sns.distplot(x1[y==2], color=\u0026#34;r\u0026#34;, bins=20, kde=False) plt.xlim(-6, 6) plt.show() Scatter (components=3) # Copy python plt.scatter(x[:, 0], x[:, 1], c=iris.target) plt.xlabel(\u0026#39;PC1\u0026#39;) plt.ylabel(\u0026#39;PC2\u0026#39;) plt.show() 최적의 PCA 개수 # 데이터셋의 분산 정도 확인 # model.explained_variance_ratio_ components가 전체 분산 정도 중 몇 퍼센트인지 확인 합쳐서 95퍼센트를 넘는 PCA 개수가 최적의 개수 최적의 PCA 개수 확인 # Copy python np.argmax(np.cumsum(model.explained_variance_ratio_) \u0026gt;= 0.95 ) + 1 np.cumsum: 값의 누적된 합계 계산 np.argmax: 주어진 값들 중 가장 큰 값의 인덱스 번호 최적의 PCA 개수 적용 # Copy python model = decomposition.PCA(n_components=0.95) model.fit(x) x = model.transform(x) "},{"id":85,"href":"/blog/aischool-06-06-k-means/","title":"[AI SCHOOL 5기] 머신 러닝 실습 - K-Means","section":"Posts","content":"1. K-Means Algorithm # K는 전체 데이터를 몇 개의 그룹으로 묶어낼 것인지 결정하는 상수 어떤 K 값이 적절한 것인지 파악하는 것이 중요 각각의 데이터마다 중심값까지의 거리를 계속 물어보기 때문에 계산량이 많음 클러스터링 성능을 향상시키기 위해 GPU Accelerated t-SNE for CUDA 활용 Clustering Process # K개의 임의의 중심값을 선택 각 데이터마다 중심값까지의 거리를 계산하여 가까운 중심값의 클러스터에 할당 각 클러스터에 속한 데이터들의 평균값으로 각 중심값을 이동 데이터에 대한 클러스터 할당이 변하지 않을 때까지 2와 3을 반복 2. Learning Process # Model Fitting # Copy python from sklearn import cluster kmeans = cluster.KMeans(n_clusters=2, random_state=0).fit(X) kmeans.labels_: 클러스터 번호 kmeans.cluster_centers_: 학습이 끝난 중심값 Model Predict # Copy python kmeans.predict([[0, 0], [8, 4]])) 각각의 번호가 어떤 클러스터에 속하는지 예측 3. K-Means for Iris Data # Import Libraries # Copy python import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from sklearn import cluster from sklearn import datasets from sklearn import metrics Axes3D: 3D 공간에서 시각화하는 함수 Model Fitting # Copy python estimators = [(\u0026#39;k=8\u0026#39;, cluster.KMeans(n_clusters=8)), (\u0026#39;k=3\u0026#39;, cluster.KMeans(n_clusters=3)), (\u0026#39;k=3(r)\u0026#39;, cluster.KMeans(n_clusters=3, n_init=1, init=\u0026#39;random\u0026#39;))] Plot Model # Copy python fignum = 1 titles = [\u0026#39;8 clusters\u0026#39;, \u0026#39;3 clusters\u0026#39;, \u0026#39;3 clusters, bad initialization\u0026#39;] for name, est in estimators: fig = plt.figure(fignum, figsize=(7, 7)) ax = Axes3D(fig, elev=48, azim=134) est.fit(X) labels = est.labels_ ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(np.float), edgecolor=\u0026#39;w\u0026#39;, s=100) ... fignum = fignum + 1 plt.show() plt.figure(fignum): plot을 여러 개 생성 (subplot()은 하나의 plot을 분리) Axes3D(elev, azim): elevation (축의 고도), azimuth (방위각) astype: 색깔 칠해주는 옵션 edgecolor: 테두리 1번 모델은 8개의 클러스터로 나눈 모델 (불필요하게 세분화시킴) 3번 모델은 초기 중앙값을 랜덤으로 잡아서 특정 클러스터에 데이터가 몰림 Ground Truth (원본) # Copy python fig = plt.figure(figsize=(7, 7)) ax = Axes3D(fig, elev=48, azim=134) for name, label in [(\u0026#39;Setosa\u0026#39;, 0), (\u0026#39;Versicolour\u0026#39;, 1), (\u0026#39;Virginica\u0026#39;, 2)]: ax.text3D(X[y == label, 3].mean(), X[y == label, 0].mean(), X[y == label, 2].mean()+2, name, horizontalalignment=\u0026#39;center\u0026#39;) ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor=\u0026#39;w\u0026#39;, s=100) ... plt.show() 4. 최적의 클러스터 개수 # 최적의 클러스터 기준 # 같은 클러스터에 있는 데이터끼리 뭉쳐 있음 서로 다른 클러스터에 있는 데이터끼리 멀리 떨어져 있음 Elbow 기법 # SSE(Sum of Squared Errors)의 값이 점점 줄어들다가 어느 순간\n줄어드는 비율이 급격하게 작아지는 부분이 발생 결과물인 그래프 모양을 보면 팔꿈치에 해당하는 부분이 최적의 클러스터 개수가 됨 Copy python def elbow(X): total_distance = [] for i in range(1, 11): model = cluster.KMeans(n_clusters=i, random_state=0) model.fit(X) total_distance.append(model.inertia_) plt.plot(range(1, 11), total_distance, marker=\u0026#39;o\u0026#39;) plt.xlabel(\u0026#39;# of clusters\u0026#39;) plt.ylabel(\u0026#39;Total distance (SSE)\u0026#39;) plt.show() elbow(X) model.inertia_: 샘플에 대해 가장 가까운 클러스터와의 거리 제곱의 합 inertia 값은 클러스터 수가 늘어날수록 감소 같은 클러스터에 있는 데이터끼리 뭉쳐있는 정도만 확인 가능 Silhouette # 클러스터링의 품질을 정량적으로 계산해주는 방법 (모든 클러스터링 기법에 적용 가능) i번째 데이터 x(i)에 대한 실루엣 계수 s(i) 값은 아래의 식으로 정의 a(i): 클러스터 내 데이터 응집도(cohesion) 를 나타내는 값\n== 데이터 x(i)와 동일한 클러스터 내의 나머지 데이터들과의 평균 거리 b(i): 클러스터 간 분리도(separation) 를 나타내는 값\n== 데이터 x(i)와 가장 가까운 클러스터 내의 모든 데이터들과의 평균 거리 클러스터의 개수가 최적화되어 있으면 실루엣 계수의 값은 1에 가까운 값이 됨 실루엣 계수의 평균이 0.7 이상이면 안정적 Copy python from sklearn.metrics import silhouette_score silhouette_avg = silhouette_score(X, y_fitted) silhouette_avg: 실루엣 계수의 평균 클러스터링의 기준이 이론적으로는 맞을 수 있어도 실용적으로는 다를 수 있음\n(판단 기준이 없을 때 활용) "},{"id":86,"href":"/blog/aischool-06-05-kernelized-svm/","title":"[AI SCHOOL 5기] 머신 러닝 실습 - Kernelized SVM","section":"Posts","content":"Support Vector Machine # 패턴 인식을 위한 지도 학습 모델 데이터를 분류하는 Margin을 최대화하는 결정 경계(Decision Boundary)를 찾는 기법 결정 경계와 가장 가까운 데이터를 가로지르는 선을 기준으로 Plus \u0026amp; Minus Plane 설정 Support Vector: 결정 경계와 가장 가까운 데이터의 좌표 Margin: b11(plus-plane)과 b12(minus-plane) 사이의 거리, 2/w 기존의 Hard Margin SVM은 소수의 Noise로 인해 결정 경계를 찾지 못할 수 있음 Plus \u0026amp; Minus Plane에 약간의 여유 변수를 두어 에러를 무시하는 Soft Margin SVM로 발전 arg min # $$arg\\ min\\lbrace\\frac{1}{2}{||w||}^2+C\\Sigma^n_{i=1}\\xi_i\\rbrace$$ $$\\text{단, }y_i({w}\\cdot{x_i}-b)\\ge{1-\\xi_i},\\quad{\\xi_i\\ge{0}},\\quad{\\text{for all }1\\le{i}\\le{n}}$$\n중괄호 안의 값(w, ξ, b)을 최소화하는 값을 찾는 것 Margin을 최대화하는 목적 함수(Objective Function) Margin(2/w)의 최대화는 w/2의 최소화와 같음 (제곱은 미분 편의성) Margin을 침범한 에러(ξ)를 모두 더하고(Σ), Hyper-Parameter인 C를 곱함 C: 얼마 만큼 여유를 가지고 오류를 허용할 것인지 판단해주는 값 C가 작을수록 에러를 무시, C가 크면 에러에 민감 Kernel Support Vector Machine # 데이터가 선형적으로 분리되지 않을 경우(Lineaerly Unseparable)에 결정 경계를 찾는 기법 원본 데이터가 놓여있는 차원을 비선형 매핑을 통해 고차원 공간으로 변환 Hyper-Parameter인 커널 함수를 컴퓨터가 스스로 찾아내는 것이 Deep Learning 커널 함수는 인공신경망의 레이어와 비슷 (Learnable Kernel) Feature Crosses # 데이터를 인공신경망에 밀어넣기 이전에, 기존 열들의 입력값을 조합해서 새로운 데이터 생성 @ http://j.mp/2p5CbO2 SVC Models # Linear SVC # Copy python from sklearn.svm import LinearSVC linear_svm = LinearSVC().fit(X, y) Kernelized SVC # Copy python from sklearn.svm import SVC X, y = custom_mglearn.make_handcrafted_dataset() svm = SVC(kernel=\u0026#39;rbf\u0026#39;, C=10, gamma=0.1).fit(X, y) rbf: 대표적인 커널 함수 (Radial Basis Function, 가우시안 커널 함수) gamma: 가우시안 함수의 단면을 r 이라 할 때, 반지름의 역수(1/r) gamma가 작아질수록 반지름이 커져 선과 같은 형태가 됨 Learning Process # Copy python cancer = load_breast_cancer() # 유방암 데이터 (569행, 30열) X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state=0) svc = SVC(gamma=\u0026#39;auto\u0026#39;) svc.fit(X_train, y_train) train_test_split()에 test_size가 없으면 기본값 0.25를 Test Data로 지정 SVC(gamma='auto'): gamma를 알아서 지정 분류 모델의 경우, model.score(X, Y)를 실행하면 Accuracy Score 계산 svc.score(X_train, y_train): Training Data의 Accuracy svc.score(X_test, y_test): Test Data의 Accuracy 두 데이터의 Accuracy 차이가 심한 것은 열마다 Scale이 달라서 발생하는 문제 Scaling 후 Hyper-Parameter를 바꿨을 때 두 데이터의 차이가 심하면 Overfitting Feature Normalization (Scaling) # min, max, mean, std를 계산할 때 Training Data만 사용 Min-Max Normalization # min(열) = 0, max(열) = 1 new_X = (old_X - min) / (max - min) Standardization # mean(열) = 0, std(열) = 1 new_X = (old_X - mean) / std Scaler Model # Copy python sc = StandardScaler() # 또는 MinMaxScaler() sc.fit(train_X) train_X_scaled = sc.transform(train_X) test_X_scaled = sc.transform(test_X) 새로운 데이터가 들어오면 2차원 행렬 상태로 Scaler를 통과시키고 모델에 입력 모델을 저장하고 불러올 때 Scaler도 같이 가져와야 함 Stanardization이 Min-Max 알고리즘보다 성능이 좋음 (예외 있음) Grid-Search # Copy python from sklearn.model_selection import GridSearchCV param_grid = {\u0026#39;C\u0026#39; : [0.1, 1, 10, 100, 1000], \u0026#39;gamma\u0026#39; : [1, 0.1, 0.01, 0.001, 0.0001], \u0026#39;kernel\u0026#39; : [\u0026#39;rbf\u0026#39;]} grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=1) grid.fit(X_train_scaled, y_train) C와 gamma의 후보군을 조합 내부적으로 K-Fold로 검증하기 때문에 후보군이 많아질수록 시간이 오래걸림 refit=True: GridSearchCV를 다시 트레이닝 시킴 verbose: 값이 커질수록 설명을 상세하게 적어줌 GridSearch가 SVC 모델이 되어 Model Fitting, Model Predict 등 과정 진행 grid.best_params_: 최적의 C와 gamma의 조합 반환 내부적으로 검증을 거친 상태이기 때문에 Training Data를 한 번 돌린 것과는 정확도가 다름 Model Tuning (HPO) # Grid-Search (Machine Learning) Randomized-Search (Deep Learning) Bayesian-Search (ML/DL) Model Predict # Copy python from sklearn.metrics import classification_report grid_predictions = grid.predict(X_test_scaled) print(classification_report(y_test, grid_predictions)) precision: 모델이 양성으로 분류한 것 중 진짜 걸린 것 recall: 양성인 것 중 모델이 양성이라 맞춘 것 f1-score: Precision과 Recall의 조합 support: Test Data confusion_matrix도 import해서 사용 가능 precision recall f1-score support 0 0.98 0.94 0.96 53 1 0.97 0.99 0.98 90 accuracy 0.97 143 macro avg 0.97 0.97 0.97 143 weighted avg 0.97 0.97 0.97 143 "},{"id":87,"href":"/blog/aischool-06-04-knn/","title":"[AI SCHOOL 5기] 머신 러닝 실습 - KNN","section":"Posts","content":"K-Nearest Neightbor Algorithm # 기존의 가까운 이웃 데이터를 살펴 새로운 데이터를 분류하는 알고리즘 K=3일 경우, 가장 가까운 나머지 3개 중 2개가 Red면 Red로 판단 K 값이 작아질수록 아주 작은 영향에로 판단이 바뀌는 Overfitting 발생 K 값이 커질수록 멀리보고 결정이 느려져 Overfitting 감소 Learning Process # Load Data # Copy python iris = datasets.load_iris() # 붓꽃 데이터 (150행, 4열) Select Feature # Copy python x = iris.data[:, :2] # [꽃받침 길이, 꽃받침 넓이] y = iris.target Create Model # Copy python model = neighbors.KNeighborsClassifier(6) Model Fitting # Copy python model.fit(x, y) Model Predict # Copy python model.predict([[9, 2.5], [3.5, 11]]) # 각각의 분류 표시 Plot Model # Data Points # Copy python plt.figure(figsize=(10,5)) plt.scatter(x[:, 0], x[:, 1]) plt.title(\u0026#34;Data points\u0026#34;) plt.show() Plot KNN # Copy python x_min, x_max = x[:,0].min() - 1, x[:,0].max() + 1 y_min, y_max = x[:,1].min() - 1, x[:,1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) Copy python cmap_light = ListedColormap([\u0026#39;#FFAAAA\u0026#39;, \u0026#39;#AAFFAA\u0026#39;,\u0026#39;#00AAFF\u0026#39;]) cmap_bold = ListedColormap([\u0026#39;#FF0000\u0026#39;, \u0026#39;#00FF00\u0026#39;,\u0026#39;#0000FF\u0026#39;]) plt.figure(figsize=(10,5)) plt.pcolormesh(xx, yy, Z, cmap=cmap_light) plt.scatter(x[:, 0], x[:, 1], c=y, cmap=cmap_bold, edgecolors=\u0026#39;gray\u0026#39;) plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.title(\u0026#34;3-Class classification (k = 1)\u0026#34;) plt.show() K를 높일수록 결정 경계가 부드러워짐 "},{"id":88,"href":"/blog/aischool-06-03-gradient-boosting/","title":"[AI SCHOOL 5기] 머신 러닝 실습 - Gradient Boosting","section":"Posts","content":"XG Boost # Extreme Gradient Boosting 대용량 분산 처리를 위한 Gradient Boosting 라이브러리 Decision Tree(의사결정나무) 에 Boosting 기법을 적용한 알고리즘 AdaBoost는 학습 성능은 좋으나, 모델의 학습 시간이 오래 걸리는 단점 병렬 처리 기법을 적용하여 Gradient Boost보다 학습 속도를 끌어올림 Hyper-Parameter가 너무 많기 때문에 권장 세팅 사용 @ http://j.mp/2PukeTS Decision Tree # 이해하기 쉽고 해석도 용이함 입력 데이터의 작은 변동에도 Tree의 구성이 크게 달라짐 과적합이 쉽게 발생 (중간에 멈추지 않으면 Leaf 노드에 하나의 데이터만 남게 됨) 의사결정나무의 문제를 해결하기 위해 Boosting 기법 활용 ex) 테니스를 쳤던 과거 데이터를 보고 날씨 정보를 이용해 의사결정 AdaBoost # Adaptive Boosting 데이터를 바탕으로 여러 weak learner 들을 반복적으로 생성 앞선 learner가 잘못 예측한 데이터에 가중치를 부여하고 학습 최종적으로 만들어진 strong learner를 이용하여 실제 예측 진행 에러를 최소화하는 weight를 매기기 위해 경사 하강법 사용 ex) Regression: 평균/가중평균, Classification: 투표 XG Boost References # NGBoost Explained (Comparison to LightGBM and XGBoost) Gradient Boosting Interactive Playground Gradient Boosting explained Comparison for hyperparams of XGBoost \u0026amp; LightGBM XGBoost Parameters XG Boost 하이퍼 파라미터 상세 설명 Complete Guide to Parameter Tuning in XGBoost (with python codes) Microsoft EBM (Explainable Boosting Machine) 정형데이터를 위한 인공신경망 모델, TabNet Ensemble # 주어진 데이터를 이용하여 여러 개의 서로 다른 예측 모형을 생성한 후,\n예측 모형의 예측 결과를 종합하여 하나의 최종 예측결과를 도출해내는 방법 다양한 모델이 문제 공간의 다른 측면을 보면서 각기 다른 방식으로 오점이 있다고 가정\n(모델 별로 약점을 보완) Boosting # weak learner들을 strong learner로 변환시키는 알고리즘\n(약한 학습기를 여러개 사용해서 하나의 강건한 학습기를 만들어내는 것) 의사결정나무 모델을 합리적인 수준(60~70% 성능)에서 여러 종류 생성 ex) AdaBoost Gradient Boosting # 경사 하강법을 사용해서 AdaBoost보다 성능을 개선한 Boosting 기법 AdaBoost는 높은 가중치를 가진 지점이 존재하게 되면 성능이 크게 떨어지는 단점\n(높은 가중치를 가진 지점과 가까운 다른 데이터들이 잘못 분류될 가능성이 높음) Gradient Boosting 기법은 이전 모델에 종속적이기 때문에 병렬 처리가 불가능 Bagging # Bootstrap Aggregating 가중치를 매기지 않고 각각의 모델이 서로 독립적 x 데이터 열들의 서로 다른 조합으로 독립적인 모델을 여러 종류 생성 ex) Random Forest Random Forest # 각 모델은 서로 다른 샘플 데이터셋을 활용 (Bootstrap Sampling \u0026amp; Bagging) 각 데이터셋은 복원추출을 통해 원래 데이터셋 만큼의 크기로 샘플링 (누락 \u0026amp; 중복 발생) 위 서로 다른 샘플로 각 모델 생성 시, 각 노드 지점마다 x열 n개 중 랜덤하게 m개 중 분기 선택 Classification에서는 root n을 m으로 사용 Regression에서는 n/3을 m으로 사용 @ https://j.mp/3rZ05bN \u0026amp; https://j.mp/3GJ7QqH Learning Process # @ https://j.mp/3jRJH6n Import Libraries # Copy python import numpy as np import matplotlib.pyplot as plt from sklearn import ensemble from sklearn import datasets from sklearn.utils import shuffle from sklearn.metrics import mean_squared_error Load Data # Copy python boston = datasets.load_boston() X, y = shuffle(boston.data, boston.target, random_state=13) X = X.astype(np.float32) offset = int(X.shape[0] * 0.9) X_train, y_train = X[:offset], y[:offset] X_test, y_test = X[offset:], y[offset:] load 부분에서 다른 데이터도 사용 가능 boston.data: x data boston.target: y data int(X.shape[0] * 0.9): 전체 행 중 90 퍼센트 의미 boston.feature_names: 데이터셋에서 열 이름 boston.DESCR: 데이터에 대한 상세 설명 Model Fitting # Copy python params = {\u0026#39;n_estimators\u0026#39;: 500, \u0026#39;max_depth\u0026#39;: 4, \u0026#39;min_samples_split\u0026#39;: 2, \u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;loss\u0026#39;: \u0026#39;ls\u0026#39;} clf = ensemble.GradientBoostingRegressor(**params) clf.fit(X_train, y_train) mse = mean_squared_error(y_test, clf.predict(X_test)) print(\u0026#34;MSE: %.4f\u0026#34; % mse) **params: 딕셔너리를 파라미터로 변환 @ https://j.mp/2IPuJzY clf: Classifier Plot Deviance # Copy python test_score = np.zeros((params[\u0026#39;n_estimators\u0026#39;],), dtype=np.float64) for i, y_pred in enumerate(clf.staged_predict(X_test)): test_score[i] = clf.loss_(y_test, y_pred) plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\u0026#39;Deviance\u0026#39;) plt.plot(np.arange(params[\u0026#39;n_estimators\u0026#39;]) + 1, clf.train_score_, \u0026#39;b-\u0026#39;, label=\u0026#39;Training Set Deviance\u0026#39;) plt.plot(np.arange(params[\u0026#39;n_estimators\u0026#39;]) + 1, test_score, \u0026#39;r-\u0026#39;, label=\u0026#39;Test Set Deviance\u0026#39;) plt.legend(loc=\u0026#39;upper right\u0026#39;) plt.xlabel(\u0026#39;Boosting Iterations\u0026#39;) plt.ylabel(\u0026#39;Deviance\u0026#39;) Deviance: 편차값, 에러 n_estimators: 의사결정나무 모델을 몇 개 만들었는지 Test Data에 대한 에러 라인이 튕겨올라가는 지점이 Overfitting Plot Feature Importance # Copy python feature_importance = clf.feature_importances_ feature_importance = 100.0 * (feature_importance / feature_importance.max()) sorted_idx = np.argsort(feature_importance) pos = np.arange(sorted_idx.shape[0]) + .5 plt.subplot(1, 2, 2) plt.barh(pos, feature_importance[sorted_idx], align=\u0026#39;center\u0026#39;) plt.yticks(pos, boston.feature_names[sorted_idx]) plt.xlabel(\u0026#39;Relative Importance\u0026#39;) plt.title(\u0026#39;Variable Importance\u0026#39;) plt.show() feature_importances_: 트리 기반 모델이 가지고 있는 변수, 각각의 열마다의 중요도 Feature Importance는 상대적인 중요도이기 때문에 합계가 1 LSTAT(인구 중 하위 계층 비율)이 집값을 예측할 때 가장 중요함 Feature Importance References # Feature Importance Analysis (LIME) Permutation importance 한글 설명 Permutation importance with Pipeline "},{"id":89,"href":"/blog/aischool-06-02-logistic-regression/","title":"[AI SCHOOL 5기] 머신 러닝 실습 - 로지스틱 회귀","section":"Posts","content":"Logistic Regression # 이진 분류(0 또는 1) 문제를 해결하기 위한 모델 다항 로지스틱 회귀(k-class), 서수 로지스틱 회귀(k-class \u0026amp; ordinal)도 존재 Sigmoid Function을 이용하여 입력값이 양성 클래스에 속할 확률을 계산 로지스틱 회귀를 MSE 식에 넣으면 지수 함정의 특징 때문에 함정이 많은 그래프가 나옴 분류를 위한 Cost Function인 Cross-Entropy 활용 성능 지표로는 Cross-Entropy 외에 Accuracy 등을 같이 사용 ex) 스팸 메일 분류, 질병 양성/음성 분류 등 양성/음성 분류 모델 # 선형 모델은 새로운 데이터가 들어오면 양성/음성 판단 기준이 크게 바뀜 모델을 지수 함수인 Sigmoid Function으로 변경 Sigmoid Function # θ 값에 따라 기울기나 x축의 위치가 바뀌는 지수 함수 y축을 이동하는 선형 함수와 다르게 x축을 이동 y가 0.5가 되는 지점을 기준으로 대칭되는 형태 y값은 조건부 확률로 해석 (X가 있을 때 양성 클래스일 확률값) Cross-Entropy Function # 예측값의 분포와 실제값의 분포를 비교하여 그 차이를 Cost로 결정 인공신경망에 각 행을 열단위로 쪼개 입력으로 넣었을 때 마지막에 카테고리 개수만큼의 수를 반환 Softmax Function을 통과시키면 개수는 그대로지만 합쳤을 때 1이되는 숫자로 변경 인공신경망 결과로 뱉어낸 카테고리별 확률을 정답과 비교해 Cross-Entropy 계산 One-Hot Encoding이 적용된 정답을 One-Hot Label이라 부름 정답 확률이 높고 오답 확률이 낮은 모델이 나은 모델 Cross-Entropy는 하나의 분포를 다른 분포로 옮겨내는 거리라고도 불림 Softmax Algorithm # $$S(y_i)=\\frac{e^{y_i}}{\\Sigma_j{e^{y_i}}}$$\n다중 클래스 분류 문제를 위한 알고리즘 모델의 결과에 해당하는 점수를 각 클래스에 소속될 확률에 해당하는 값들의 벡터로 변환\n(클래스 개수만큼의 숫자를 입력 받으면 합이 1이 되는 확률값으로 변환) ROC Curve # Receiver Operating Characteristic Curve 얼마나 신호에 민감하게 반응할지를 그려낸 곡선 Threshold를 끌어올리거나 끌어내리는 과정에서 발생 Threshold를 끌어롤리면 양성 기준이 엄격해지고 끌어내리면 양성 기준이 너그러워짐 모델이 엄격할 때는 양성율이 낮지만 이를 억지로 끌어올리는 과정에서 위양성율이 발생 선이 직각일수록 이상적인 모델, 반면 모델의 성능이 안좋아 실수가 많으면 선이 아래로 내려옴 Confusion Matrix # 분류 모델이 학습 결과를 뱉어낸 것을 바탕으로 만든 표 모델이 얼마나 혼동하고 있는지를 나타냄 Accuracy(정확성): (참긍정 + 참부정) / 총 예시 수 Recall(재현율): 정답에서 참인 것을 골라냄, TP / (FN + TP) Precision(정밀도): 분류한 것들 중 정답을 골라냄, TP / (TP + FP) 스팸 메일 분류의 경우 정밀도를 높일 필요가 있음 F1-Score: Recall과 Precision의 조화평균, 2RP / (R + P) F-beta score: Recall과 Precision에 가중치 부여 AUC # Area Under the ROC Curve ROC 커브 밑에 있는 영역의 크기 0.5에서 1 사이의 값이 나오며, 0.7 후반을 쓸만한 모델로 판단 Learning Process # Load Data # pd.read_excel()로 엑셀 데이터 불러오기 엑셀 데이터를 np.array() 안에 넣어 Numpy Array 형태로 변경 Select Feature # 떨어진 열들을 꺼낼 때 data[:, (5, 12)] 형식으로 열을 꺼냄 Training \u0026amp; Test Set # Copy python from sklearn import model_selection x_train, x_test, y_train, y_test = \\ model_selection.train_test_split(boston_X, boston_Y, test_size=0.3, random_state=0) Create Model # Copy python from sklearn import linear_model model = linear_model.LogisticRegression() Model Fitting # Copy python model.fit(x_train, y_train) Model Predict # Copy python model.predict(x_train) Accuracy # Copy python from sklearn.metrics import accuracy_score print(accuracy_score(model.predict(x_test), y_test)) 양성/음성 확률 # Copy python model.predict_proba(x_test) Plot ROC Curve # Copy python from sklearn.metrics import roc_curve, auc fpr, tpr, _ = roc_curve(y_true=y_test, y_score=pred_test[:,1]) roc_auc = auc(fpr, tpr) y_true가 정답, y_score가 양성이 나올 확률 fpr: ROC 커브를 그리기 위한 x좌표 tpr: ROC 커브를 그리기 위한 y좌표 Copy python plt.figure(figsize=(10, 10)) plt.plot(fpr, tpr, color=\u0026#39;darkorange\u0026#39;, lw=2, label=\u0026#39;ROC curve (area = %0.2f)\u0026#39; % roc_auc) plt.plot([0, 1], [0, 1], color=\u0026#39;navy\u0026#39;, lw=2, linestyle=\u0026#39;--\u0026#39;) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(\u0026#39;False Positive Rate\u0026#39;) plt.ylabel(\u0026#39;True Positive Rate\u0026#39;) plt.legend(loc=\u0026#34;lower right\u0026#34;) plt.title(\u0026#34;ROC curve\u0026#34;) plt.show() "},{"id":90,"href":"/blog/aischool-06-00-machine-learning/","title":"[AI SCHOOL 5기] 머신 러닝","section":"Posts","content":"인공지능 # Intelligent Agents를 만드는 것 주변 환경들을 인식하고 원하는 행동을 취하여 목표를 성취하는 것 Artificial Narrow Intelligence # 제한된 기능만 수행할 수 있는 인공지능 weak AI Artificial General Intelligence # 사람만큼 다양한 분야에서 기능을 수행할 수 있는 인공지능 strong AI Artificial Super Intelligence # 모든 분야에서 사람보다 뛰어난 인공지능 모델 # 데이터를 가장 잘 설명할 수 있는 함수 (y = ax + b) 모델에서 θ는 Parameter(가중치, Weight) 의미 모델에서 h(x)는 Hypotheses(가설) 의미 모델에서 b는 Bias(편향, 보정치) 의미 머신러닝 # 어떠한 과제를 해결하는 과정에서 특정한 평가 기준을 바탕으로 학습의 경험을 쌓는 프로그램 머신러닝 분류 # Supervised # 입력값에 대한 정답을 예측하기 위해 학습 데이터와 정답이 같이 존재 회귀(Regression): 결과가 실수 영역 전체에서 나타남 분류(Classification): 결과가 특정 분류에 해당하는 불연속값으로 나타남 ex) 주식 가격 예측, 이미지 인식 등 Unsupervised # 입력값 속에 숨어있는 규칙성을 찾기 위해 학습 정답이 없는 데이터를 주고 비슷한 집단을 분류 ex) 고객군 분류, 장바구니 분석(Association Rule) 등 Reinforcement # Trial \u0026amp; Error를 통한 학습 최종적으로 얻게 될 기대 보상을 최대화하기 위한 행동 선택 정책 학습 각 상태에 대해 결정한 행동을 통해 환경으로부터 받는 보상을 학습 ex) 로봇 제어, 공정 최적화 등 Automated ML # 어떤 모델(함수, 알고리즘)을 써야할지를 컴퓨터가 알아서 정하게 함 인공신경망 레이어의 범위, 후보 등을 정해놓고 그 안에서 가장 좋은 조합을 찾음 ex) AutoML Tables (행의 수가 1000건이 넘어야하는 제약) 학습 # 데이터를 가장 잘 설명하는 방법을 찾는 과정 데이터에 맞는 모델을 찾는 과정 (= Model Fitting) 실제 정답과 예측 결과 사이의 오차(Loss, Cost, Error)를 줄여나가는 최적화 과정 학습 과정 # 초기 모델에 데이터를 입력 결과를 평가 (예측/분류의 정확도 등) 결과를 개선하기 위해 모델을 수정 (모델 내부 Parameter 수정 등) Model\u0026rsquo;s Capacity # 2번 모델은 3번 모델보다 오차가 크지만 새로운 데이터가 생겼을 때 비슷하게 예측 가능 3번 모델은 오차가 가장 적지만 새로운 데이터가 생겼을 때 오차가 매우 커질 수 있음 3번 모델과 같은 Overfitting(과적합)이 발생하기 전에 학습을 멈춤 Cross Validation # 새로운 데이터들에 대해서도 좋은 결과를 내게 하기 위해 데이터를 3개 그룹으로 나눠 학습 60%의 Training Data로 모델을 학습 20%의 Validation Data로 모델을 최적화/선택 20%의 Test Data로 모델을 평가 데이터를 분리하는 비율은 모델에 따라 달라짐 K-Fold Cross Validation # 후보 모델 간 비교 및 선택을 위한 알고리즘 Training Data를 K 등분하고 그 중 하나를 Validation Data로 설정 K 값은 자체적으로 결정하며 보통 10-Fold 사용 (시간이 없으면 5-Fold) 머신러닝에서 K는 주로 사용자가 결정하는 상수 Stratified: 층화 표집 방법, 데이터의 분류 별 비율이 다르면 K-Fold 조각 안에서 비율을 유지시킴 10-Fold 학습 과정 # 데이터를 80%의 Training Data와 20%의 Test Data로 나누고 Training Data를 10등분\nPhase 1. Training Data(0:9) + Validation Data(TD 9)를 사용해 점수 측정\nPhase 2. Training Data(0:8,9) + Validation Data(TD 8)를 사용해 점수 측정\n. . .\nPhase 10. Training Data(1:10) + Validation Data(TD 0)를 사용해 점수 측정\n마지막으로 Training Data 전체를 학습하고 Test Data로 검증\nScikit-learn # 파이썬으로 전통적인 머신 러닝 알고리즘들을 구현한 오픈 소스 라이브러리 다른 라이브러리들과의 호환성이 좋음 (Numpy, Pandas, Matplotlib 등) 머신러닝 학습 과정 # 데이터셋 불러오기 Copy python sklearn.load_[DATA]() Train/Test set으로 데이터 나눔 Copy python sklearn.model_selection.train_test_split(X, Y, test_size) 모델 객체 생성 Copy python sklearn.linear_model.LinearRegression() sklearn.linear_model.LogisticRegression() sklearn.neighbors.KNeighborsClassifier(n_neighbors) sklearn.cluster.KMeans(n_clusters) sklearn.decomposition.PCA(n_components) sklearn.svm.SVC(kernel, C, gamma) 모델 학습 시키기 Copy python model.fit(train_X, train_Y) 모델로 새로운 데이터 예측 (Scaler를 적용했으면 새로운 데이터에도 적용) Copy python model.predict(test_X) model.predict_proba(test_X) sklearn.metrics.mean_squared_error(predicted_Y, test_Y) sklearn.metrics.accuracy_score(predicted_Y, test_Y) sklearn.metrics.precision_score sklearn.metrics.recall_score 머신러닝 분류 기준 # Choosing the right estimator Feature Normalization # Numeric Column # Min-Max Algorithm Standardization Categorical Column # One-Hot Encoding, One-Hot Vector 열과 목록 개수만큼 0으로 채워진 행렬을 만들고 값이 해당하는 위치에 1을 표시 범주형 데이터(카테고리)의 숫자가 크고 작음에 관계없이 카테고리의 위치값만을 판단 Supervised Algorithm # Linear Regression (선형 회귀) # 종속 변수 y와 독립 변수 x 사이의 선형 상관 관계를 모델링하는 회귀분석 기법 Logistic Regression (로지스틱 회귀) # 이진 분류(0 또는 1) 문제를 해결하기 위한 모델 ex) 스팸 메일 분류, 질병 양성/음성 분류 등 Gradient Boosting Regression (XG Boost) # 대용량 분산 처리를 위한 Gradient Boosting 라이브러리 ex) 테니스를 쳤던 과거 데이터를 보고 날씨 정보를 이용해 의사결정 K-Nearest Neightbor Algorithm (KNN) # 기존의 가까운 이웃 데이터를 살펴 새로운 데이터를 분류하는 알고리즘 Kernel Support Vector Machine (KSVM) # 데이터를 분류하는 Margin을 최대화하는 결정 경계를 찾는 기법 Unsupervised Algorithm # K-Means Algorithm # 데이터를 K개의 클러스터로 분류하는 알고리즘 Principal Component Analysis # 차원 축소를 통해 최소 차원의 정보로 원래 차원의 정보를 모사하는 알고리즘 Model Saving \u0026amp; Loading # Model Saving # Copy python import joblib joblib.dump(model, \u0026#39;model_v1.pkl\u0026#39;, compress=True) Model Loading # Copy python import joblib model_loaded = joblib.load(\u0026#39;model_v1.pkl\u0026#39;) "},{"id":91,"href":"/blog/aischool-06-01-linear-regression/","title":"[AI SCHOOL 5기] 머신 러닝 실습 - 선형 회귀","section":"Posts","content":"Linear Regression # 종속 변수 y와 독립 변수 x 사이의 선형 상관 관계를 모델링하는 회귀분석 기법 정답이 있는 데이터의 추세를 잘 설명하는 선형 함수를 찾아 x에 대한 y를 예측 Linear Combination (선형 결합): 더하기와 곱하기로만 이루어진 식 단순 회귀분석: 1개의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때 다중 회귀분석: 2개 이상의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때 선형 회귀는 가장 적합한 θ들의 집합을 찾는 것이 목표 Cost Function # 예측 값과 실제 값의 차이를 기반으로 모델의 성능(정확도)을 판단하기 위한 함수 Objective (MIN or MAX) 함수 안에 Cost Function이 존재 선형 회귀에서는 Mean Squre(d) Error Function (평균 제곱 오차 함수) 활용 MSE(Cost)가 최소가 되는 θ(a \u0026amp; b)를 찾아야하며,\n이를 위한 최적화 기법으로 Gradient Descent Algorithm (경사하강법) 활용 Mean Squre Error Function # 회귀 분석을 위한 Cost Function y축 방향의 차이를 에러로 판단하는데 전체 에러를 단순하게 합칠 경우\n양 에러와 음 에러가 상쇄되어 올바른 판단을 할 수 없음 부호를 제거하기 위해 모든 에러에 제곱을 취하고 그 평균을 구한 것이 MSE MSE(Cost)가 0에 가까울수록 에러가 적다고 판단 값에 제곱을 취하기 때문에 이상치가 있으면 영향을 많이 받아 이상치를 찾아내기 쉬움 제곱 대신에 절댓값을 사용하는 MAE Function은 이상치에 영향을 덜 받음 Gradient Descent Algorithm # Cost Function의 값을 최소로 만드는 θ를 찾아나가는 방법 Cost Function의 Gradient(기울기)에 상수를 곱한 값을 빼서 θ를 조정 어느 방향으로 θ를 움직이면 Cost가 작아지는지 현재 위치에서 함수를 미분하여 판단 변수(θ)를 움직이면서 전체 Cost 값이 변하지 않거나 매우 느리게 변할 때까지 접근 MSE를 미분했을 때 0이 나오는 지점을 찾아도 되지만, 빅데이터에서 x 데이터 역행렬이 오래걸림 그래프 중간에 함정처럼 페인 부분을 Local Minima라 부름 (목표점은 Global Minima) 가던 방향에서 조금 더 가는 발전된 Gradient Descent 기법을 통해 함정을 빠져나감 Local Minima도 Global Minima와 비슷하게 떨어지기 때문에 에러가 적음 $$\\text{repeat until convergence}\\ { \\theta_j:=\\theta_j-{\\alpha}\\frac{\\delta}{\\delta\\theta_j}J(\\theta_0,\\theta_1)\\quad(\\text{for}j=0\\text{and}j=1) }$$\n계산식에서 J(θ0, θ1)는 MSE를 의미하며 이를 미분한 것에 ⍺를 곱함 ⍺는 한 번 이동하는 길이를 결정하는 상수 (Step Size, 보폭, Learning Rate) ⍺와 같이 사람이 결정해야 하는 값을 Hyper-Parameter라 부름 Hyper-Parameter # 사람이 결정하는 파라미터, 모델 클래스 생성 시 집어 넣는 파라미터 모델을 선택하는 것, 인공신경망의 층을 몇개로 구성할 것인지 등 Hyper-Parameter를 설정하는 것을 Model Tuning, Hyper-Parmas Tuning,\n또는 Hyper-Parameter Optimizator(HPO)라 부름 AutoML # Hyper-Parameter Tuning을 컴퓨터에게 맡김 Automated FE (Feature Engineering): 결측치 채움, x열(feature) 생성 Automated MS (Model Selection) Automated HPO (Hyper Parameter Optimization) Learning Process # Load Data # pd.read_excel()로 엑셀 데이터 불러오기 엑셀 데이터를 np.array() 안에 넣어 Numpy Array 형태로 변경 Select Feature # Numpy Array는 2차원 행렬이어야 하기 때문에 data[:, 1:2] 형식으로 열을 꺼냄 (data[:, 1]는 1차원 행렬을 반환) Training \u0026amp; Test Set # Copy python from sklearn import model_selection x_train, x_test, y_train, y_test = \\ model_selection.train_test_split(boston_X, boston_Y, test_size=0.3, random_state=0) Create Model # Copy python from sklearn import linear_model model = linear_model.LinearRegression() Model Fitting # Copy python model.fit(x_train, y_train) model.coef_: a에 해당하는 θ 값 model.intercept_: b에 해당하는 θ 값 (y 절편) Model Predict # Copy python model.predict(x_train) MSE # Copy python print(np.mean((model.predict(x_train) - y_train) ** 2)) Copy python from sklearn.metrics import mean_squared_error print(mean_squared_error(model.predict(x_train), y_train)) Training Data와 Test Data의 MSE 차이를 원본 데이터와 함께 비교하여 Overfitting 판단 선형회귀는 성능을 기대하기 어려움 Plot Linear Model # Copy python plt.figure(figsize=(10, 10)) plt.scatter(x_test, y_test, color=\u0026#34;black\u0026#34;) # Test data plt.scatter(x_train, y_train, color=\u0026#34;red\u0026#34;, s=1) # Train data plt.plot(x_test, model.predict(x_test), color=\u0026#34;blue\u0026#34;, linewidth=3) plt.show() "},{"id":92,"href":"/blog/aischool-05-03-merge/","title":"[AI SCHOOL 5기] SQL 프로그래밍 실습 - Merge","section":"Posts","content":"INNER JOIN # Copy sql SELECT l.Title, r.Name FROM albums AS l INNER JOIN artists AS r ON r.ArtistId = l.ArtistId; Copy sql SELECT Title, Name FROM albums INNER JOIN artists USING(ArtistId); LEFT JOIN # Copy sql SELECT Name, Title FROM artists LEFT JOIN albums ON artists.ArtistId = albums.ArtistId ORDER BY Name; SELF JOIN # Copy sql SELECT m.firstname || \u0026#39; \u0026#39; || m.lastname AS \u0026#39;Manager\u0026#39;, e.firstname || \u0026#39; \u0026#39; || e.lastname AS \u0026#39;Receives reports from\u0026#39; FROM employees e INNER JOIN employees m ON m.employeeid = e.reportsto ORDER BY manager; \u0026lsquo;A 테이블\u0026rsquo;과 A 테이블의 복사본인 \u0026lsquo;B 테이블\u0026rsquo;을 합치기 Grouping Data # Copy sql SELECT albumid, COUNT(trackid) FROM tracks GROUP BY albumid ORDER BY COUNT(trackid) DESC; Copy sql SELECT albumid, COUNT(trackid) FROM tracks GROUP BY albumid HAVING albumid = 1; Copy sql SELECT albumid, COUNT(trackid) FROM tracks WHERE COUNT(trackid) BETWEEN 18 AND 20 GROUP BY albumid; 에러발생: WHERE문에는 집계함수 사용 불가 WHERE가 집계함수보다 우선적으로 실행되기 때문 Copy sql SELECT tracks.albumid, title, MIN(tracks.milliseconds), MAX(tracks.milliseconds), ROUND(AVG(tracks.milliseconds), 2) FROM tracks INNER JOIN albums ON albums.albumid = tracks.albumid GROUP BY tracks.albumid; ROUND는 n번째 자리까지 나타나도록 반올림 Subquery # Copy sql SELECT AVG(SUM(bytes)) FROM tracks GROUP BY albumid; SELECT 문에서 집계함수의 결과 값에 바로 중첩하여 집계함수 적용 불가 Copy sql SELECT AVG(SIZE) FROM (SELECT SUM(bytes) AS SIZE FROM tracks GROUP BY albumid); "},{"id":93,"href":"/blog/aischool-05-02-sql-crud/","title":"[AI SCHOOL 5기] SQL 프로그래밍 실습 - SQL CRUD","section":"Posts","content":"SELECT # Copy sql SELECT 10 / 5, 2 * 4; Copy sql SELECT trackid, name FROM tracks; Copy sql SELECT * FROM tracks; INSERT # Copy sql INSERT INTO artists (name) VALUES(\u0026#39;Bud Powell\u0026#39;); Copy python script = \u0026#34;\u0026#34;\u0026#34; INSERT INTO artists (name) VALUES (\u0026#34;?\u0026#34;); \u0026#34;\u0026#34;\u0026#34; data = [ (\u0026#34;Buddy Rich\u0026#34;), (\u0026#34;Candido\u0026#34;), (\u0026#34;Charlie Byrd\u0026#34;) ] cur.executemany(script, data) Copy sql SELECT ArtistId, Name FROM Artists ORDER BY ArtistId DESC; UPDATE # Copy sql UPDATE employees SET lastname = \u0026#39;Smith\u0026#39; WHERE employeeid = 3; Copy sql UPDATE employees SET city = \u0026#39;Toronto\u0026#39;, state = \u0026#39;ON\u0026#39;, postalcode = \u0026#39;M5P 2N7\u0026#39; WHERE employeeid = 4; Copy sql UPDATE employees SET email = UPPER(firstname || \u0026#34;.\u0026#34; || lastname || \u0026#34;@corp.co.kr\u0026#34;); Sorting # Copy sql SELECT TrackId, Name, Composer FROM tracks ORDER BY Composer; NULL Data인 None은 SQLite3에서 가장 작은 값으로 인식 Filtering # DISTINCT # Copy sql SELECT DISTINCT city FROM customers; NULL을 포함한 중복값을 하나만 남기고 제외 Copy sql SELECT DISTINCT city, country FROM customers; 2개 열의 값이 모두 동일한 행들을 제외 WHERE # Copy sql SELECT name, milliseconds, bytes, albumid FROM tracks WHERE (albumid = 10) AND (milliseconds \u0026gt; 250000); WHERE \u0026amp; LIKE # Copy sql SELECT trackid, name FROM tracks WHERE name LIKE \u0026#39;Wild%\u0026#39;; WHERE \u0026amp; IN # Copy sql SELECT TrackId, Name, MediaTypeId FROM Tracks WHERE MediaTypeId IN (1, 2) WHERE \u0026amp; LIMIT/OFFSET # Copy sql SELECT trackId, name FROM tracks LIMIT 10 OFFSET 7; LIMIT: 불러오는 값의 수 OFFSET: OFFSET에 해당하는 수만큼 떼어내 그 이후의 데이터 불러옴 WHERE \u0026amp; BETWEEN # Copy sql SELECT InvoiceId, BillingAddress, Total FROM invoices WHERE Total BETWEEN 14.91 AND 18.86 ORDER BY Total; WHERE \u0026amp; IS NULL # Copy sql SELECT Name, Composer FROM tracks WHERE Composer IS NULL ORDER BY Name; "},{"id":94,"href":"/blog/aischool-05-00-sql-programming/","title":"[AI SCHOOL 5기] SQL 프로그래밍","section":"Posts","content":"DBMS # DataBase Management System 하드웨어에 저장된 데이터베이스를 관리해주는 소프트웨어 관계형 데이터베이스(RDBMS)가 주로 사용 Oracle, MySQL(MariaDB), SQLite, MS SQL, PstgreSQL 데이터 모델링 # 현실 세계 E-R 다이어그램 (개념 스키마) Relation 모델 (논리적 스키마) 물리적인 SQL 코드 (데이터베이스 스키마) 개념적 데이터 모델링 # 현실 세계로부터 개체를 추출, 개체들의 관계를 정의, E-R 다이어그램 생성 개체(Entity): 회원, 제품 등 저장할 가치가 있는 데이터를 포함한 개체 속성(Attribute): 이름, 이메일 등 의미 있는 데이터의 가장 작은 논리적 단위 관계(Relationship): 구매 등 개체와 개체 사이의 연관성 및 개체 집합 간 대응 관계 논리적 데이터 모델링 # E-R 다이어그램을 바탕으로 논리적인 구조를 Relation 모델로 표현 릴레이션(Relation): 개체에 대한 데이터를 2차원 테이블 구조로 표현한 것 속성(Attribute): 열, 필드 튜플(Tuble): 행, 레코드, 인스턴스 차수(Degree): 릴레이션 내 속성(Column)의 총 개수 카디널리티(Cardinality): 릴레이션 내 튜플(Row)의 총 개수 물리적 데이터 모델링 # Relation 모델을 물리 저장 장치에 저장할 수 있는 물리적 구조로 구현 SQL # Structured Query Language RDBMS에서 데이터를 관리 및 처리하기 위해 만들어진 언어 DDL(Data Definition Language): CREATE, ALTER, DROP DML(Data Manipulation Language): SELECT, INSERT, UPDATE, DELETE DCL(Data Control Language): GRANT, REVOKE NoSQL # 관계형 모델을 사용하지 않음, 명시적인 스키마가 없음 대용량 데이터 분산 저장에 특화 Kye-Value, Document, Wide Column, Graph 등 "},{"id":95,"href":"/blog/aischool-05-01-sqlite3/","title":"[AI SCHOOL 5기] SQL 프로그래밍 실습 - SQLite3","section":"Posts","content":"Connect SQLite3 # Copy python import sqlite3 dbpath = \u0026#34;maindb.db\u0026#34; conn = sqlite3.connect(dbpath) cur = conn.cursor() connnect(): DBMS와 연결 conn.commit(): 현재 변경사항 저장 conn.rollback(): 마지막 commit 시점으로 되돌리기 cursor(): DB에서 SQL문을 실행하는 객체 Execute Scripts # Datatypes # NULL: 결측치 INTEGER (or INT): 정수 (양수 또는 음수), int 값 REAL: 실수, float 값 TEXT (or VARCHAR): 텍스트, string 값 BLOB: 모든 종류의 파일을 저장하는 바이너리 객체 Scripts # DROP TABLE IF EXISTS: 테이블이 이미 있으면 제거 CREATE TABLE: 테이블 생성 AUTOINCREMENT: 값을 따로 입력하지 않으면 자동 증가 숫자 부여 NOT NULL: 빈 값이 저장되는 것을 허용하지 않음 INSERT INTO TABLE(FIELD, \u0026hellip;) VALUES(VALUE, \u0026hellip;):\n테이블에 데이터 추가, 전체 필드에 값 추가 시 필드명 생략 가능 --: 한 줄 주석, /* ... */: 여러 줄 주석 Excecute # conn.executescript(): 스크립트 구문 실행 cur.executemany(): 많은 데이터를 한번에 INSERT/UPDATE/DELETE\n(\u0026quot;INSERT INTO ... VALUES(?, ?, ?, ?, ?);\u0026quot;, date) cur.execute(): 하나의 SQL문 실행 cur.fetchall(): SQL문 실행 결과를 모두 반환 (튜플 형태) cur.description: 테이블 정보 conn.close(): DB 연결 해제 To Dataframe # pd.read_sql_query(query, conn) CREATE Table # Copy sql CREATE TABLE devices ( name TEXT NOT NULL, model TEXT NOT NULL, Serial INTEGER NOT NULL UNIQUE Copy sql CREATE TABLE contact_groups( contact_id INTEGER, group_id INTEGER, PRIMARY KEY (contact_id, group_id), FOREIGN KEY (contact_id) REFERENCES contacts(contact_id) ON DELETE CASCADE, FOREIGN KEY (group_id) REFERENCES groups(group_id) ON DELETE CASCADE ); CASCADE: css cascade와 동일 ALTER Table # Copy sql ALTER TABLE devices RENAME TO equipment; Copy sql ALTER TABLE equipment ADD COLUMN location text; Copy sql ALTER TABLE equipment RENAME COLUMN location TO loc; DROP Table # Copy sql DROP TABLE equipment ; Pandas로 삭제된 테이블 요청 시 no such table 에러 발생 DB 내 테이블 목록/구조 확인 # Copy sql SELECT name FROM sqlite_master WHERE type =\u0026#39;table\u0026#39; AND name NOT LIKE \u0026#39;sqlite_%\u0026#39;; sqlite_master는 기본적으로 생성되는 테이블 sqlite_master 테이블에서 생성된 모든 테이블 목록/구조 확인 가능 "},{"id":96,"href":"/blog/aischool-04-04-ab-test/","title":"[AI SCHOOL 5기] 통계분석 실습 - A/B Test","section":"Posts","content":"마케팅 비용 분석 # 매월 유튜브에 광고 비용을 지출하여 신규 유저(구매 고객 or 회원가입 고객)를 획득 월별로 10,000원 단위의 유튜브 광고 비용과 해당 월에 신규로 획득된 유저 수가 측정되었다고 가정 비교 데이터 # 단순 CAC 계산 # CAC(Customer Acquisition Cost, 신규고객 유치 비용) @ https://j.mp/35O5NRe Copy python cac = ad_df[\u0026#39;Marketing_Costs\u0026#39;].sum() / ad_df[\u0026#39;User_Acquired\u0026#39;].sum() print(cac * 10000) # Output 446원 위의 금액에 추가로 획득하기를 원하는 유저 수를 곱한 금액을\n유튜브 광고 비용으로 쓰면 그만큼 유저가 늘어날까?\n== 위의 금액 만큼 유튜브 광고에 쓰면 정말로 유저가 1명 늘어날까?\n피어슨 상관계수 # Copy python stats.pearsonr(ad_df[\u0026#39;Marketing_Costs\u0026#39;], ad_df[\u0026#39;User_Acquired\u0026#39;]) # Output 피어슨 상관계수 : 0.8035775069546849 p-value : 0.0016386012345537505 p-value가 0.0016(\u0026lt;0.05)이므로,\n월별 유튜브 광고 비용과 신규 유저 수가 통계적으로 유의미한 상관관계가 없다 월별 유튜브 광고 비용과 신규 유저 수 사이에는\n통계적으로 유의미한 강한 상관관계(+0.8)가 있다 피어슨 상관계수 값에 대한 해석 기준 (Strong/Moderate/Weak) @ https://j.mp/3mH8FWN 파이썬 프로그래밍 없이 상관관계 분석을 진행할 수 있는 도구 @ https://j.mp/324551c A/B Test (독립표본) # 페이지 구성과 세부 디자인을 다르게 만든 2개의 웹사이트 시안을 기반으로 A/B Test 진행 웹사이트 시안 A와 B 각각에 유입된 유저들이\n실제로 각 웹사이트 내에서 이탈하기까지의 시간(체류시간, Duration time)을 측정 T-Test를 진행하기 이전에 Null인 데이터들은 제외 비교 데이터 # 독립표본 T-Test # Copy python stats.ttest_ind(web_a.Duration_A.values, web_b.Duration_B.values, equal_var=False) # Output Ttest_indResult(statistic=3.0165632092150694, pvalue=0.008734970056646718) equal_var=True : 두 개의 열의 분산값이 동일할거라 가정 equal_var=False: 분산값이 동일하지 않기 때문에 False로 설정 2개의 측정 그룹이 동일한 수가 아니거나 유사한 분산값을 갖지 않을 경우\nWelch\u0026rsquo;s t-test를 사용 @ https://j.mp/3kLFwcE p-value가 0.0087(\u0026lt;0.05)이므로,\n웹 시안 A와 B에 대한 체류시간의 평균값이 통계적으로 유의미한 차이가 없다 이러한 전제 하에 위와 같은 체류시간 측정 결과가 나올 확률이 0.87%라고 이해할 수 있음 웹 시안 A와 B에 대한 유저들의 체류 시간 사이에는 통계적으로 유의미한 차이가 있다 A/B Test (카이제곱 검정) # 최종 구매를 위한 버튼을 2개의 서로 다른 시안으로 제작해 각기 다른 유저들에게 노출 해당 버튼을 누르면 구매가 확정된다고 가정 비교 데이터 # Conversion Rate (전환율) # 전체 웹사이트 사용자 중에서 얼마나 많은 사용자들이 동작을 마치는지에 대한 지표 Copy python conversion_rate = click_df[\u0026#39;Clicked\u0026#39;] / (click_df[\u0026#39;Clicked\u0026#39;] + click_df[\u0026#39;Unclicked\u0026#39;]) * 100 # Output Button_A 5.746209 Button_B 7.737226 dtype: float64 Click-Through Rate (CTR, 클릭율) # 얼마나 많은 사용자들이 웹사이트에 접근하기 위해 광고를 클릭하는지에 대한 지표 Button_A는 5.75% Button_B는 7.73% 카이제곱 검정 # 버튼 A/B에 대한 클릭 여부가 정리된 위 테이블 == Contingency Table(분할표)\n@ https://j.mp/384CcFR chi2_contingency 함수 활용 시 Contingency Table을 기반으로 카이제곱 검정\n@ https://j.mp/3mH1Nsr Copy python stats.chi2_contingency([click_df[\u0026#39;Clicked\u0026#39;], click_df[\u0026#39;Unclicked\u0026#39;]])[1] # Output 0.004968535119697213 p-value가 0.0049(\u0026lt;0.05)이므로,\n버튼 A/B에 대한 클릭 여부가 통계적으로 유의미한 연관성이 없다는 전제 하에,\n이러한 클릭 수 측정 결과가 나올 확률이 0.49%라고 이해할 수 있음 배너 버튼 시안 A와 B에 대한 유저들의 클릭 수 사이에는 통계적으로 유의미한 차이가 있다 A/B Test References # A/B Testing에 대한 기초적인 정보들 @ https://j.mp/3eeo2TA 데이터를 활용한 디지털 마케팅 효과분석 @ https://j.mp/2P7YHCO P-hacking에 대하여 @ https://j.mp/3mLMOgP / https://j.mp/31XJBTF p-value # p-value의 높고 낮음과 별개로 실제 실험의 효과 크기 역시도 중요하게 고려해야한다.\n예를 들어 어떤 웹사이트의 구매 버튼의 디자인을 변경하여 구매 수가 n 만큼 증가되었고,\n디자인 변경 전/후에 대한 구매 버튼 클릭 수 사이의 관계를 대상으로 통계 검정 후 p-value가 0.05보다 낮게 나왔더라도,\n정작 증가된 구매 수에 해당하는 n이 미미하다면 낮은 p-value에도 불구하고 디자인 변경의 실질적인 효용이 적기 때문이다.\n(통계적으로만 유의미할 뿐 독립변수의 변화에 따른 종속변수의 변화값이 실질적/실용적인 의미를 갖지 않음)\n"},{"id":97,"href":"/blog/aischool-04-03-test-statistics/","title":"[AI SCHOOL 5기] 통계분석 실습 - T-Test \u0026 상관관계 분석","section":"Posts","content":"Import Libraries # Copy python import pandas as pd import seaborn as sns import scipy as sp from scipy import stats import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) 교차분석 # 교차표 (Cross-Table) # Copy python crosstab = pd.crosstab(df.propensity, df.skin, margins=True) crosstab.columns=[] crosstab.index=[] margins: 합계(All) 추가 여부 normalize: Normalization 여부 Chi-square 검정 # 두 범주형 변수 사이의 관계가 있는지 없는지를 검정 (독립성 검정) 귀무가설: Indepedent (vice versa) 대립가설: Not Independent Copy python stats.chisquare(df.column1, df.column2) # Output Power_divergenceResult(statistic=291.8166666666667, pvalue=0.023890557260065975) p-value # 관찰 데이터의 검정 통계량이 귀무가설을 지지하는 정도 귀무가설이 참이라는 전제 하에, 관찰이 완료된 값이 표본을 통해 나타날 확률 p-value가 0.05(5%) 미만일 경우, 관측치가 나타날 확률이 매우 낮다고 판단하여 귀무가설 기각 p-value가 0.05(5%) 이상일 경우, 관측치가 나타날 확률이 충분하다고 판단하여 귀무가설 지지 p-value가 0.05 이하라는 것이 항상 대립가설을 의미하는 것은 아님 (5%만큼 귀무가설이 참일 가능성) Copy python crosstab.plot.bar(stacked=True) 독립표본 T-test 분석 시각화 # 서로 다른 집단에서 같은 열 비교 두 집단 간의 평균 차이를 검정 ex) \u0026ldquo;서로 다른\u0026rdquo; 성별 간에 전반적인 만족도의 평균값 사이에 유의미한 차이가 \u0026ldquo;없다\u0026rdquo; Copy python stats.ttest_ind(column1.values, column2.values) # Output Ttest_indResult(statistic=-0.494589803056421, pvalue=0.6213329051985961) Box-Plot # Histogram # Copy python sns.distplot(male, kde=False, fit=stats.norm, hist_kws={\u0026#39;color\u0026#39;: \u0026#39;r\u0026#39;, \u0026#39;alpha\u0026#39;: 0.2}, fit_kws={\u0026#39;color\u0026#39;: \u0026#39;r\u0026#39;}) sns.distplot(female, kde=False, fit=stats.norm, hist_kws={\u0026#39;color\u0026#39;: \u0026#39;g\u0026#39;, \u0026#39;alpha\u0026#39;: 0.2}, fit_kws={\u0026#39;color\u0026#39;: \u0026#39;g\u0026#39;}) 대응표본 T-test 분석 시각화 # 동일한 집단에서 서로 다른 열 비교 동일한 모집단으로부터 추출된 두 변수의 평균값을 비교 분석 \u0026ldquo;동일한\u0026rdquo; 고객 집단이 평가한 구매 가격에 대한\n만족도와 구매 문의에 대한 만족도의 평균값 사이에 유의미한 차이가 있다. Copy python stats.ttest_rel(df[\u0026#34;satisf_b\u0026#34;], df[\u0026#34;satisf_i\u0026#34;]) # Output Ttest_relResult(statistic=-7.155916401026872, pvalue=9.518854506666398e-12) Histogram # Copy python sns.distplot(df[\u0026#34;satisf_b\u0026#34;], kde=False, fit=stats.norm, hist_kws={\u0026#39;color\u0026#39;: \u0026#39;r\u0026#39;, \u0026#39;alpha\u0026#39;: 0.2}, fit_kws={\u0026#39;color\u0026#39;: \u0026#39;r\u0026#39;}) sns.distplot(df[\u0026#34;satisf_i\u0026#34;], kde=False, fit=stats.norm, hist_kws={\u0026#39;color\u0026#39;: \u0026#39;g\u0026#39;, \u0026#39;alpha\u0026#39;: 0.2}, fit_kws={\u0026#39;color\u0026#39;: \u0026#39;g\u0026#39;}) 분산분석 시각화 # 분산분석(ANalysis Of VAriance, ANOVA) 세 개의 집단에서 적어도 하나의 유의미한 차이가 있는가 ex) 3가지 구매 동기에 따른 전반적인 만족도의 평균값 중 적어도 하나는 유의미한 차이가 있다. Copy python stats.f_oneway(anova1, anova2, anova3) # Output F_onewayResult(statistic=4.732129410493065, pvalue=0.009632034309915485) Histogram # Copy python sns.distplot(anova1, kde=False, fit=sp.stats.norm, hist_kws={\u0026#39;color\u0026#39;: \u0026#39;r\u0026#39;, \u0026#39;alpha\u0026#39;: 0.2}, fit_kws={\u0026#39;color\u0026#39;: \u0026#39;r\u0026#39;}) sns.distplot(anova2, kde=False, fit=sp.stats.norm, hist_kws={\u0026#39;color\u0026#39;: \u0026#39;g\u0026#39;, \u0026#39;alpha\u0026#39;: 0.2}, fit_kws={\u0026#39;color\u0026#39;: \u0026#39;g\u0026#39;}) sns.distplot(anova3, kde=False, fit=sp.stats.norm, hist_kws={\u0026#39;color\u0026#39;: \u0026#39;b\u0026#39;, \u0026#39;alpha\u0026#39;: 0.2}, fit_kws={\u0026#39;color\u0026#39;: \u0026#39;b\u0026#39;}) 상관관계 분석 시각화 # 연속형 변수열들끼리 비교 피어슨 상관계수 # Copy python df.corr.corr() -1에 가까울수록 음의 상관관계 1에 가까울수록 양의 상관관계 0에 가까울수록 상관관계가 적음 PairPlot # Copy python sns.parilpot(df_corr) Iris Data # 붓꽃의 품종에 대한 데이터 Copy text iris = sns.load_dataset(\u0026#34;iris\u0026#34;) sns.pairplot(iris, kind=\u0026#34;reg\u0026#34;) reg: 추세선 sepal_width \u0026lt;-\u0026gt; petal_length\n두 그룹으로 나눠져 있어 음의 상관관계라 보기 어려움 그룹을 나눠서 비교 (1. 양의 상관관계, 2. 관계 없음) Simpson\u0026rsquo;s paradox # 심슨의 역설 @ https://j.mp/31Kd6v7 \u0026amp; https://j.mp/3IswbTj 사례로 알아보는 심슨의 역설 @ https://j.mp/3ICKS6q 전체를 봤을 때와 일부를 나눠서 봤을 때 다른 결과가 나옴 신장 결석 크기를 치료법 a, b를 사용해서 얼마나 빨리 치료하는지 작은 결석/큰 결석 인원수가 달라서 b 치료법 확률이 낮아짐 \u0026gt; 합계는 높음 "},{"id":98,"href":"/blog/aischool-04-02-descriptive-statistics/","title":"[AI SCHOOL 5기] 통계분석 실습 - 빈도 분석 \u0026 기술통계량 분석","section":"Posts","content":"Chart # Pie Chart # Copy python df[\u0026#39;column\u0026#39;].value_counts().plot(kind = \u0026#39;pie\u0026#39;) Bar Chart # Copy python df[\u0026#39;column\u0026#39;].value_counts().plot(kind = \u0026#39;bar\u0026#39;) Descriptive Statistics # df['column'].max(): 최댓값 (행방향 기준: axis=1) df['column'].min(): 최솟값 df['column'].sum(): 합계 df['column'].mean(): 평균 df['column'].variance(): 분산 df['column'].std(): 표준편차 df['column'].describe(): 기술통계량 분포의 왜도와 첨도 # df['column'].hist(): 히스토그램 df['column'].skew(): 왜도 (분포가 좌우로 치우쳐진 정도) 왜도(Skewness): 0에 가까울수록 정규분포 (절대값 기준 3 미초과)\n우측으로 치우치면 음(negative)의 왜도, 좌측으로 치우치면 양(positive)의 왜도 df['column'].kurtosis(): 첨도 (분포가 뾰족한 정도) 첨도(Kurtosis): 1에 가까울수록 정규분포 (절대값 기준 8 또는 10 미초과) 왜도가 0, 정도가 1일 때 완전한 정규분포로 가정 sns.distplot(df['column'], rug=True): distribution plot\nrug: 막대 그래프를 표시할지 여부 sns.jointplot(x='column1', y='column2', data=df): 산점도와 히스토그램 한번에 표시 sns.jointplot(..., kind=\u0026quot;kde\u0026quot;): 밀집된 분포 곡선을 표시 Outlier 탐지 및 제거 # df.boxplot(column='column'): 데이터 전체에 걸쳐서 분포 밀집도를 표시 IQR 활용 # IQR(Inter-Quantile Range): 바닥부터 75% 지점의 값 - 바닥부터 25% 지점의 값 상한치: 바닥부터 75% 지점의 값 + IQR의 1.5배 하한치: 바닥부터 25% 지점의 값 - IQR의 1.5배 상한/하한치를 넘으면 Outlier로 판단 Copy python Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 df_IQR = df[ (df[\u0026#39;column\u0026#39;] \u0026lt; Q3 + IQR * 1.5) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026gt; Q1 - IQR * 1.5) ] df_IQR.boxplot(column=\u0026#39;column\u0026#39;) Outlier 제거 전후 분포 비교 # Histogram # Before After Joint-Plot # Before After Log 함수를 활용한 데이터 스케일링 # 왜도 혹은 첨도가 너무 큰 경우, Log 함수를 적용해 왜도/첨도를 낮춰주는 전처리를 적용 processed_df['log_column'] = np.log(processed_df['column']) Before After "},{"id":99,"href":"/blog/aischool-04-01-numpy-pandas/","title":"[AI SCHOOL 5기] 통계분석 실습 - Numpy \u0026 Pandas","section":"Posts","content":"Numpy # Numpy Array 내부의 데이터는 하나의 자료형으로 통일 Numpy Array에 값을 곱하면 전체 데이터 그대로 복사되는 리스트와 달리 데이터에 각각 곱해짐 np.array([]): Numpy Array 생성 np.dtype: Numpy Array의 Data Type np.shape: Numpy Array 모양(차원) np.arange(): range를 바탕으로 Numpy Array 생성 np.reshape(): Numpy Array 모양을 변경, 열에 -1을 입력하면 자동 계산 np.dot(): 행렬곱 Pandas # pd.Series([], index=[]): Key가 있는 리스트(Series) 생성 Series.values: Series의 값 Series.index: Series의 키 값 df.ammount: 띄어쓰기 없이 영단어로 구성된 열은 변수처럼 꺼내 쓸 수 있음 df.insert(column, 'key', 'value'): index 기준으로 특정 위치에 새로운 열 삽입 df[(con1) \u0026amp; (con2)]: 여러 개의 조건을 사용할 땐 각각의 조건을 괄호 안에 묶어야 함 df['key'].value_counts(): 값의 출현 빈도 합계 (sort=False로 정렬 해제) df['key'].value_counts().plot(kind='pie'): 빈도수를 기준으로 원형차트 생성 df['key'].apply(): 조건에 따라 변환된 값을 가진 열 반환 df['key'].replace(): 변환값이 1대1 대응 시 apply() 대신 replace() 사용 가능\ndf['gender'].replace([1, 2], ['male', 'female']) "},{"id":100,"href":"/blog/aischool-03-04-web-crawling/","title":"[AI SCHOOL 5기] 웹 크롤링 실습 - 웹 크롤링","section":"Posts","content":"Wadis 마감 상품 재고 체크 # Google 메일 설정 # Copy python import smtplib from email.mime.text import MIMEText def sendMail(sender, receiver, msg): smtp = smtplib.SMTP_SSL(\u0026#39;smtp.gmail.com\u0026#39;, 465) smtp.login(sender, \u0026#39;your google app password\u0026#39;) msg = MIMEText(msg) msg[\u0026#39;Subject\u0026#39;] = \u0026#39;Product is available!\u0026#39; smtp.sendmail(sender, receiver, msg.as_string()) smtp.quit() Wadis 상품 재고 체크 # Copy python # 라이브러리 선언 check_status = 1 url = \u0026#39;https://www.wadiz.kr/web/campaign/detail/{item_number}\u0026#39; # 상품 재고가 확인되어 메일이 발송되면 종료 while check_status: webpage = urlopen(url) source = BeautifulSoup(webpage, \u0026#39;html.parser\u0026#39;) target = source.find_all(\u0026#39;button\u0026#39;, {\u0026#39;class\u0026#39;:\u0026#39;rightinfo-reward-list\u0026#39;}) for item in target: # 가격이 \u0026#39;179,000\u0026#39;원 상품 중 if \u0026#39;179,000\u0026#39; in item.find(\u0026#39;dt\u0026#39;).get_text().strip(): # \u0026#39;블루\u0026#39; 색상인 상품에 대하여 if \u0026#39;블루\u0026#39; in item.find(\u0026#39;p\u0026#39;).get_text().strip(): # 판매 중인 상태가 되면 (마감된 상품엔 \u0026#34;soldout\u0026#34; 클래스가 추가) if len(item.attrs[\u0026#39;class\u0026#39;]) == 2: sendMail(sender, receiver, msg) check_status = 0 서울상권분석서비스 # 웹 스크래핑 시도 # Copy python url = \u0026#39;https://golmok.seoul.go.kr/regionAreaAnalysis.do\u0026#39; response = requests.get(url).content web_page = BeautifulSoup(response, \u0026#39;html.parser\u0026#39;) 해당 웹 페이지는 POST 요청으로 데이터를 주고 받기 때문에 GET 방식으로는 접근 불가 개발자 도구의 Network 탭을 확인하면 JSON 데이터 확인 가능 POST 요청할 때 Payload를 변경하여 JSON 파일 종류 변경 가능 POST 요청 # Copy python # Payload 설정 data = {\u0026#39;stdrYyCd\u0026#39;: \u0026#39;2021\u0026#39;, \u0026#39;stdrQuCd\u0026#39;: \u0026#39;4\u0026#39;, \u0026#39;stdrSlctQu\u0026#39;: \u0026#39;sameQu\u0026#39;, \u0026#39;svcIndutyCdL\u0026#39;: \u0026#39;CS000000\u0026#39;, \u0026#39;svcIndutyCdM\u0026#39;: \u0026#39;all\u0026#39;} response = requests.post(\u0026#39;https://golmok.seoul.go.kr/region/selectRentalPrice.json\u0026#39;, data=data).content result = json.loads(response) Output\nCopy json [{\u0026#39;GBN_CD\u0026#39;: \u0026#39;11\u0026#39;, \u0026#39;NM\u0026#39;: \u0026#39;서울시 전체\u0026#39;, \u0026#39;GUBUN\u0026#39;: \u0026#39;si\u0026#39;, \u0026#39;BF1_FST_FLOOR\u0026#39;: \u0026#39;132504\u0026#39;, ... 네이버 금융 Top 종목 # TOP 종목 테이블 값 추출 # Copy python url = \u0026#39;http://finance.naver.com\u0026#39; response = requests.get(url).content web_page = BeautifulSoup(response, \u0026#39;html.parser\u0026#39;) top_items = web_page.find(\u0026#39;tbody\u0026#39;, {\u0026#39;id\u0026#39;:\u0026#39;_topItems1\u0026#39;}) item_rows = top_items.find_all(\u0026#39;tr\u0026#39;) TOP 종목 테이블 값 표시 # Copy python for item in item_rows: item_name = item.find(\u0026#39;th\u0026#39;).get_text() item_price = item.find_all(\u0026#39;td\u0026#39;)[0].get_text() item_delta_price = item.find_all(\u0026#39;td\u0026#39;)[1].get_text() item_delta_percent = item.find_all(\u0026#39;td\u0026#39;)[2].get_text().strip() print(\u0026#39;{} : 현재가 {}, 어제보다 {} {}, 백분율 변환 시 {}\u0026#39;.format( item_name, item_price, item_delta_price[3:], item_delta_price[:2], item_delta_percent)) 부동산 매매 내역 # 공공데이터포털 API 발급 # https://www.data.go.kr/data/15057267/openapi.do 상업업무용 부동산 매매 신고 자료 활용신청 상세 정보는 API 기술문서 참조 Python3 샘플 코드 # Copy python import requests url = \u0026#39;http://openapi.molit.go.kr/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcNrgTrade\u0026#39; params ={\u0026#39;serviceKey\u0026#39; : \u0026#39;서비스키\u0026#39;, \u0026#39;LAWD_CD\u0026#39; : \u0026#39;11110\u0026#39;, \u0026#39;DEAL_YMD\u0026#39; : \u0026#39;201512\u0026#39; } response = requests.get(url, params=params) print(response.content) request.get 요청 시 params를 사용해서 파라미터 한번에 입력 LAWD_CD: 법정동코드 10자리 중 앞 5자리 @ https://www.code.go.kr/index.do 부동산 매매 신고 자료 XML 요청 # Copy python response = requests.get(url, params=params).content web_page = BeautifulSoup(response, \u0026#39;lxml-xml\u0026#39;) 매매 내역을 DataFrame에 저장 # Copy python loc_code = [] loc = [] date = [] price = [] building_usage = [] for item in items: try: loc_code.append(item.find(\u0026#39;지역코드\u0026#39;).get_text()) loc.append(item.find(\u0026#39;시군구\u0026#39;).get_text() + item.find(\u0026#39;법정동\u0026#39;).get_text()) date.append(item.find(\u0026#39;년\u0026#39;).get_text() + item.find(\u0026#39;월\u0026#39;).get_text() + item.find(\u0026#39;일\u0026#39;).get_text()) price.append(item.find(\u0026#39;거래금액\u0026#39;).get_text()) building_usage.append(item.find(\u0026#39;건물주용도\u0026#39;).get_text()) except: pass import pandas as pd df = pd.DataFrame({\u0026#39;지역코드\u0026#39;:loc_code, \u0026#39;부동산 위치\u0026#39;:loc, \u0026#39;거래 일자\u0026#39;:date, \u0026#39;거래 금액\u0026#39;:price, \u0026#39;부동산 용도\u0026#39;:building_usage}) "},{"id":101,"href":"/blog/aischool-03-03-selenium/","title":"[AI SCHOOL 5기] 웹 크롤링 실습 - 셀레니움","section":"Posts","content":"Selenium # 브라우저의 기능을 체크할 때 사용하는 도구 브라우저를 조종해야할 때도 사용 Import Libraries # Copy python # 크롬 드라이버 파일 자동 다운로드 from webdriver_manager.chrome import ChromeDriverManager # 크롬 드라이버를 파일에 연결 from selenium.webdriver.chrome.service import Service from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys from bs4 import BeautifulSoup import time import pandas as pd import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) # 불필요한 Warning 메시지 무시 Virtual Browser # Copy python # 크롬 드라이버 파일을 다운로드 후 세팅 service = Service(executable_path=ChromeDriverManager().install()) # 세팅된 크롬 드라이버를 연결해 가상 브라우저 실행 driver = webdriver.Chrome(service=service) driver.maximize_window(): 가상 브라우저 크기 최대화 올바른 실행을 위해 가상 브라우저의 내부는 건들지 않아야 함 Google Translation # Google 번역 페이지 접속 # Copy python translate_url = \u0026#39;https://translate.google.co.kr/?sl=auto\u0026amp;tl=en\u0026amp;op=translate\u0026amp;hl=ko\u0026#39; driver.get(translate_url) driver.current_url: 가상 브라우저가 접속한 페이지의 URL 주소 반환 driver.page_source: 가상 브라우저가 접속한 페이지의 소스코드 반환 driver.find_element: BeautifulSoup의 find와 같음 driver.find_elements: BeautifulSoup의 find_all과 같음 원본 텍스트 입력 # 클래스나 ID를 통한 접근이 어려울 경우 XPath를 통해 접근 개발자 도구에서 full XPath를 복사 Copy python origin_xpath = \u0026#39;원본 텍스트 부분에 해당하는 XPath\u0026#39; driver.find_element_by_xpath(origin_xpath).clear() driver.find_element_by_xpath(origin_xpath).send_keys(\u0026#39;원본 텍스트\u0026#39;) .click(): 특정 부분 클릭 .clear(): 특정 부분에 입력된 값 지우기 .send_keys(): 특정 부분에 값 입력 번역된 텍스트 가져오기 # Copy python translation_xpath = \u0026#39;번역된 텍스트 부분에 해당하는 XPath\u0026#39; translated_contents = driver.find_element_by_xpath(translation_xpath).text .text: 번역된 텍스트 가상 브라우저 종료 # Copy python driver.close() driver.quit() Translated Word Cloud # Translated Word Cloud 생성 방법 # 기사글 전체를 번역하고 단어를 빈도수 순으로 정렬 빈도수를 기반으로 단어를 선정하고 해당 단어들만을 번역 * 선정된 단어들을 번역 # Copy python for key in translation_target: # key를 원본 텍스트 부분에 입력 time.sleep(3) # translated_contents 변수에 번역된 텍스트를 가져와서 저장 translation_result[translated_contents] = translation_target[key] driver.close() driver.quit() Translated Word Cloud # 파파고 번역 # 파파고 번역 페이지는 키워드 입력 후 번역된 결과를 보이는 시간 간격이 김 딜레이를 지정하고 반복문을 수행할 경우 번역이 끝나지 않아 잘못된 결과를 가져올 가능성 번역이 완료될 경우 나타나는 태그를 기준으로 대기 시간 설정 파파고에서는 번역된 단어의 발음에 해당하는 \u0026lt;p\u0026gt; 태그가 나타날 때를 번역 완료로 판단 expected_conditions를 활용해 특정한 태그의 로딩이 완료될 때까지 대기 선정된 단어들을 번역 # Copy python from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions from selenium.webdriver.common.by import By # 가상 브라우저 실행 # 파파고 번역 페이지(https://papago.naver.com/?sk=ko\u0026amp;tk=en) 이동 for key in translation_target: driver.find_element_by_id(\u0026#39;txtSource\u0026#39;).clear() driver.find_element_by_id(\u0026#39;txtSource\u0026#39;).send_keys(key) time.sleep(3) wait = WebDriverWait(driver, timeout=10) wait.until(expected_conditions.presence_of_element_located((By.CSS_SELECTOR, \u0026#34;#targetEditArea \u0026gt; p\u0026#34;))) translated_contents = driver.find_element_by_id(\u0026#39;txtTarget\u0026#39;).text translation_result_papago[translated_contents] = translation_target[key] # 가상 브라우저 종료 WebDriverWait(): 가상 브라우저가 timeout을 초과하면 에러 발생 wait.until(expected_conditions): 지정한 Tag가 포착될 때까지 대기 expected_conditions Documentation @ https://j.mp/3mCnc5G 인터파크 투어 # 여행지 검색 # Copy python # 가상 브라우저 실행 # 인터파크 투어 페이지(http://tour.interpark.com/) 이동 driver.find_element_by_id(\u0026#39;SearchGNBText\u0026#39;).send_keys(\u0026#39;보라카이\u0026#39;) driver.find_element_by_class_name(\u0026#39;search-btn\u0026#39;).click() 여행지 검색 결과 크롤링 # Copy python # 더보기 버튼 클릭 driver.find_element_by_class_name(\u0026#39;moreBtn\u0026#39;).click() # 2페이지로 변경 driver.find_element_by_xpath(\u0026#39;/html/body/div[3]/div/div[1]/div[2]/div[4]/div[3]/ul/li[2]\u0026#39;).click() "},{"id":102,"href":"/blog/aischool-02-04-word-cloud/","title":"[AI SCHOOL 5기] 텍스트 분석 실습 - 워드클라우드","section":"Posts","content":"Okt Library # 한국어 형태소 분석기 KoNLPy 패키지에 속한 라이브러리 KoNLPy 테스트 # Copy python from konlpy.tag import Okt tokenizer = Okt() tokens = tokenizer.pos(\u0026#34;아버지 가방에 들어가신다.\u0026#34;, norm=True, stem=True) print(tokens) norm: 정규화(Normalization), \u0026lsquo;안녕하세욯\u0026rsquo; -\u0026gt; \u0026lsquo;안녕하세요\u0026rsquo; stem: 어근화(Stemming, Lemmatization), (\u0026lsquo;한국어\u0026rsquo;, \u0026lsquo;Noun\u0026rsquo;) Pickle Library (Extra) # 파이썬 변수를 pickle 파일로 저장/불러오기 Copy python with open(\u0026#39;raw_pos_tagged.pkl\u0026#39;, \u0026#39;wb\u0026#39;) as f: pickle.dump(raw_pos_tagged, f) with open(\u0026#39;raw_pos_tagged.pkl\u0026#39;,\u0026#39;rb\u0026#39;) as f: data = pickle.load(f) 크롤링 데이터 전처리 # 크롤링 데이터 불러오기 # Copy python df = pd.read_excel(\u0026#39;result_220328_1314.xlsx\u0026#39;) articles = df[\u0026#39;Article\u0026#39;].tolist() articles = \u0026#39;\u0026#39;.join(articles) Article 데이터를 불러와서 리스트화 시키고 다시 하나의 문자열로 변환 형태소 단위 분해 # Copy python from konlpy.tag import Okt tokenizer = Okt() raw_pos_tagged = tokenizer.pos(articles, norm=True, stem=True) 단어 등장 빈도 시각화 # Copy python word_cleaned = [\u0026#39;불용어가 제거된 단어 목록\u0026#39;] # NLTK의 Text() 클래스에서 matplotlib의 plot 기능 제공 word_counted = nltk.Text(word_cleaned) plt.figure(figsize=(15, 7)) word_counted.plot(50) 단어 등장 빈도 시각화 (막대그래프) # Copy python # NLTK의 FreqDist() 클래스를 선언하면 인덱스 열이 지정된 객체 생성 word_frequency = nltk.FreqDist(word_cleaned) df = pd.DataFrame(list(word_frequency.values()), word_frequency.keys()) result = df.sort_values([0], ascending=False) result.plot(kind=\u0026#39;bar\u0026#39;, legend=False, figsize=(15,5)) plt.show() Word Cloud # Import Libraries # Copy python from wordcloud import WordCloud import matplotlib.pyplot as plt from PIL import Image import numpy as np import matplotlib.pyplot as plt Create WordCloud # Copy python word_cloud = WordCloud(font_path=\u0026#34;malgun.ttf\u0026#34;, width=2000, height=1000, background_color=\u0026#39;white\u0026#39;).generate_from_frequencies(word_dic) width, height: 워드클라우드 해상도 background_color: 배경색 max_words: 단어 최대 갯수 (default: 200) max_font_size: 최대 글자 크기 prefer_horizontal: 가로로 보여주는 정도, 가로로만 그리려면 1.0 설정 Show WordCloud # Copy python plt.figure(figsize=(15,15)) # 화면에 보여지는 크기 plt.imshow(word_cloud) plt.axis(\u0026#34;off\u0026#34;) plt.tight_layout(pad=0) plt.show() Masking # Copy python python_coloring = np.array(Image.open(\u0026#34;python_mask.jpg\u0026#34;)) word_cloud = WordCloud(font_path=\u0026#34;malgun.ttf\u0026#34;, width=2000, height=1000, mask=python_coloring, background_color=\u0026#39;white\u0026#39;).generate_from_frequencies(word_dic) np.array로 이미지 파일을 열면 픽셀 단위의 행렬 생성 mask 파라미터에 Numpy Array 전달 WordCloud의 해상도는 원본 이미지의 해상도에 영향을 받음 Coloring # Copy python from wordcloud import ImageColorGenerator image_colors = ImageColorGenerator(python_coloring) ... plt.imshow(word_cloud.recolor(color_func=image_colors), interpolation=\u0026#39;bilinear\u0026#39;) ImageColorGenerator 객체를 통해 이미지로부터 색상을 추출 recolor 함수를 통해 이미지 컬러 다시 칠하기 interpolation: 비어있는 픽셀 값을 칠하는 방법, bilinear(보간법) colormap: 임의로 색상 지정 ('Reds', 'Blues' 등) Save to Image File # Copy python word_cloud.to_file(\u0026#34;word_cloud_completed.png\u0026#34;) "},{"id":103,"href":"/blog/aischool-03-02-web-scraping-advanced/","title":"[AI SCHOOL 5기] 웹 크롤링 실습 - 웹 스크래핑 심화","section":"Posts","content":"Import Libraries # Copy python import requests from bs4 import BeautifulSoup import pandas as pd from datetime import datetime import time # time.sleep() import re 뉴스 검색 결과에서 네이버 뉴스 추출 # 네이버 뉴스 검색 결과 URL 분석 # Copy html https://search.naver.com/search.naver? where=news\u0026amp; sm=tab_jum\u0026amp; \u0026lt;!-- 불필요 --\u0026gt; query=데이터분석 네이버 뉴스 검색 URL 불러오기 # Copy python query = input() # 데이터분석 url = f\u0026#39;https://search.naver.com/search.naver?where=news\u0026amp;query={query}\u0026#39; web = requests.get(url).content source = BeautifulSoup(web, \u0026#39;html.parser\u0026#39;) 네이버 뉴스 기사 주제 가져오기 # Copy python news_subjects = source.find_all(\u0026#39;a\u0026#39;, {\u0026#39;class\u0026#39; : \u0026#39;news_tit\u0026#39;}) subject_list = [] for subject in news_subjects: subject_list.append(subject.get_text()) 네이버 뉴스 기사 링크 가져오기 # Copy python urls_list = [] for urls in source.find_all(\u0026#39;a\u0026#39;, {\u0026#39;class\u0026#39; : \u0026#39;info\u0026#39;}): if urls.attrs[\u0026#39;href\u0026#39;].startswith(\u0026#39;https://news.naver.com\u0026#39;): urls_list.append(urls.attrs[\u0026#39;href\u0026#39;]) 단일 뉴스 페이지 분석 # ConnectionError # Copy python web_news = requests.get(urls_list[0]).content source_news = BeautifulSoup(web_news, \u0026#39;html.parser\u0026#39;) Copy bash ConnectionError: (\u0026#39;Connection aborted.\u0026#39;, RemoteDisconnected(\u0026#39;Remote end closed connection without response\u0026#39;)) 브라우저를 거치지 않고 HTML 코드를 요청하면 ConnectionError 발생 사용자임을 알리는 헤더 추가 Copy python headers = {\u0026#39;User-Agent\u0026#39;:\u0026#39;Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36\u0026#39;} web_news = requests.get(urls_list[0], headers=headers).content source_news = BeautifulSoup(web_news, \u0026#39;html.parser\u0026#39;) 기사 제목 / 발행 날짜 추출 # Copy python title = source_news.find(\u0026#39;h3\u0026#39;, {\u0026#39;id\u0026#39; : \u0026#39;articleTitle\u0026#39;}).get_text() date = source_news.find(\u0026#39;span\u0026#39;, {\u0026#39;class\u0026#39; : \u0026#39;t11\u0026#39;}).get_text() Pandas Timestamp # Copy python # 2022.03.25. 오전 10:18 date = source_news.find(\u0026#39;span\u0026#39;, {\u0026#39;class\u0026#39; : \u0026#39;t11\u0026#39;}).get_text() # 2022.03.25.10:18am pd_date = pd.Timestamp(reformatted_date) 기사 본문 추출 # Copy python article = source_news.find(\u0026#39;div\u0026#39;, {\u0026#39;id\u0026#39; : \u0026#39;articleBodyContents\u0026#39;}).get_text() article = article.replace(\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;) article = article.replace(\u0026#34;// flash 오류를 우회하기 위한 함수 추가function _flash_removeCallback() {}\u0026#34;, \u0026#34;\u0026#34;) article = article.replace(\u0026#34;동영상 뉴스 \u0026#34;, \u0026#34;\u0026#34;) article = article.replace(\u0026#34;동영상 뉴스\u0026#34;, \u0026#34;\u0026#34;) article = article.strip() 기사 발행 언론사 추출 # Copy python press_company = source_news.find(\u0026#39;address\u0026#39;, {\u0026#39;class\u0026#39; : \u0026#39;address_cp\u0026#39;}).find(\u0026#39;a\u0026#39;).get_text() print(press_company) 여러 뉴스 데이터 수집 # 각 기사들의 데이터를 수집해 리스트에 추가 # Copy python for url in urls_list: ... titles.append(title) dates.append(date) articles.append(article) article_urls.append(url) press_companies.append(press_company) 데이터에 대한 DataFrame 생성 # Copy python article_df = pd.DataFrame({\u0026#39;Title\u0026#39;:titles, \u0026#39;Date\u0026#39;:dates, \u0026#39;Article\u0026#39;:articles, \u0026#39;URL\u0026#39;:article_urls, \u0026#39;PressCompany\u0026#39;:press_companies}) 여러 페이지의 뉴스 데이터 수집 # 각각의 페이지에 해당하는 쿼리 리스트 생성 # Copy python max_page = int(input()) # 5 query = input() # 데이터분석 start_points = [] for point in range(1, max_page*10+1, 10): start_points.append(str(point)) 각각의 페이지에 대한 반복문 실행 # Copy python current_call = 1 last_call = (max_page - 1) * 10 + 1 while current_call \u0026lt;= last_call: url = \u0026#34;https://search.naver.com/search.naver?where=news\u0026amp;query=\u0026#34; + query + \\ \u0026#34;\u0026amp;start=\u0026#34; + str(current_call) web = requests.get(url).content source = BeautifulSoup(web, \u0026#39;html.parser\u0026#39;) ... # 대량의 데이터를 크롤링할 때는 요청 사이에 딜레이 생성 time.sleep(5) current_call += 10 날짜 지정하여 크롤링 # 네이버 뉴스 날짜 지정 검색 결과 URL 분석 # Copy html https://search.naver.com/search.naver?where=news \u0026amp;query=데이터분석 \u0026amp;sm=tab_opt \u0026amp;sort=0 \u0026amp;photo=0 \u0026amp;field=0 \u0026amp;pd=4 \u0026amp;ds= \u0026amp;de= \u0026amp;docid= \u0026amp;related=0 \u0026amp;mynews=0 \u0026amp;office_type=0 \u0026amp;office_section_code=0 \u0026amp;news_office_checked= \u0026lt;!-- 날짜 지정 (from{YYYYMMDD}to{YYYYMMDD}) --\u0026gt; \u0026amp;nso=so%3Ar%2Cp%3Afrom20220101to20220301 \u0026amp;is_sug_officeid=0 날짜에 해당하는 쿼리 생성 # Copy python start_date = input() # 2022.01.01 end_date = input() # 2022.03.01 start_date = start_date.replace(\u0026#34;.\u0026#34;, \u0026#34;\u0026#34;) end_date = end_date.replace(\u0026#34;.\u0026#34;, \u0026#34;\u0026#34;) ... while current_call \u0026lt;= last_call: url = \u0026#34;https://search.naver.com/search.naver?where=news\u0026amp;query=\u0026#34; + query \\ + \u0026#34;\u0026amp;nso=so%3Ar%2Cp%3Afrom\u0026#34; + start_date \\ + \u0026#34;to\u0026#34; + end_date \\ + \u0026#34;%2Ca%3A\u0026amp;start=\u0026#34; + str(current_call) web = requests.get(url).content source = BeautifulSoup(web, \u0026#39;html.parser\u0026#39;) ... 기사 정렬 순서 지정하여 크롤링 # 네이버 뉴스 기사 정렬 순서 검색 결과 URL 분석 # Copy html https://search.naver.com/search.naver?where=news \u0026amp;query=데이터분석 \u0026amp;sm=tab_opt \u0026lt;!-- 관련도순: 0, 최신순: 1, 오래된순: 2 --\u0026gt; \u0026amp;sort=0 ... 정렬 순서에 해당하는 쿼리 생성 # Copy python query = input() # \u0026#34;데이터분석\u0026#34; \u0026lt; 정확한 검색 sort_type = int(input()) # 1 데이터를 엑셀 파일로 저장 # Copy python article_df.to_excel(\u0026#39;result_{}.xlsx\u0026#39;.format(datetime.now().strftime(\u0026#39;%y%m%d_%H%M\u0026#39;)), index=False, encoding=\u0026#39;utf-8\u0026#39;) "},{"id":104,"href":"/blog/leetcode-problems-1337/","title":"[LeetCode 1337] The K Weakest Rows in a Matrix (Python)","section":"Posts","content":"문제 링크 # https://leetcode.com/problems/the-k-weakest-rows-in-a-matrix/ 개요 # 2차원 배열에 대해 각각의 리스트의 합을 기준으로 정렬을 하고 그 순서를 반환하는 문제이다. 파이썬에서는 내장함수 sort()를 사용하면 쉽게 풀 수 있다. 문제 해설 # 입력으로 2차원 배열 mat과 출력값의 개수를 의미하는 정수 k가 주어진다. mat에 있는 각각의 리스트는 0과 1의 조합으로 이루어져 있으며 1의 개수가 많은 리스트가 강한 리스트이다. 문제에서 요구하는 것은 1. 리스트를 약한 순으로 정렬하고\n2. 정렬하기 전의 인덱스 번호를 정렬된 순서대로 반환하는 것이다. 이를 위해 리스트의 인덱스 번호와 리스트의 합을 따로 저장할 필요가 있으므로 for문을 통해 mat을 순회한다. 순회하면서 mat의 각 리스트 내용을 [인덱스 번호, 리스트의 합]으로 덮어쓰고\n이후에 1번 원소(리스트의 합)을 기준으로 mat를 정렬한다. 마지막에 정렬된 mat의 0번 원소(인덱스 번호)를 k개 만큼만 추출해서 반환하면 된다. 해설 코드 # Copy python class Solution(object): def kWeakestRows(self, mat, k): \u0026#34;\u0026#34;\u0026#34; :type mat: List[List[int]] :type k: int :rtype: List[int] \u0026#34;\u0026#34;\u0026#34; for i in range(len(mat)): mat[i] = [i, sum(mat[i])] mat.sort(key=lambda x: x[1]) return [mat[i][0] for i in range(k)] "},{"id":105,"href":"/blog/boj-problems-2805/","title":"[백준 2805] 나무 자르기 (PyPy3)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/2805 개요 # 이분 탐색으로 해결할 수 있는 문제이다. Python3을 사용하면 시간초과가 발생하므로 PyPy3를 사용한다. 문제 조건 # 일정 높이에 대해 모든 나무를 잘랐을 때, 조건을 만족하는 절단기의 최대 높이(H)를 구하는 문제이다. 잘린 나무의 길이의 합은 상근이가 필요로 하는 나무의 길이(M)보다 크거나 같아야 한다. 문제 해설 # 나무의 수(N)의 최댓값이 1,000,000이므로 모든 범위에 대해 반복하는 순차 탐색을 이용할 경우 시간초과가 발생한다. 시간 복잡도가 O(log n)인 이분 탐색을 이용하면 시간 복잡도가 O(n)인 순차 탐색을 쓰는 것보다 훨씬 빠르다. 이분 탐색은 중간값(md)을 기준으로 시작하여 조건에 따라\n최대/최솟값의 포인터(mx/mn)를 조정하는 탐색 알고리즘이다. 잘린 나무의 길이의 합(total)을 구할 때 잘리지 않은 나무에 대한 음수값을 포함하지 않도록 주의한다. 조건을 만족할 경우 최솟값(mn)을 중간값(md)보다 크게 맞추며,\n반대의 경우 최댓값(mx)을 중간값(md)보다 작게 조정한다. 이분 탐색을 마치면 최댓값(mx)에 조건을 만족하는 최대 높이(H)의 값이 남게 된다. 시간 복잡도 # 시간 복잡도가 O(log N)인 이분 탐색의 매 반복마다 시간 복잡도가 O(n)인 for문을 실행하므로\n시간 복잡도는 O(N log N) 이상이 된다. N의 최댓값 1,000,000에 대해 20,000,000번이 넘는 연산이 실행되므로 Python3으로는 시간 제한 1초를 초과한다. PyPy3에 대한 이해가 깊은 편이 아니라 자세한 설명은 어렵지만,\n메모리를 더 사용하는 대신 코드를 캐싱하는 PyPy3를 사용하면 시간 제한 안에 해결할 수 있다. 해설 코드 # Copy python N, M = map(int, input().split()) trees = list(map(int, input().split())) mn, md, mx = 0, 0, max(trees) while mn \u0026lt;= mx: md = (mx + mn) // 2 total = 0 for tree in trees: total += tree - md if tree \u0026gt; md else 0 if total \u0026gt;= M: mn = md + 1 else: mx = md - 1 print(mx) "},{"id":106,"href":"/blog/aischool-02-03-text-analysis/","title":"[AI SCHOOL 5기] 텍스트 분석 실습 - 텍스트 분석","section":"Posts","content":"Scikit-learn Library # Traditional Machine Learning (vs DL, 인공신경을 썼는지의 여부) Copy python from sklearn import datasets, linear_model, model_selection, metrics data_total = datasets.load_boston() x = data_total.data y = data_total.target train_x, test_x, train_y, test_y = model_selection.train_test_split(x, y, test_size=0.3) # 학습 전의 모델 생성 model = linear_model.LinearRegression() # 모델에 학습 데이터를 넣으면서 학습 진행 model.fit(train_x, train_y) # 모델에게 새로운 데이터를 주면서 예측 요구 predictions = model.predict(test_x) # 예측 결과를 바탕으로 성능 점수 확인 metrics.mean_squared_error(predictions, test_y) 실습에서 사용할 패키지 Copy python from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics.pairwise import cosine_similarity Vectorizer # Copy python corpus = [doc1, doc2, doc3] vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(corpus).todense() # vertorizer.fit(corpus) # X = vectorizer.trasnform(corpus) Vectorizer는 리스트로 묶인 다수의 데이터에 대한 벡터 생성 fit_transform()은 기본적으로 메모리를 차지하는 \u0026lsquo;0\u0026rsquo;이라는 값들을 제외하고\n좌표에 대한 값의 형태로 표시 .todense(): \u0026lsquo;0\u0026rsquo;이라는 값들을 포함하여 행렬의 형태로 표시 X.shape: Vectorizer의 모양 확인 (row, column) CountVectorizer(): 단어의 출연 횟수만으로 벡터 생성 Print Output # Copy python [[0.0071001 0.00332632 0. ... 0. 0.00166316 0. ] [0.00889703 0. 0.00138938 ... 0.00138938 0. 0.00138938]] Pandas Output # 3. Cosine Similarity # Copy python cosine_similarity(X[0], X[1]) [하나의 행 vs 전체 행] 구도로 표시 # Copy python similarity = cosine_similarity(X[0], X) # 위에서부터 순서대로 보기 위해 전치 행렬(Transpose)로 표시 pd.DataFrame(similarity.T) [각 행 vs 전체 행] 구도로 표시 # Copy python similarity = cosine_similarity(X, X) result = pd.DataFrame(similarity) result.columns = [\u0026#39;Shawshank\u0026#39;, \u0026#39;Godfather\u0026#39;, \u0026#39;Inception\u0026#39;] result.index = [\u0026#39;Shawshank\u0026#39;, \u0026#39;Godfather\u0026#39;, \u0026#39;Inception\u0026#39;] 유사도 행렬 시각화 # Copy python import seaborn as sns import matplotlib.pyplot as plt plt.figure(figsize=(10, 10)) sns.heatmap(result, annot=True, fmt=\u0026#39;f\u0026#39;, linewidths=5, cmap=\u0026#39;RdYlBu\u0026#39;) sns.set(font_scale=1.5) plt.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False) plt.show() "},{"id":107,"href":"/blog/aischool-02-02-text-data-exploration/","title":"[AI SCHOOL 5기] 텍스트 분석 실습 - 텍스트 데이터 분석","section":"Posts","content":"Tokenizing Text Data # Import Libraries # Copy python import nltk from nltk.corpus import stopwords from collections import Counter Set Stopwords # Copy python stop_words = stopwords.words(\u0026#34;english\u0026#34;) stop_words.append(\u0026#39;,\u0026#39;) stop_words.append(\u0026#39;.\u0026#39;) stop_words.append(\u0026#39;’\u0026#39;) stop_words.append(\u0026#39;”\u0026#39;) stop_words.append(\u0026#39;—\u0026#39;) Open Text Data # Copy python file = open(\u0026#39;movie_review.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#34;utf-8\u0026#34;) lines = file.readlines() Tokenize # Copy python tokens = [] for line in lines: tokenized = nltk.word_tokenize(line) for token in tokenized: if token.lower() not in stop_words: tokens.append(token) Counting Nouns # POS Tagging # Copy python tags = nltk.pos_tag(tokens) word_list = [] for word, tag in tags: if tag.startswith(\u0026#39;N\u0026#39;): word_list.append(word.lower()) Counting Nouns # Copy python counts = Counter(word_list) print(counts.most_common(10)) Output\nCopy python [(\u0026#39;movie\u0026#39;, 406), (\u0026#39;batman\u0026#39;, 303), (\u0026#39;film\u0026#39;, 284), (\u0026#39;joker\u0026#39;, 219), (\u0026#39;dark\u0026#39;, 136), (\u0026#39;ledger\u0026#39;, 131), (\u0026#39;knight\u0026#39;, 124), (\u0026#39;time\u0026#39;, 112), (\u0026#39;heath\u0026#39;, 110), (\u0026#39;performance\u0026#39;, 87)] Counting Adjectives # POS Tagging # Copy python tags = nltk.pos_tag(tokens) word_list = [] for word, tag in tags: if tag.startswith(\u0026#39;J\u0026#39;): word_list.append(word.lower()) Counting Adjectives # Copy python counts = Counter(word_list) print(counts.most_common(10)) Output\nCopy python [(\u0026#39;good\u0026#39;, 141), (\u0026#39;best\u0026#39;, 102), (\u0026#39;great\u0026#39;, 78), (\u0026#39;many\u0026#39;, 54), (\u0026#39;much\u0026#39;, 52), (\u0026#39;comic\u0026#39;, 43), (\u0026#39;real\u0026#39;, 29), (\u0026#39;bad\u0026#39;, 28), (\u0026#39;little\u0026#39;, 26), (\u0026#39;new\u0026#39;, 25)] Counting Verbs # POS Tagging # Copy python tags = nltk.pos_tag(tokens) word_list = [] for word, tag in tags: if tag.startswith(\u0026#39;V\u0026#39;): word_list.append(word.lower()) Counting Verbs # Copy python counts = Counter(word_list) print(counts.most_common(10)) Output\nCopy python [(\u0026#39;see\u0026#39;, 59), (\u0026#39;get\u0026#39;, 54), (\u0026#39;made\u0026#39;, 49), (\u0026#39;think\u0026#39;, 46), (\u0026#39;seen\u0026#39;, 45), (\u0026#39;make\u0026#39;, 45), (\u0026#39;say\u0026#39;, 41), (\u0026#34;\u0026#39;ve\u0026#34;, 37), (\u0026#34;\u0026#39;m\u0026#34;, 32), (\u0026#39;going\u0026#39;, 31)] Visualizing Tokens # Import Libraries # Copy python import matplotlib.pyplot as plt import re 정규표현식으로 토큰 분류 # Copy python tokens = [] for line in lines: tokenized = nltk.word_tokenize(line) for token in tokenized: if token.lower() not in stop_words: if re.match(\u0026#39;^[a-zA-Z]+\u0026#39;, token): tokens.append(token) 정규표현식 개념 소개 @ https://j.mp/3bJQJHg 정규표현식 기본 문법 정리 @ https://j.mp/3bLXSqB 상세한 정규표현식 설명 @ http://j.mp/2PzgFO8 상세한 정규표현식 예제 @ https://hamait.tistory.com/342 점프 투 파이썬 정규표현식 @ https://wikidocs.net/4308 Visualizing Tokens # Copy python plt.figure(figsize=(10, 3)) plt.title(\u0026#39;Top 25 Words\u0026#39;,fontsize=30) corpus.plot(25) Top 25 Words Chart # Similar Words # Copy python print(\u0026#39;Similar words : \u0026#39;) corpus.similar(\u0026#39;batman\u0026#39;) Output\nCopy bash Similar words : superhero film action movie character better iconic seen acting actor heath performance modern difficult villain second end good come best Collocation # Copy python print(\u0026#39;Collocation\u0026#39;) corpus.collocation() Output\nCopy bash Collocation Dark Knight; Heath Ledger; Christian Bale; comic book; Harvey Dent; Christopher Nolan; Bruce Wayne; Aaron Eckhart; Morgan Freeman; Gary Oldman; Batman Begins; Two Face; Gotham City; Maggie Gyllenhaal; Rachel Dawes; Michael Caine; special effect; Tim Burton; Jack Nicholson; dark knight "},{"id":108,"href":"/blog/aischool-02-01-processing-text-data/","title":"[AI SCHOOL 5기] 텍스트 분석 실습 - 텍스트 분석","section":"Posts","content":"NLTK Library # NLTK(Natural Language Toolkit)은 자연어 처리를 위한 라이브러리 Copy python import nltk nltk.download() 문장을 단어 수준에서 토큰화 # Copy python sentence = \u0026#39;NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\u0026#39; nltk.word_tokenize(sentence) Output\nCopy bash [\u0026#39;NLTK\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;leading\u0026#39;, \u0026#39;platform\u0026#39;, ... POS Tagging # Copy python tokens = nltk.word_tokenize(sentence) nltk.pos_tag(tokens) Output\nCopy bash [(\u0026#39;NLTK\u0026#39;, \u0026#39;NNP\u0026#39;), (\u0026#39;is\u0026#39;, \u0026#39;VBZ\u0026#39;), (\u0026#39;a\u0026#39;, \u0026#39;DT\u0026#39;), (\u0026#39;leading\u0026#39;, \u0026#39;VBG\u0026#39;), (\u0026#39;platform\u0026#39;, \u0026#39;NN\u0026#39;), ... NLTK POS Tags List # Stopwords 제거 # Copy python from nltk.corpus import stopwords stop_words = stopwords.words(\u0026#39;english\u0026#39;) stop_words.append(\u0026#39;,\u0026#39;) stop_words.append(\u0026#39;.\u0026#39;) result = [] for token in tokens: if token.lower() not in stopWords: result.append(token) Output\nCopy python [\u0026#39;NLTK\u0026#39;, \u0026#39;leading\u0026#39;, \u0026#39;platform\u0026#39;, \u0026#39;building\u0026#39;, \u0026#39;Python\u0026#39;, \u0026#39;programs\u0026#39;, \u0026#39;work\u0026#39;, \u0026#39;human\u0026#39;, \u0026#39;language\u0026#39;, \u0026#39;data\u0026#39;, \u0026#39;provides\u0026#39;, \u0026#39;easy-to-use\u0026#39;, \u0026#39;interfaces\u0026#39;, \u0026#39;50\u0026#39;, \u0026#39;corpora\u0026#39;, \u0026#39;lexical\u0026#39;, \u0026#39;resources\u0026#39;, \u0026#39;WordNet\u0026#39;, \u0026#39;along\u0026#39;, \u0026#39;suite\u0026#39;, \u0026#39;text\u0026#39;, \u0026#39;processing\u0026#39;, \u0026#39;libraries\u0026#39;, \u0026#39;classification\u0026#39;, \u0026#39;tokenization\u0026#39;, \u0026#39;stemming\u0026#39;, \u0026#39;tagging\u0026#39;, \u0026#39;parsing\u0026#39;, \u0026#39;semantic\u0026#39;, \u0026#39;reasoning\u0026#39;, \u0026#39;wrappers\u0026#39;, \u0026#39;industrial-strength\u0026#39;, \u0026#39;NLP\u0026#39;, \u0026#39;libraries\u0026#39;, \u0026#39;active\u0026#39;, \u0026#39;discussion\u0026#39;, \u0026#39;forum\u0026#39;] Lemmatizing # Lemmatization: 단어의 형태소적/사전적 분석을 통해 파생적 의미를 제거하고,\n어근에 기반하여 기본 사전형인 lemma를 찾는 것 Copy python lemmatizer = nltk.wordnet.WordNetLemmatizer() print(lemmatizer.lemmatize(\u0026#34;cats\u0026#34;)) # cat print(lemmatizer.lemmatize(\u0026#34;geese\u0026#34;)) # goose print(lemmatizer.lemmatize(\u0026#34;better\u0026#34;)) # better print(lemmatizer.lemmatize(\u0026#34;better\u0026#34;, pos=\u0026#34;a\u0026#34;)) # good print(lemmatizer.lemmatize(\u0026#34;ran\u0026#34;)) # ran print(lemmatizer.lemmatize(\u0026#34;ran\u0026#34;, \u0026#39;v\u0026#39;)) # run default로 n 이므로 \u0026lsquo;cats\u0026rsquo;, \u0026lsquo;geese\u0026rsquo; 들은 기본명사형을 반환 형용사 \u0026lsquo;better\u0026rsquo;는 pos에 a를 함께 입력해주어야 원형인 \u0026lsquo;good\u0026rsquo;을 반환 동사 \u0026lsquo;ran\u0026rsquo;은 pos에 v를 함께 입력해주어야 원형인 \u0026lsquo;run\u0026rsquo;을 반환 영화 리뷰 데이터 전처리 # Copy python file = open(\u0026#39;moviereview.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) lines = file.readlines() sentence = lines[1] tokens = nltk.word_tokenize(sentence) lemmas = [] for token in tokens: if token.lower() not in stop_words: lemmas.append(lemmatizer.lemmatize(token)) "},{"id":109,"href":"/blog/aischool-03-01-web-scraping-basic/","title":"[AI SCHOOL 5기] 웹 크롤링 실습 - 웹 스크래핑 기본","section":"Posts","content":"BeautifulSoup Library # Copy python from bs4 import BeautifulSoup from urllib.request import urlopen 단어의 검색 결과 출력 # 다음 어학사전 URL 불러오기 # Copy python # 찾는 단어 입력 word = \u0026#39;happiness\u0026#39; url = f\u0026#39;https://alldic.daum.net/search.do?q={word}\u0026#39; web = urlopen(url) web_page = BeautifulSoup(web, \u0026#39;html.parser\u0026#39;) 찾는 단어 출력 # Copy python text_search = web_page.find(\u0026#39;span\u0026#39;, {\u0026#39;class\u0026#39;: \u0026#39;txt_emph1\u0026#39;}) print(f\u0026#39;찾는 단어: {text_search.get_text()}\u0026#39;) 단어의 뜻 출력 # Copy python list_search = web_page.find(\u0026#39;ul\u0026#39;, {\u0026#39;class\u0026#39;: \u0026#39;list_search\u0026#39;}) list_text = list_search.find_all(\u0026#39;span\u0026#39;, {\u0026#39;class\u0026#39;: \u0026#39;txt_search\u0026#39;}) definitions = [definition.get_text() in list_text] print(f\u0026#39;단어의 뜻: {definitions}\u0026#39;) Output\nCopy vim 찾는 단어: happiness 단어의 뜻: [\u0026#39;행복\u0026#39;, \u0026#39;만족\u0026#39;, \u0026#39;기쁨\u0026#39;, \u0026#39;행운\u0026#39;] 영화 정보 출력 # 네이버 영화 URL 불러오기 # Copy python # 찾는 영화 번호 입력 (향후 영화 제목으로 검색 구현) movie = 208077 url = f\u0026#39;https://movie.naver.com/movie/search/result.naver?query={movie}\u0026#39; web = urlopen(url) web_page = BeautifulSoup(web, \u0026#39;html.parser\u0026#39;) 영화 제목 출력 # Copy python title = web_page.find(\u0026#39;h3\u0026#39;, {\u0026#39;class\u0026#39;:\u0026#39;h_movie\u0026#39;}).find(\u0026#39;a\u0026#39;) print(f\u0026#39;Movie Title: {title.get_text()}\u0026#39;) 네이버 영화 배우/제작진 URL 불러오기 # Copy python url = f\u0026#39;https://movie.naver.com/movie/bi/mi/detail.naver?query={movie}\u0026#39; web = urlopen(url) web_page = BeautifulSoup(web, \u0026#39;html.parser\u0026#39;) 감독 이름 출력 # Copy python director = web_page.find(\u0026#39;div\u0026#39;, {\u0026#39;class\u0026#39;:\u0026#39;dir_product\u0026#39;}).find(\u0026#39;a\u0026#39;) print(f\u0026#39;Director: {director.get_text()}\u0026#39;) 출연 배우들 이름 출력 # Copy python actor_list = web_page.find(\u0026#39;ul\u0026#39;, {\u0026#39;class\u0026#39;:\u0026#39;lst_people\u0026#39;}) actor_names = actor_list.find_all(\u0026#39;a\u0026#39;, {\u0026#39;class\u0026#39;:\u0026#39;k_name\u0026#39;}) actors = [actor.get_text() for actor in actor_names] print(f\u0026#39;Actors: {actors}\u0026#39;) Output\nCopy text Movie Title: 스파이더맨: 노 웨이 홈 Director: 존 왓츠 Actors: [\u0026#39;톰 홀랜드\u0026#39;, \u0026#39;젠데이아 콜먼\u0026#39;, \u0026#39;베네딕트 컴버배치\u0026#39;, \u0026#39;존 파브로\u0026#39;, \u0026#39;제이콥 배덜런\u0026#39;, \u0026#39;마리사 토메이\u0026#39;, \u0026#39;알프리드 몰리나\u0026#39;] 티스토리 게시글 출력 및 저장 # 티스토리 게시글 URL 불러오기 # Copy python # 찾는 글 번호 입력 post_number = 22 url = f\u0026#39;https://minyeamer.tistory.com/{post_number}\u0026#39; web = urlopen(url) source = BeautifulSoup(web, \u0026#39;html.parser\u0026#39;) 티스토리 게시글 출력 # Copy python all_text = source.find(\u0026#39;article\u0026#39;,{\u0026#39;class\u0026#39;: \u0026#39;content\u0026#39;}) tags = [\u0026#39;h2\u0026#39;, \u0026#39;h3\u0026#39;, \u0026#39;h4\u0026#39;, \u0026#39;li\u0026#39;, \u0026#39;p\u0026#39;, \u0026#39;blockquote\u0026#39;, \u0026#39;code\u0026#39;] article = all_text.find_all(tags) print(article) 티스토리 게시글 저장 # Copy python from urllib.request import HTTPError for post_number in range(10): try: url = f\u0026#39;https://minyeamer.tistory.com/{post_number}\u0026#39; web = urlopen(url) source = BeautifulSoup(web, \u0026#39;html.parser\u0026#39;) except HTTPError: print(f\u0026#39;{i}번 글에서 에러가 발생했습니다.\u0026#39;) pass with open(\u0026#39;tistory_all.txt\u0026#39;, \u0026#39;a\u0026#39;, encoding = \u0026#39;utf-8\u0026#39;) as f: all_text = source.find(\u0026#39;article\u0026#39;,{\u0026#39;class\u0026#39;: \u0026#39;content\u0026#39;}) tags = [\u0026#39;h2\u0026#39;, \u0026#39;h3\u0026#39;, \u0026#39;h4\u0026#39;, \u0026#39;li\u0026#39;, \u0026#39;p\u0026#39;, \u0026#39;blockquote\u0026#39;, \u0026#39;code\u0026#39;] article = all_text.find_all(tags) for content in article: f.write(content.get_text() + \u0026#39;\\n\u0026#39;) "},{"id":110,"href":"/blog/aischool-03-00-web-crawling/","title":"[AI SCHOOL 5기] 웹 크롤링","section":"Posts","content":"Web Crawling vs Web Scraping # Web Crawling: Bot이 web을 link를 통해 돌아다니는 것 Web Scraping: Webpage에서 원하는 자료를 긇어오는 것 HTML Tags # Tag\u0026rsquo;s Name: html, head, body, p, span, li, ol, ul, div Tag\u0026rsquo;s Attribute: class, id, style, href, src The Process of Web Scraping # URL 분석 (query 종류 등) URL 구성 HTTP Response 얻기 (urlopen(URL) or request.get(URL).content) HTTP source 얻기 (BeautifulSoup(HTTP Response, 'html.parser')) HTML Tag 꺼내기 (.find('tag_name', {'attr_name':'attr_value'})) Tag로부터 텍스트 혹은 Attribute values 꺼내기 (Tag.get_text() or Tag.attrs) The Process of Data Analysis for Text Data # 텍스트 데이터를 str 자료형으로 준비 Tokenize (형태소 분석) POS Tagging (Part-of-speech, 품사 표시) Stopwords 제거 (불용어 제거) 단어 갯수 카운팅 \u0026amp; 단어 사전 생성 단어 사전 기반 데이터 시각화 (+ 머신러닝/딥러닝 모델 적용) TF-IDF # Term Frequency - Inverse Document Frequency 특정 단어가 문서에서 어떤 중요도를 가지는지를 나타내는 지표 많은 문서에 공통적으로 들어있는 단어는 문서 구별 능력이 떨어진다 판단하여 가중치 축소 Count Vectorizer # 단어의 빈도수만을 사용해서 벡터 생성 Document That Nice Car John Has Red A 1 1 1 0 0 0 B 1 0 1 1 1 1 TF-IDF Vectorizer # 단어의 빈도수(TF)를 TF-IDF 값으로 변경하여 가중치가 조정된 벡터 생성 Cosine Similarity # 두 벡터 사이 각도의 코사인 값을 이용하여 두 벡터의 유사한 정도 측정 유사도가 -1이면 서로 완전히 반대되는 경우 유사도가 0이면 서로 독립적인 경우 유사도가 1이면 서로 완전히 같은 경우 텍스트 매칭에 적용될 경우 두 벡터에 해당 문서에서의 단어 빈도가 적용 Embedding # 한국어 임베딩 @ https://j.mp/3mduiBk "},{"id":111,"href":"/blog/aischool-01-03-data-visualization/","title":"[AI SCHOOL 5기] 데이터 분석 실습 - 데이터 시각화","section":"Posts","content":"Visualization Libraries # Plotly Altair Bokeh (Website Graph) @ https://j.mp/30772sU Data Chart Types # Numeric: 숫자 자체에 의미가 있음 (온도 등), 연속형 Categoric: 숫자 너머에 의미가 있음 (성별, 강아지 품종 등), 불연속형 @ https://goo.gl/ErLHCY @ http://j.mp/2JcEENe GeoJSON Data # Copy python import json # 한국의 지도 데이터 참조 # @ https://github.com/southkorea/southkorea-maps geo_path = \u0026#39;skorea_municipalities_geo_simple.json\u0026#39; geo_str = json.load(open(geo_path, encoding=\u0026#39;utf-8\u0026#39;)) JSON(Javascript Object Notation): 데이터 교환을 위한 표준 포맷 GeoJSON: 지도 데이터 포맷 json.load: JSON 파일 불러오기 json.dump: JSON 파일 저장하기 PyPrnt Library # Copy python from pyprnt import prnt prnt(geo_str, truncate=True, width=80) PyPrnt: JSON 구조 파악에 용의한 도구 @ http://j.mp/2WVZuGy Folium Library # Copy python import folium # Folium 공식문서 @ https://goo.gl/5UgneX seoul_map = folium.Map(location=, zoom_start=, tiles=) Folium: 지도 데이터 시각화 라이브러리 localtion: 초기 지도 시작 위치 zoom_start: 초기 지도 확대 정도 tiles: 지도 타입 (default \u0026ldquo;Stamen Terrain\u0026rdquo; or \u0026ldquo;Stamen Toner\u0026rdquo;) 초기 좌표를 [37.5502, 126.982], 확대 정도를 11로 설정하면 서울을 표시 살인사건 발생건수 시각화 # Copy python # Choropleth map @ https://goo.gl/yrTRHU seoul_map.choropleth(geo_data = geo_str, data = gu_df[\u0026#39;살인\u0026#39;], columns = [gu_df.index, gu_df[\u0026#39;살인\u0026#39;]], fill_color = \u0026#39;PuRd\u0026#39;, key_on = \u0026#39;feature.id\u0026#39;) geo_data: GeoJSON 데이터 data: 시각화의 대상이 될 데이터 columns: DataFrame의 index column을 가져와 인식 fill_color: matplolib colormap과 유사 @ http://colorbrewer2.org key_on: GeoJSON 규약을 따름, JSON 파일 feature의 id에 매칭 경찰서별 검거율 점수 계산 # 경찰서별 검거율에 대한 시각화 시 문제점\n경찰서별 검거율의 최대-최소 차이가 17로 매우 적음 검거율을 원형 차트로 만들었을 때 각각의 차이가 적어서 직관적이지 못함 Min-Max Algorithm을 사용하여 검거율 점수 계산\n$$z_i=\\frac{x_i-\\text{min}(x)}{\\text{max}(x)-\\text{min}(x)}$$\nCopy python def re_range(x, oldMin, old_max, new_min, new_max): return (x - old_min)*(new_max - new_min) / (old_max - old_min) + new_min df[\u0026#39;점수\u0026#39;] = re_range(df[\u0026#39;검거율\u0026#39;], min(df[\u0026#39;검거율\u0026#39;]), max(df[\u0026#39;검거율\u0026#39;]), 1, 100) 경찰서별 좌표 데이터 수집 # 강남경찰서 좌표 데이터 내용 확인\nCopy python import googlemaps gmaps = googlemaps.Client(key=\u0026#39;your-api-key\u0026#39;) gangnam_police_map = gmaps.geocode(\u0026#39;서울강남경찰서\u0026#39;, language=\u0026#34;ko\u0026#34;) Copy json [{\u0026#39;address_components\u0026#39;: [{\u0026#39;long_name\u0026#39;: \u0026#39;１１\u0026#39;, \u0026#39;short_name\u0026#39;: \u0026#39;１１\u0026#39;, \u0026#39;types\u0026#39;: [\u0026#39;premise\u0026#39;]}, {\u0026#39;long_name\u0026#39;: \u0026#39;테헤란로114길\u0026#39;, \u0026#39;short_name\u0026#39;: \u0026#39;테헤란로114길\u0026#39;, \u0026#39;types\u0026#39;: [\u0026#39;political\u0026#39;, \u0026#39;sublocality\u0026#39;, \u0026#39;sublocality_level_4\u0026#39;]}, {\u0026#39;long_name\u0026#39;: \u0026#39;강남구\u0026#39;, \u0026#39;short_name\u0026#39;: \u0026#39;강남구\u0026#39;, \u0026#39;types\u0026#39;: [\u0026#39;political\u0026#39;, \u0026#39;sublocality\u0026#39;, \u0026#39;sublocality_level_1\u0026#39;]}, {\u0026#39;long_name\u0026#39;: \u0026#39;서울특별시\u0026#39;, \u0026#39;short_name\u0026#39;: \u0026#39;서울특별시\u0026#39;, \u0026#39;types\u0026#39;: [\u0026#39;administrative_area_level_1\u0026#39;, \u0026#39;political\u0026#39;]}, {\u0026#39;long_name\u0026#39;: \u0026#39;대한민국\u0026#39;, \u0026#39;short_name\u0026#39;: \u0026#39;KR\u0026#39;, \u0026#39;types\u0026#39;: [\u0026#39;country\u0026#39;, \u0026#39;political\u0026#39;]}, {\u0026#39;long_name\u0026#39;: \u0026#39;06175\u0026#39;, \u0026#39;short_name\u0026#39;: \u0026#39;06175\u0026#39;, \u0026#39;types\u0026#39;: [\u0026#39;postal_code\u0026#39;]}], \u0026#39;formatted_address\u0026#39;: \u0026#39;대한민국 서울특별시 강남구 테헤란로114길 11\u0026#39;, \u0026#39;geometry\u0026#39;: {\u0026#39;location\u0026#39;: {\u0026#39;lat\u0026#39;: 37.5094352, \u0026#39;lng\u0026#39;: 127.0669578}, \u0026#39;location_type\u0026#39;: \u0026#39;ROOFTOP\u0026#39;, \u0026#39;viewport\u0026#39;: {\u0026#39;northeast\u0026#39;: {\u0026#39;lat\u0026#39;: 37.5107841802915, \u0026#39;lng\u0026#39;: 127.0683067802915}, \u0026#39;southwest\u0026#39;: {\u0026#39;lat\u0026#39;: 37.5080862197085, \u0026#39;lng\u0026#39;: 127.0656088197085}}}, \u0026#39;partial_match\u0026#39;: True, \u0026#39;place_id\u0026#39;: \u0026#39;ChIJcbaB0UakfDURoyy8orQOWFg\u0026#39;, \u0026#39;plus_code\u0026#39;: {\u0026#39;compound_code\u0026#39;: \u0026#39;G358+QQ 대한민국 서울특별시\u0026#39;, \u0026#39;global_code\u0026#39;: \u0026#39;8Q99G358+QQ\u0026#39;}, \u0026#39;types\u0026#39;: [\u0026#39;establishment\u0026#39;, \u0026#39;point_of_interest\u0026#39;, \u0026#39;police\u0026#39;]}] gmaps.geocode: Google Maps의 Geocoding에 대한 함수, 위도/경도 및 우편번호 등 반환 gmaps.reverse_geocode((lng, lat), lang=): 위도/경도 값으로 주소값 반환 formatted_address: 도로명 주소 반환값 geometry.location: 위도/경도 반환값 (lat / lng) 경찰서별 좌표 데이터 수집\nCopy python lat = [] lng = [] for name in df[\u0026#39;경찰서\u0026#39;]: police_map = gmaps.geocode(name, language=\u0026#39;ko\u0026#39;) police_loc = seoul_police_map[0].get(\u0026#39;geometry\u0026#39;) lat.append(police_loc[\u0026#39;location\u0026#39;][\u0026#39;lat\u0026#39;]) lng.append(police_loc[\u0026#39;location\u0026#39;][\u0026#39;lng\u0026#39;]) df[\u0026#39;lat\u0026#39;] = lat df[\u0026#39;lng\u0026#39;] = lng 경찰서별 검거율 데이터 시각화 # Copy python police_map = folium.Map(location=, zoom_start=) for n in df.index: folium.CircleMarker([df.at[n, \u0026#39;lat\u0026#39;], df.at[n, \u0026#39;lng\u0026#39;]], radius=df.at[n, \u0026#39;점수\u0026#39;]*0.5, # meter 단위 color=\u0026#39;#3186cc\u0026#39;, fill=True, fill_color=\u0026#39;#3186cc\u0026#39;).add_to(map) 시각화된 데이터 종합 # Copy python police_map = folium.Map(location=, zoom_start=) police_map.choropleth(geo_data = geo_str, data = crime_ratio[\u0026#39;전체발생비율\u0026#39;], columns = [crime_ratio.index, crime_ratio[\u0026#39;전체발생비율\u0026#39;]], fill_color = \u0026#39;PuRd\u0026#39;, key_on = \u0026#39;feature.id\u0026#39;) for n in df.index: folium.CircleMarker([df.at[n, \u0026#39;lat\u0026#39;], df.at[n, \u0026#39;lng\u0026#39;]], radius=df.at[n, \u0026#39;점수\u0026#39;]*0.7, color=\u0026#39;#3186cc\u0026#39;, fill=True, fill_color=\u0026#39;#3186cc\u0026#39;).add_to(police_map) Export DataFrame # DataFrame to csv file Copy python df.to_csv(\u0026#39;processed_data.csv\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) DataFrame to Excel file Copy python from pandas import ExcelWriter writer = ExcelWriter(\u0026#39;file_name.xlsx\u0026#39;) df.to_excel(writer) writer.save() Saving a folium map as an HTML file Copy python folium_map.save(\u0026#39;folium_map.html\u0026#39;) HTML 파일로 시각화된 지도 데이터 추출\npolice-map.html GeoJSON Data (Not Simplified) # Copy python geo_path = \u0026#39;skorea-2018-municipalities-geo.json\u0026#39; geo_str = json.load(open(geo_path, encoding=\u0026#39;utf-8\u0026#39;)) features.properties.code에서 서울 내 지역코드는 11로 시작 기존 features.id는 feature.properties.name과 매칭 choropleth 실행 시 key_on에 feature.properties.name 입력 서울 내 지역만 수집 # Copy python in_seoul = [] for feature in geo_str[\u0026#39;features\u0026#39;]: if feature[\u0026#39;properties\u0026#39;][\u0026#39;code\u0026#39;].startswith(\u0026#39;11\u0026#39;): in_seoul.append(feature) 구체적인 데이터로 구현한 지도 # "},{"id":112,"href":"/blog/boj-problems-11650/","title":"[백준 11650] 좌표 정렬하기 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/11650 개요 # 배열 형태의 자료들을 정렬하는 간단한 문제이다. 파이썬에서는 내장 함수 sort()를 사용하면 쉽게 풀 수 있다. 문제 해설 # 문제에서 요구하는 것은 x좌표 값과 y좌표 값으로 구성된 배열들의 리스트를 x 값, y 값 순으로 정렬하는 것이다. 배열의 자료구조는 인덱싱으로 접근이 가능한 것이면 아무거나 상관없기에 좌표 표현에 직관적인 튜플을 사용한다. 정렬의 기준이 반대였으면 람다 식을 써야겠지만 좌표의 위치가 곧 정렬 순서이기 때문에 Key값은 사용하지 않는다. 해설 코드 # Copy python import sys input = sys.stdin.readline points = [] for _ in range(int(input())): points.append(tuple(map(int, input().split()))) for point in sorted(points): print(point[0], point[1]) "},{"id":113,"href":"/blog/aischool-01-02-data-exploration/","title":"[AI SCHOOL 5기] 데이터 분석 실습 - 데이터 탐색","section":"Posts","content":"Visualization Library # Copy python import seaborn as sns sns.heatmap(gu_df[]) Visualization Issues # 한글 데이터 표시 오류 서로 다른 자릿수로 구성된 열에 동일한 스케일 적용 시각화된 테이블 형태의 비직관성 문제 인구수가 고려되지 않은 부정확한 데이터 한글 데이터 시각화 # Copy python matplotlib inline # Windows font_name = font_manager.FontProperties(fname=\u0026#34;C:/~/malgun.ttf\u0026#34;).get_name() rc(\u0026#39;font\u0026#39;, family=font_name) # Mac rc(\u0026#39;font\u0026#39;, family=\u0026#39;AppleGothic\u0026#39;) Feature Scaling/Normalization # Min-Max Algorithm 열에 대한 최솟값(min)을 0, 열에 대한 최댓값(max)를 1로 맞춤 기존 열을 old_x, 새로운 열을 new_x라 할 때,\nnew_x = ( old_x - min(column) ) / ( max(column) - min(column) ) Standardization 열에 대한 평균값(mean)을 0, 열에 대한 표준편차 값(std)를 1로 맞춤 기존 열을 old_x, 새로운 열을 new_x라 할 때,\nnew_x = ( old_x - mean(column) ) / std(column) 표준 점수 (Z-score)와 동일 시각화 개선 # 전체 테이블의 사이즈 조정\npit.figure(figsize = (x, y)) 셀 형식 및 색상 등 변경\nsns.heatmap(norm, annot=, fmt=, linewidths=, cmap=) annot: 셀 내에 수치 입력 여부 (defualt False) fmt: 셀 내에 입력될 수치의 format ('f' == float) linewidths: 셀 간 거리 (내부 테두리) cmap: matplotlib colormap @https://goo.gl/YWpBES 테이블 제목 설정\nplt.tile() 시각화 설정된 테이블 표시\nplt.show() 데이터 정확성 개선 # 범죄 발생 횟수에 인구수 반영 Copy python # 시각화된 데이터에서 열방향(axis=0)을 기준으로 인구수 데이터를 나눔 (인구 10만 단위) crime_ratio = crime_count_norm.div(gu_df[\u0026#39;인구수\u0026#39;], axis=0) * 100000 구별 5대 범죄 발생 수치 평균 계산 Copy python crime_ratio[\u0026#39;전체발생비율\u0026#39;] = crime_ratio.mean(axis=1) 각 사건들의 중형도를 고려하지 못할 수 있음 \u0026lsquo;살인\u0026rsquo; 열에 스케일 적용 중 이미 큰 값이 곱해졌기 때문에 가중치가 적용되었다고도 판단 가능 Improved Visualization # Copy python plt.figure(figsize = (10,10)) sns.heatmap(crime_ratio.sort_values(by=\u0026#39;전체발생비율\u0026#39;, ascending=False), annot=True, fmt=\u0026#39;f\u0026#39;, linewidths=.5, cmap=\u0026#39;Reds\u0026#39;) plt.title(\u0026#39;범죄 발생(전체발생비율로 정렬) - 각 항목을 정규화한 후 인구로 나눔\u0026#39;) plt.show() "},{"id":114,"href":"/blog/aischool-01-01-data-analysis/","title":"[AI SCHOOL 5기] 데이터 분석 실습 - 데이터 분석","section":"Posts","content":"Practice Data # 서울시 범죄현황 통계자료 범죄별로 검거율 계산 # Copy python # gu_df는 실습 자료에 서울시 경찰청의 소속 구 데이터를 추가한 DataFrame gu_df[\u0026#39;강간검거율\u0026#39;] = gu_df[\u0026#39;강간(검거)\u0026#39;]/gu_df[\u0026#39;강간(발생)\u0026#39;]*100 gu_df[\u0026#39;강도검거율\u0026#39;] = gu_df[\u0026#39;강도(검거)\u0026#39;]/gu_df[\u0026#39;강도(발생)\u0026#39;]*100 gu_df[\u0026#39;살인검거율\u0026#39;] = gu_df[\u0026#39;살인(검거)\u0026#39;]/gu_df[\u0026#39;살인(발생)\u0026#39;]*100 gu_df[\u0026#39;절도검거율\u0026#39;] = gu_df[\u0026#39;절도(검거)\u0026#39;]/gu_df[\u0026#39;절도(발생)\u0026#39;]*100 gu_df[\u0026#39;폭력검거율\u0026#39;] = gu_df[\u0026#39;폭력(검거)\u0026#39;]/gu_df[\u0026#39;폭력(발생)\u0026#39;]*100 gu_df[\u0026#39;검거율\u0026#39;] = gu_df[\u0026#39;소계(검거)\u0026#39;]/gu_df[\u0026#39;소계(발생)\u0026#39;]*100 해당 계산법의 문제:\n이전 연도에 발생한 사건이 많이 검거될 경우 검거율이 100%를 초과 발생 건수가 0인 경우 검거율에 결측치(N/A)가 발생 초과된 검거율을 최댓값으로 조정:\nCopy python # 검거율에 해당되는 열의 집합 columns columns = [\u0026#39;강간검거율\u0026#39;, \u0026#39;강도검거율\u0026#39;, \u0026#39;살인검거율\u0026#39;, \u0026#39;절도검거율\u0026#39;, \u0026#39;폭력검거율\u0026#39;] 모든 행에 대해 반복문 실행 Copy python for row_index, row in gu_df_rate.iterrows(): for column in columns: if row[column] \u0026gt; 100: gu_df.at[row_index, column] = 100 Masking 기법 활용 Copy python gu_df[ gu_df[columns] \u0026gt; 100 ] = 100 gu_df[columns] \u0026gt; 100은 True와 False로 이루어진 행렬을 반환 해당 행렬을 gu_df의 Key로 사용하면 True에 해당하는 값이 채로 친듯 솎아 걸러짐 조건이 두 개 이상일 경우 괄호로 감싸주어야 함 Pandas에서는 and, or, not이 동작하지 않기 때문에 \u0026amp;, |, ~ 사용 gu_df[ (gu_df['살인(발생)'] \u0026gt; 7) \u0026amp; (gu_df['폭력(발생)'] \u0026gt; 2000) ] 결측치를 의미있는 값으로 변경:\nCopy python gu_df[\u0026#39;살인검거율\u0026#39;] = gu_df[\u0026#39;살인검거율\u0026#39;].fillna(100) 인구 데이터 Merge # Copy python # 인구 데이터에 해당하는 csv 파일 불러오기 popul_df = pd.read_csv(\u0026#39;pop_kor.csv\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) Pandas Merge Functions # Join: A.join(B) A와 B의 index 열이 동일해야 함 Merge: pd.merge(A, B, left_on=, right_on=, how=) DataFrame A의 기준 left_on과 DataFrame B의 기준 right_on을 비교 how 옵션에는 inner (교집합), full_outer (합집합),\nleft_outer (A 기준 합), right_outer (B 기준 합) 가능 Concatenate: pd.concat([A, B], axis=) 무조건 갖다 붙이기 때문에 사용에 주의 Additional Pandas Functions # df.iterrow(): 반복문을 사용해 모든 행을 참조할 때 사용, (행 이름, Series) 반환 df.at[]: DataFrame에서 단일 값 추출 (단일 인덱싱에서 df.loc[]보다 빠름) df.[].fillna(): 결측치(N/A)를 의미있는 값으로 바꿈 (Series가 가지고 있는 함수) df[df[]].str.contains()]: 특정 문자가 포함된 행을 표시 Setting Index pd.read_csv('pop_kor.csv', encoding='utf-8', index_col=) pd.read_csv('pop_kor.csv', encoding='utf-8').set_index() "},{"id":115,"href":"/blog/aischool-01-00-data-analysis/","title":"[AI SCHOOL 5기] 데이터 분석","section":"Posts","content":"Data Types # Structured Data Relational Database Spread Sheets Semi-structured Data System Logs Sensor Data HTML Unstructured Data Image / Video Sound Document Data Collection Tools # Logstash: 로그 데이터 (SQL 구조화) Elasticsearch: 데이터가 자유로움 Kibana: 그래프 자동화 Elastic Stack, Zepplin API Meanings # 웹 상에서의 API 라이브러리/프로그램 도구 (텐서플로우에서의 함수 등) Open API # 공익적인 목적 서비스 활성화 목적 (서드파티 앱 지원) SNS에서 무분별한 크롤링으로 인한 서버 과부하 대비 Missing Data Handling # 랜덤하게 채워넣기 주변 (행의) 값들로 채워넣기 열의 대푯값을 계싼해서 채워넣기 (mea, median) 전체 행들을 그룹으로 묶어낸 후 그룹 내 해당 열의 값을 예측해 채워넣기 나머지 열들로 머신러닝 예측모델을 만든 후 해당 열의 값을 예측해 채워넣기 특정 기준 비율 이상으로 빠져있을 시 해당 열 삭제 Pandas Functions # Referring # df = pd.read_excel(): 엑셀 파일 열기 (엑셀 파일 원본은 행과 열로 구성된 pandas.DataFrame() 타입) df.head(): 위에서부터 값을 참조 (default 5) df.tail(): 밑에서부터 값을 참조 (default 5) df.describe(): 기술 통계량 반환 (평균, 최솟값 등) df.info(): DataFrame 정보 반환 (Non-Null 행에서 유효성 확인) df.loc[row]: DataFrame에서 행 꺼내기 (추가로 column도 지정 가능) df.iloc[row]: DataFrame에서 인덱스 번호를 기준으로 행 꺼내기 df[column]: DataFrame에서 열 꺼내기 df[column].apply(lambda x: x+1): 특정 열에 속한 값에 1씩 더해서 반환 Modifiying # df.drop([row]): DataFrame에서 행 삭제 del df[column]: DataFrame에서 열 삭제 df.rename(columns=, inplace=True):\nDataFrame에서 열 이름 바꾸기 (inplace 옵션은 덮어쓰기를 의미) df.sort_values(by=, inplace=True):\nDataFrame에서 열 내부의 값을 정렬 (내림차순 정렬 시 ascending=False 옵션 추가) pd.pivot_table(df, index=, aggfunc=np.mean):\n기존 DataFrame에서 특정 행을 index로 설정한 새로운 DataFrame 생성 (피벗 테이블) aggfunc 옵션에 계산식을 넣을 수 있음 (count, np.sum 등) Copying # df_2 = df: 얕은 복사 (원본 변경 시 복사본도 같이 변경) df_3 = df.copy(): 깊은 복사 (원본 변경이 복사본에 영향을 미치지 않음) "},{"id":116,"href":"/blog/boj-problems-4949/","title":"[백준 4949] 균형잡힌 세상 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/4949 개요 # 스택을 이용하여 풀 수 있는 문제이다. 문자열 처리에 관한 능력이 추가로 요구된다. 최대 입력 크기가 정해지지 않았기에 시간 복잡도는 무시한다. 문제 해설 # 해당 문제에서 고려해야할 문자는 종료 조건인 점(\u0026rsquo;.\u0026rsquo;)을 제외하면 소괄호와 대괄호 뿐이다. 균형잡힌 문장의 구분 여부는 1. 닫힌 괄호가 열린 괄호보다 앞에 나온 경우 2. 열린 괄호가 안 닫힌 경우로 판단했다. 문자 하나하나마다 확인하며 괄호를 골라낼 수도 있지만 이번엔 정규식을 사용해본다. 우선 정규식 라이브러리인 re에 속한 sub 메서드를 사용해 괄호를 제외한 모든 문자를 제거한다. 나머지 문자에 대해 for문을 돌려 열린 괄호면 스택에 추가, 닫힌 괄호면 스택에 남은 값을 뺀다. 단, 닫힌 괄호의 경우 스택에 열린 괄호가 없거나 스택 맨 위의 값이 다른 종류의 괄호면 균형이 깨졌다 판단한다. 코드의 중복을 발생시키지 않기 위해 균형이 깨진 경우를 IndexError의 발생으로 통일하고\n이 때 조건 변수를 재설정하고 반복문을 중지시킨다. 반복문이 종료된 후 스택과 조건 변수에 대한 NAND 결과를 통해 균형잡힌 문장의 여부를 판단하여 결과를 출력한다. 해설 코드 # Copy python import re match = {\u0026#39;)\u0026#39;: \u0026#39;(\u0026#39;, \u0026#39;]\u0026#39;: \u0026#39;[\u0026#39;} while True: balanced_stack = [] unbalanced = False sentence = input() if sentence == \u0026#39;.\u0026#39;: break sentence = re.sub(\u0026#39;[^\\(\\)\\[\\]]+\u0026#39;, \u0026#39;\u0026#39;, sentence) for bracket in sentence: if bracket in {\u0026#39;(\u0026#39;, \u0026#39;[\u0026#39;}: balanced_stack.append(bracket) else: try: if balanced_stack[-1] == match[bracket]: balanced_stack.pop() else: raise IndexError except IndexError: unbalanced = True break if not(balanced_stack or unbalanced): print(\u0026#39;yes\u0026#39;) else: print(\u0026#39;no\u0026#39;) "},{"id":117,"href":"/blog/boj-problems-2164/","title":"[백준 2164] 카드2 (Python)","section":"Posts","content":"문제 링크 # https://www.acmicpc.net/problem/2164 개요 # 큐를 이용하여 풀 수 있는 간단한 문제이다. 양쪽에서 데이터를 빼고 집어넣는 작업이 요구되기 때문에 deque의 사용을 권장한다. 1번 카드의 위치를 앞으로 하냐 뒤로 하냐는 크게 상관없기 때문에 앞에서부터 정의하겠다. 문제 해설 # 문제에서 제시된 행동은 1. 제일 위의 카드를 버린다 2. 제일 위에 남은 카드를 제일 아래로 옮긴다 이다. 해당 행동을 카드가 한 장이 남을 때까지 무한히 반복하면 된다. 1번 행동을 하기 위해선 1번 카드를 큐의 맨 앞으로 정했기에 큐의 왼쪽에서 값을 빼내면 된다. 큐의 왼쪽에서 값을 빼내기 위해 popleft() 함수를 사용한다. 2번 행동은 마찬가지로 큐의 왼쪽에서 값을 빼내고, 추가로 빼낸 값을 맨 뒤에 추가한다. 큐의 왼쪽에서 빼낸 값을 다시 넣어야 하기 때문에 append() 안에 popleft()를 넣어준다. 두 가지 동작을 while문 안에 넣고 카드가 1개보다 많이 남으면 반복하도록 조건을 설정한다. while문이 종료된 후 하나의 값이 남아있는 큐에서 pop()을 사용해 마지막 남은 카드를 출력한다. 시간 복잡도 # 문제에서 주어진 시간은 2초다. 가장 단순하게 리스트로 큐를 구현할 경우 del() 함수를 이용해 값을 삭제해야 할 것이고\n이를 N번 만큼 반복할 것이므로 이 때의 시간 복잡도는 O(N^2)를 초과한다. N의 최댓값이 500,000이므로 대충 어림잡아도 연산 횟수가 2억을 훌쩍 초과한다. 반면, deque로 큐를 구현한 해설의 경우 시간 복잡도가 O(1)인 popleft()를\nN번 만큼 반복하기 때문에 O(N)의 시간 복잡도를 가진다. 이는 제한 시간 내에 충분히 수행하고도 여유가 남을 알고리즘이다. 해설 코드 # Copy python from collections import deque N = int(input()) cards = deque([i for i in range(1, N+1)]) while len(cards) \u0026gt; 1: cards.popleft() cards.append(cards.popleft()) print(cards.pop()) "},{"id":118,"href":"/blog/big-o-list/","title":"Big-O List","section":"Posts","content":"List # Operation Example Big-O Index l[i] O(1) Store l[i] = 0 O(1) Length len(l) O(1) Append l.append(x) O(1) Pop l.pop() O(1) Slice l[a:b] O(b-a) Construction list(x) O(len(x)) Check l1 == l2 O(len(n)) Insert l[a:b] = x O(n) Containment x in l O(n) Copy l.copy() O(n) Remove l.remove() O(n) Count l.count(x) O(n) Index l.index(x) O(n) Pop l.pop(i) O(n) Extreme value min(l)/max(l) O(n) Iteration for v in l: O(n) Reverse l.reverse() O(n) Sort l.sort() O(n Log n) Multiply k * l O(k n) Set # Operation Example Big-O Length len(s) O(1) Add s.add(x) O(1) Containment x in s O(1) Remove s.remove() O(1) Pop s.pop() O(1) Construction set(x) O(len(x)) Check s1 == s2 O(len(s)) Union s + t O(len(s)+len(t)) Intersection s \u0026amp; t O(len(s)+len(t)) Difference s - t O(len(s)+len(t)) Symmetric Diff s ^ t O(len(s)+len(t)) Iteration for v in s: O(n) Copy s.copy() O(n) Dictionary # Operation Example Big-O Index d[k] O(1) Store d[k] = v O(1) Length len(d) O(1) Pop d.pop() O(1) View d.keys() O(1) Construction dict(x) O(len(x)) Iteration for k in d: O(n) Sort # Method Best Average Worst Insertion Sort O(n) O(n^2) O(n^2) Selection Sort O(n^2) O(n^2) O(n^2) Bubble Sort O(n^2) O(n^2) O(n^2) Shell Sort O(n) O(n^1.5) O(n^1.5) Quick Sort O(n log n) O(n log n) O(n^2) Heap Sort O(n log n) O(n log n) O(n log n) Merge Sort O(n log n) O(n log n) O(n log n) Radix Sort O(dn) O(dn) O(dn) Search # Method Search Insert Delete Sequential O(n) O(1) O(n) Binary O(log n) O(log n + n) O(log n + n) Binary Search Tree (Balanced) O(log n) O(log n) O(log n) Binary Search Tree (Left-Associative) O(n) O(n) O(n) Hashing (Best) O(1) O(1) O(1) Hashing (Worst) O(n) O(n) O(n) Heap # Operation Example Big-O Push heapq.heappush(heap, x) O(log n) Pop heapq.heappop(heap) O(log n) Construction heapq.heapify(heap) O(n) DFS/BFS # N은 노드, E는 간선일 때\n인접 리스트: O(N+E) 인접 행렬: O(N^2) "},{"id":119,"href":"/blog/aischool-00-02-python-advanced/","title":"[코드라이언] 파이썬 심화","section":"Posts","content":"Crawling # 크롤러는 웹 페이지의 데이터를 모아주는 소프트웨어 크롤링은 크롤러를 사용해 웹 페이지의 데이터를 추출해 내는 행위 Request # request 모듈의 get() 함수는 서버에게 html 정보를 요청 get() 함수는 url, 파라미터 값을 받고 request.Response를 반환 정상적인 응답을 받을 경우 Response [200] 반환 응답값을 reponse 변수에 넣고 response.text를 출력하면 html 코드 출력 BeautifulSoup # bs4 모듈의 BeautifulSoup 기능은 입력값을 의미있는 데이터로 변환 Copy python soup = BeautifulSoup(response.text, \u0026#39;html.parser\u0026#39;) soup.title # html 코드에서 title에 해당하는 태그를 반환 soup.title.string # title 태그에서 문자열 값만 뽑아 반환 soup.findAll(\u0026#39;span\u0026#39;) # 모든 span 태그를 반환 soup.findAll(\u0026#39;a\u0026#39;, \u0026#39;link_favorsch\u0026#39;) # link_favorsch 클래스만 반환 Copy python results = soup.findAll(\u0026#39;a\u0026#39;, \u0026#39;link_favorsch\u0026#39;) result.get_text() # result에서 태그를 제외하고 텍스트만 반환 File # open(file, mode): 파일을 생성, 참조, 수정할 때 사용하는 라이브러리 mode에는 r (read), w (write), a (append)가 있음 Copy python file = open(\u0026#34;rankresult.txt\u0026#34;, \u0026#34;w\u0026#34;) file.write(result.get_text()+\u0026#34;\\n\u0026#34;) Copy python file = open(\u0026#34;rankresult.txt\u0026#34;, \u0026#34;a\u0026#34;) file.write(result.get_text()+\u0026#34;\\n\u0026#34;) API # API는 누군가가 만든 프로그램을 가져와서 사용할 때 필요한 인터페이스 API Key는 API를 누가 사용하는지 알 수 있는 키 OpenWeatherMap API # Current Weather Data API 사용 https://api.openweathermap.org/data/2.5/weather?q={city name}\u0026amp;appid={API key} f-string을 이용하여 city name과 API Key를 변수로 설정 requests.get(api)로 API 요청 API 파라미터에 lang을 kr로 추가하여 반환값을 한국어로 변경 API 파라미터에 units를 metric으로 추가하여 온도 단위르 섭씨로 변경 JSON # 자바스크립트의 오브젝트에 따르는 문자 기반 데이터 포맷 json.loads(str)로 문자열을 JSON (딕셔너리 타입)으로 변경 Translator # googletrans: 언어 감지 및 번역을 도와주는 라이브러리 googletrans의 Translator를 import하고 Translator()로 Translator 생성 Copy python \u0026gt;\u0026gt;\u0026gt; translator.detect(sentence) # 문장에 대한 언어 감지 결과를 반환 \u0026gt;\u0026gt;\u0026gt; Detected(lang=ko, confidence=1.0) \u0026gt;\u0026gt;\u0026gt; translator.translate(sentence, \u0026#39;en\u0026#39;) # 문장에 대한 변역 결과를 반환 \u0026gt;\u0026gt;\u0026gt; Translated(src=ko, dest=en, text=Hello, ... Mail # IMAP # 다른 메일 서버에서 보낸 메일을 클라이언트에게 보내기 위한 프로토콜 SMTP # 간단하게 메일을 보내기 위한 프로토콜 SMTP 메일 서버를 연결한다. (smtp.gmail.com:465) Copy python smtp = smtplib.SMTP_SSL(SMTP_SERVER, SMTP_PORT) SMTP 메일 서버에 로그인한다. Copy python smtp.login(\u0026#39;MAIL_ADDRESS\u0026#39;, \u0026#39;PASSWORD\u0026#39;) SMTP 메일 서버에 메일을 보내고 연결을 끊는다. Copy python smtp.send_message() smtp.quit() MIME # 전자우편을 위한 인터넷 표준 포맷 email.message 모듈의 .EmailMessage기능 사용 MIME의 Header에는 Subject, To 등이 존재 이메일을 만든다. Copy python message = EmailMessage() 이메일에 내용을 담는다. Copy python message.set_content(\u0026#39;content\u0026#39;) 발신자, 수신자를 설정한다. Copy python message[\u0026#39;Subject\u0026#39;] = \u0026#39;subject\u0026#39; message[\u0026#39;from\u0026#39;] = \u0026#39;user1@gmail.com\u0026#39; message[\u0026#39;To\u0026#39;] = \u0026#39;user2@gmail.com Attach Image # Read Image Copy python with open(\u0026#39;codelion.png\u0026#39;,\u0026#39;rb\u0026#39;) as image: image_file = image.read() Attach Image Copy python # 이미지 파일 첨부 (메일 형식이 mixed로 바뀜) add_attachment(image, maintype=\u0026#39;image\u0026#39;, subtype=\u0026#39;png\u0026#39;) 이미지 파일의 확장자를 판단 Copy python imghdr.what(\u0026#39;filename\u0026#39;, image) Validation # 정규표현식\n^①[a-zA-Z0-9.+_-]+@②[a-zA-Z0-9]+③\\.[a-zA-z]{2,3}$\n① [a부터 z까지, A부터 Z까지, 0부터 9까지, . , + , _ , -] | +: 1회 이상 반복\n② @ | [a부터 z까지, A부터 Z까지, 0부터 9까지] | +: 1회 이상 반복\n③ . (개행문자 ) | [a부터 z까지, A부터 Z까지] | {2,3}: 최소 2회, 최대 3번 반복 Copy python re.match(reg, \u0026#39;example@gmail.com\u0026#39;) 적합하지 않은 이메일 형식일 경우 None을 반환 if문을 사용해 None이 아니면 메일을 보내도록 설정 Others # 함수: 입력한 값을 사용해 결과물을 만들어 반환하는 조립기 모듈: 함수들을 모아놓은 파일 type(): 객체의 타입을 반환 datetime.today().strftime(\u0026quot;%Y년 %m월 %d일\u0026quot;): 오늘의 날짜 반환 로봇이 아님을 알리는 헤더\nCopy python headers = {\u0026#39;User-Agent\u0026#39;:\u0026#39;Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36\u0026#39;} "},{"id":120,"href":"/blog/aischool-00-01-python-basic/","title":"[코드라이언] 파이썬 기초","section":"Posts","content":"for문 # 문장을 여러 번 실행할 떄 복사 붙여넣기로 길게 늘이지 않고 단순하게 표현하기 위한 구문 for문에 적용되는 문장은 들여쓰기를 해야 함 Copy python for _ in range(30): print(random.choice([\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;])) while문 # for문과 마찬가지로 문장을 반복실행할 수 있는 구문 조건을 충족할 경우 반복을 멈춤 True를 조건으로 사용 시 무한루프 발생 while True: break 명령어를 통해 반복문 종료 가능 변수 # 객체에 이름표를 붙이고 이름표가 불리면 내용물인 객체를 반환\nlunch = random.choice([\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;]) 딕셔너리 # \u0026ldquo;xx은 xx이다\u0026quot;를 코드로 표현한 자료구조 딕셔너리의 get 명령어는 Key에 해당하는 값을 반환 값을 추가할 때는 dict[a] = b 형식으로 추가 딕셔너리의 clear 명령어는 딕셔너리 내용을 초기화 집합 # 중복된 값을 제거하여 표현하는 자료구조 set()으로 집합 생성 합집합: set1 | set2 교집합: set1 \u0026amp; set2 차집합: set1 - set2 조건문 # 상황에 따른 처리를 하기 위한 구문 if 조건:으로 조건문 선언 같은 경우를 구할 땐 a == b 나머지 경우에 대해서는 else 사용 pip/conda # pip: 파이썬에서 지원받는 패키지만을 가져옴 (라이브러리만 맞으면 설치) conda: 아나콘다에서 지원받는 패키지만을 가져옴 (아나콘다에서 유리) conda의 장점: 기존 Python 및 라이브러리 버전 충돌을 체크함 conda의 단점: 설치 속도가 너무 느림 설치가 너무 느리거나 다른 라이브러리에 대한 영향이 없을 경우 pip 사용 라이브러리 참조 파일 생성 시 pip install -r requirements.txt 기타 명령어 # random.choice(): 리스트 안에서 랜덤한 객체 하나를 반환 time.sleep(): 입력값만큼의 시간(초) 동안 딜레이 발생 len(): 리스트/딕셔너리의 목록 개수 반환 "},{"id":121,"href":"/blog/ai-school-00-00-start/","title":"[AI SCHOOL 5기] 첫 주차","section":"Posts","content":"AI SCHOOL 지원 과정 # 아직 군에 복무 중이던 시절, 전역한 후 바로 취업하기 위해 국비, 부트캠프 과정을 탐색하던 중 AI SCHOOL을 발견했다. 이떄 개인적으로 가격 비교, 사용자 맞춤 추천 등의 기능을 포함한 서비스를 구상하고 있었는데 AI 기술이 바로 그것이었다. AI SCHOOL과 함께 눈에 들었던 게 SW마에스트로였지만 5월까지는 군인 신분인 나와는 맞지 않아 아쉽게 포기했다. AI SCHOOL의 지원 과정은 서류(자기소개서)와 과제(영상) 순으로 진행되었다. 영상을 찍어야 할 때 아직 군대 안에 있었기에 어려웠지만 모종의 방법으로 촬영에 성공했다. 이때 처음으로 영상 편집 프로그램 중 다빈치 리졸브를 사용했는데 꽤 재미있었다. 다행히 AI SCHOOL에 합격했고 조기전역 후 곧바로 데스크 세팅에 들어갔다. 첫 주차 온라인 강의 # AI SCHOOL 첫 날에 오리엔테이션을 진행하고 이 날을 포함한 4일 동안 온라인 강의를 수강했다. 일단 만드는 PYTHON, [기초] 같이 푸는 PYTHON 터미널에서 실행할 수 있는 간단한 기능을 파이썬 기초 문법만을 사용해 구현하는 강의다. 리스트, 딕셔너리, 집합 등의 자료구조와 for문, while문 등의 반복문을 배울 수 있었다. [심화] 같이 푸는 PYTHON 웹을 통해 실행할 수 있는 기능을 파이썬을 통해 구현하는 강의다. 함수나 클래스 등을 배우지 않고 바로 넘어가는 느낌은 있었지만 속도감 있어서 좋았다. 크롤링, API, 구글 번역기, 메일 전송 등의 기술을 다뤘다. 파이썬으로는 알고리즘 밖에 안해봤기에 새로운 라이브러리와 마주하여 굉장히 즐겁게 따라하면서 배웠다. 이번 주는 아직 전역 후 데스크 및 물건 정리 등으로 많이 바빠 차분히 공부하지 못했던게 아쉬웠다. 다행히 금요일 전까지 데스크 세팅은 맞춰서 정상적으로 줌에 참여할 수 있었다. 줌을 통한 첫 번째 수업은 주피터 노트북의 사용법을 익히고 앞서 온라인 강의를 통해 배웠던 파이썬 문법을 다듬었다. 1회차 챕틀리 및 특강 # 코드라이언 온라인 강의가 끝난 시점부터 1회차 챕틀리(도전과제)가 진행되었다. 딥러닝에 대한 이해 전무한 나로서 numpy 라이브러리를 사용하는 과제는 많이 생소했다. 어찌저찌 검색하며 해결할 순 있었고 검색 중에 슬쩍 본 퍼셉트론 등의 개념에 흥미가 생겼다. 토요일 오후에 챕틀리 해설을 위한 특별 강의가 진행되었다.\n(수강생들을 위해 주말에도 업무하시는 매니저님들께 감사를 표합니다.) 초청 강사 분을 통해 진행된 특별강의에서는 numpy 라이브러리를 다루는 법에 대해 배웠다. 이미 고생하면서 찾아본 내용을 복습하는 기분이었지만 제출한 코드에 문제가 없음을 확인할 수 있었다. "},{"id":122,"href":"/blog/2022-03-06/","title":"2022-03-06 Log","section":"Posts","content":"defaultdict() # collections 모듈에 포함된 dict의 서브 클래스 dict와 작동 방식은 동일하지만 인자로 주어진 객체의 기본값을 초기값으로 지정 가능 Copy python \u0026gt;\u0026gt;\u0026gt; int_dict = defaultdict(int) \u0026gt;\u0026gt;\u0026gt; int_dict \u0026gt;\u0026gt;\u0026gt; defaultdict(\u0026lt;class \u0026#39;int\u0026#39;\u0026gt;, {}) int를 인자로 넣을 경우 값을 지정하지 않은 키는 그 값이 0으로 지정됨 Copy python \u0026gt;\u0026gt;\u0026gt; int_dict[\u0026#39;key1\u0026#39;] 0 \u0026gt;\u0026gt;\u0026gt; int_dict defaultdict(\u0026lt;class \u0026#39;int\u0026#39;\u0026gt;, {\u0026#39;key\u0026#39;: 0}) infinite # 양의 무한대 float('inf') 음의 무한대 float('-inf') Prim\u0026rsquo;s Algorithm # 시작 정점을 선택한 후, 정점에 인접한 간선 중 최소 비용의 간선을 연결하여\n최소 신장 트리(MST)를 확장해가는 방식 Kruskal\u0026rsquo;s Algorithm이 비용이 가장 작은 간선부터 다음 간선을 선택하는데 반해,\nPrim\u0026rsquo;s Algorithm은 특정 정점에서부터 다음 정점을 갱신해나가며 비용이 작은 간선을 선택 Prim\u0026rsquo;s Algorithm의 시간 복잡도는 최악의 경우 O(E log E)\n(while 구문에서 모든 간선에 대해 반복하고, 최소 힙 구조를 사용) Reference: www.fun-coding.org/Chapter20-prim-live.html 파이썬 구현 코드\nCopy python def prim(edge_list: list, start_node: int) -\u0026gt; list: mst = list() adjacent_edge_list = defaultdict(list) for weight, n1, n2 in edge_list: adjacent_edge_list[n1].append((weight, n1, n2)) adjacent_edge_list[n1].append((weight, n2, n1)) connected_nodes = {start_node} candidate_edge_list = adjacent_edge_list[start_node] heapq.heapify(candidate_edge_list) while candidate_edge_list: weight, n1, n2 = heapq.heappop(candidate_edge_list) if n2 not in connected_nodes: connected_nodes.add(n2) mst.append((weight, n1, n2)) for edge in adjacent_edge_list[n2]: if edge[2] not in connected_nodes: heapq.heappush(candidate_edge_list, edge) return mst Prim\u0026rsquo;s Algorithm 개선 # 간선이 아닌 노드를 중심으로 우선순위 큐를 적용 노드마다 Key 값을 가지고 있고, Key 값을 우선순위 큐에 넣음 Key 값이 0인 정점의 인접한 정점들에 대해 Key 값과 연결된 비용을 비교하여\nKey 값이 작으면 해당 정점의 Key 값을 갱신 개선된 Prim\u0026rsquo;s Algorithm의 시간 복잡도는 O(E log V) 해당 알고리즘을 구현하기 위해 heapdict 라이브러리 사용\n(기존의 heap 내용을 업데이트하면 알아서 최소 힙의 구조로 업데이트됨) 파이썬 구현 코드\nCopy python from heapdict import heapdict def prim(graph: dict, start_node: int) -\u0026gt; (list, int): mst, keys, pi, total_weight = list(), heapdict(), dict(), 0 for node in graph.keys(): keys[node] = float(\u0026#39;inf\u0026#39;) pi[node] = None keys[start_node], pi[start_node] = 0, start_node while keys: current_node, current_key = keys.popitem() mst.append([pi[current_node], current_node, current_key]) total_weight += current_key for adjacent, weight in graph[current_node].items(): if adjacent in keys and weight \u0026lt; keys[adjacent]: keys[adjacent] = weight pi[adjacent] = current_node return mst, total_weight struggling with a problem # 백준 골드 5를 혼자서 푼 후 기고만장해져서 골드 4의 1197번 문제에 도전해보았다. 이틀에 걸쳐 도전했지만 포기하고 정답을 보게되었음에도 문제를 해결한 것 같지 않다. 해당 문제는 n개의 정점들에 대한 간선들 중에서 가장 가중치가 작은 경로의 가중치를 찾는 것이다. 처음엔 노드하면 DFS와 BFS 밖에 몰랐기 때문에 당연하게 DFS로 접근했다:\n먼저 부모, 자식, 가중치, 인덱스를 변수로 가지는 Node 클래스를 선언하여\n간선의 정보를 노드 내 인스턴스 변수에 저장하게 한다. 전체 노드 중 자식 노드를 가진 노드에 한해 가중치 최솟값을 구하는 함수를 실행한다. 해당 함수는 root에서부터 end-point까지 순회하면서 가중치 합의 최솟값을 구하는 동작을 수행한다. 함수의 결과는 따로 반환되지 않고 root 노드의 인스턴스 변수에 저장된다. 이러한 논리를 가지고 작성한 알고리즘이 글 밑에 있는 첫 번째 코드이다. 하지만 해당 코드는 1초의 시간 제한 안에 돌아가기엔 무리가 있었다. DFS로 안된다는 것을 깨닫고 질문글을 훑어본 후 크루스칼 알고리즘을 선택하기로 했다:\n우선 고려해야될 것은 크루스칼 알고리즘이 모든 노드를 연결시키기 위한 알고리즘이라는 것이다. 해당 문제는 root 노드에서부터 시작하는 모든 경로를 고려해야 하는데 크루스칼 알고리즘을\n사용할 경우 가장 작은 가중치로 시작하는 경로만을 선택하고 나머지를 무시하게 된다. 이 경우 발생하는 반례가 다음과 같다. Copy bash 3 3 1 2 2 1 3 3 2 3 9999 output: 10001 answer: 3 크루스칼 알고리즘에 의해 1 -\u0026gt; 2의 간선을 선택하고 1 -\u0026gt; 3의 간선을 무시할 경우\n최종적으로는 1 -\u0026gt; 2 -\u0026gt; 3의 경로에 대한 가중치 10001을 결과로 얻게 된다. 이에 대한 해결책으로 생각한 것이 EtherChannel의 Active/Passive 개념이다. 앞서 시도한 DFS 기반 알고리즘에 크루스칼 알고리즘을 조합해서 모든 경로를 탐색하는데\n가중치가 가장 작은 경로로 이어지는 자식 노드를 Active로, 나머지를 Passive로 분류한다. 만약 한 노드에 새로운 자식 노드가 추가되면 자식 노드들의 가중치를 비교해서 Active를 갱신하고\n해당 노드의 부모 노드를 타고 올라가며 동일한 작업을 반복한다. 해당 알고리즘은 root 노드에서부터 모든 자식 노드를 탐색해야 했던 DFS 기반 알고리즘과는 반대로\n자식 노드에서부터 root 노드까지의 경로만을 탐색하기 때문에 시간 초과를 피할 수 있었다. 하지만 여러 조건들을 고려하다보니 작성자인 나조차도 알아보기 힘들정도로 코드가 많이 복잡해졌고\nroot 노드가 기준인데 굳이 아래서부터 위를 탐색하는 방식이 마음에 들지 않았다. 그리고 가장 큰 문제는 해당 알고리즘에도 반례가 있어서 정답이 될 수 없었다는 것이다. 하루동안 고민한 끝에 크루스칼 알고리즘을 포기하고 이와 비슷하다는 프림 알고리즘을 선택하게 되었다:\n이제까지 사용했던 Node 인스턴스 내에 모든 정보를 저장하는 접근방식을 버리고\n프림 알고리즘의 기본에 집중했다. 부모 노드의 값을 자식 노드의 배열 값에 저장하는 Union-Find 알고리즘을 기반으로 그래프를 그리고\n모든 노드에 대해 프림 알고리즘을 수행하여 최소 가중치를 구하는 방식을 구상했다. 하지만 이 경우에 두 가지 문제점이 있었다. 프림 알고리즘도 결국 모든 노드를 연결하기 위한 알고리즘이기 때문에,\nroot에서 end-point까지 갔다 하더라도 거기서 멈추지 않고 다른 경로를 탐색하는 문제가 생긴다. 해당 문제에 대한 해결책으로 Find 연산을 응용한 깊이 탐색 과정을 추가했다. 매 반복마다 현재 노드에 대해 Find 연산을 수행하고 재귀한 횟수 반환하여 깊이로 지정한다. 깊이가 지속적으로 증가하지 않을 경우 end-point까지 도달했다 판단하여 반복을 멈춘다. 모든 경로의 깊이가 1일 경우 1번 조건을 무시하고 다른 경로를 탐색하는 문제가 있다. root 노드에서 시작했는데 다시 root 노드로 돌아올 경우 해당 노드 자체를 무시한다. 위 조건에 걸릴 경우 양의 무한대 값을 반환하여 가중치 판단 과정에서 제외시킬 수 있었다. 이렇게 많은 시행착오를 거쳤지만 하나를 해결하면 다른 빈틈이 생겨버려 포기할 수밖에 없었다. 심지어 백준에서는 heapdict 모듈을 지원하지 않아 해당 알고리즘을 활용할 수도 없었다. 언젠가 이 문제를 완벽하게 해결하기 위해 디버그 값을 남긴다. Copy bash 3 3 1 2 2 1 3 3 2 3 9999 graph = {1: {2: 1, 3: 3}, 2: {1: 1, 3: 2}, 3: {2: 2, 1: 3}} mst1 = [[1, 1, 0], [1, 2, 1], [2, 3, 2]], weight: 3 mst2 = [[2, 2, 0], [2, 1, 1], [2, 3, 2]], weight: 3 mst3 = [[3, 3, 0], [3, 2, 2], [2, 1, 1]], weight: 3 output: 3 answer: 3 Copy bash 6 8 1 3 -1 1 5 3 1 6 2 2 5 5 2 6 6 3 4 9 3 5 -1 5 6 -1 graph = {1: {3: -1, 5: 3, 6: 2}, 3: {1: -1, 4: 9, 5: -1}, 5: {1: 3, 2: 5, 3: -1, 6: -1}, 6: {1: 2, 2: 6, 5: -1}, 2: {5: 5, 6: 6}, 4: {3: 9}} mst1 = [[1, 1, 0], [1, 3, -1], [3, 5, -1], [5, 6, -1], [5, 2, 5], [3, 4, 9]], w = 11 mst2 = [[3, 3, 0], [3, 5, -1], [5, 6, -1], [3, 1, -1], [5, 2, 5], [3, 4, 9]], w = 11 mst3 = [[5, 5, 0], [5, 6, -1], [5, 3, -1], [3, 1, -1], [5, 2, 5], [3, 4, 9]] 11 mst4 = [[6, 6, 0], [6, 5, -1], [5, 3, -1], [3, 1, -1], [5, 2, 5], [3, 4, 9]] 11 mst5 = [[2, 2, 0], [2, 5, 5], [5, 6, -1], [5, 3, -1], [3, 1, -1], [3, 4, 9]] 11 mst6 = [[4, 4, 0], [4, 3, 9], [3, 5, -1], [5, 6, -1], [3, 1, -1], [5, 2, 5]] 11 output: 11 answer: -3 Copy bash 3 3 1 2 2 1 3 3 2 3 9999 graph = {1: {2: 2, 3: 3}, 2: {1: 2, 3: 9999}, 3: {1: 3, 2: 9999}} mst1 = [[1, 1, 0], [1, 2, 2], [1, 3, 3]], weight = 5 mst2 = [[2, 2, 0], [2, 1, 2], [1, 3, 3]], weight = 5 mst3 = [[3, 3, 0], [3, 1, 3], [1, 2, 2]], weight = 5 output: 5 answer: 3 결론:\n해당 문제에 대한 정답을 찾아본 결과 프림 알고리즘을 heapdict 없이 구현한 알고리즘을 보았는데\n노드에 대한 방문 여부를 판단하여 경로를 구하는 방식이었다. 백준에서는 해당 문제가 통과되었지만 위 세 개의 데이터를 넣었을 때 예상과 다른 값이 나왔다. 아마 내가 문제를 제대로 이해하지 못했거나 채점 데이터 자체가 적어서 그랬을 것이다. 결과적으로 다른 사람이 작성한 정답을 보게 됐지만 완전히 납득하지는 못했다. My First Algorithm (DFS) class Node: def __init__(self, index): self.index = index self.data = 2147483647 self.parent = [] self.child = [] def print_node(self): print(self.index, self.data, self.parent, self.child) def spanning_tree(nodes, check, root, parent, data): for child in parent.child: weight = data + child[1] child = nodes[child[0]] if child.child: if not check[child.index]: spanning_tree(nodes, check, root, child, weight) else: check[parent.index] = True if weight \u0026lt; root.data: root.data = weight\nV, E = map(int, input().split()) graph = [Node(i) for i in range(V+1)] visited = [False for _ in range(V+1)]\nfor _ in range(E): A, B, C = map(int, input().split()) graph[A].child.append((B,C)) graph[B].parent.append((A,C))\nmin_weight = 2147483647\nfor node in graph: if node.child and not node.parent: spanning_tree(graph, visited, node, node, 0) if node.data \u0026lt; min_weight: min_weight = node.data\nprint(min_weight) My Second Algorithm (Kruskal's Algorithm) class Node: def __init__(self, index): self.index = index self.data = 0 self.root = self self.parent = self self.active = None self.passive = [] def get_branch(self): if self.active: return self.passive + [self.active] else: return [] def set_branch(self, node, data): if self.root == node.root: if data \u0026lt; node.data: node.parent = self node.data = data else: node.root = self.root node.parent = self node.data += data if not self.active: self.active = node self.data += node.data node.data = self.data else: self.passive.append(node) self.update_data() def update_data(self): branch = self.get_branch() branch.sort(key=lambda n: n.data, reverse=True) active = branch.pop() if active != self.active: self.active = active self.passive = branch self.data = self.active.data def union_root(source: Node, target: Node, data: int) -\u0026gt; None: root = source.root if target.root in [source, source.root, target]: source.set_branch(target, data) while source != root: source = source.parent source.update_data()\nV, E = map(int, input().split())\ngraph = [Node(i) for i in range(V + 1)] edge_dict = {}\nfor _ in range(E): A, B, C = map(int, input().split()) edge_dict[(A, B)] = C\nedge_list = sorted(edge_dict.items(), key=lambda x: [x[1], x[0]])\nfor (a, b), c in edge_list: node_a, node_b = graph[a], graph[b] if node_a.parent != node_b.parent: union_root(node_a, node_b, c)\nweight = 2147483647\nfor edge_node in graph: if (edge_node.root == edge_node) and edge_node.get_branch(): if edge_node.data \u0026lt; weight: weight = edge_node.data\nprint(weight) My Third Algorithm (Prim's Algorithm) def prim(nodes: dict, start: int) -\u003e int or float: mst, keys, pi = [], heapdict(), dict() depth, total_weight = -1, 0 for n in nodes.keys(): keys[n] = float('inf') pi[n] = None keys[start], pi[start] = 0, start while keys: current_node, current_key = keys.popitem() current_depth = get_depth(pi, start, current_node, 0) if current_depth \u0026lt;= depth: if pi[current_node] == start: return float('inf') break depth = current_depth mst.append([pi[current_node], current_node, current_key]) total_weight += current_key for adjacent, weight in nodes[current_node].items(): if adjacent in keys and weight \u0026lt; keys[adjacent]: keys[adjacent] = weight pi[adjacent] = current_node return total_weight def get_depth(nodes: dict, root: int, start: int, data: int) -\u0026gt; int: if start == root: return data if nodes[start] == root: return data+1 return get_depth(nodes, root, nodes[start], data+1)\nV, E = map(int, input().split()) graph = defaultdict(dict)\nfor _ in range(E): A, B, C = map(int, input().split()) graph[A][B] = C graph[B][A] = C\nweight_list = [] for node in graph.keys(): heapq.heappush(weight_list, prim(graph, node))\nprint(heapq.heappop(weight_list)) Answer Algorithm V, E = map(int, input().split()) graph = [[] for _ in range(V+1)] visited = [False for _ in range(V+1)] heap = [[0, 1]] for _ in range(E): A, B, C = map(int, input().split()) graph[A].append([C, B]) graph[B].append([C, A]) total_weight = 0 node_cnt = 0 while heap: if node_cnt == V: break weight, node = heapq.heappop(heap) if not visited[node]: visited[node] = True total_weight += weight node_cnt += 1 for i in graph[node]: heapq.heappush(heap, i)\nprint(total_weight) Userful Reference\nGraph Editor\n"},{"id":123,"href":"/blog/2022-03-05/","title":"2022-03-05 Log","section":"Posts","content":"Operator Overloading # 연산자 오버로딩은 인스턴스 객체끼리 서로 연산을 할 수 있게 기존 연산자의 기능을 중복으로 정의하는 것 연산자 오버로딩의 예시 Method Operator Example __add__(self, other) + (Binomial) A + B, A += B __pos__(self) + (Unary) +A _sub__(self, other) - (Binomial) A - B, A -= B __neg__(self) - (Unary) -A __mul__(self, other) * A * B, A *= B __truediv__(self, other) / A / B, A /= B __floordiv__(self, other) // A // B, A //= B __mod__(self, other) % A % B, A %= B __pow__(self, other) pow(), ** pow(A, B), A ** B __eq__(self, other) == A == B __lt__(self, other) \u0026lt; A \u0026lt; B __gt__(self, other) \u0026gt; A \u0026gt; B __lshift__(self, other) \u0026laquo; A \u0026laquo; B __rshift__(self, other) \u0026raquo; A \u0026raquo; B __and__(self, other) \u0026amp; A \u0026amp; B, A \u0026amp;= B __xor__(self, other) ^ A ^ B, A ^= B __or__(self, other) | A | B, A |= B __invert__(self) ~ ~A __abs__(self) abs() abs(A) Union-Find Algorithm # 두 노드가 같은 그래프에 속하는지 판별하는 알고리즘 노드를 합치는 Union 연산과 루트 노드를 찾는 Find 연산으로 이루어짐 배열에 나열된 모든 노드들은 기본적으로 자기 자신의 값을 가짐 노드를 합칠 때 자식 노드의 배열 값에 부모 노드의 배열 값을 넣음 간단한 구현 코드\nCopy python def find(graph: list, x: int) -\u0026gt; int: if graph[x] == x: return x graph[x] = find(graph, graph[x]) def union(graph: list, x: int, y: int) -\u0026gt; None: x = find(graph, x) y = find(graph, y) if x == y: return graph[y] = x Kruskal\u0026rsquo;s Algorithm # 가장 적은 비용으로 모든 노드를 연결하기 위해 사용하는 알고리즘 (최소 비용 신장 트리) 모든 간선 정보를 오름차순으로 정렬한 뒤 비용이 적은 간선부터 그래프에 포함 Reference: https://blog.naver.com/ndb796/221230994142 간단한 구현 코드\nCopy python class Edge: def __init__(self, a: int, b: int, cost: int): self.parent = a self.child = b self.cost = cost def get_parent(graph: list, x: int) -\u0026gt; int: if graph[x] == x: return x graph[x] = get_parent(graph, graph[x]) def union_parent(graph: list, a: int, b: int) -\u0026gt; None: a = get_parent(graph, a) b = get_parent(graph, b) if a \u0026lt; b: graph[b] = a else: graph[a] = b def find(graph: list, a: int, b: int) -\u0026gt; int: a = get_parent(graph, a) b = get_parent(graph, b) if a == b: return True else: return False def sort_edge(edge_list: list) -\u0026gt; list: return sorted(edge_list, key=lambda x: [x.cost, x.parent, x.child]) def union_edge(graph: list, edge_list: list) -\u0026gt; int: cost = 0 for edge in edge_list: if not find(graph, edge.parent, edge.child): cost += edge.cost union_parent(graph, edge.parent, edge.child) return cost "},{"id":124,"href":"/blog/2022-03-04/","title":"2022-03-04 Log","section":"Posts","content":"1. Set # 백준 1107번(리모컨) 문제를 풀 때 유용하게 사용 해당 문제는 특정 길이의 문자열에 대해 가능한 모든 조합을 탐색해야 하는데\n시간복잡도를 줄이기 위해 중복이 없는 집합을 사용 빈집합은 set() 명령어로 간단하게 정의 Set은 Dictionary와 동일한 Hash Table 기반이기 때문에\nx in s 연산의 시간복잡도가 O(1)\n리스트의 x in s 연산 시간복잡도가 O(n)인 것과는 큰 차이 Set을 응용해 작성한 코드 일부\nCopy python buttons = set([str(i) for i in range(10)]) channels = {N,} diff = {abs(int_N-100)} if M \u0026gt; 0: buttons -= set(list(input().split())) channels = set() for i in range(1, count+1): product = itertools.product(buttons, repeat=i) channels |= set(map(\u0026#39;\u0026#39;.join, product)) min_chan, max_chan = \u0026#39;0\u0026#39;, \u0026#39;1\u0026#39; for _ in range(count-1): min_chan += max(buttons) for _ in range(count): max_chan += min(buttons) if set(max_chan) \u0026amp; buttons == set(max_chan): channels.add(max_chan) if set(min_chan) \u0026amp; buttons == set(min_chan): channels.add(min_chan) 2. Dictionary # 백준 1620번(나는야 포켓몬 마스터 이다솜) 문제를 풀 때 사용 해당 문제는 문자열 또는 인덱스를 입력했을 때 대칭되는 값을 출력해야 하는데\n처음엔 시간복잡도가 O(n)인 List의 index(x)를 사용하여 시간초과가 발생 문자열과 인덱스의 관계를 Dictionary로 구현해 탐색 시간복잡도를 O(1)로 개선 3. Coutner # 백준 10816번(숫자카드 2) 문제를 풀 때 사용 해당 문제는 숫자 카드의 값을 입력했을 때 해당 카드의 개수를 출력해야 하는데\n처음엔 시간복잡도가 O(n)인 List의 count(x)를 사용하여 시간초과가 발생 전체 카드에 대한 Counter를 정의하여 탐색 시간복잡도를 O(1)로 개선 4. Combination # 백준 1010번(다리 놓기) 문제를 풀 때 사용 해당 문제는 강에 다리를 놓는 경우의 수를 출력해야 하는데\nmath 모듈의 comb 함수를 이용해 경우의 수를 계산 5. Permutation # 백준 1107번(리모컨) 문제를 풀 때 유용하게 사용 해당 문제에서 특정 길이의 문자열에 대해 가능한 모든 조합을 나열하는데,\n순서를 고려하고 중복을 허용하기 위해 중복 순열(Product)을 사용 Permutation을 응용해 작성한 코드 일부\nCopy python buttons = set([str(i) for i in range(10)]) ... for i in range(1, count+1): product = itertools.product(buttons, repeat=i) channels |= set(map(\u0026#39;\u0026#39;.join, product)) 6. Binary Search # 백준 1654번(랜선 자르기) 문제를 풀 때 유용하게 사용 해당 문제는 서로 다른 길이의 선들을 동일한 길이로 가장 길게 잘라야 되는데\n처음엔 가장 긴 선부터 가장 짧은 선까지의 범위 내에서 완전탐색을 진행하여 시간초과가 발생 완전탐색을 이분탐색으로 대체하여 시간복잡도 개선 Binary Search를 응용해 작성한 코드 일부\nCopy python while mn \u0026lt; mx: md = (mx + mn) // 2 count = 0 for i in range(K): count += k[i] // md if count \u0026lt; N: mx = md else: mn = md + 1 7. Heap # 백준 7662번(이중 우선순위 큐) 문제를 풀 때 유용하게 사용 해당 문제는 최솟값과 최댓값 삭제 기능을 모두 가지고 있는 이중 우선순위 큐를 구현하는 것 처음엔 List의 pop(x), index(x), max(x)/min(x)를 혼합하여 사용한 것 때문에\nO(n^3) 이상의 시간복잡도를 만들어서 시간초과가 발생 두번째 시도에선 List의 pop(x)와 heapq 모듈의 heappop(x)를 사용해 시간복잡도를 O(1()로 개선\n하지만, Heap은 이진트리 기반으로 리스트와는 구조가 다르기 때문에 인덱스로 참조 시 에러가 발생 세번째 시도에선 단일 큐를 Max Heap과 Min Heap으로 나누고 각각에서 heappop(x), heappush(x)를 수행\n하지만, Max Heap 또는 Min Heap에서 삭제된 값이 반대쪽 Heap에서 남아있는 경우가 있어 에러가 발생 해당 에러에 대한 해결책으로 Max Heap과 Min Heap을 동기화를 시키는 방법도 있지만,\n값이 유효한지 판단하는 Dictionary를 구현해 값에 대한 참/거짓 여부를 참조하는 방법을 이용\n해당 Dictionary를 heappop(x) 사용 시 한 번, 최대/최솟값 출력 시 한 번씩 참조해 에러 해결 Heap을 응용해 작성한 코드 일부\nCopy python if cmd == \u0026#39;I\u0026#39;: n = int(n) heapq.heappush(min_Q, n) heapq.heappush(max_Q, -n) try: valid[n] += 1 except KeyError: valid[n] = 1 ins += 1 elif cmd == \u0026#39;D\u0026#39;: try: if n == \u0026#39;1\u0026#39;: max_pop = -heapq.heappop(max_Q) while not valid[max_pop]: max_pop = -heapq.heappop(max_Q) valid[max_pop] -= 1 ins -= 1 elif n == \u0026#39;-1\u0026#39;: min_pop = heapq.heappop(min_Q) while not valid[min_pop]: min_pop = heapq.heappop(min_Q) valid[min_pop] -= 1 ins -= 1 except IndexError: min_Q, max_Q = [], [] continue Copy python max_pop, min_pop = 0, 0 while True: max_pop = -heapq.heappop(max_Q) if valid[max_pop]: break while True: min_pop = heapq.heappop(min_Q) if valid[min_pop]: break print(max_pop, min_pop) 8. DFS/BFS # 백준 1260번(DFS와 BFS) 문제를 풀 때 사용 해당 문제는 DFS와 BFS로 탐색했을 때의 결과를 출력하는 기본적인 문제 DFS는 깊이를 우선적으로 탐색, BFS는 너비를 우선적으로 탐색 DFS는 경로의 특징을 저장할 때 사용, BFS는 최단거리를 구할 때 사용 DFS는 스택 또는 재귀함수로 구현, BFS는 큐(데크)를 이용해서 구현 DFS/BFS를 응용해 작성한 코드 일부\nCopy python def dfs(nodes, visited, node): visited[node] = True next_nodes = nodes[node] while next_nodes: next_node = heapq.heappop(next_nodes) if not visited[next_node]: print(next_node, end=\u0026#39; \u0026#39;) dfs(nodes, visited, next_node) Copy python def bfs(nodes, visited, root): queue = deque() visited[root] = True queue.append(root) while queue: node = queue.popleft() visited[node] = True print(node, end=\u0026#39; \u0026#39;) next_nodes = nodes[node] while next_nodes: next_node = heapq.heappop(next_nodes) if not visited[next_node]: visited[next_node] = True queue.append(next_node) "},{"id":125,"href":"/blog/police-map/","title":"Police Map","section":"Posts","content":"\u003c!DOCTYPE html\u003e "}]