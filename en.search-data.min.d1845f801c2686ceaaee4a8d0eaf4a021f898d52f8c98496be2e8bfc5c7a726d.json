[{"id":0,"href":"/blog/hugo-blog-3/","title":"Hugo 블로그 만들기 (3) - Taxonomies로 태그/카테고리 페이지 커스터마이징","section":"Posts","content":""},{"id":1,"href":"/blog/hugo-blog-2/","title":"Hugo 블로그 만들기 (2) - 메인 레이아웃 커스터마이징 (메뉴, 목차, 헤더)","section":"Posts","content":""},{"id":2,"href":"/blog/hugo-blog-1/","title":"Hugo 블로그 만들기 (1) - 프로젝트 구성과 GitHub Pages 배포 (Submodule 활용)","section":"Posts","content":""},{"id":3,"href":"/blog/openup-handson/","title":"[OSSCA] 2025 오픈소스 컨트리뷰션 아카데미 - PyTorch 문서 한글화 참여 후기","section":"Posts","content":""},{"id":4,"href":"/blog/uv-project/","title":"[Python] uv로 프로젝트 구성하고 PyPI 배포하기 - Rust 기반 고속 패키지 관리","section":"Posts","content":""},{"id":5,"href":"/blog/spark-study-8/","title":"Apache Spark - 사용자 정의 함수(UDF)와 고차 함수 활용하기","section":"Posts","content":"User-Defined Functions # 스파크는 자신의 기능을 정의할 수 있는 유연성을 제공한다. 이를 사용자 정의 함수(User-Defined Function, UDF)라고 한다.\nUDF를 생성하는 이점은 스파크 SQL 안에서 이를 사용할 수 있다는 것이다.\nSpark SQL UDF 활용 # 다음은 스파크 SQL UDF를 만드는 예시로, 인수를 세제곱하는 함수 cubed() 를 생성한다.\nCopy python from pyspark.sql.types import LongType # 큐브 함수 생성 def cubed(s): return s * s * s # UDF로 등록 spark.udf.register(\u0026#34;cubed\u0026#34;, cubed, LongType()) 스파크 SQL을 사용하여 cubed() 함수를 실행할 수 있다.\nCopy python # 임시 뷰 생성 spark.range(1, 9).createOrReplaceTempView(\u0026#34;udf_test\u0026#34;) # 큐브 UDF를 사용하여 쿼리 spark.sql(\u0026#34;SELECT id, cubed(id) AS id_cubed FROM udf_test\u0026#34;).show() Copy bash +---+--------+ | id|id_cubed| +---+--------+ | 1| 1| | 2| 8| | 3| 27| | 4| 64| | 5| 125| | 6| 216| | 7| 343| | 8| 512| +---+--------+ 스파크 SQL 평가 순서 # 스파크 SQL은 하위 표현식의 평가 순서를 보장하지 않는다. 예를 들어, 다음 쿼리에서 s IS NOT NULL 절이 strlen(s) \u0026gt; 1 절 이전에 실행된다는 것을 보장할 수 없다.\nCopy python spark.sql(\u0026#34;SELECT s FROM test1 WHERE s IS NOT NULL AND strlen(s) \u0026gt; 1\u0026#34;) 따라서, 다음 두 가지 null 검사 방식을 수행하는 것이 좋다.\nUDF 안에서 null 을 인식하고 null 검사를 수행할 필요가 있다. SQL문에서 IF 또는 CASE WHEN 식을 사용하여 null 검사를 수행하고 조건에 맞으면 UDF를 호출한다. Pandas UDF # PySpark UDF는 JVM과 파이썬 사이의 데이터 이동을 필요로 해서 Scala UDF보다 성능이 느렸다.\n이 문제를 해결하기 위해 Pandas UDF가 스파크 2.3 버전부터 도입되었다. Pandas UDF는 Apache Arrow를 사용하여 Pandas UDF를 정의하거나 함수 자체를 래핑할 수 있다. Apache Arrow 형식에 포함된 데이터라면 더이상 JVM으로 데이터를 전달하기 위해 직렬화나 피클할 필요가 없다.\nPandas UDF는 pandas.Series, pandas.DataFrame 과 같은 파이썬 유형 힌트로 유추한다. 예시로, 앞에서 정의한 큐브 함수를 Pandas UDF로 만들면 아래와 같다.\nCopy python import pandas as pd from pyspark.sql.functions import col, pandas_udf from pyspark.sql.types import LongType # 큐브 함수 생성 def cubed(a: pd.Series) -\u0026gt; pd.Series: return a * a * a # 큐브 함수에 대한 Pandas UDF 생성 cubed_udf = pandas_udf(cubed, returnType=LongType()) Pandas UDF는 아래와 같이 실행할 수 있다.\nCopy python # 스파크 데이터프레임 생성 df = spark.range(1, 4) # Pandas UDF를 실행 df.select(\u0026#34;id\u0026#34;, cubed_udf(col(\u0026#34;id\u0026#34;))).show() Copy bash +---+---------+ | id|cubed(id)| +---+---------+ | 1| 1| | 2| 8| | 3| 27| +---+---------+ 스파크 UI에서 시각화된 pandas_udf 함수의 실행 단계에 대한 DAG을 조회할 수 있다. Stage 0에서 ArrowEvalPython 연산이 Pandas UDF를 평가하는 단계이다.\n고차 함수 # 복잡한 데이터 유형은 단순한 데이터 유형의 결합이기 때문에 다음과 같이 조작할 수 있다.\n중첩된 구조를 개별 행으로 분해하고 각각에 함수를 적용한 후 중첩된 구조를 다시 만드는 방법 사용자 정의 함수를 사용하는 방법 하지만 배열과 같은 중첩된 구조를 분해하고 다시 만든다고 가정할 때, 셔플 작업이 발생해 결과 배열의 순서가 원래 배열의 순서와 동일하지 않을 수 있다.\n사용자 정의 함수를 사용할 경우에는 정렬 문제는 해결할 수 있지만, 직렬화 및 역직렬화 과정을 거치면서 발생하는 비용이 크다는 문제가 있다.\n내장 함수 # 복잡한 데이터 유형에 대해 스파크 2.4 이상 버전에 포함된 내장 함수를 사용할 수 있다. 자세한 건 공식 문서를 참고해볼 수 있는데, 그 중에서 배열과 맵에 대해서 일부를 알아본다.\n배열과 관련된 함수는 공식 문서에서 array 문단부터 시작하는 함수들을 참고하면 된다. 대표적으로 array_distinct 함수는 배열 내 중복을 제거하고, array_sort 함수는 배열을 오름차순으로 정렬한다. array로 시작하지 않는 함수 중에서도 concat 함수는 복수 개의 배열을 받아 하나의 배열로 합쳐주고, flatten 함수는 2차원 이상 중첩된 배열을 단일 배열로 플랫화한다. sequence 함수로 시작과 끝에 대한 배열을 생성할 수 있고, slice 함수로 배열의 특정 부분만 잘라낼 수도 있다.\n맵과 관련된 함수는 공식 문서에서 map 문단부터 시작하는 함수들을 참고하면 된다. 대표적으로 map_concat 함수는 복수 개의 맵을 하나의 맵으로 합쳐주고, map_keys 함수로 맵에서 키 배열만 추출할 수도 있다. map으로 시작하지 않는 함수 중에서도 element_at 함수는 주어진 키에 대한 값을 반환하고, cardinality 함수는 맵의 크기를 반환한다.\ntransform() # 내장 함수 외에도 익명 람다 함수를 인수로 사용하는 고차 함수 transform() 이 있다.\n고차 함수를 실행해보기 위해 아래와 같이 샘플 데이터 tC 를 만들어본다.\nCopy python from pyspark.sql.types import * schema = StructType([StructField(\u0026#34;celsius\u0026#34;, ArrayType(IntegerType()))]) t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]] t_c = spark.createDataFrame(t_list, schema) t_c.createOrReplaceTempView(\u0026#34;tC\u0026#34;) t_c.show() Copy bash +--------------------+ | celsius| +--------------------+ |[35, 36, 32, 30, ...| |[31, 32, 34, 55, 56]| +--------------------+ Celsius 단위를 Fahrenheit 단위로 바꾸는 transform() 함수를 사용해, celsius 열로부터 fahrenheit 열을 계산했다. 출력 결과는 아래와 같다.\nCopy python spark.sql(\u0026#34;\u0026#34;\u0026#34; SELECT celsius, transform(celsius, t -\u0026gt; ((t * 9) div 5) + 32) AS fahrenheit FROM tc \u0026#34;\u0026#34;\u0026#34;).show() Copy bash +--------------------+--------------------+ | celsius| fahrenheit| +--------------------+--------------------+ |[35, 36, 32, 30, ...|[95, 96, 89, 86, ...| |[31, 32, 34, 55, 56]|[87, 89, 93, 131,...| +--------------------+--------------------+ filter() # filter() 함수는 입력한 배열의 요소 중 부울 함수가 참인 요소만으로 구성된 배열을 생성한다.\nCopy python spark.sql(\u0026#34;\u0026#34;\u0026#34; SELECT celsius, filter(celsius, t -\u0026gt; t \u0026gt; 38) AS high FROM tc \u0026#34;\u0026#34;\u0026#34;).show() Copy bash +--------------------+--------+ | celsius| high| +--------------------+--------+ |[35, 36, 32, 30, ...|[40, 42]| |[31, 32, 34, 55, 56]|[55, 56]| +--------------------+--------+ exists() # exists() 함수는 입력한 배열의 요소 중 불린 함수를 만족시키는 것이 존재할 때 참을 반환한다.\nCopy python spark.sql(\u0026#34;\u0026#34;\u0026#34; SELECT celsius, exists(celsius, t -\u0026gt; t = 38) AS threshold FROM tc \u0026#34;\u0026#34;\u0026#34;).show() Copy bash +--------------------+---------+ | celsius|threshold| +--------------------+---------+ |[35, 36, 32, 30, ...| true| |[31, 32, 34, 55, 56]| false| +--------------------+---------+ 스파크 SQL 작업 # 스파크 SQL의 기능 중 일부는 DataFrame의 다양한 기능에서 유래된다. 이용가능한 작업에는 집계 함수, 수집 함수, 날짜 함수, 수학 함수, 정렬 함수, 문자열 함수, 윈도우 함수 등 매우 광범위하다.\nUnion # Union은 동일한 스키마를 가진 두 개의 서로 다른 DataFrame을 하나로 합치는 작업이다.\nSQL문으로 다음과 같이 표현할 수 있다.\nCopy sql (SELECT * FROM first_half) UNION ALL (SELECT * FROM second_half); 파이썬으로는 다음과 같이 표현할 수 있다.\nCopy python result = first_half.union(second_half) Join # Join은 두 개 이상의 DataFrame을 특정 조건을 기준으로 결합하여 하나로 합치는 작업이다.\nSQL문으로 다음과 같이 표현할 수 있다.\nCopy sql SELECT p.id AS productId, p.storeId, s.name AS storeName, p.name AS productName FROM product AS p LEFT JOIN store AS s ON p.storeId = s.id; 파이썬으로는 다음과 같이 표현할 수 있다.\nCopy python from pyspark.sql.functions import col product.join( store, product.storeId == store.id, how = \u0026#34;left\u0026#34; ).select( col(\u0026#34;p.id\u0026#34;).alias(\u0026#34;productId\u0026#34;), \u0026#34;p.storeId\u0026#34;, col(\u0026#34;s.name\u0026#34;).alias(\u0026#34;storeName\u0026#34;), col(\u0026#34;p.name\u0026#34;).alias(\u0026#34;productName\u0026#34;) ).show() 윈도우 # 윈도우 함수를 사용하면 모든 입력 행에 대해 단일값을 반환하면서 행 그룹에 대해 작업할 수 있다.\n순위를 매기는 작업과 관련해서는 RANK, DENSE_RANK, PERCENT_RANK, NTILE, ROW_NUMBER 함수가 있고, 집계와 관련해서는 MAX, MIN, COUNT, SUM, AVG 등의 함수가 있다.\nSQL문으로 다음과 같이 표현할 수 있다.\nCopy sql SELECT name, dept, salary, RANK() OVER (PARTITION BY dept ORDER BY salary) AS rank FROM employees; 파이썬으로는 다음과 같이 표현할 수 있다.\nCopy python from pyspark.sql.functions import rank from pyspark.sql import Window window = rank().over(Window.partitionBy(\u0026#34;dept\u0026#34;).orderBy(\u0026#34;salary\u0026#34;)).alias(\u0026#34;rank\u0026#34;) employees.select(\u0026#34;name\u0026#34;, \u0026#34;dept\u0026#34;, \u0026#34;salary\u0026#34;, window).show() 수정 # DataFrame 자체는 변경할 수 없지만, 열을 가공하여 새로운 DataFrame을 만드는 것과 같은 작업을 통해 수정할 수 있다.\n파이썬으로 활용 가능한 다음과 같은 예시가 있다.\nCopy python from pyspark.sql.functions import expr # 열 추가 foo2 = foo.withColumn( \u0026#34;status\u0026#34;, expr(\u0026#34;CASE WHEN delay \u0026lt;= 10 THEN \u0026#39;On-time\u0026#39; ELSE \u0026#39;Delayed\u0026#39; END\u0026#34;)) # 열 삭제 foo3 = foo2.drop(\u0026#34;delay\u0026#34;) # 칼럼명 바꾸기 foo4 = foo3.withColumnRenamed(\u0026#34;status\u0026#34;, \u0026#34;flight_status\u0026#34;) 피벗 # 로우와 칼럼을 바꿔야 하는 경우가 있다. 이 경우에 pivot 함수를 지원한다.\n피벗을 실행해보기 위해 아래와 같이 샘플 데이터를 만들어본다.\nCopy python from pyspark.sql import Row df1 = spark.createDataFrame([ Row(course=\u0026#34;dotNET\u0026#34;, year=2012, earnings=10000), Row(course=\u0026#34;Java\u0026#34;, year=2012, earnings=20000), Row(course=\u0026#34;dotNET\u0026#34;, year=2012, earnings=5000), Row(course=\u0026#34;dotNET\u0026#34;, year=2013, earnings=48000), Row(course=\u0026#34;Java\u0026#34;, year=2013, earnings=30000),]) df1.show() Copy bash +------+----+--------+ |course|year|earnings| +------+----+--------+ |dotNET|2012| 10000| | Java|2012| 20000| |dotNET|2012| 5000| |dotNET|2013| 48000| | Java|2013| 30000| +------+----+--------+ 위 데이터에서 course 를 열로, year 를 행으로, earnings 를 값으로 sum 집계해 구성한 피벗 테이블을 아래와 같이 만들 수 있다.\nCopy python df1.groupBy(\u0026#34;year\u0026#34;).pivot( \u0026#34;course\u0026#34;, [\u0026#34;dotNET\u0026#34;, \u0026#34;Java\u0026#34;]).sum(\u0026#34;earnings\u0026#34;).sort(\u0026#34;year\u0026#34;).show() Copy bash +----+------+-----+ |year|dotNET| Java| +----+------+-----+ |2012| 15000|20000| |2013| 48000|30000| +----+------+-----+ References # https://spark.apache.org/docs/latest/sql-ref-functions-udf-scalar.html https://books.japila.pl/pyspark-internals/sql/ArrowEvalPython/#evalType https://spark.apache.org/docs/latest/api/sql/index.html https://docs.databricks.com/aws/en/semi-structured/higher-order-functions https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.rank.html https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.pivot.html "},{"id":6,"href":"/blog/kube-installtion/","title":"쿠버네티스 클러스터 구축하기 - Ubuntu 24.04에서 kubeadm으로 멀티노드 설치","section":"Posts","content":"실습 개요 # 쿠버네티스 공식문서 중 시작하기 항목에 있는 컨테이너 런타임 과 kubeadm 설치하기 를 따릅니다.\nOS 선정 # 관련한 블로그 게시글 중에는 Rocky Linux OS를 사용한 사례가 많았는데, 이미 과거에 Mac에 Ubuntu Server 24.04 설치하기 (UTM) 라는 게시글로 Ubuntu Server 가상머신을 생성한 이력이 있어서 Rocky Linux 대신에 Ubuntu Server를 사용할 것입니다.\nUbuntu Server를 선택한 또 다른 이유는, Rocky Linux의 원본인 RHEL 8버전과 Ubuntu Server의 공식문서를 참고하여 아래와 같이 최소사양을 비교한데 있습니다. 서비스 운영 환경으로 사용할 것이 아니라서 단순히 숫자만 놓고 봤을 때 Ubuntu Server가 더 가벼워서 실습 환경으로 선택했습니다.\n하드웨어 Ubuntu Server 24.04.2 LTS RHEL 8 CPU 1GHz 이상 64비트 1.5GHz 이상 64비트, 2코어 권장 RAM 최소 1.5GB (ISO), 권장 3GB 이상 최소 1.5GiB~3GiB, 권장 1GB/CPU 저장공간 최소 5GB, 권장 25GB 이상 최소 10GiB, 권장 20GiB 이상 Apple Silicon 환경에서 리눅스 가상머신을 실행하기 때문에, OS와 컨테이너 런타임 등은 모두 arm64 호환 버전을 사용합니다.\n실습 환경 # UTM 소프트웨어를 사용해 Ubuntu Server 가상머신이 만들어졌음을 가정하고 진행합니다. 가상머신 생성이 필요한 경우 이전 게시글을 참고할 수 있습니다.\n가용 자원 # M4 Mac Mini 한 대의 자원을 사용할 수 있습니다.\nM4 Mac mini\nMemory : 16GB CPU : 10코어 IP Address : 192.168.50.227/24\n실습 자원 # 마스터 노드 1대와 워커 노드 2대로 구성된 쿠버네티스 클러스터를 구성합니다.\nk8s-master\nMemory : 4GB CPU : 4코어 IP Address : 192.168.50.13/24\nk8s-worker1\nMemory : 4GB CPU : 4코어 IP Address : 192.168.50.14/24\nk8s-worker2\nMemory : 4GB CPU : 4코어 IP Address : 192.168.50.15/24\n네트워크 # 통신 편의성을 위해 가상머신 네트워크는 브릿지 모드로 설정합니다. Ubuntu Server 설치 시 기본 설정인 DHCP 대신에 Static IP 주소를 설정합니다.\n실습 중 minyeamer 호스트에서 쿠버네티스 노드로 SSH 연결하여 명령어를 입력합니다.\n쿠버네티스 노드는 역할을 명시하기 위해 k8s-master, k8s-worker1, k8s-worker2 호스트로 정의합니다. k8s-master 가 마스터 노드 역할을 하고, 나머지가 워커 노드 역할을 담당합니다.\n설치하기 (공통) # 우선, 마스터 노드와 워커 노드에서 공통적으로 컨테이너 런타임과 쿠버네티스를 설치합니다.\n1. Ubuntu Server 기본 설정 # Copy bash echo \u0026#39;===== [1] Ubuntu Server 기본 설정 =====\u0026#39; echo \u0026#39;===== [1-1] 타임존 설정 =====\u0026#39; sudo timedatectl set-timezone Asia/Seoul echo \u0026#39;===== [1-2] hosts 설정 =====\u0026#39; cat \u0026lt;\u0026lt; EOF | sudo tee -a /etc/hosts 192.168.50.13 k8s-master 192.168.50.14 k8s-worker1 192.168.50.15 k8s-worker2 EOF 쿠버네티스 노드 간 호스트명으로 통신하기 위해 /etc/hosts 에 호스트명과 IP 주소를 맵핑합니다. 맥에서도 쿠버네티스 노드에 SSH로 접근하기 위해 편의상 /etc/hosts 를 동일하게 설정했습니다.\n2. kubeadm 설치 전 사전작업 # 쿠버네티스 공식문서 kubeadm 설치하기 에 따르면 시작하기 전에,\n컴퓨터의 특정 포트들 개방. 자세한 내용은 여기 를 참고한다. 스왑의 비활성화. kubelet이 제대로 작동하게 하려면 반드시 스왑을 사용하지 않도록 설정한다. 위 사항을 만족해야 한다고 알려줍니다.\n쿠버네티스 클러스터 노드 간 다양한 포트를 통해 통신이 이루어지는데, 방화벽 설정에 의해 통신이 차단되어 클러스터 구성이 실패할 수 있습니다. 쿠버네티스 API 서버가 6443 포트를 사용하고, 파드 네트워크 플러그인마다 또 필요한 포트가 있습니다. 실습 환경에선 방화벽 자체를 해제합니다.\n또한, 쿠버네티스의 구성요소인 Kubelet은 메모리 할당 및 관리를 위해 설계되었는데, Swap을 사용하게 되면 예상치 못한 성능 저하나 불안정성을 초래할 수 있어 Swap을 해제해야 합니다.\n이러한 과정을 아래와 같은 명령어로 처리할 수 있습니다.\nCopy bash echo \u0026#39;===== [2] kubeadm 설치 전 사전작업 =====\u0026#39; echo \u0026#39;===== [2-1] 방화벽 해제 =====\u0026#39; sudo systemctl stop ufw sudo systemctl disable ufw sudo ufw disable echo \u0026#39;===== [2-2] Swap 비활성화 =====\u0026#39; sudo swapoff -a sudo sed -i \u0026#39;/ swap / s/^/#/\u0026#39; /etc/fstab echo \u0026#39;===== [2-3] 필수 패키지 설치 =====\u0026#39; sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gpg iproute2 추가로, 컨테이너 런타임과 쿠버네티스 설치 전 필요한 필수 패키지를 미리 설치합니다.\n3. 컨테이너 런타임 설치 전 사전작업 # 컨테이너 런타임과 관련된 쿠버네티스 공식문서의 IPv4를 포워딩하여 iptables가 브리지된 트래픽을 보게 하기 문단에 따라 패킷 포워딩 등의 기능 제공을 위해 iptables 설정을 합니다.\n해당 과정은 아래와 같이 공식문서에서 제공됩니다.\nCopy bash echo \u0026#39;===== [3] 컨테이너 런타임 설치 전 사전작업 =====\u0026#39; echo \u0026#39;===== [3-1] 필요한 모듈 로드 =====\u0026#39; cat \u0026lt;\u0026lt;EOF | sudo tee /etc/modules-load.d/containerd.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter echo \u0026#39;===== [3-2] iptables 세팅 =====\u0026#39; sudo tee /etc/sysctl.d/kubernetes.conf \u0026lt;\u0026lt;EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF echo \u0026#39;===== [3-3] 설정값 적용 =====\u0026#39; sudo sysctl --system 4. 컨테이너 런타임 설치 # 컨테이너 런타임은 Docker 공식문서 Install Docker Engine on Ubuntu 에 따라 도커 엔진을 설치합니다. 도커 엔진을 설치하고 그 안에 포함되는 containerd 를 쿠버네티스의 컨테이너 런타임으로 사용합니다.\n해당 과정은 아래와 같이 공식문서에서 제공됩니다.\n참고로, containerd 설치 시 amd64와 arm64 아키텍처 중 하나를 지정해야 하는데, $(dpkg --print-architecture) 명령어로 현재 환경의 아키텍처를 출력하여 변수로 사용합니다.\nCopy bash echo \u0026#39;===== [4] 컨테이너 런타임 설치 =====\u0026#39; echo \u0026#39;===== [4-1] 도커 공식 GPG 키 추가 =====\u0026#39; sudo apt-get update sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc echo \u0026#39;===== [4-2] 도커 Apt 저장소 추가 =====\u0026#39; echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;${UBUNTU_CODENAME:-$VERSION_CODENAME}\u0026#34;) stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null echo \u0026#39;===== [4-3] containerd 설치 =====\u0026#39; sudo apt-get update sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin echo \u0026#39;===== [4-4] CRI 활성화 =====\u0026#39; sudo mkdir -p /etc/containerd sudo containerd config default | sudo tee /etc/containerd/config.toml sudo sed -i \u0026#39;s/SystemdCgroup = false/SystemdCgroup = true/g\u0026#39; /etc/containerd/config.toml sudo systemctl restart containerd 컨테이너 런타임과 관련된 쿠버네티스 공식문서의 containerd 문단을 보면 아래와 같은 참고사항을 알려줍니다.\n만약 containerd 를 패키지(RPM, .deb 등)를 통해 설치하였다면, CRI integration 플러그인은 기본적으로 비활성화되어 있다.\n쿠버네티스에서 containerd를 사용하기 위해서는 CRI support가 활성화되어 있어야 한다. cri가 /etc/containerd/config.toml 파일 안에 있는 disabled_plugins 목록에 포함되지 않도록 주의하자. 만약 해당 파일을 변경하였다면, containerd 를 다시 시작한다.\n따라서, containerd 설치 후 CRI를 직접 활성화합니다.\n컨테이너 런타임을 설치한 후 containerd 버전을 확인해보니 1.2.27 버전이 설치되었습니다.\nCopy bash $ containerd --version containerd containerd.io 1.7.27 05044ec0a9a75232cad458027ca83437aae3f4da 5. 쿠버네티스 설치 # 쿠버네티스 공식문서의 kubeadm, kubelet 및 kubectl 설치 문단에 따라 쿠버네티스 운영에 필요한 패키지를 설치합니다. 각 패키지에 대해 아래와 같은 설명을 참고할 수 있습니다.\nkubeadm : 클러스터를 부트스트랩하는 명령이다. kubelet : 클러스터의 모든 머신에서 실행되는 파드와 컨테이너 시작과 같은 작업을 수행하는 컴포넌트이다. kubectl : 클러스터와 통신하기 위한 커맨드 라인 유틸리티이다. 해당 과정은 아래와 같이 공식문서에서 제공됩니다.\nCopy bash echo \u0026#39;===== [5] kubeadm 설치 =====\u0026#39; echo \u0026#39;===== [5-1] 쿠버네티스 패키지 저장소의 공개키 다운로드 =====\u0026#39; curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg echo \u0026#39;===== [5-2] 쿠버네티스 Apt 저장소 추가 =====\u0026#39; # sudo mkdir -p -m 755 /etc/apt/keyrings \u0026lt;\u0026lt; Ubuntu 22.04 이전 버전 echo \\ \u0026#34;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/kubernetes.list echo \u0026#39;===== [5-3] kubelet, kubeadm, kubectl 설치 =====\u0026#39; sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl echo \u0026#39;===== [5-4] kubelet 서비스 활성화 =====\u0026#39; sudo systemctl enable --now kubelet 쿠버네티스를 설치한 후 kubeadm, kubelet, kubectl 버전을 확인했습니다. 모두 동일하게 1.33.2 버전이 설치되었습니다.\nCopy bash $ kubeadm version kubeadm version: \u0026amp;version.Info{ Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;33\u0026#34;, EmulationMajor:\u0026#34;\u0026#34;, EmulationMinor:\u0026#34;\u0026#34;, MinCompatibilityMajor:\u0026#34;\u0026#34;, MinCompatibilityMinor:\u0026#34;\u0026#34;, GitVersion:\u0026#34;v1.33.2\u0026#34;, GitCommit:\u0026#34;a57b6f7709f6c2722b92f07b8b4c48210a51fc40\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2025-06-17T18:39:42Z\u0026#34;, GoVersion:\u0026#34;go1.24.4\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/arm64\u0026#34;} Copy bash $ kubelet --version Kubernetes v1.33.2 Copy bash $ kubectl version Client Version: v1.33.2 Kustomize Version: v5.6.0 Server Version: v1.33.2 설정하기 (마스터 노드) # 마스터 노드에서 우선 클러스터 설정을 진행합니다.\n6. 쿠버네티스 클러스터 설정 # 쿠버네티스 클러스터를 초기화하면 다양한 쿠버네티스 컴포넌트들이 파드로 만들어집니다. 또한, 실습 과정에서 새로운 파드를 생성하게 됩니다.\n컨테이너들 간 통신을 위해 CNI(Container Network Interface)라는 표준 인터페이스가 필요합니다. 쿠버네티스는 기본적으로 Kubelet 이라는 자체적인 CNI 플러그인을 제공하지만 네트워크 기능이 매우 제한적인 단점이 있습니다.\n그 단점을 보완하기 위해, 서드파티 플러그인으로 Flannel, Calico 등이 존재합니다. 이 중에서 Calico 네트워크 정책 및 보안, 확장성, 성능 측면에서 뛰어나기 때문에 실전 환경에서 주로 사용됩니다.\nCalico를 개발하고 관리하는 Tigera에서 제공하는 공식문서 Install Calico networking and network policy for on-premises deployments 를 참고하여 아래와 같이 Calico를 설치했습니다.\nCopy bash echo \u0026#39;===== [6] 쿠버네티스 클러스터 설정 =====\u0026#39; echo \u0026#39;===== [6-1] 클러스터 초기화 =====\u0026#39; sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address 192.168.50.13 echo \u0026#39;===== [6-2] kubectl 설정 =====\u0026#39; mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config echo \u0026#39;===== [6-3] Pod Network 설치 (Calico) =====\u0026#39; kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.30.2/manifests/operator-crds.yaml kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.30.2/manifests/tigera-operator.yaml echo \u0026#39;===== [6-4] 사용자 정의 리소스 다운로드 =====\u0026#39; curl https://raw.githubusercontent.com/projectcalico/calico/v3.30.2/manifests/custom-resources.yaml -O sed -i \u0026#39;s/192.168.0.0/10.244.0.0/g\u0026#39; custom-resources.yaml echo \u0026#39;===== [6-5] Calico를 설치하기 위한 manifest 생성 =====\u0026#39; kubectl create -f custom-resources.yaml 설치가 완료되면 calico-system 이라는 네임스페이스 아래에 calico-node 등의 파드가 만들어집니다. 하지만, 위 명령어 중 사용자 정의 리소스 다운로드 부분에 calico-node 를 정의하는 custom-resources.yaml 파일의 기본 네트워크 대역이 192.168.0.0/16 대역으로 작성되어 있어서 오류가 발생했습니다.\ncustom-resources.yaml 파일 내용에서 기본 네트워크 대역을 클러스터 초기화 시에 입력한 pod-network-cidr 대역 10.244.0.0/16 으로 변경해줍니다.\nCopy bash $ kubectl get pods -n calico-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-6565cb8dfb-dfb6m 1/1 Running 1 (23h ago) 3d calico-node-2xznr 1/1 Running 0 3d calico-node-7zlgx 1/1 Running 0 3d calico-node-jgddh 0/1 Running 1 (23h ago) 3d calico-typha-6fd8bff95d-k2bgm 1/1 Running 0 3d csi-node-driver-2llx6 2/2 Running 0 3d csi-node-driver-mj8js 2/2 Running 0 3d csi-node-driver-nzr55 2/2 Running 2 (23h ago) 3d goldmane-5f56496f4c-5szmn 1/1 Running 1 (23h ago) 3d whisker-78db647586-dpsdl 2/2 Terminating 0 3d whisker-78db647586-tmbnp 2/2 Running 0 31m 설정하기 (워커 노드) # 워커 노드에서 마스터 노드에 연결하는 설정을 진행합니다.\n7. 쿠버네티스 클러스터 연결 # 마스터 노드에서 쿠버네티스 클러스터를 초기화하고 새 노드가 클러스터에 합류할 수 있도록 아래와 같이 토큰을 생성할 수 있습니다.\nCopy bash k8s-master$ sudo kubeadm token create --print-join-command \u0026gt; ~/join.sh k8s-master$ cat join.sh kubeadm join 192.168.50.13:6443 --token ... 마스터 노드에서 kubeadm token create 의 출력 결과를 복사해서 워커 노드에서 실행할 수도 있지만, 향후 쿠버네티스 설치 과정을 Ansible 등을 사용해 자동화할 계획이 있어서 GUI를 거치지 않고 SSH 통신을 통해 출력 결과를 전달했습니다.\nCopy bash k8s-master$ scp ~/join.sh minyeamer@k8s-worker1:/home/minyeamer/ k8s-worker1$ sudo ./join.sh k8s-master$ scp ~/join.sh minyeamer@k8s-worker2:/home/minyeamer/ k8s-worker2$ sudo ./join.sh 마스터 노드로부터 전달받은 join.sh 파일, 즉 kubeadm join 명령어를 워커 노드에서 실행하고 기다리면 모든 노드가 활성화되는 것을 확인할 수 있습니다.\nCopy bash k8s-master$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready control-plane 29m v1.33.2 k8s-worker1 NotReady \u0026lt;none\u0026gt; 16m v1.33.2 k8s-worker2 NotReady \u0026lt;none\u0026gt; 15m v1.33.2 Copy bash k8s-master$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready control-plane 30m v1.33.2 k8s-worker1 Ready \u0026lt;none\u0026gt; 16m v1.33.2 k8s-worker2 Ready \u0026lt;none\u0026gt; 16m v1.33.2 설정하기 (기타) # kubectl 명령어를 입력하는 마스터 노드에서 편의를 위한 설정을 진행합니다.\n8. 쿠버네티스 편의기능 설치 # 쿠버네티스 공식문서 중 kubectl 치트 시트 에 따라 kubectl 명령어를 k 로 단축하는 것과 같은 자동완성 설정을 할 수 있습니다.\nCopy bash sudo apt-get update sudo apt-get install bash-completion echo \u0026#39;source \u0026lt;(kubectl completion bash)\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;alias k=kubectl\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;complete -o default -F __start_kubectl k\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc 9. 쿠버네티스 대시보드 배포 # 쿠버네티스 공식문서 중 쿠버네티스 대시보드를 배포하고 접속하기 에 따르면 아래와 같은 웹 UI(쿠버네티스 대시보드)를 배포할 수 있습니다.\n쿠버네티스 대시보드 웹 UI에 대한 파드를 생성하고, 파드의 443 포트를 호스트의 포트(8443 등)에 포워딩하여 웹 UI에 접속할 수 있습니다. 해당 과정은 아래에서 제시합니다.\nCopy bash kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.1/aio/deploy/recommended.yaml kubectl create serviceaccount admin-user -n kubernetes-dashboard kubectl create clusterrolebinding admin-user \\ --clusterrole=cluster-admin \\ --serviceaccount=kubernetes-dashboard:admin-user kubectl create token admin-user -n kubernetes-dashboard \u0026gt; ~/admin-token kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard 8443:443 \u0026amp; 이제 막 쿠버네티스를 설치한 직후라 그런지, 아니면 사용량이 올바르게 수집되지 않아서인진 모르겠지만 아무런 내용도 나타나지 않습니다. 나중에 쿠버네티스를 충분히 실습하고 나서 대시보드에 대해 다시 알아보겠습니다.\nReferences # https://kubernetes.io/ko/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ https://kubernetes.io/ko/docs/setup/production-environment/container-runtimes/ https://docs.tigera.io/calico/latest/getting-started/kubernetes/self-managed-onprem/onpremises https://www.inflearn.com/course/쿠버네티스-어나더-클래스-지상편-sprint1 https://cafe.naver.com/kubeops/91 https://captcha.tistory.com/78 https://kubernetes.io/ko/docs/reference/kubectl/cheatsheet/ https://kubernetes.io/ko/docs/tasks/access-application-cluster/web-ui-dashboard/ "},{"id":7,"href":"/blog/container-history/","title":"컨테이너의 역사와 쿠버네티스 등장 배경 - LXC에서 containerd까지","section":"Posts","content":"Study Overview # 쿠버네티스 어나더 클래스 (지상편) - Sprint 1, 2 과정을 따릅니다. [따배쿠] 쿠버네티스 시리즈 과정(유튜브 공개 영상)을 따릅니다. 목적 # 쿠버네티스가 무엇이고, 쿠버네티스로 어떤 것을 할 수 있는지 알기 위해 학습합니다. 두 가지 커리큘럼을 참고하지만, 그대로 따라하지 않고 개발 환경에 맞춰서 설치 버전 등을 변경합니다. 이론보다는 실습 위주로 직접 명령어를 입력하고 결과를 보면서 쿠버네티스를 익힙니다. Container History # LXC # 컨테이너의 역사는 chroot 를 발표한 것으로 시작합니다. chroot 는 \u0026quot;change root\u0026quot;의 약자로, 프로세스의 root 디렉터리를 변경하는 기능입니다. 해당 프로세스가 지정된 디렉터리 이하로만 접근할 수 있도록 하여 격리시킬 수 있습니다.\ncgroup 은 \u0026quot;control group\u0026quot;의 약자로, 프로세스의 리소스 사용량(CPU, 메모리 등)을 제한 또는 격리할 수 있습니다. 시스템 관리자는 cgroup 을 통해 특정 프로세스 그룹에 할당할 수 있는 리소스의 양을 제한하거나, 우선순위를 지정하고, 리소스 사용 현황을 모니터링할 수 있습니다. 컨테이너 환경에서 여러 프로세스가 실행될 때 각 컨테이너별로 자원을 분리하고 관리하는 데 핵심적으로 사용됩니다.\nnamespace 는 시스템 리소스(프로세스, 네트워크 등)를 서로 독립적으로 분리해서 사용할 수 있게 만듭니다. 컨테이너 기술의 핵심 기반으로, 각각의 컨테이너가 독립된 환경처럼 동작할 수 있게 해줍니다.\nLXC 또는 리눅스 컨테이너는 cgroup, namesapce 와 같은 리눅스 커널의 기능을 활용하여 하나의 리눅스 시스템에서 여러 개의 격리된 리눅스 환경(컨테이너)을 실행할 수 있게 해주는 운영체제 수준의 가상화 기술입니다.\nRed Hat의 토픽 Linux 컨테이너란? 에 따르면, LXC 이전에 FreeBSD Jail 이라고, FreeBSD 시스템에서 여러 개의 독립적인 환경을 실행할 수 있게 해주는 컨테이너의 시초가 있었다고 합니다. 이후에 VServer 프로젝트를 통해 Linux에 격리된 환경이 구현되었습니다.\nDocker # Docker 는 LXC 에서 출발한 상위 레벨의 컨테이너 기술로, 간편한 CLI와 서버 데몬, 사전 쿠축된 컨테이너 이미지 라이브러리 및 레지스트리 서버의 개념 등 여러 가지의 새로운 개념과 툴을 선보였습니다.\nrkt 는 Docker 이후에 등장한, 보안과 표준 준수를 중시한 컨테이너 런타임입니다. rkt 는 CoreOS에서 개발한 오픈소스 컨테이너 런타임이었는데, CoreOS가 Red Hat에 인수되면서 Fedora CoreOS로 이름이 바뀌었습니다. Red Hat의 토픽 rkt란 무엇일까요? 에 따르면, rkt 의 핵심 실행 단위가 pod 인데, 공유 컨텍스트에서 실행되는 하나 이상의 애플리케이션(컨테이너) 집합을 의미하며, 쿠버네티스의 최소 배포 단위로 사용되는 개념입니다.\nKubernetes # Docker 가 나온 이후 Kubernetes, Docker Swarm 등 컨테이너 오케스트레이션이라는 컨테이너의 배포, 관리, 확장 및 네트워킹을 자동화하는 프로세스가 만들어졌습니다. 현재는 Kubernetes 가 주로 사용됩니다.\n따라서, Kubernetes 와 호환성이 좋은지가 컨테이너를 선택하는 중요한 결정요소가 되었는데, Docker 는 컨테이너 런타임 뿐 아니라 빌드, CLI, 데몬 등 여러 구성요소를 포함해 Kubernetes 의 런타임으로 Docker 를 사용할 때 불필요한 오버헤드와 복잡성이 발생했습니다.\nKubernetes 는 다양한 컨테이너 런타임과의 호환을 위해 CRI(Container Runtime Interface)라는 표준을 도입했습니다. Docker 는 CRI를 준수하지 않아 Dockershim 이라는 별도의 계층을 만들어야 했습니다.\ncontainerd # containerd 는 Docker 에서 불필요한 기능을 제거한 경량 런타임으로, 리소스 사용량이 적고 성능이 뛰어납니다. 따라서, Docker 를 런타임으로 쓸 떄보다 CPU나 메모리 사용량이 줄고 관리 포인트가 단순화됩니다. container 는 CNCF(Cloud Native Computing Foundation)에 기부되어 관리되며, OCI(Open Container Initiative) 표준을 준수합니다. Kubernetes 내부에서 컨테이너를 실행하는 엔진으로 containerd 가 사용됩니다.\nKubernetes 와 Docker 의 관계에 대한 자세한 내용은 쿠버네티스 블로그의 게시글 당황하지 마세요. 쿠버네티스와 도커 를 참고할 수 있습니다.\nCRI-O 는 CNCF에서 관리되는 containerd 와 달리, Red Hat 등이 관리하며 엔터프라이즈 Kubernetes 환경에서 널리 사용됩니다. 범용 컨테이너 런타임 목적의 containerd 와 다르게, CRI-O 는 Kubernetes 와의 통합만을 목적으로 불필요한 기능을 배제한 최소한의 런타임입니다.\nCRI-O 에 대한 자세한 설명은 Red Hat의 공식문서 CRI-O Runtime 을 참고할 수 있습니다.\nKubernetes History # Docker # LXC 는 리눅스 커널의 기능을 가지고 만든 Low Level 컨테이너 런타임입니다. Docker 는 LXC 를 기반으로 libcontainer 라는 Low Level 컨테이너 런타임을 만들었습니다. Docker 는 libcontainer 를 기반으로 사용자 친화적으로 만든 High Level 컨테이너 런타임입니다.\nDocker 에는 CLI, 로그 관리, 저장공간, 네트워크 등의 부가 기능이 많아 사용자 편의가 좋지만, 이 중에서 컨테이너를 만드는 역할은 containder 가 담당하며 libcontainer 를 이용합니다.\nkubelet # Kubernetes 는 kube-apiserver 와 kubelet 이라는 컴포넌트가 있습니다. Kubernetes 에는 컨테이너를 만드는 명령어는 없지만, Pod 안에 컨테이너를 하나 이상 명시할 수 있습니다.\nkube-apiserver 가 Pod를 생성해달라는 호출을 받으면 kubelet 에게 전달하고, kubelet 은 컨테이너 런타임한테 컨테이너를 생성해달라는 요청을 보냅니다. 그러면, 컨테이너 런타임이 직접적으로 컨테이너를 생성합니다.\nkubelet 은 Pod의 생성부터 실행, 상태 관리, 리소스 모니터링을 담당합니다. kubelet 이 중단되면 해당 노드에서 Pod의 생성 및 관리가 불가능해져, 클러스터의 정상 운영에 큰 영향을 미칩니다.\nCRI # kubelet 은 Docker 또는 다른 컨테이너 런타임에 알맞은 API를 호출합니다. 하지만, containerd 가 분리되고 CRI-O 가 추가되는 등 컨테이너 런타임이 늘어나게 되면서 다양한 컨테이너 런타임과 통신하기 위한 표준이 필요했습니다.\nCRI는 다양한 컨테이너 런타임과 표준화된 방식으로 통신할 수 있게 해주는 핵심 인터페이스입니다. CRI는 gRPC 프로토콜을 사용하여 kubelet 과 컨테이너 런타임이 효율적으로 명령과 상태 정보를 주고받도록 합니다. CRI를 통해 컨테이너 런타임 종류에 관계없이 일관된 방식으로 컨테이너와 이미지를 관리할 수 있게 되었습니다.\nOCI # 컨테이너의 종류가 많아지면서 컨테이너 런타임과 관련 업계 개방형 표준을 만들기 위해 Red Hat과 Docker 등 여러 기업에 의해 설립된 단체가 OCI입니다. OCI는 컨테이너 런타임과 이미지 포맷의 업계 표준을 정의하여, 다양한 플랫폼과 도구 간의 호환성을 마련하는 것이 핵심 목적입니다.\nDocker 는 OCI 규격을 맞추기 위해 Low Level로 runC 를 만들고 containerd 에서도 runC 를 사용하게 됩니다. runC 가 기존 libcontainer 와의 차이점은 LXC 를 이용하지 않고 바로 커널 레벨의 가상화 기술을 사용한다는 것입니다.\nCRI-Plugin # Docker 와 같은 컨테이너 런타임에 새 기능이 생기면 쿠버네티스도 같이 패치를 해야합니다. 이때마다 CRI를 수정하기 위해 쿠버네티스를 패치해야 하는 구조를, kubelet 에서 컨테이너 런타임으로 바로 받을 수 있게 구조를 바꿉니다.\n이런 구조를 지원하기 위해 containerd 에서는 CRI-Plugin 이라는 기능을 추가했습니다. CRI-Plugin 은 kubelet 과 다양한 컨테이너 런타임 사이의 표준 통신을 가능하게 해주는 플러그인 형태의 구현체로, CRI 명세를 실제로 구현한 모듈입니다.\n해당 과정에서 Docker 를 지원하던 dockershim 은 관리가 잘 안되는 문제로 1.24 버전부터 제거되었습니다. 1.24 버전부터는 Mirantis가 Docker 를 인수하여 cri-dockerd 라는 어댑터를 만들고 Mirantis Container Runtime이란 이름으로 Docker 를 지원합니다.\nReferences # https://www.inflearn.com/course/쿠버네티스-어나더-클래스-지상편-sprint1 https://www.redhat.com/ko/topics/containers/linux-keonteineolan https://www.redhat.com/ko/topics/containers/what-is-rkt https://kubernetes.io/ko/blog/2020/12/02/dont-panic-kubernetes-and-docker/ https://docs.redhat.com/en/documentation/openshift_container_platform/3.11/html-single/cri-o_runtime/index "},{"id":8,"href":"/blog/spark-study-7/","title":"Apache Spark - 외부 데이터베이스 연동 (PostgreSQL, MySQL)","section":"Posts","content":"Spark SQL CLI # 스파크 SQL 쿼리를 실행하는 쉬운 방법은 spark-sql CLI이다. 스파크 SQL CLI는 Hive 메타스토어와 서비스와 통신하는 대신 Thrift JDBC 서버와 통신할 수 없다.\nHive 설치 # 진행하기 전에 Hive가 설치되어 있지 않아서 설치해야 했다.\n설치 과정은 [Hive] virtual box linux [ubuntu 18.04]에 하이브 설치,다운로드 4.ubuntu 에 Hive(하이브) 다운로드 게시글을 참고했다.\n브라우저 또는 curl, wget 등 명령어를 통해 압축 파일을 내려받는다. Copy bash wget https://dlcdn.apache.org/hive/hive-4.0.1/apache-hive-4.0.1-bin.tar.gz 압축 해제 프로그램을 사용하거나, 터미널에서 아래 명령어를 입력하여 압축 해제한다. Copy bash tar zxvf apache-hive-4.0.1-bin.tar.gz Hive 경로에 접근하기 위해 ~/.zshrc 에 환경변수를 설정한다. Copy bash export HIVE_HOME=/Users/{username}/hive-4.0.1 export PATH=$PATH:$HIVE_HOME/bin 변경 사항을 적용하기 위해 터미널을 재시작하거나 아래 명령어를 실행한다. Copy bash source ~/.zshrc $HIVE_HOME/bin/hive-config.sh 파일에 HDFS 경로를 추가한다. Copy bash export HADOOP_HOME=/Users/{username}/hadoop-3.4.0 HDFS에 Hive 디렉터리를 생성한다. Copy bash $HADOOP_HOME/sbin/start-all.sh Copy bash hdfs dfs -mkdir /tmp hdfs dfs -chmod g+w /tmp Copy bash hdfs dfs -mkdir -p /user/hive/warehouse hdfs dfs -chmod g+w /user/hive/warehouse Copy bash % hdfs dfs -ls / drwxrwxr-x - user supergroup 0 2025-07-12 10:08 /tmp drwxr-xr-x - user supergroup 0 2025-07-12 10:08 /user $HIVE_HOME/conf/hive-site.xml 파일에 아래 속성을 맨 윗부분에 추가한다. 파일이 없을 경우 동일한 경로의 hive-default.xml.template 파일을 hive-site.xml 이름의 파일로 복사한다. Copy xml \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;system:java.io.tmpdir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/tmp/hive/java\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;system:user.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;${user.name}\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; Derby DB를 시작한다. 오류가 발생할 경우 참고한 게시글을 확인해볼 수 있다. Copy bash $HIVE_HOME/bin/schematool -initSchema -dbType derby Copy bash Initializing the schema to: 4.0.0 Metastore connection URL:\tjdbc:derby:;databaseName=metastore_db;create=true Metastore connection Driver :\torg.apache.derby.jdbc.EmbeddedDriver Metastore connection User:\tAPP Starting metastore schema initialization to 4.0.0 Initialization script hive-schema-4.0.0.derby.sql ... Initialization script completed Hive CLI를 시작해본다. Copy bash $HIVE_HOME/bin/hive Copy bash Beeline version 4.0.1 by Apache Hive beeline\u0026gt; Hive 메타스토어 서버를 실행한다. Copy bash hive --service metastore \u0026amp; hive-site.xml 편집하기 # Hive Tables 공식문서에 따르면, Spark SQL로 Hive에 저장된 데이터에 액세스하려면 hive-site.xml, core-site.xml, hdfs-site.xml 파일들을 $SPARK_HOME/conf/ 경로에 배치해야 한다.\n그런데 위 파일들을 복사한 후 spark-sql 을 실행하니까 WARN HiveConf 메시지가 460줄이나 발생했다.\nCopy bash 25/07/12 11:00:36 WARN HiveConf: HiveConf of name hive.repl.dump.metadata.only.for.external.table does not exist 25/07/12 11:00:36 WARN HiveConf: HiveConf of name hive.druid.rollup does not exist 25/07/12 11:00:36 WARN HiveConf: HiveConf of name hive.repl.retain.prev.dump.dir does not exist ... 단순히 Hive 경로에서 $SPARK_HOME/conf/ 경로로 hive-site.xml 파일을 복사했는데, Spark가 사용하지 않는 속성들이 들어있어서 이러한 메시지가 발생했다.\n실제 동작에는 영향을 주지 않지만 spark-sql 을 실행할 때마다 이런 메시지를 볼 수는 없어서 hive-site.xml 파일에서 문제되는 속성들을 전부 삭제했다.\n속성을 하나씩 삭제하기에는 너무 많아서 파이썬 코드를 사용해 hive-site.xml 파일을 수정했다. properties 변수에 문제되는 속성의 이름에 대한 문자열 리스트를 할당하고 코드를 실행한다.\nCopy python import xml.etree.ElementTree as ET import os SPARK_HOME = os.environ.get(\u0026#34;SPARK_HOME\u0026#34;) properties = [] # 제거하고 싶은 속성 이름 리스트 tree = ET.parse(f\u0026#34;{SPARK_HOME}/conf/hive-site.xml\u0026#34;) root = tree.getroot() targets = [] # 삭제 대상 property 수집 for prop in root.findall(\u0026#34;property\u0026#34;): name = prop.find(\u0026#34;name\u0026#34;) if (name is not None) and (name.text in properties): targets.append(prop) for prop in targets: root.remove(prop) # 속성 삭제 tree.write(f\u0026#34;{SPARK_HOME}/conf/hive-site.cleaned.xml\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;, xml_declaration=True) 생성된 hive-site.cleaned.xml 내용을 확인하고 hive-site.xml 로 바꿔준다.\nspark-sql # $SPARK_HOME/bin/spark-sql 스크립트를 실행해 스파크 SQL CLI를 시작한다.\nCopy bash $SPARK_HOME/bin/spark-sql 셸을 시작하면 스파크 SQL 쿼리를 대화 형식으로 수행할 수 있다. --help 옵션을 통해 아래와 같은 CLI 옵션을 확인할 수 있다.\nCopy text CLI options: -d,--define \u0026lt;key=value\u0026gt; Hive 쿼리에서 사용할 변수(key)와 값(value)을 지정 --database \u0026lt;databasename\u0026gt; 사용할 데이터베이스 지정 -e \u0026lt;quoted-query-string\u0026gt; 명령어 입력창에서 직접 SQL 쿼리를 실행할 때 사용 -f \u0026lt;filename\u0026gt; SQL 쿼리가 작성된 파일을 실행할 때 사용 -H,--help 도움말 제공 --hiveconf \u0026lt;property=value\u0026gt; Hive 설정값을 지정할 때 사용 --hivevar \u0026lt;key=value\u0026gt; Hive 쿼리에서 사용할 변수(key)와 값(value)을 지정 -i \u0026lt;filename\u0026gt; CLI 실행 시 먼저 실행될 쿼리 파일 제공 -S,--silent 대화형 셸에서 결과만 출력하고 기타 정보는 무시 -v,--verbose SQL 쿼리문을 콘솔에 출력 스파크 SQL 테이블을 생성하려면 다음 쿼리를 실행한다.\nCopy bash spark-sql (default)\u0026gt; CREATE TABLE people (name STRING, age INT); Time taken: 0.685 seconds 테이블이 생성되었는지 확인한다.\nCopy bash spark-sql (default)\u0026gt; SHOW TABLES; people Time taken: 0.239 seconds, Fetched 1 row(s) 테이블을 생성하고 테이블에 데이터를 삽입한다.\nCopy bash spark-sql (default)\u0026gt; INSERT INTO people VALUES (\u0026#34;Michael\u0026#34;, NULL); Time taken: 1.728 seconds spark-sql (default)\u0026gt; INSERT INTO people VALUES (\u0026#34;Andy\u0026#34;, 30); Time taken: 0.601 seconds spark-sql (default)\u0026gt; INSERT INTO people VALUES (\u0026#34;Samantha\u0026#34;, 19); Time taken: 0.149 seconds 테이블에서 20세 미만의 사람들이 몇 명인지 확인해본다.\nCopy bash spark-sql (default)\u0026gt; SELECT * FROM people WHERE age \u0026lt; 20; Samantha\t19 Time taken: 0.285 seconds, Fetched 1 row(s) 비라인 작업 # 비라인은 SQLLine CLI를 기반으로 하는 JDBC 클라이언트다. 동일한 유틸리티를 사용해 스파크 쓰리프트 서버에 대해 스파크 SQL 쿼리를 실행할 수 있다.\n스파크 쓰리프트 JDBC/ODBC 서버를 시작하려면 $SPARK_HOME/sbin/start-thriftserver.sh 스크립트를 실행한다.\nCopy bash $SPARK_HOME/sbin/start-thriftserver.sh 비라인을 사용하여 쓰리프트 JDBC/ODBC 서버를 테스트한다.\nCopy bash $SPARK_HOME/bin/beeline 비라인을 구성하여 로컬 쓰리프트 서버에 연결한다. 사용자 이름은 로그인 계정을 입력하고 비밀번호는 비어 있다.\nCopy bash beeline\u0026gt; !connect jdbc:hive2://localhost:10000 Connecting to jdbc:hive2://localhost:10000 Enter username for jdbc:hive2://localhost:10000: user Enter password for jdbc:hive2://localhost:10000: Connected to: Spark SQL (version 4.0.0) Driver: Hive JDBC (version 2.3.10) Transaction isolation: TRANSACTION_REPEATABLE_READ 0: jdbc:hive2://localhost:10000\u0026gt; 비라인에서 스파크 SQL 쿼리를 실행할 수 있다.\nCopy bash 0: jdbc:hive2://localhost:10000\u0026gt; SHOW TABLES; +------------+------------+--------------+ | namespace | tableName | isTemporary | +------------+------------+--------------+ | default | people | false | +------------+------------+--------------+ 1 row selected (0.297 seconds) Copy bash 0: jdbc:hive2://localhost:10000\u0026gt; SELECT * FROM people; +-----------+-------+ | name | age | +-----------+-------+ | Samantha | 19 | | Andy | 30 | | Michael | NULL | +-----------+-------+ 3 rows selected (1.44 seconds) 쓰리프트 서버를 중지할 때는 stop-thriftserver.sh 스크립트를 실행한다.\nCopy bash $SPARK_HOME/sbin/stop-thriftserver.sh 외부 데이터 소스 # JDBC # 스파크 SQL에는 JDBC를 사용하여 다른 데이터베이스에서 데이터를 읽을 수 있는 데이터 소스 API가 포함되어 있다. 스파크 SQL의 이점을 활용하여 쿼리 결과를 DataFrame으로 반환받을 수 있다.\nJDBC 데이터 소스에 연결하려면 JDBC 드라이버를 지정해야 한다. spark-shell 을 실행할 때 클래스 경로를 지정할 수 있다. 클래스 경로에 특정 데이터베이스용 JDBC 드라이버를 포함해야 한다.\nCopy bash $SPARK_HOME/bin/spark-shell --driver-class-path $database.jar --jars $database.jar 데이터 소스 옵션 # 사용자는 데이터 소스 옵션에서 JDBC 연결 속성을 지정할 수 있다. 다음과 같은 일반적인 연결 속성을 제공한다.\nuser, password : 데이터 소스에 로그인하기 위한 계정 정보 url : JDBC 연결 URL, jdbc:subprotocol:subname 와 같은 형식 dbtable : 읽거나 쓸 JDBC 테이블, query 옵션과 동시에 사용할 수는 없다. query : 스파크로 데이터를 읽어오는 데 사용되는 쿼리, dbtable 옵션과 동시에 사용할 수는 없다. driver : 지정한 URL에 연결하는 데 사용할 JDBC 드라이버의 클래스 이름 스파크 SQL과 JDBC 외부 소스 간에 많은 양의 데이터를 전송할 때 데이터 소스를 분할할 필요가 있다. 대규모 작업에서 다음과 같은 속성을 사용할 수 있다.\nnumPartitions : 테이블 읽기 및 쓰기에서 병렬 처리를 위해 사용할 수 있는 최대 파티션 수, 또는 최대 동시 JDBC 연결 수 partitionColumn : 외부 소스를 읽을 때 파티션을 결정하기 위해 사용되는 칼럼 (숫자, 날짜, 또는 타임스탬프) lowerBound : 파티션 크기에 대한 파티션 열의 최솟값 upperBound : 파티션 크기에 대한 파티션 열의 최댓값 numPartitions 는 스파크 워커 수의 배수를 사용하는 것이 좋지만, 소스 시스템이 읽이 요청을 얼마나 잘 처리할 수 있는지 확인해야 한다.\npartitionColumn 은 데이터 스큐를 방지하기 위해 균일하게 분산될 수 있는 열을 선택해야 한다.\n예를 들어, {numPartitions : 10, lowerBound : 1000, upperBound : 10000} 을 선택했지만 대부분이 2000에서 3000 사이의 값을 요청하는 경우 다른 partitionColumn 을 사용하거나 새 항목을 생성하는 것이 좋다.\nPostgreSQL # PostgreSQL 실행 # PostgreSQL은 따로 설치하지 않고 Docker 컨테이너로 실행했다.\nCopy bash % docker run --name postgres13 -d -p 5432:5432 postgres:13 % docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6d3a827005a6 postgres:13 \u0026#34;docker-entrypoint.s…\u0026#34; 1 seconds ago Up 2 seconds 0.0.0.0:5432-\u0026gt;5432/tcp, [::]:5432-\u0026gt;5432/tcp postgres13 postgres13 컨테이너에 접속하면서 PostgreSQL 프롬프트에 진입한다.\nCopy bash % docker exec -it postgres13 psql -U postgres psql (13.21 (Debian 13.21-1.pgdg120+1)) Type \u0026#34;help\u0026#34; for help. postgres=# SparkSession에서 접속해보기 위해 임시로 사용자, 스키마, 테이블을 생성했다.\nCopy sql CREATE USER spark WITH PASSWORD \u0026#39;spark\u0026#39;; CREATE SCHEMA spark_schema AUTHORIZATION spark; CREATE TABLE spark_schema.users ( id SERIAL PRIMARY KEY, name VARCHAR(100) NOT NULL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); GRANT ALL PRIVILEGES ON SCHEMA spark_schema TO spark; GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA spark_schema TO spark; 3개 행만 추가해보고 내용을 확인해봤다.\nCopy sql INSERT INTO spark_schema.users (name) VALUES (\u0026#39;김민수\u0026#39;); INSERT INTO spark_schema.users (name) VALUES (\u0026#39;이민수\u0026#39;); INSERT INTO spark_schema.users (name) VALUES (\u0026#39;박민수\u0026#39;); Copy sql postgres=# SELECT * FROM spark_schema.users; id | name | created_at ----+--------+---------------------------- 1 | 김민수 | 2025-07-12 11:29:42.40485 2 | 이민수 | 2025-07-12 11:29:47.036362 3 | 박민수 | 2025-07-12 11:29:50.087099 (3 rows) PostgreSQL 드라이버 다운로드 # PostgreSQL 데이터베이스에 연결하려면 JDBC 드라이버 파일을 클래스 경로에 추가한다.\n이미지 링크로 연결된 위 웹사이트에서 Java 버전에 맞는 파일을 다운로드 받을 수 있는데 Java 8 이상인 경우 아래 URL을 통해 직접 다운로드 받을 수도 있다.\nCopy bash wget https://jdbc.postgresql.org/download/postgresql-42.7.7.jar PostgreSQL 데이터 읽기 # SparkSession을 생성할 때 앞단계에서 내려받은 JDBC 드라이버 파일의 경로를 spark.driver.extraClassPath 설정값으로 전달한다.\nCopy python from pyspark.sql import SparkSession import os SPARK_HOME = os.environ.get(\u0026#34;SPARK_HOME\u0026#34;) spark = (SparkSession .builder .config(\u0026#34;spark.driver.extraClassPath\u0026#34;, f\u0026#34;{SPARK_HOME}/jars/postgresql-42.7.7.jar\u0026#34;) \\ .appName(\u0026#34;PostgresExample\u0026#34;) .getOrCreate()) postgres 데이터베이스의 spark_schema.users 테이블의 데이터를 가져온다. 데이터를 출력해보면 앞에서 추가한 3개 행이 반환되는 것을 볼 수 있다.\nCopy python df = spark.read.format(\u0026#34;jdbc\u0026#34;) \\ .option(\u0026#34;url\u0026#34;, \u0026#34;jdbc:postgresql://localhost:5432/postgres\u0026#34;) \\ .option(\u0026#34;dbtable\u0026#34;, \u0026#34;spark_schema.users\u0026#34;) \\ .option(\u0026#34;user\u0026#34;, \u0026#34;spark\u0026#34;) \\ .option(\u0026#34;password\u0026#34;, \u0026#34;spark\u0026#34;) \\ .load() df.show() Copy sql +---+------+--------------------+ | id| name| created_at| +---+------+--------------------+ | 1|김민수|2025-07-12 11:29:...| | 2|이민수|2025-07-12 11:29:...| | 3|박민수|2025-07-12 11:29:...| +---+------+--------------------+ PostgreSQL 데이터 쓰기 # 반대로 DataFrame을 PostgreSQL에 새로운 테이블로 저장할 수도 있다.\nCopy python df.write.format(\u0026#34;jdbc\u0026#34;) \\ .option(\u0026#34;url\u0026#34;, \u0026#34;jdbc:postgresql://localhost:5432/postgres\u0026#34;) \\ .option(\u0026#34;dbtable\u0026#34;, \u0026#34;spark_schema.new_users\u0026#34;) \\ .option(\u0026#34;user\u0026#34;, \u0026#34;spark\u0026#34;) \\ .option(\u0026#34;password\u0026#34;, \u0026#34;spark\u0026#34;) \\ .save() PostgreSQL에서 새로운 테이블을 조회했을 때 동일한 데이터가 저장된 것을 볼 수 있다.\nCopy sql postgres=# SELECT * FROM spark_schema.new_users; id | name | created_at ----+--------+------------------------------- 1 | 김민수 | 2025-07-12 02:29:42.40485+00 2 | 이민수 | 2025-07-12 02:29:47.036362+00 3 | 박민수 | 2025-07-12 02:29:50.087099+00 (3 rows) 또한, 기존 테이블에 새로운 행으로 추가할 수도 있다.\nCopy python df.select(\u0026#34;name\u0026#34;).write.format(\u0026#34;jdbc\u0026#34;) \\ .option(\u0026#34;url\u0026#34;, \u0026#34;jdbc:postgresql://localhost:5432/postgres\u0026#34;) \\ .option(\u0026#34;dbtable\u0026#34;, \u0026#34;spark_schema.users\u0026#34;) \\ .option(\u0026#34;user\u0026#34;, \u0026#34;spark\u0026#34;) \\ .option(\u0026#34;password\u0026#34;, \u0026#34;spark\u0026#34;) \\ .mode(\u0026#34;append\u0026#34;) \\ .save() spark_schema.users 테이블의 id 열이 SERIAL 타입인데, 시퀀스에 대한 권한이 없어서 위 명령어를 실행하면 오류가 발생했다. 그래서 PostgreSQL에서 spark 사용자에게 권한을 부여했다.\nCopy sql GRANT USAGE, SELECT ON SEQUENCE spark_schema.users_id_seq TO spark; 스파크의 DataFrameWriter를 통해 spark_schema.users 테이블에 새로운 행을 추가하고 데이터를 조회하면 아래와 같이 3개의 행이 더 추가된 것을 볼 수 있다.\nCopy sql postgres=# SELECT * FROM spark_schema.users; id | name | created_at ----+--------+---------------------------- 1 | 김민수 | 2025-07-12 11:29:42.40485 2 | 이민수 | 2025-07-12 11:29:47.036362 3 | 박민수 | 2025-07-12 11:29:50.087099 4 | 김민수 | 2025-07-12 20:42:51.519473 5 | 이민수 | 2025-07-12 20:42:51.519473 6 | 박민수 | 2025-07-12 20:42:51.519473 (6 rows) 비슷한 시간에 데이터를 추가했는데 컨테이너는 UTC 시간대고 SparkSession은 KST 시간대에 있어서 created_at 이 9시간 차이가 나는 것 같다.\nMySQL # MySQL 실행 # 마찬가지로 MySQL 컨테이너를 실행한다.\nCopy bash % docker run --name mysql8 -e MYSQL_ROOT_PASSWORD=root -d -p 3306:3306 mysql:8 % docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 48d3c4f28fd6 mysql:8 \u0026#34;docker-entrypoint.s…\u0026#34; 11 seconds ago Up 10 seconds 0.0.0.0:3306-\u0026gt;3306/tcp, [::]:3306-\u0026gt;3306/tcp mysql8 6d3a827005a6 postgres:13 \u0026#34;docker-entrypoint.s…\u0026#34; About an hour ago Up About an hour 0.0.0.0:5432-\u0026gt;5432/tcp, [::]:5432-\u0026gt;5432/tcp postgres13 mysql8 컨테이너에 접속하면서 MySQL 프롬프트에 진입한다.\nCopy bash % docker exec -it mysql8 mysql -u root -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 9 Server version: 8.4.5 MySQL Community Server - GPL ... mysql\u0026gt; SparkSession에서 접속해보기 위해 임시로 사용자, 스키마, 테이블을 생성했다.\nCopy sql CREATE USER \u0026#39;spark\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;spark\u0026#39;; CREATE DATABASE spark_db; USE spark_db; CREATE TABLE users ( id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(100) NOT NULL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); GRANT ALL PRIVILEGES ON spark_db.* TO \u0026#39;spark\u0026#39;@\u0026#39;%\u0026#39;; FLUSH PRIVILEGES; 3개 행만 추가해보고 내용을 확인해봤다.\nCopy sql INSERT INTO users (name) VALUES (\u0026#39;kim\u0026#39;); INSERT INTO users (name) VALUES (\u0026#39;lee\u0026#39;); INSERT INTO users (name) VALUES (\u0026#39;park\u0026#39;); Copy sql mysql\u0026gt; SELECT * FROM users; +----+------+---------------------+ | id | name | created_at | +----+------+---------------------+ | 1 | kim | 2025-07-12 12:08:41 | | 2 | lee | 2025-07-12 12:08:45 | | 3 | park | 2025-07-12 12:08:48 | +----+------+---------------------+ 3 rows in set (0.01 sec) MySQL 드라이버 다운로드 # MySQL 데이터베이스에 연결하려면 JDBC 드라이버 파일을 클래스 경로에 추가한다.\n이미지 링크로 연결된 위 웹사이트 또는 아래와 같은 curl, wget 명령어 등으로 버전에 맞는 압축 파일을 다운로드 받을 수 있다.\nCopy bash wget https://downloads.mysql.com/archives/get/p/3/file/mysql-connector-j-8.4.0.tar.gz 압축 파일을 해제하면 JDBC 드라이버 파일을 확인할 수 있다.\nCopy bash % tar zxvf mysql-connector-j-8.4.0.tar.gz % ls -la mysql-connector-j-8.4.0 total 5888 drwxr-xr-x@ 10 user staff 320 Mar 13 2024 . drwx------@ 49 user staff 1568 Jul 12 21:12 .. -rw-r--r--@ 1 user staff 282811 Mar 13 2024 CHANGES -rw-r--r--@ 1 user staff 188 Mar 13 2024 INFO_BIN -rw-r--r--@ 1 user staff 134 Mar 13 2024 INFO_SRC -rw-r--r--@ 1 user staff 82896 Mar 13 2024 LICENSE -rw-r--r--@ 1 user staff 1624 Mar 13 2024 README -rw-r--r--@ 1 user staff 91633 Mar 13 2024 build.xml -rw-r--r--@ 1 user staff 2533399 Mar 13 2024 mysql-connector-j-8.4.0.jar drwxr-xr-x@ 8 user staff 256 Mar 13 2024 src MySQL 데이터 읽기 # SparkSession을 생성할 때 MySQL JDBC 드라이버 파일의 경로를 spark.driver.extraClassPath 설정값으로 전달한다.\nCopy python from pyspark.sql import SparkSession import os SPARK_HOME = os.environ.get(\u0026#34;SPARK_HOME\u0026#34;) spark = (SparkSession .builder .config(\u0026#34;spark.driver.extraClassPath\u0026#34;, f\u0026#34;{SPARK_HOME}/jars/mysql-connector-j-8.4.0.jar\u0026#34;) \\ .appName(\u0026#34;MySQLExample\u0026#34;) .getOrCreate()) spark_db 데이터베이스의 users 테이블의 데이터를 가져온다. 데이터를 출력해보면 앞에서 추가한 3개 행이 반환되는 것을 볼 수 있다.\nCopy python df = spark.read.format(\u0026#34;jdbc\u0026#34;) \\ .option(\u0026#34;url\u0026#34;, \u0026#34;jdbc:mysql://localhost:3306/spark_db\u0026#34;) \\ .option(\u0026#34;dbtable\u0026#34;, \u0026#34;users\u0026#34;) \\ .option(\u0026#34;user\u0026#34;, \u0026#34;spark\u0026#34;) \\ .option(\u0026#34;password\u0026#34;, \u0026#34;spark\u0026#34;) \\ .load() df.show() Copy sql +---+----+-------------------+ | id|name| created_at| +---+----+-------------------+ | 1| kim|2025-07-12 12:08:41| | 2| lee|2025-07-12 12:08:45| | 3|park|2025-07-12 12:08:48| +---+----+-------------------+ MySQL 데이터 쓰기 # 반대로 DataFrame을 MySQL에 새로운 테이블로 저장할 수도 있다.\nCopy python df.write.format(\u0026#34;jdbc\u0026#34;) \\ .option(\u0026#34;url\u0026#34;, \u0026#34;jdbc:mysql://localhost:3306/spark_db\u0026#34;) \\ .option(\u0026#34;dbtable\u0026#34;, \u0026#34;new_users\u0026#34;) \\ .option(\u0026#34;user\u0026#34;, \u0026#34;spark\u0026#34;) \\ .option(\u0026#34;password\u0026#34;, \u0026#34;spark\u0026#34;) \\ .save() MySQL에서 새로운 테이블을 조회했을 때 동일한 데이터가 저장된 것을 볼 수 있다.\nCopy sql mysql\u0026gt; SELECT * FROM new_users; +------+------+---------------------+ | id | name | created_at | +------+------+---------------------+ | 1 | kim | 2025-07-12 12:08:41 | | 2 | lee | 2025-07-12 12:08:45 | | 3 | park | 2025-07-12 12:08:48 | +------+------+---------------------+ 3 rows in set (0.00 sec) 또한, 기존 테이블에 새로운 행으로 추가할 수도 있다.\nCopy python df.select(\u0026#34;name\u0026#34;).write.format(\u0026#34;jdbc\u0026#34;) \\ .option(\u0026#34;url\u0026#34;, \u0026#34;jdbc:mysql://localhost:3306/spark_db\u0026#34;) \\ .option(\u0026#34;dbtable\u0026#34;, \u0026#34;users\u0026#34;) \\ .option(\u0026#34;user\u0026#34;, \u0026#34;spark\u0026#34;) \\ .option(\u0026#34;password\u0026#34;, \u0026#34;spark\u0026#34;) \\ .mode(\u0026#34;append\u0026#34;) \\ .save() 스파크의 DataFrameWriter를 통해 users 테이블에 새로운 행을 추가하고 데이터를 조회하면 아래와 같이 3개의 행이 더 추가된 것을 볼 수 있다.\nCopy sql mysql\u0026gt; SELECT * FROM users; +----+------+---------------------+ | id | name | created_at | +----+------+---------------------+ | 1 | kim | 2025-07-12 12:08:41 | | 2 | lee | 2025-07-12 12:08:45 | | 3 | park | 2025-07-12 12:08:48 | | 4 | kim | 2025-07-12 12:24:29 | | 5 | lee | 2025-07-12 12:24:29 | | 6 | park | 2025-07-12 12:24:29 | +----+------+---------------------+ 6 rows in set (0.00 sec) References # https://spark.apache.org/docs/latest/sql-distributed-sql-engine-spark-sql-cli.html https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html https://spidyweb.tistory.com/215 https://dlcdn.apache.org/hive/hive-4.0.1/ https://kevin717.tistory.com/50 https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html https://jdbc.postgresql.org/download/ https://downloads.mysql.com/archives/c-j/ "},{"id":9,"href":"/blog/spark-study-6/","title":"Apache Spark - 다양한 데이터 소스 읽기/쓰기 (Parquet, JSON, CSV, Avro)","section":"Posts","content":"Data Source API # DataFrameReader # DataFrameReader는 데이터 소스에서 DataFrame으로 데이터를 읽는 방식이다. 아래와 같이 권장되는 사용 패턴이 있다.\nCopy python DataFrameReader .format(args) # 데이터 소스 형식 .option(\u0026#34;key\u0026#34;, \u0026#34;value\u0026#34;) # 키/값 쌍으로 연결되는 옵션 .schema(args) # DDL 문자열 또는 StructType .load() # 데이터 소스의 경로 데이터 소스 형식에는 인수로 \u0026quot;parquet\u0026quot;, \u0026quot;csv\u0026quot;, \u0026quot;txt\u0026quot;, \u0026quot;json\u0026quot;, \u0026quot;jdbc\u0026quot;, \u0026quot;orc\u0026quot;, \u0026quot;avro\u0026quot; 등이 전달된다. 기본값은 \u0026quot;parquet\u0026quot; 또는 spark.sql.sources.default 에 지정된 항목이 설정된다.\nJSON이나 CSV 형식은 option() 함수에서 스키마를 유추하는 inferSchema 옵션을 적용할 수 있지만, 스키마를 제공하면 로드 속도가 빨라진다.\nSparkSession 인스턴스를 통해서 DataFrame에 액세스할 경우 read() 또는 readStream() 을 사용할 수 있다. read() 는 정적 데이터 소스에서 DataFrame을 읽어 오며, readStream() 은 스트리밍 소스에서 인스턴스를 반환한다.\nDataFrameWriter # DataFrameWriter는 데이터 소스에 데이터를 저장하거나 쓰는 작업을 수행한다. 권장되는 사용 형식은 다음과 같다.\nCopy python DataFrameWriter .format(args) # 데이터 소스 형식 .option(args) # 키/값 쌍으로 연결되는 옵션 .bucketBy(args) # 버킷 개수 및 버킷 기준 칼럼 이름 .partitionBy(args) # 데이터 소스의 경로 .save(path) # 저장할 테이블 DataFrame에서 인스턴스에 액세스할 경우 write() 또는 writeStream() 을 사용할 수 있다.\nData Sources # Parquet # 스파크의 기본 데이터 소스인 Parquet는 다양한 I/O 최적화를 제공하는 오픈소스 칼럼 기반 파일 형식이다. 압축을 통해 저장 공간을 절약하고 데이터 칼럼에 대한 빠른 액세스를 허용한다.\nParquet 파일은 데이터 파일, 메타데이터, 여러 압축 파일 및 일부 상태 파일이 포함된 디렉터리 구조가 저장된다. 메타데이터에는 파일 형식의 버전, 스키마, 경로 등의 칼럼 데이터가 포함된다.\ndatabricks/LearningSparkV2의 databricks-datasets/learning-spark-v2/flights/summary-data/parquet 경로에서 2010-summary.parquet/ 디렉터리를 가져온다.\nParquet 파일의 디렉터리에는 다음과 같은 파일 집합이 포함된다.\nCopy bash % ls -la data/flights/summary-data/parquet/2010-summary.parquet/ -rwxr-xr-x@ ... 0 ... _SUCCESS -rwxr-xr-x@ ... 3921 ... part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet Parquet 파일을 DataFrame으로 읽으려면 형식과 경로를 지정하기만 하면 된다. spark.sql.sources.default 설정을 하지 않았다면 .format(\u0026quot;parquet\u0026quot;) 함수는 생략해도 된다.\nCopy python file = \u0026#34;data/flights/summary-data/parquet/2010-summary.parquet\u0026#34; df = spark.read.format(\u0026#34;parquet\u0026#34;).load(file) df.show(5) Copy bash +-----------------+-------------------+-----+ |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| +-----------------+-------------------+-----+ | United States| Romania| 1| | United States| Ireland| 264| | United States| India| 69| | Egypt| United States| 24| |Equatorial Guinea| United States| 1| +-----------------+-------------------+-----+ Parquet 파일을 Spark SQL 테이블로 읽으려면 아래와 같은 스파크 SQL을 사용할 수 있다.\nCopy sql CREATE OR REPLACE TEMPORARY VIEW delay_flights USING parquet OPTIONS ( path \u0026#34;data/flights/summary-data/parquet/2010-summary.parquet\u0026#34;) 메타데이터가 궁금해서 parquet-tools 라이브러리를 설치하고, inspect 명령어로 part-XXXX 압축 파일을 조회했다. 아래와 같이 출력되었는데, 3개의 칼럼 DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count 에 대한 데이터 타입 등의 정보가 상세히 적혀 있다. 스파크는 해당 데이터 타입을 읽기 때문에, 위 DataFrame에 printSchema() 출력한 결과는 아래 칼럼별 데이터 타입과 같다.\nCopy bash % parquet-tools inspect data/flights/summary-data/parquet/2010-summary.parquet/part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet ############ file meta data ############ created_by: parquet-mr (build 32c46643845ea8a705c35d4ec8fc654cc8ff816d) num_columns: 3 num_rows: 255 num_row_groups: 1 format_version: 1.0 serialized_size: 658 ############ Columns ############ DEST_COUNTRY_NAME ORIGIN_COUNTRY_NAME count ############ Column(DEST_COUNTRY_NAME) ############ name: DEST_COUNTRY_NAME path: DEST_COUNTRY_NAME max_definition_level: 1 max_repetition_level: 0 physical_type: BYTE_ARRAY logical_type: String converted_type (legacy): UTF8 compression: GZIP (space_saved: 37%) ############ Column(ORIGIN_COUNTRY_NAME) ############ name: ORIGIN_COUNTRY_NAME path: ORIGIN_COUNTRY_NAME max_definition_level: 1 max_repetition_level: 0 physical_type: BYTE_ARRAY logical_type: String converted_type (legacy): UTF8 compression: GZIP (space_saved: 39%) ############ Column(count) ############ name: count path: count max_definition_level: 1 max_repetition_level: 0 physical_type: INT64 logical_type: None converted_type (legacy): NONE compression: GZIP (space_saved: 53%) DataFrame을 DataFrameWriter를 사용해 Parquet 파일로 저장할 때는 아래와 같은 함수를 사용할 수 있다.\nCopy python (df.write.format(\u0026#34;parquet\u0026#34;) .mode(\u0026#34;overwrite\u0026#34;) .option(\u0026#34;compression\u0026#34;, \u0026#34;snappy\u0026#34;) .save(\u0026#34;/tmp/data/parquet/df_parquet\u0026#34;)) 압축 방식으로 snappy 를 사용하여 snappy 압축 파일이 생성된다. DataFrame을 직접 저장하면 아래와 같은 파일이 생성된다.\nCopy bash % ls -la /tmp/data/parquet/df_parquet/ -rw-r--r--@ ... 8 ... ._SUCCESS.crc -rw-r--r--@ ... 52 ... .part-00000-9828b287-9956-40cb-9c33-d59bea52e5be-c000.snappy.parquet.crc -rw-r--r--@ ... 0 ... _SUCCESS -rw-r--r--@ ... 5442 ... part-00000-9828b287-9956-40cb-9c33-d59bea52e5be-c000.snappy.parquet JSON # JSON 데이터 형식은 XML에 비해 읽기 쉽고, 구문을 분석하기 쉬운 형식이다. 단일 라인 모드와 다중 라인 모드를 모두 지원한다. 단일 라인 모드에서는 각 라인이 단일 JSON 개체를 나타내지만, 다중 라인 모드에서는 전체 라인 객체가 단일 JSON 개체를 구성한다. option() 함수에서 multiLine 옵션에 ture 또는 false 를 설정할 수 있다.\n단일 라인 모드의 JSON 데이터는 아래와 같이 구성된다.\nCopy bash % head -n 5 data/flights/summary-data/json/2010-summary.json {\u0026#34;ORIGIN_COUNTRY_NAME\u0026#34;:\u0026#34;Romania\u0026#34;,\u0026#34;DEST_COUNTRY_NAME\u0026#34;:\u0026#34;United States\u0026#34;,\u0026#34;count\u0026#34;:1} {\u0026#34;ORIGIN_COUNTRY_NAME\u0026#34;:\u0026#34;Ireland\u0026#34;,\u0026#34;DEST_COUNTRY_NAME\u0026#34;:\u0026#34;United States\u0026#34;,\u0026#34;count\u0026#34;:264} {\u0026#34;ORIGIN_COUNTRY_NAME\u0026#34;:\u0026#34;India\u0026#34;,\u0026#34;DEST_COUNTRY_NAME\u0026#34;:\u0026#34;United States\u0026#34;,\u0026#34;count\u0026#34;:69} {\u0026#34;ORIGIN_COUNTRY_NAME\u0026#34;:\u0026#34;United States\u0026#34;,\u0026#34;DEST_COUNTRY_NAME\u0026#34;:\u0026#34;Egypt\u0026#34;,\u0026#34;count\u0026#34;:24} {\u0026#34;ORIGIN_COUNTRY_NAME\u0026#34;:\u0026#34;United States\u0026#34;,\u0026#34;DEST_COUNTRY_NAME\u0026#34;:\u0026#34;Equatorial Guinea\u0026#34;,\u0026#34;count\u0026#34;:1} JSON 파일을 DataFrame으로 읽으려면 아래처럼 .format(\u0026quot;json\u0026quot;) 을 지정한다.\nCopy python file = \u0026#34;data/flights/summary-data/json/*\u0026#34; df = spark.read.format(\u0026#34;json\u0026#34;).load(file) df.show(5) Copy bash +-----------------+-------------------+-----+ |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| +-----------------+-------------------+-----+ | United States| Romania| 15| | United States| Croatia| 1| | United States| Ireland| 344| | Egypt| United States| 15| | United States| India| 62| +-----------------+-------------------+-----+ JSON 데이터 소스에 대해 DataFrameReader 및 DataFrameWriter 에 대한 일반적인 옵션은 아래와 같다.\ncompression : 압축 코덱을 쓰기 시에 사용할 수 있다. bzip2, gzip, snappy 등이 값으로 전달된다. dateFormat : Java의 DateTimeFormatter에서 제공하는 모든 형식을 사용할 수 있다. (yyyy-MM-dd 등) multiLine : true 를 지정하면 다중 라인 모드를 사용한다. 기본값은 false 이다. allowUnquotedFileNames : 따옴표로 묶이지 않은 JSON 필드 이름을 허용한다. 기본값은 false 이다. CSV # 쉼표로 각 데이터 또는 필드를 구분하며, 쉼표로 구분된 각 줄은 레코드를 나타낸다. 쉼표가 데이터의 일부인 경우, 값을 쌍따옴표로 감싸주거나, 다른 구분 기호를 사용하여 필드를 분리할 수 있다.\n일반적인 CSV 데이터는 아래와 같이 구성된다.\nCopy bash % head -n 5 data/flights/summary-data/csv/2010-summary.csv DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count United States,Romania,1 United States,Ireland,264 United States,India,69 Egypt,United States,24 CSV 파일을 DataFrame으로 읽으려면 아래처럼 .format(\u0026quot;csv\u0026quot;) 을 지정한다. 위 파일과 같이 헤더가 있는 경우 header 옵션에 true 를 설정한다. nullValue 옵션을 사용해 null 데이터를 특정 값으로 교체할 수 있다.\nCopy python file = \u0026#34;data/flights/summary-data/csv/*\u0026#34; df = spark.read.format(\u0026#34;csv\u0026#34;).option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;).load(file) df.show(5) Copy bash +-----------------+-------------------+-----+ |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| +-----------------+-------------------+-----+ | United States| Romania| 1| | United States| Ireland| 264| | United States| India| 69| | Egypt| United States| 24| |Equatorial Guinea| United States| 1| +-----------------+-------------------+-----+ CSV 데이터 소스에 대해 DataFrameReader 및 DataFrameWriter 에 대한 일반적인 옵션은 아래와 같다.\ncompression : 압축 코덱을 쓰기 시에 사용할 수 있다. bzip2, gzip, snappy 등이 값으로 전달된다. dateFormat : Java의 DateTimeFormatter에서 제공하는 모든 형식을 사용할 수 있다. (yyyy-MM-dd 등) multiLine : true 를 지정하면 다중 라인 모드를 사용한다. 기본값은 false 이다. interSchema : true 를 지정하면 스파크가 칼럼 데이터 유형을 결정한다. 기본값은 false 이다. sep : 칼럼을 구분하기 위한 문자이며, 기본 구분 기호는 쉼표(,)다. escape : 따옴표를 이스케이프할 때 사용하는 문자이며, 기본값은 / 다. header : 첫 번째 줄이 칼럼명을 나타내는 헤더일 경우 true 를 지정하고, 기본값은 false 이다. Avro # 스파크 2.4에 내장된 데이터 소스인 Avro 형식은 특히 아파치 카프카에서 메시지를 직렬화 및 역직렬화할 때 사용된다. JSON에 대한 직접 매핑, 속도와 효율성 등 많은 이점을 제공한다.\n스파크 공식 문서의 Apache Avro Data Source Guide에 따르면, spark-avro 모듈은 외부 모듈로 spark-submit 또는 spark-shell 에 포함되어 있지 않다. 따라서, 아래와 같이 --packages 를 사용하여 종속성을 추가할 수 있다.\nCopy bash ./bin/spark-shell --packages org.apache.spark:spark-avro_2.13:4.0.0 ... SparkSession 인스턴스를 사용할 경우, spark.jars.packages 설정에 spark-avro_2.13 종속성을 추가한다.\nCopy python from pyspark.sql import SparkSession spark = (SparkSession .builder .appName(\u0026#34;SparkAvroExampleApp\u0026#34;) .config(\u0026#34;spark.jars.packages\u0026#34;, \u0026#34;org.apache.spark:spark-avro_2.13:4.0.0\u0026#34;) .getOrCreate()) Avro 파일을 DataFrame으로 읽으려면 아래처럼 .format(\u0026quot;avro\u0026quot;) 을 지정한다.\nCopy python file = \u0026#34;data/flights/summary-data/avro/*\u0026#34; df = spark.read.format(\u0026#34;avro\u0026#34;).load(file) df.show(5) Copy bash +-----------------+-------------------+-----+ |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| +-----------------+-------------------+-----+ | United States| Romania| 1| | United States| Ireland| 264| | United States| India| 69| | Egypt| United States| 24| |Equatorial Guinea| United States| 1| +-----------------+-------------------+-----+ Avro 데이터 소스에 대해 DataFrameReader 및 DataFrameWriter 에 대한 일반적인 옵션은 아래와 같다.\navroSchema : JSON 형식으로 제공할 수 있는 Avro 스키마이다. Avro 데이터나 카탈리스트 데이터와 일치하지 않으면 읽기/쓰기 작업이 실패한다. recordName : Avro 사양에 필요한 쓰기 결과의 최상위 레코드명이다. recrodNamespace : 쓰기 결과에 네임스페이스를 기록한다. ignoreExtension : 확장자가 .avro 인지 여부에 관계없이 모든 파일을 읽어들인다. compression : 쓰기에 사용할 압축 코덱을 지정할 수 있다. Image # 스파크 2.4에서 머신러닝 프레임워크를 지원하기 위해 새로운 데이터 소스인 이미지 파일을 도입했다.\ndatabricks/LearningSparkV2의 databricks-datasets/learning-spark-v2 경로에서 cctvVideos/ 디렉터리를 가져온다. 해당 디렉터리의 구조는 다음과 같다.\nCopy text cctvVideos/ ├── README.md └── train_images/ ├── label=0/ │ ├── Browse2frame0000.jpg │ ├── Browse2frame0001.jpg │ ├── Browse2frame0002.jpg │ ├── ... | └── Walk2frame0042.jpg └── label=1/ ├── Fight_Chaseframe0012.jpg ├── Fight_Chaseframe0013.jpg ├── Fight_Chaseframe0014.jpg ├── ... └── Rest_WiggleOnFloorframe0050.jpg 이미지 파일은 아래와 같이 DataFrameReader 함수로 읽을 수 있다. 이미지 파일을 읽을 때 numpy 라이브러리가 필요하다.\nCopy python from pyspark.ml import image image_dir = \u0026#34;data/cctvVideos/train_images\u0026#34; images_df = spark.read.format(\u0026#34;image\u0026#34;).load(image_dir) images_df.printSchema() Copy bash root |-- image: struct (nullable = true) | |-- origin: string (nullable = true) | |-- height: integer (nullable = true) | |-- width: integer (nullable = true) | |-- nChannels: integer (nullable = true) | |-- mode: integer (nullable = true) | |-- data: binary (nullable = true) |-- label: integer (nullable = true) 이미지의 높이 및 너비와 같은 정보는 아래와 같이 조회할 수 있다.\nCopy python images_df.select(\u0026#34;image.height\u0026#34;, \u0026#34;image.width\u0026#34;, \u0026#34;image.nChannels\u0026#34;, \u0026#34;image.mode\u0026#34;, \u0026#34;label\u0026#34;).show(5) Copy bash +------+-----+---------+----+-----+ |height|width|nChannels|mode|label| +------+-----+---------+----+-----+ | 288| 384| 3| 16| 0| | 288| 384| 3| 16| 1| | 288| 384| 3| 16| 0| | 288| 384| 3| 16| 0| | 288| 384| 3| 16| 0| +------+-----+---------+----+-----+ Binary File # 이진 파일을 읽으려면 데이터 소스 형식을 binaryFile 로 지정해야 한다. DataFrameReader는 이진 파일을 원본 내용과 메타데이터를 포함하는 단일 DataFrame 행으로 변환한다.\npathGlobFilter 를 사용하면 지정된 전역 패턴과 일치하는 경로로 파일을 로드할 수 있다. 아래 코드는 디렉터리에서 모든 .jpg 파일을 읽는다.\nCopy python path = \u0026#34;data/cctvVideos/train_images\u0026#34; binary_files_df = spark.read.format(\u0026#34;binaryFile\u0026#34;).option(\u0026#34;pathGlobFilter\u0026#34;, \u0026#34;*.jpg\u0026#34;).load(path) binary_files_df.show(5) Copy bash +--------------------+-------------------+------+--------------------+-----+ | path| modificationTime|length| content|label| +--------------------+-------------------+------+--------------------+-----+ |file:/Users/cuz/D...|2025-01-28 13:30:40| 55037|[FF D8 FF E0 00 1...| 0| |file:/Users/cuz/D...|2025-01-28 13:30:40| 54634|[FF D8 FF E0 00 1...| 1| |file:/Users/cuz/D...|2025-01-28 13:30:40| 54624|[FF D8 FF E0 00 1...| 0| |file:/Users/cuz/D...|2025-01-28 13:30:40| 54505|[FF D8 FF E0 00 1...| 0| |file:/Users/cuz/D...|2025-01-28 13:30:40| 54475|[FF D8 FF E0 00 1...| 0| +--------------------+-------------------+------+--------------------+-----+ References # https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ https://github.com/databricks/LearningSparkV2/tree/master/databricks-datasets/learning-spark-v2/flights/summary-data https://spark.apache.org/docs/latest/api/python/tutorial/sql/python_data_source.html https://spark.apache.org/docs/latest/sql-data-sources.html https://spark.apache.org/docs/latest/sql-data-sources-avro.html "},{"id":10,"href":"/blog/install-ubuntu-server/","title":"Mac에서 UTM으로 Ubuntu Server 24.04 설치하기","section":"Posts","content":"리눅스 개발 환경이 필요해졌는데 단순 테스트를 위해 새로운 PC를 사거나 기존에 사용하는 Mac에 직접 설치하는건 비용이나 리스크가 있어 간단하게 가상환경을 이용하려 합니다.\nMac에서 무료로 이용할 수 있는 가상화 소프트웨어인 UTM을 사용해 우분투 서버를 설치하는 과정을 진행합니다.\n1. Ubuntu 이미지 다운로드 # Ubuntu Server 24.04.2 LTS 이미지를 다운로드 받습니다. (이미지 클릭 시 다운로드 경로로 이동)\n주의할 점은, 기본 다운로드 경로인 https://ubuntu.com/download/server로 접속하면 x86 아키텍처와 호환되는 amd64 이미지로 연결되기 때문에, 애플 실리콘 기반의 Mac이라면 다운로드 받는 파일이 arm64 이미지인지 확인해야 합니다.\n또는 터미널에서 내려받을 수도 있습니다.\nCopy bash curl -O -L https://cdimage.ubuntu.com/releases/24.04/release/ubuntu-24.04.2-live-server-arm64.iso Copy bash % ls -la ubuntu-24.04.2-live-server-arm64.iso -rw-r--r--@ 1 user group 2922393600 Jul 6 11:29 ubuntu-24.04.2-live-server-arm64.iso 2. UTM 설치하기 # UTM 최신 버전(작성일 기준 4.6.5)을 설치합니다. (이미지 클릭 시 다운로드 경로로 이동)\n앱스토어에서도 설치할 수 있는데 $9.99를 지불해야 합니다.\nUTM을 실행하면 다음과 같은 화면이 나타납니다.\n3. 가상머신 생성하기 # 새 가상머신 만들기를 선택합니다.\nStart 화면에서 Virtualize를 선택합니다.\n운영체제는 Linux를 선택합니다.\n이미지 파일 추가 # Boot ISO Image에 앞에서 다운로드 받았던 Ubuntu 이미지 파일을 추가합니다.\n하드웨어 설정 # 메모리와 CPU 크기는 목적에 맞게 설정합니다.\n저장공간도 목적에 맞게 설정합니다.\n가상머신 이름 설정 # 저장공간 설정 후에 나오는 공유폴더 설정은 무시합니다. 마지막으로 요약 화면이 나오는데 가상머신 이름을 설정합니다.\n저장을 누르면 가상머신이 생성된 것을 확인할 수 있습니다.\n가상머신 네트워크 설정 # 추가로, Ubuntu를 설치하기 전에 가상머신에서 네트워크 설정을 적용했습니다. 향후 여러 대의 가상머신과 Mac 간의 통신을 원활히 하기 위해 브릿지 모드를 선택했습니다. 이러한 경우가 아니라면 기본 설정인 Shared Network(NAT) 모드를 사용해도 됩니다.\n4. Ubuntu 설치하기 # 앞에서 생성한 가상머신을 실행합니다. \u0026quot;Try or Install Ubuntu Server\u0026quot; 를 선택합니다.\n언어 및 설치 유형 # 언어 및 키보드 레이아웃은 기본값인 \u0026quot;English\u0026quot; 를 선택합니다.\n설치 유형은 기본값인 \u0026quot;Ubuntu Server\u0026quot; 를 선택합니다.\n네트워크 설정 # 네트워크 설정에선 기본적으로 DHCP를 통한 동적 IP 주소가 적용되어 있습니다.\n향후 여러 가상머신 간 고정된 IP 주소를 가지고 통신할 필요가 있기 때문에 정적으로 IP 주소를 지정하겠습니다. 이러한 경우가 아니라면 DHCP를 유지한채 넘어가도 무방합니다.\n가상머신을 실행하기 전에 네트워크 설정에서 브릿지 모드로 변경했기 때문에 맥의 네트워크와 동일한 대역을 사용할 수 있습니다. NAT 모드로 가상머신을 실행 중이라면 DHCP를 통해 배정된 IP 주소를 바탕으로 대역을 추정해 IP 주소를 지정해야 합니다.\n정적 IP 주소를 할당했다면 다음과 같이 static 으로 표시됩니다.\n프록시 및 미러 서버 설정 # 프록시 서버는 기본값으로 무시합니다.\n미러 서버는 소프트웨어 패키지를 다운로드 받는 공식 서버의 복제본입니다. 보통 패키지를 다운로드 받을 때 미러 서버를 통해 받습니다. 기본값으로는 \u0026quot;kr.ports.ubuntu.com/ubuntu-ports\u0026quot; 로 지정되어 있는데, 속도가 더 빠른 카카오 미러 서버 \u0026quot;mirror.kakao.com\u0026quot; 로 변경했습니다.\n저장공간 설정 # 저장공간도 기본 설정인 \u0026quot;Use an entire disk\u0026quot; 를 적용합니다. 목적에 따라 파티션을 분리할 수도 있지만, 현재는 파티션을 나눌 필요가 없습니다.\n설치를 진행하게 되면 디스크 포맷을 통해 저장된 데이터가 삭제될 수 있다고 경고하는데 그대로 진행합니다.\n프로필 설정 # 사용자 이름, 서버 이름 등을 설정합니다.\nYour name : 이름 정보 (서버 운영과 무관) Your server's name : 서버 호스트명 Pick a username : 로그인 사용자 이름 Choose a password : 로그인 사용자 비밀번호 Confirm your password : 로그인 사용자 비밀번호 확인 기타 설정 및 설치 # Ubuntu Pro 업그레이드 여부를 묻는데 사용하지 않으므로 넘어갑니다.\nOpenSSH 서버 설치를 묻는데 SSH 서버를 사용하기 위해 체크합니다.\n설치 패키지 선택창이 나오는데 필요한건 직접 설치할 것이기 때문에 다음으로 넘어갑니다.\n설치가 진행되고, 설치가 완료되면 \u0026quot;Reboot Now\u0026quot; 선택지가 생깁니다. 재부팅을 수행합니다.\n5. Ubuntu 접속 # 최초 설치 후 재부팅하면 더이상 진행되지 않고 커서만 깜빡이는데, 일단 종료하고 UTM 화면으로 돌아갑니다. 가상머신에서 부팅용 이미지 파일을 초기화한 후 다시 실행합니다.\n가상머신을 실행하면 로그인 화면이 나타납니다. 프로필 설정에 지정한 사용자 이름과 비밀번호를 순차적으로 입력합니다.\n정상적으로 로그인되었다면 아래와 같이 명령어를 입력할 수 있는 프롬프트가 나타납니다.\n참고 자료 # https://ubuntu.com/download/server/arm https://mac.getutm.app/ https://gymdev.tistory.com/75 https://moneymentors.tistory.com/entry/우분투Ubuntu-22042-LTS-서버-설치-방법 https://mirror.kakao.com/ "},{"id":11,"href":"/blog/spark-study-5/","title":"Apache Spark - 스파크 SQL과 테이블/뷰 관리","section":"Posts","content":"Spark SQL # 스파크 SQL은 다음과 같은 특징을 갖는다.\n정형화 API가 엔진으로 제공한다. 다양한 정형 데이터(Parquet 등)를 읽거나 쓸 수 있다. 외부 BI 툴(태블로 등)의 데이터 소스나 RDBMS(MySQL 등)의 데이터를 쿼리할 수 있다. 정형 데이터에 대해 SQL 쿼리를 실행할 수 있는 대화형 쉘을 제공한다. Spark SQL 사용법 # Copy python spark.sql(\u0026#34;SELECT * FROM table\u0026#34;) SparkSession 객체에 sql() 함수를 사용한다. 쿼리 결과로는 DataFrame 객체가 반환된다.\nSpark SQL 활용 (Python) # databricks/LearningSparkV2의 databricks-datasets/learning-spark-v2/flights 경로에서 미국 항공편 운항 지연 데이터세트 departuredelays.csv 를 가져온다. 해당 데이터를 활용해 아래와 같이 임시뷰를 생성한다.\nCopy python from pyspark.sql import SparkSession spark = (SparkSession .builder .appName(\u0026#34;SparkSQLExampleApp\u0026#34;) .getOrCreate()) # 데이터 경로 csv_file = \u0026#34;data/flights/departuredelays.csv\u0026#34; # 스키마를 추론하여 데이터를 읽기 df = (spark.read.format(\u0026#34;csv\u0026#34;) .option(\u0026#34;inferSchema\u0026#34;, \u0026#34;true\u0026#34;) .option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;) .load(csv_file)) # 데이터로부터 임시뷰를 생성 df.createOrReplaceTempView(\u0026#34;delay_flights\u0026#34;) 스파크 SQL을 사용해 임시뷰에 대해 SQL 쿼리를 실행할 수 있다. 스파크 SQL은 ANSI:2003과 호환되는 SQl 인터페이스를 제공한다.\nCopy python spark.sql(\u0026#34;\u0026#34;\u0026#34; SELECT distance, origin, destination FROM delay_flights WHERE distance \u0026gt; 1000 ORDER BY distance DESC;\u0026#34;\u0026#34;\u0026#34;).show(10) Copy bash +--------+------+-----------+ |distance|origin|destination| +--------+------+-----------+ | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| +--------+------+-----------+ only showing top 10 rows 위의 쿼리는 아래 파이썬 예제와 같이 동등한 DataFrame API로 표현할 수 있다.\nCopy python from pyspark.sql.functions import col, desc (df.select(\u0026#34;distance\u0026#34;, \u0026#34;origin\u0026#34;, \u0026#34;destination\u0026#34;) .where(col(\u0026#34;distance\u0026#34;) \u0026gt; 1000) .orderBy(desc(\u0026#34;distance\u0026#34;))).show(10) SQL 쿼리로 단순 SELECT 조회뿐 아니라 현재 생성된 임시뷰 목록을 조회할 수도 있다.\nCopy python spark.sql(\u0026#34;SHOW TABLES;\u0026#34;).show() Copy bash +---------+-------------+-----------+ |namespace| tableName|isTemporary| +---------+-------------+-----------+ | |delay_flights| true| +---------+-------------+-----------+ 앞에서 스키마를 추론하여 DataFrame을 읽었는데, SQL 쿼리로 어떤 스키마로 인식되었는지 확인해 보았다. DataFrame 객체의 스키마 df.schema 의 출력 결과와 동일하다.\nCopy python spark.sql(\u0026#34;DESC delay_flights;\u0026#34;).show() Copy bash +-----------+---------+-------+ | col_name|data_type|comment| +-----------+---------+-------+ | date| int| NULL| | delay| int| NULL| | distance| int| NULL| | origin| string| NULL| |destination| string| NULL| +-----------+---------+-------+ Table \u0026amp; View # 스파크는 테이블을 위한 별도 메타스토어를 생성하지 않고 기본적으로 /user/hive/warehouse 경로에 있는 아파치 하이브 메타스토어를 사용해 테이블에 대한 모든 메타데이터를 유지한다.\n스파크는 관리형과 비관리형이라는 두 가지 유형의 테이블로 만들 수 있다.\n관리형 테이블은 스파크가 메타데이터와 파일 저장소의 데이터를 모두 관리한다. 따라서, DROP TABLE 과 같은 SQL 명령에 대해 메타데이터와 실제 데이터를 모두 삭제한다.\n반면에 비관리형 테이블은 스파크가 메타데이터만 관리하고 외부 데이터 소스에서 데이터를 직접 관리한다. 그래서, DROP TABLE 명령에도 실제 데이터는 그대로 두고 메타데이터만 삭제한다.\n테이블 생성하기 # 스파크는 기본적으로 default 데이터베이스 안에 테이블을 생성한다. SparkSession을 열고 현재 접속한 데이터베이스를 조회하면 알 수 있다.\nCopy python spark.sql(\u0026#34;SELECT current_database();\u0026#34;).show() Copy bash +----------------+ |current_schema()| +----------------+ | default| +----------------+ 우선, SQL 명령어를 실행하여 새로운 데이터베이스 learn_spark_db 를 생성할 수 있다. 생성한 데이터베이스를 사용하고 다시 현재 접속한 데이터베이스를 확인해 보았다.\nCopy python spark.sql(\u0026#34;CREATE DATABASE learn_spark_db;\u0026#34;) spark.sql(\u0026#34;USE learn_spark_db;\u0026#34;) spark.sql(\u0026#34;SELECT current_database();\u0026#34;).show() Copy bash +----------------+ |current_schema()| +----------------+ | learn_spark_db| +----------------+ 관리형 테이블 생성하기 # CREATE 문을 사용하여 현재 데이터베이스 안에 관리형 테이블을 생성할 수 있다.\nCopy python table = \u0026#34;managed_delay_flights\u0026#34; schema = \u0026#34;date STRING, delay INT, distaince INT, origin STRING, destination STRING\u0026#34; spark.sql(\u0026#34;CREATE TABLE {} ({});\u0026#34;.format(table, schema)) 위 SQL 쿼리는 마찬가지로 아래처럼 DataFrame API로 표현할 수도 있다. 이미 테이블을 만들었을 경우, 아래 파이썬 예제를 그대로 실행하면 TABLE_OR_VIEW_ALREADY_EXISTS 에러가 발생하므로 mode=\u0026quot;overwrite\u0026quot; 옵션을 넣어주어 기존 테이블을 덮어쓴다.\nCopy python csv_file = \u0026#34;data/flights/departuredelays.csv\u0026#34; flights_df = spark.read.csv(csv_file, schema=schema) flights_df.write.saveAsTable(table, mode=\u0026#34;overwrite\u0026#34;) 테이블을 생성하게 되면 현재 위치 아래의 spark-warehouse/{{DB명}}.db/{{테이블명}} 경로에 .parquet 파일들이 생성된다. 스파크 공식문서 중 Hive Table을 참고하면, 기본 디렉토리인 spark-warehouse 는 SparkSession을 실행할 때 spark.sql.warehouse.dir 설정을 통해 변경할 수 있다.\nCopy python warehouse_location = abspath(\u0026#39;spark-warehouse\u0026#39;) spark = SparkSession \\ .builder \\ .appName(\u0026#34;Python Spark SQL Hive integration example\u0026#34;) \\ .config(\u0026#34;spark.sql.warehouse.dir\u0026#34;, warehouse_location) \\ .enableHiveSupport() \\ .getOrCreate() 정적 설정이라 세션 실행 중에는 변경할 수 없어서 세션을 종료하고 다시 실행했다. saved 경로로 변경하고 다시 관리형 테이블을 생성해보니 해당 위치에 Parquet 파일들이 만들어졌다.\nCopy python from pyspark.sql import SparkSession from pathlib import Path warehouse_location = Path(\u0026#34;saved\u0026#34;) warehouse_location.mkdir(exist_ok=True) spark = (SparkSession .builder .appName(\u0026#34;SparkSQLExampleApp\u0026#34;) .config(\u0026#34;spark.sql.warehouse.dir\u0026#34;, str(warehouse_location.absolute())) .getOrCreate()) 비관리형 테이블 생성하기 # 기존 CREATE 문을 사용하는 SQL 쿼리에서 뒤에 USING 키워드로 시작하는 CSV 파일 경로를 붙여주어 외부 데이터 소스로부터 비관리형 테이블을 생성할 수 있다.\nCopy python import os table = \u0026#34;unmanaged_delay_flights\u0026#34; schema = \u0026#34;date STRING, delay INT, distaince INT, origin STRING, destination STRING\u0026#34; csv_file = os.path.join(os.getcwd(), \u0026#34;data/flights/departuredelays.csv\u0026#34;) spark.sql(\u0026#34;CREATE TABLE {} ({}) USING csv OPTIONS (PATH \u0026#39;{}\u0026#39;);\u0026#34;.format(table, schema, csv_file)) CSV 파일의 상대경로로 SQL 쿼리를 실행해보니까 FileStreamSink: Assume no metadata directory. 경고 메시지가 발생했는데 절대경로로 바꿔주니까 해결되었다.\nSQL 쿼리를 DataFrame API로 표현하면 아래와 같다. 관리형 테이블을 만드는 구문이랑 거의 비슷한데, path 옵션으로 /tmp 경로를 지정하는데 차이가 있다.\nCopy python (flights_df .write .option(\u0026#34;path\u0026#34;, \u0026#34;/tmp/data/delay_flights\u0026#34;) .saveAsTable(table, mode=\u0026#34;overwrite\u0026#34;)) 뷰 생성하기 # 기존 테이블을 토대로 뷰를 만들 수 있다. 전역(모든 SparkSession) 또는 세션 범위에서 생성할 수 있고, Spark Application이 종료되면 뷰는 사라진다.\n전역 임시 뷰는 SQL 쿼리에서는 GLOBAL TEMP VIEW 키워드를 추가하고, 파이썬에서는 createOrReplaceGlobalTempView() 함수를 호출하여 생성할 수 있다.\nCopy python table = \u0026#34;us_origin_airport_SFO\u0026#34; spark.sql(\u0026#34;\u0026#34;\u0026#34; CREATE OR REPLACE GLOBAL TEMP VIEW {} AS SELECT date, delay, origin, destination FROM delay_flights WHERE origin = \u0026#39;SFO\u0026#39;; \u0026#34;\u0026#34;\u0026#34;.format(table)) Copy python from pyspark.sql.functions import col table = \u0026#34;us_origin_airport_SFO\u0026#34; (df.select(\u0026#34;date\u0026#34;, \u0026#34;delay\u0026#34;, \u0026#34;origin\u0026#34;, \u0026#34;destination\u0026#34;) .where(col(\u0026#34;origin\u0026#34;) == \u0026#34;SFO\u0026#34;) .createOrReplaceGlobalTempView(table)) 전역 임시 뷰는 global_temp 라는 전역 임시 데이터베이스에 생성되며, global_temp.\u0026lt;view_name\u0026gt; 처럼 명시하여 뷰 테이블에 접근할 수 있다. 일반 임시 뷰는 현재 데이터베이스에 생성되므로 global_temp 접두사를 붙일 필요가 없다.\nCopy python spark.sql(\u0026#34;SELECT * FROM global_temp.{};\u0026#34;.format(table)).show(5) 메타데이터 보기 # 스파크는 관리형 및 비관리형 테이블에 대한 메타데이터를 관리한다. 메타데이터는 스파크 SQL의 상위 추상화 모듈인 카탈로그에 저장된다. 카탈로그에서 아래와 같이 데이터베이스, 테이블, 칼럼 목록을 조회할 수 있다.\nCopy python \u0026gt;\u0026gt;\u0026gt; spark.catalog.listDatabases() [Database(name=\u0026#39;default\u0026#39;, catalog=\u0026#39;spark_catalog\u0026#39;, description=\u0026#39;default database\u0026#39;, locationUri=\u0026#39;file:/Users/cuz/Documents/Github/study/spark/saved\u0026#39;)] Copy python \u0026gt;\u0026gt;\u0026gt; spark.catalog.listTables() [Table(name=\u0026#39;delay_flights\u0026#39;, catalog=None, namespace=[], description=None, tableType=\u0026#39;TEMPORARY\u0026#39;, isTemporary=True)] Copy python \u0026gt;\u0026gt;\u0026gt; spark.catalog.listColumns(\u0026#34;delay_flights\u0026#34;) [Column(name=\u0026#39;date\u0026#39;, description=None, dataType=\u0026#39;int\u0026#39;, nullable=True, isPartition=False, isBucket=False, isCluster=False), Column(name=\u0026#39;delay\u0026#39;, description=None, dataType=\u0026#39;int\u0026#39;, nullable=True, isPartition=False, isBucket=False, isCluster=False), Column(name=\u0026#39;distance\u0026#39;, description=None, dataType=\u0026#39;int\u0026#39;, nullable=True, isPartition=False, isBucket=False, isCluster=False), Column(name=\u0026#39;origin\u0026#39;, description=None, dataType=\u0026#39;string\u0026#39;, nullable=True, isPartition=False, isBucket=False, isCluster=False), Column(name=\u0026#39;destination\u0026#39;, description=None, dataType=\u0026#39;string\u0026#39;, nullable=True, isPartition=False, isBucket=False, isCluster=False)] SQL 테이블 캐싱하기 # 스파크 공식문서 중 CACHE TABLE을 참고하면, CACHE TABLE 쿼리를 사용하여 임시 뷰를 캐싱할 수 있다. CACHE LAZY TABLE 과 같이 LAZY 파라미터를 추가하면 테이블이 사용되는 시점까지 캐싱을 미룰 수 있다.\nCopy python spark.sql(\u0026#34;CACHE TABLE delay_flights;\u0026#34;) 테이블 캐시가 활성화 상태인지 보려면 카탈로그가 가진 isCached 함수를 참고할 수 있다. 캐시가 활성화되었다면 True, 아니면 False 를 반환한다.\nCopy python \u0026gt;\u0026gt;\u0026gt; spark.catalog.isCached(\u0026#34;delay_flights\u0026#34;) True Copy python \u0026gt;\u0026gt;\u0026gt; spark.catalog.isCached(\u0026#34;global_temp.us_origin_airport_sfo\u0026#34;) False 테이블 캐시를 삭제하고 싶다면 UNCACHE TABLE 쿼리를 사용한다.\nCopy python \u0026gt;\u0026gt;\u0026gt; spark.sql(\u0026#34;UNCACHE TABLE delay_flights;\u0026#34;) \u0026gt;\u0026gt;\u0026gt; spark.catalog.isCached(\u0026#34;delay_flights\u0026#34;) False 테이블을 DataFrame으로 변환하기 # SQL 쿼리로 테이블 전체를 읽을 수도 있지만, table() 함수를 사용할 수도 있다.\nCopy python spark.sql(\u0026#34;SELECT * FROM delay_flights;\u0026#34;) Copy python spark.table(\u0026#34;delay_flights\u0026#34;) sql() 과 table() 모두 DataFrame 객체를 반환한다.\nReferences # https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ https://github.com/databricks/LearningSparkV2/tree/master/databricks-datasets/learning-spark-v2/flights https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html https://spark.apache.org/docs/latest/sql-ref-syntax-aux-cache-cache-table.html https://spark.apache.org/docs/latest/sql-ref-syntax-aux-cache-uncache-table.html "},{"id":12,"href":"/blog/programmers-sql-4-5/","title":"프로그래머스 SQL Lv.4, 5 완전 정복 - 20문제 상세 풀이 (JOIN, GROUP BY, 윈도우 함수)","section":"Posts","content":"이직 준비를 하면서 오랜만에 코딩테스트를 보게되었는데, SQL 코딩테스트는 어떻게 나오나 궁금해서 프로그래머스 Lv.4, 5 수준의 문제를 모두 풀어보았습니다.\n모든 문제 # 표에서 ID를 클릭하면 프로그래머스 문제 풀이로 이동합니다. 문제는 최신순으로 정렬되어 있는데 풀이 순서는 표에서 역순으로, 즉 오래된순으로 나열합니다.\n문제를 순서대로 풀어보면 유사한 데이터를 사용하는 경우가 있어 비슷한건 큰 제목으로 묶었습니다. 테이블 구조는 생략하고 문제 요약 \u0026gt; 문제 해석 + 풀이 \u0026gt; SQL문 순서로 구성합니다.\nID제목유형난이도정답률문제풀기풀이보기 301651멸종위기의 대장균 찾기SELECTLv.521%링크링크301650특정 세대의 대장균 찾기SELECTLv.461%링크링크284528연간 평가점수에 해당하는 평가 등급 및 성과금 조회하기GROUP BYLv.472%링크링크276036언어별 개발자 분류하기GROUP BYLv.441%링크링크276035FrontEnd 개발자 찾기JOINLv.451%링크링크157339특정 기간동안 대여 가능한 자동차들의 대여비용 구하기JOINLv.449%링크링크151141자동차 대여 기록 별 대여 금액 구하기String, DateLv.451%링크링크144856저자 별 카테고리 별 매출액 집계하기GROUP BYLv.476%링크링크133027주문량이 많은 아이스크림들 조회하기JOINLv.474%링크링크132204취소되지 않은 진료 예약 조회하기String, DateLv.479%링크링크131537오프라인/온라인 판매 데이터 통합하기SELECTLv.467%링크링크131534상품을 구매한 회원 비율 구하기JOINLv.546%링크링크131532년, 월, 성별 별 상품 구매 회원 수 구하기GROUP BYLv.475%링크링크131124그룹별 조건에 맞는 식당 목록 출력하기JOINLv.471%링크링크131118서울에 위치한 식당 목록 출력하기SELECTLv.475%링크링크1311175월 식품들의 총매출 조회하기JOINLv.484%링크링크131116식품분류별 가장 비싼 식품의 정보 조회하기GROUP BYLv.485%링크링크62284우유와 요거트가 담긴 장바구니GROUP BYLv.474%링크링크59413입양 시각 구하기(2)GROUP BYLv.461%링크링크59045보호소에서 중성화한 동물JOINLv.485%링크링크 동물 입양 테이블 # 보호소에서 중성화한 동물 # ANIMAL_INS 테이블은 동물 보호소에 들어온 동물의 정보를 담은 테이블입니다. ANIMAL_OUTS 테이블은 동물 보호소에서 입양 보낸 동물의 정보를 담은 테이블입니다.\n보호소에서 중성화 수술을 거친 동물 정보를 알아보려 합니다. 보호소에 들어올 당시에는 중성화되지 않았지만, 보호소를 나갈 당시에는 중성화된 동물의 아이디와 생물 종, 이름을 조회하는 아이디 순으로 조회하는 SQL 문을 작성해주세요.\n보호소에 들어올 당시에 대한 테이블 ANIMAL_INS 에서 특정 조건만 선택하고, 보호소를 나갈 당시에 대한 테이블 ANIMAL_OUTS 에서 특정 조건만 선택해서, 두 테이블을 INNER JOIN 하여 두 조건을 만족하는 경우만 선택하는 문제입니다.\nANIMAL_INS 테이블에서는 중성화되지 않은 조건인, 성별 및 중성화 여부(SEX_UPON_INTAKE)에 \u0026quot;Intact\u0026quot; 단어가 포함된 경우를 선택하면 됩니다. 중성화 여부를 나타내는 단어는 앞에 있기 때문에 LIKE 'Intact%' 조건식을 사용했습니다.\nANIMAL_OUTS 테이블에서는 중성화된 조건인, 성별 및 중성화 여부(SEX_UPON_INTAKE)에 \u0026quot;Spayed\u0026quot; 또는 \u0026quot;Neutered\u0026quot; 단어가 포함된 경우를 선택하면 됩니다. 너무 길게 쓰는건 좋아하지 않아서 정규표현식을 활용하여 REGEXP '^Spayed|Neutered' 로 표현했습니다.\nCopy sql SELECT INS.ANIMAL_ID, INS.ANIMAL_TYPE, INS.NAME FROM ANIMAL_INS AS INS INNER JOIN ANIMAL_OUTS AS OUTS ON INS.ANIMAL_ID = OUTS.ANIMAL_ID WHERE INS.SEX_UPON_INTAKE LIKE \u0026#39;Intact%\u0026#39; AND OUTS.SEX_UPON_OUTCOME REGEXP \u0026#39;^Spayed|Neutered\u0026#39; ORDER BY ANIMAL_ID; 입양 시각 구하기(2) # ANIMAL_OUTS 테이블은 동물 보호소에서 입양 보낸 동물의 정보를 담은 테이블입니다.\n보호소에서는 몇 시에 입양이 가장 활발하게 일어나는지 알아보려 합니다. 0시부터 23시까지, 각 시간대별로 입양이 몇 건이나 발생했는지 조회하는 SQL문을 작성해주세요. 이때 결과는 시간대 순으로 정렬해야 합니다.\n입양일(DATETIME)에서 시간만 추출하고, 추출한 값을 기준으로 COUNT 집계하는 문제입니다.\n주의할 점은, 0시부터 23시까지 모든 행이 존재해야 합니다. ANIMAL_OUTS 테이블에 모든 시간대가 있지 않기 때문에 0시부터 23시까지 값을 가지는 테이블을 만들어야 합니다.\n잘 안써본 구문이라 찾아봤는데 RECURSIVE 라는 재귀적 쿼리를 활용하면 간단하게 해결될 것 같았습니다. 재귀적 쿼리를 사용한 HOURS 임시 테이블을 생성하고, 여기에 ANIMAL_OUTS 테이블을 COUNT 집계한 COUNTS 임시 테이블을 LEFT JOIN 으로 붙여서 모든 시간대를 표시했습니다.\nCopy sql WITH RECURSIVE HOURS AS ( SELECT 0 AS HOUR UNION ALL SELECT HOUR + 1 FROM HOURS WHERE HOUR \u0026lt; 23 ), COUNTS AS ( SELECT HOUR(DATETIME) AS HOUR, COUNT(ANIMAL_ID) AS COUNT FROM ANIMAL_OUTS GROUP BY HOUR ) SELECT HR.HOUR, COALESCE(CNT.COUNT, 0) AS COUNT FROM HOURS AS HR LEFT JOIN COUNTS AS CNT ON HR.HOUR = CNT.HOUR ORDER BY HOUR; 식품 테이블 # 우유와 요거트가 담긴 장바구니 # CART_PRODUCTS 테이블은 장바구니에 담긴 상품 정보를 담은 테이블입니다.\n데이터 분석 팀에서는 우유(Milk)와 요거트(Yogurt)를 동시에 구입한 장바구니가 있는지 알아보려 합니다. 우유와 요거트를 동시에 구입한 장바구니의 아이디를 조회하는 SQL 문을 작성해주세요. 이때 결과는 장바구니의 아이디 순으로 나와야 합니다.\n장바구니의 아이디(CART_ID) 그룹으로 집계하면서, 상품 종류(NAME) 에 \u0026quot;Milk\u0026quot;가 있는 경우와 \u0026quot;Yogurt\u0026quot;가 있는 경우를 모두 만족하는 아이디만 선택하는 문제입니다.\n자주 쓰는 Python에서는 Boolean 타입의 True / False 를 각각 정수 1과 0으로 인식할 수 있는데, SQL도 그렇지 않을까 싶어서 SUM 집계해봤습니다. 두 가지 조건에 대해 SUM 집계 결과가 1 이상인 경우만 선택해서 아이디만 조회했습니다.\nCopy sql WITH COUNTS AS ( SELECT CART_ID, SUM(NAME = \u0026#34;Milk\u0026#34;) AS MILK_COUNT, SUM(NAME = \u0026#34;Yogurt\u0026#34;) AS YOG_COUNT FROM CART_PRODUCTS GROUP BY CART_ID ) SELECT CART_ID FROM COUNTS WHERE MILK_COUNT \u0026gt; 0 AND YOG_COUNT \u0026gt; 0 ORDER BY CART_ID; 식품분류별 가장 비싼 식품의 정보 조회하기 # FOOD_PRODUCT 테이블은 식품의 정보를 담은 테이블입니다.\nFOOD_PRODUCT 테이블에서 식품분류별로 가격이 제일 비싼 식품의 분류, 가격, 이름을 조회하는 SQL문을 작성해주세요. 이때 식품분류가 '과자', '국', '김치', '식용유'인 경우만 출력시켜 주시고 결과는 식품 가격을 기준으로 내림차순 정렬해주세요.\n식품분류(CATEGORY) 그룹에서 가격(PRICE)이 가장 큰 항목만 선택하는 문제입니다.\n가격(PRICE)이 MAX 집계 결과와 동일한 항목만 선택하는 경우도 있지만, 더 간단하게 WINDOW 함수를 사용했습니다. 식품분류(CATEGORY) 파티션에 대해 가격(PRICE)을 내림차순으로 정렬한 순번(ROW_NUMBER)이 1인 경우만 선택하면 동일한 결과를 조회할 수 있습니다.\nCopy sql WITH HIGHEST AS ( SELECT PRODUCT_ID, ROW_NUMBER() OVER (PARTITION BY CATEGORY ORDER BY PRICE DESC) AS SEQ FROM FOOD_PRODUCT WHERE CATEGORY IN (\u0026#34;과자\u0026#34;,\u0026#34;국\u0026#34;,\u0026#34;김치\u0026#34;,\u0026#34;식용유\u0026#34;) ) SELECT PRD.CATEGORY, PRD.PRICE AS MAX_PRICE, PRD.PRODUCT_NAME FROM FOOD_PRODUCT AS PRD INNER JOIN (SELECT * FROM HIGHEST WHERE SEQ = 1) AS HGT ON PRD.PRODUCT_ID = HGT.PRODUCT_ID ORDER BY MAX_PRICE DESC; 5월 식품들의 총매출 조회하기 # FOOD_PRODUCT 테이블은 식품의 정보를 담은 테이블입니다. FOOD_ORDER 테이블은 식품의 주문 정보를 담은 테이블입니다.\nFOOD_PRODUCT 와 FOOD_ORDER 테이블에서 생산일자가 2022년 5월인 식품들의 식품 ID, 식품 이름, 총매출을 조회하는 SQL문을 작성해주세요. 이때 결과는 총매출을 기준으로 내림차순 정렬해주시고 총매출이 같다면 식품 ID를 기준으로 오름차순 정렬해주세요.\n식품 ID(PRODUCT_ID) 그룹에 대해 총매출(SUM(AMOUNT * PRICE))을 계산하는 문제입니다.\nJOIN 을 먼저해서 매출(AMOUNT * PRICE)을 계산하고 SUM 집계하는 경우도 가능한데, 개인적으로는 GROUP BY 시에 식품 이름(PRODUCT_NAME)과 같은 긴 문자열을 포함하는 것을 선호하지 않습니다.\n입고일(PRODUCE_DATE)에 따라 가격(PRICE)이 바뀌는 것도 아니기 때문에 JOIN 과 GROUP BY 의 순서를 신경쓸 필요도 없습니다. 물론, FOOD_PRODUCT 테이블에서 식품 ID(PRODUCT_ID)에 중복이 없음을 전제로 합니다.\n따라서, 식품 ID(PRODUCT_ID)에 대해서 먼저 GROUP BY 하여 총주문량(SUM(AMOUNT))을 계산하고, 가격(PRICE)을 나중에 곱해 총매출(SUM(AMOUNT) * PRICE)을 만들었습니다.\nCopy sql WITH AMOUNTS AS ( SELECT PRODUCT_ID, SUM(AMOUNT) AS TOTAL_AMOUNT FROM FOOD_ORDER WHERE PRODUCE_DATE BETWEEN \u0026#39;2022-05-01\u0026#39; AND \u0026#39;2022-05-31\u0026#39; GROUP BY PRODUCT_ID ) SELECT AMT.PRODUCT_ID, PRD.PRODUCT_NAME, (AMT.TOTAL_AMOUNT * PRD.PRICE) AS TOTAL_SALES FROM AMOUNTS AS AMT INNER JOIN FOOD_PRODUCT AS PRD ON AMT.PRODUCT_ID = PRD.PRODUCT_ID ORDER BY TOTAL_SALES DESC, PRODUCT_ID ASC; 식당 테이블 # 서울에 위치한 식당 목록 출력하기 # REST_INFO 테이블은 식당의 정보를 담은 테이블입니다. REST_REVIEW 테이블은 식당의 리뷰 정보를 담은 테이블입니다.\nREST_INFO 와 REST_REVIEW 테이블에서 서울에 위치한 식당들의 식당 ID, 식당 이름, 음식 종류, 즐겨찾기수, 주소, 리뷰 평균 점수를 조회하는 SQL문을 작성해주세요. 이때 리뷰 평균점수는 소수점 세 번째 자리에서 반올림 해주시고 결과는 평균점수를 기준으로 내림차순 정렬해주시고, 평균점수가 같다면 즐겨찾기수를 기준으로 내림차순 정렬해주세요.\nREST_REVIEW 테이블에서 식당 ID(REST_ID) 그룹에 대해 리뷰 평균 점수(AVG(REVIEW_SCORE))를 계산하고, REST_INFO 테이블에 계산 결과를 결합하는 문제입니다.\nREST_INFO 테이블을 조회하면 서울에 있는 식당은 전부 주소(ADDRESS)가 \u0026quot;서울\u0026quot;로 시작합니다. 정렬 기준만 맞춰서 조회하면 정답을 도출할 수 있습니다.\nCopy sql WITH REVIEWS AS ( SELECT REST_ID, ROUND(AVG(REVIEW_SCORE),2) AS SCORE FROM REST_REVIEW GROUP BY REST_ID ) SELECT INFO.REST_ID, INFO.REST_NAME, INFO.FOOD_TYPE, INFO.FAVORITES, INFO.ADDRESS, RVW.SCORE FROM REST_INFO AS INFO INNER JOIN REVIEWS AS RVW ON INFO.REST_ID = RVW.REST_ID WHERE INFO.ADDRESS LIKE \u0026#34;서울%\u0026#34; ORDER BY SCORE DESC, FAVORITES DESC; 그룹별 조건에 맞는 식당 목록 출력하기 # MEMBER_PROFILE 테이블은 고객의 정보를 담은 테이블입니다. REST_REVIEW 테이블은 식당의 리뷰 정보를 담은 테이블입니다.\nMEMBER_PROFILE 와 REST_REVIEW 테이블에서 리뷰를 가장 많이 작성한 회원의 리뷰들을 조회하는 SQL문을 작성해주세요. 회원 이름, 리뷰 텍스트, 리뷰 작성일이 출력되도록 작성해주시고, 결과는 리뷰 작성일을 기준으로 오름차순, 리뷰 작성일이 같다면 리뷰 텍스트를 기준으로 오름차순 정렬해주세요.\nREST_REVIEW 테이블에서 리뷰 수(COUNT(REVIEW_ID))가 가장 큰 회원 ID(MEMBER_ID)에 대한 리뷰 목록을 조회하는 문제입니다.\n회원 ID(MEMBER_ID) 그룹별 리뷰 수(COUNT(REVIEW_ID))를 계산하고, 내림차순 정렬하여 첫 번째 항목만 조회하여 리뷰를 가장 많이 작성한 회원을 구합니다.\n해당 회원 ID(MEMBER_ID)에 대한 REST_REVIEW 항목을 모두 조회하면서, MEMBER_PROFILE 테이블을 결합해 회원 이름(MEMBER_NAME)을 같이 표시합니다. DATE 타입의 리뷰 작성일(REVIEW_DATE)은 그대로 출력하면 안되고 %Y-%m-%d 날짜 포맷팅을 해야합니다.\nCopy sql WITH BEST_MEMBER AS ( SELECT MEMBER_ID, COUNT(REVIEW_ID) AS REVIEW_COUNT FROM REST_REVIEW GROUP BY MEMBER_ID ORDER BY REVIEW_COUNT DESC LIMIT 1 ) SELECT MEM.MEMBER_NAME, RVW.REVIEW_TEXT, DATE_FORMAT(RVW.REVIEW_DATE, \u0026#39;%Y-%m-%d\u0026#39;) AS REVIEW_DATE FROM REST_REVIEW AS RVW INNER JOIN BEST_MEMBER AS BST ON RVW.MEMBER_ID = BST.MEMBER_ID INNER JOIN MEMBER_PROFILE AS MEM ON RVW.MEMBER_ID = MEM.MEMBER_ID ORDER BY REVIEW_DATE; 쇼핑몰 테이블 # 년, 월, 성별 별 상품 구매 회원 수 구하기 # USER_INFO 테이블은 의류 쇼핑몰에 가입한 회원 정보를 담은 테이블입니다. ONLINE_SALE 테이블은 온라인 상품 판매 정보를 담은 테이블입니다.\nUSER_INFO 테이블과 ONLINE_SALE 테이블에서 년, 월, 성별 별로 상품을 구매한 회원수를 집계하는 SQL문을 작성해주세요. 결과는 년, 월, 성별을 기준으로 오름차순 정렬해주세요. 이때, 성별 정보가 없는 경우 결과에서 제외해주세요.\n두 테이블을 INNER JOIN 하면서 성별(GENDER)이 있는(IS NOT NULL) 경우만 선택합니다.\n판매일(SALES_DATE)에서 각각 연도(YEAR), 월(MONTH)을 추출하고, 성별(GENDER)과 함께 세 가지 기준에 대해 GROUP BY 합니다. 그룹 내에서 중복 없는 회원 ID(DISTINCT USER_ID)를 COUNT 집계해 회원수를 계산합니다.\nCopy sql SELECT EXTRACT(YEAR FROM SALES.SALES_DATE) AS YEAR, EXTRACT(MONTH FROM SALES.SALES_DATE) AS MONTH, USR.GENDER, COUNT(DISTINCT SALES.USER_ID) AS USERS FROM ONLINE_SALE AS SALES INNER JOIN USER_INFO AS USR ON SALES.USER_ID = USR.USER_ID WHERE USR.GENDER IS NOT NULL GROUP BY YEAR, MONTH, GENDER ORDER BY YEAR, MONTH, GENDER; 상품을 구매한 회원 비율 구하기 # USER_INFO 테이블은 의류 쇼핑몰에 가입한 회원 정보를 담은 테이블입니다. ONLINE_SALE 테이블은 온라인 상품 판매 정보를 담은 테이블입니다.\nUSER_INFO 테이블과 ONLINE_SALE 테이블에서 2021년에 가입한 전체 회원들 중 상품을 구매한 회원수와 상품을 구매한 회원의 비율(=2021년에 가입한 회원 중 상품을 구매한 회원수 / 2021년에 가입한 전체 회원 수)을 년, 월 별로 출력하는 SQL문을 작성해주세요. 상품을 구매한 회원의 비율은 소수점 두번째자리에서 반올림하고, 전체 결과는 년을 기준으로 오름차순 정렬해주시고 년이 같다면 월을 기준으로 오름차순 정렬해주세요.\n2021년에 가입한 회원수와, 그중에서 상품을 구매한 회원수를 집계해 그 비율을 계산하는 문제입니다.\n우선, 대상을 2021년에 가입한 회원으로 좁히기 위해 USER_INFO 에서 가입일(JOINED)의 연도가 2021인 항목만 선택한 TARGET_USERS 임시 테이블 결과를 ONLINE_SALE 에 INNER JOIN 으로 연결합니다.\n가입일(JOINED)에서 연도(YEAR) 와 월(MONTH)을 추출하고, 두 가지 기준에 대해 GROUP BY 합니다. 그룹 내에서 중복 없는 회원 ID(DISTINCT USER_ID)를 COUNT 집계해 회원수를 계산하는데, 이때 판매량(SALES_AMOUNT)이 1 이상인, 즉 상품을 구매한 회원 ID만 고려합니다. 조건에 맞지 않는 회원 ID는 NULL 로 바꿔서 COUNT 집계에서 제외했습니다.\n이렇게 계산한 PURCHASED_USERS 값을 분자로, 회원 ID(USER_ID)를 COUNT 집계한 값을 분모로 하여 상품을 구매한 회원의 비율(PURCHASED_RATIO)을 계산합니다.\nCopy sql WITH TARGET_USERS AS ( SELECT DISTINCT USER_ID FROM USER_INFO WHERE EXTRACT(YEAR FROM JOINED) = 2021 ), PURCHASED AS ( SELECT EXTRACT(YEAR FROM SALES.SALES_DATE) AS YEAR, EXTRACT(MONTH FROM SALES.SALES_DATE) AS MONTH, COUNT(DISTINCT IF(SALES.SALES_AMOUNT \u0026gt; 0, USERS.USER_ID, NULL)) AS PURCHASED_USERS FROM ONLINE_SALE AS SALES INNER JOIN TARGET_USERS AS USERS ON SALES.USER_ID = USERS.USER_ID GROUP BY YEAR, MONTH ) SELECT YEAR, MONTH, PURCHASED_USERS, ROUND(PURCHASED_USERS / (SELECT COUNT(USER_ID) FROM TARGET_USERS), 1) AS PUCHASED_RATIO FROM PURCHASED ORDER BY YEAR, MONTH; 오프라인/온라인 판매 데이터 통합하기 # ONLINE_SALE 테이블은 온라인 상품 판매 정보를 담은 테이블입니다. ONLINE_SALE 테이블은 온라인 상품 판매 정보를 담은 테이블입니다.\nONLINE_SALE 테이블과 OFFLINE_SALE 테이블에서 2022년 3월의 오프라인/온라인 상품 판매 데이터의 판매 날짜, 상품ID, 유저ID, 판매량을 출력하는 SQL문을 작성해주세요. OFFLINE_SALE 테이블의 판매 데이터의 USER_ID 값은 NULL 로 표시해주세요. 결과는 판매일을 기준으로 오름차순 정렬해주시고 판매일이 같다면 상품 ID를 기준으로 오름차순, 상품ID까지 같다면 유저 ID를 기준으로 오름차순 정렬해주세요.\n두 테이블을 결과를 세로로 결합하고 정렬하는 문제입니다.\nONLINE_SALE 테이블은 판매일(SALES_DATE), 상품 ID(PRODUCT_ID), 회원 ID(USER_ID) 그룹별로 판매량(SALES_AMOUNT)을 SUM 집계했습니다. GROUP BY 하라는 지문이 없어 안해도 되었던 것 같은데 개인적으로 판매 데이터를 보면 버릇처럼 GROUP BY 하게 됩니다.\nOFFLINE_SALE 테이블은 회원 ID(USER_ID)가 없지만, UNION ALL 연산을 위해 ONLINE_SALE 테이블과 구성을 맞춰줄 목적으로 NULL 값을 가지는 USER_ID 열을 추가합니다.\nDATE 타입의 판매일(REVIEW_DATE)은 그대로 출력하면 안되고 %Y-%m-%d 날짜 포맷팅을 해야합니다.\nCopy sql SELECT DATE_FORMAT(TOTAL.SALES_DATE, \u0026#39;%Y-%m-%d\u0026#39;) AS SALES_DATE, TOTAL.PRODUCT_ID, TOTAL.USER_ID, TOTAL.SALES_AMOUNT FROM ( SELECT SALES_DATE, PRODUCT_ID, USER_ID, SUM(SALES_AMOUNT) AS SALES_AMOUNT FROM ONLINE_SALE WHERE SALES_DATE BETWEEN \u0026#39;2022-03-01\u0026#39; AND \u0026#39;2022-03-31\u0026#39; GROUP BY SALES_DATE, PRODUCT_ID, USER_ID UNION ALL SELECT SALES_DATE, PRODUCT_ID, NULL AS USER_ID, SUM(SALES_AMOUNT) AS SALES_AMOUNT FROM OFFLINE_SALE WHERE SALES_DATE BETWEEN \u0026#39;2022-03-01\u0026#39; AND \u0026#39;2022-03-31\u0026#39; GROUP BY SALES_DATE, PRODUCT_ID ) AS TOTAL ORDER BY SALES_DATE, PRODUCT_ID, USER_ID; 진료 테이블 # 취소되지 않은 진료 예약 조회하기 # PATIENT 테이블은 환자 정보를 담은 테이블입니다. DOCTOR 테이블은 의사 정보를 담은 테이블입니다. APPOINTMENT 테이블은 진료 예약목록을 담은 테이블입니다.\nPATIENT, DOCTOR 그리고 APPOINTMENT 테이블에서 2022년 4월 13일 취소되지 않은 흉부외과(CS) 진료 예약 내역을 조회하는 SQL문을 작성해주세요. 진료예약번호, 환자이름, 환자번호, 진료과코드, 의사이름, 진료예약일시 항목이 출력되도록 작성해주세요. 결과는 진료예약일시를 기준으로 오름차순 정렬해주세요.\n세 개의 테이블에서 조건에 맞는 항목들만 선택해 결합하는 문제입니다.\nAPPOINTMENT 테이블에서는 진료 예약일시(APNT_YMD)가 '2022-04-13' 과 동일하고 예약취소여부(APNT_CNCL_YN)가 'N'에 해당하는 경우만 선택합니다.\nDOCTOR 테이블에서는 진료과코드(MCDP_CD)가 'CS'에 해당하는 경우만 선택합니다.\nPATIENT 테이블은 환자이름(PT_NAME)을 가져오기 위한 목적이며 따로 조건은 없습니다.\n세 개의 테이블을 INNER JOIN 하여 조건을 결합하고 필요한 항목들을 출력합니다.\nCopy sql SELECT APP.APNT_NO, PAT.PT_NAME, APP.PT_NO, APP.MCDP_CD, DOC.DR_NAME, APP.APNT_YMD FROM APPOINTMENT AS APP INNER JOIN DOCTOR AS DOC ON APP.MDDR_ID = DOC.DR_ID INNER JOIN PATIENT AS PAT ON APP.PT_NO = PAT.PT_NO WHERE DATE_FORMAT(APP.APNT_YMD, \u0026#39;%Y-%m-%d\u0026#39;) = \u0026#39;2022-04-13\u0026#39; AND APP.APNT_CNCL_YN = \u0026#39;N\u0026#39; AND DOC.MCDP_CD = \u0026#39;CS\u0026#39; ORDER BY APNT_YMD 판매 테이블 # 주문량이 많은 아이스크림들 조회하기 # FIRST_HALF 테이블은 아이스크림 가게의 상반기 주문 정보를 담은 테이블입니다. JULY 테이블은 7월의 아이스크림 주문 정보를 담은 테이블입니다.\n7월 아이스크림 총 주문량과 상반기의 아이스크림 총 주문량을 더한 값이 큰 순서대로 상위 3개의 맛을 조회하는 SQL 문을 작성해주세요.\n아이스크림 총주문량(TOTAL_ORDER)을 내림차순 정렬하여 상위 3개 항목만 조회하는 문제입니다.\n구성이 동일한 FIRST_HALF 테이블과 JULY 테이블을 UNION ALL 결합하고 아이스크림 맛(FLAVOR) 그룹별로 아이스크림 총주문량(TOTAL_ORDER)을 SUM 집계합니다.\n이때, 집계한 값을 출력할 필요는 없기 때문에 굳이 SELECT 하지는 않고 ORDER BY 구문에서 직접적으로 사용합니다. 집계한 값을 내림차순 정렬한 후 LIMIT 3 으로 상위 3개 항목만 조회합니다.\nCopy sql SELECT FLAVOR FROM ( SELECT * FROM FIRST_HALF UNION ALL SELECT * FROM JULY ) AS TOTAL GROUP BY FLAVOR ORDER BY SUM(TOTAL_ORDER) DESC LIMIT 3 저자 별 카테고리 별 매출액 집계하기 # BOOK 테이블은 각 도서의 정보를 담은 테이블입니다. AUTHOR 테이블은 도서의 저자의 정보를 담은 테이블입니다. BOOK_SALES 테이블은 각 도서의 날짜 별 판매량 정보를 담은 테이블입니다.\n2022년 1월의 도서 판매 데이터를 기준으로 저자 별, 카테고리 별 매출액(TOTAL_SALES = 판매량 * 판매가) 을 구하여, 저자 ID(AUTHOR_ID), 저자명(AUTHOR_NAME), 카테고리(CATEGORY), 매출액(SALES) 리스트를 출력하는 SQL문을 작성해주세요. 결과는 저자 ID를 오름차순으로, 저자 ID가 같다면 카테고리를 내림차순 정렬해주세요.\n세 개의 테이블을 결합하고 제시된 그룹별 매출액을 계산하는 문제입니다.\nBOOK_SALES 테이블에서 BOOK_ID, AUTHOR_ID 를 기준으로 각각 BOOK, AUTHOR 테이블과 INNER JOIN 합니다. BOOK_SALES 테이블의 판매량(SALES)과 BOOK 테이블의 판매가(PRICE)를 곱한 매출액을 SUM 집계하여 총매출액(TOTAL_SALES)을 계산합니다.\nCopy sql SELECT BK.AUTHOR_ID, AU.AUTHOR_NAME, BK.CATEGORY, SUM(SALES.SALES * BK.PRICE) AS TOTAL_SALES FROM BOOK_SALES AS SALES INNER JOIN BOOK AS BK ON SALES.BOOK_ID = BK.BOOK_ID INNER JOIN AUTHOR AS AU ON BK.AUTHOR_ID = AU.AUTHOR_ID WHERE SALES.SALES_DATE BETWEEN \u0026#39;2022-01-01\u0026#39; AND \u0026#39;2022-01-31\u0026#39; GROUP BY AUTHOR_ID, AUTHOR_NAME, CATEGORY ORDER BY AUTHOR_ID, CATEGORY DESC; 자동차 대여 테이블 # 자동차 대여 기록 별 대여 금액 구하기 # CAR_RENTAL_COMPANY_CAR 테이블은 대여 중인 자동차들의 정보를 담은 테이블입니다. CAR_RENTAL_COMPANY_RENTAL_HISTORY 테이블은 자동차 대여 기록 정보를 담은 테이블입니다. CAR_RENTAL_COMPANY_DISCOUNT_PLAN 테이블은 자동차 종류 별 대여 기간 종류 별 할인 정책 정보를 담은 테이블 입니다.\nCAR_RENTAL_COMPANY_CAR 테이블과 CAR_RENTAL_COMPANY_RENTAL_HISTORY 테이블과 CAR_RENTAL_COMPANY_DISCOUNT_PLAN 테이블에서 자동차 종류가 '트럭'인 자동차의 대여 기록에 대해서 대여 기록 별로 대여 금액(컬럼명: FEE)을 구하여 대여 기록 ID와 대여 금액 리스트를 출력하는 SQL문을 작성해주세요. 결과는 대여 금액을 기준으로 내림차순 정렬하고, 대여 금액이 같은 경우 대여 기록 ID를 기준으로 내림차순 정렬해주세요.\n세 개의 테이블을 결합하는데, 우선 대여 시작일(START_DATE)과 대여 종료일(END_DATE) 기간에 대한 날짜수를 기준으로 할인율 적용 기준을 분류합니다. 할인율 적용 기준을 통해 할인율(DISCOUNT_RATE) 수치를 도출하여 대여 금액(FEE)에 할인 적용을 하는 문제입니다.\n일수(DAYS)를 계산하는 방법으로 DATEDIFF 함수를 사용합니다. 시작일과 종료일도 기간에 포함되기 때문에 함수의 결과에 +1 을 더합니다. 일수(DAYS)에 대해 CASE 조건문을 통해 대여 기간 종류(DURATION_TYPE)에 해당하는 4가지 분류로 구분합니다. 대여 기간 종류(DURATION_TYPE)는 그대로 LEFT JOIN 의 기준으로써, CAR_RENTAL_COMPANY_DISCOUNT_PLAN 테이블로부터 할인율(DISCOUNT_RATE)을 도출하는데 사용됩니다.\n대여 금액(FEE)은 일일 대여 요금(DAILY_FEE)에 앞에서 구한 일수(DAYS)와 할인율(DISCOUNT_RATE)을 곱한 결과입니다. 이때, 정수 타입인 할인율(DISCOUNT_RATE)은 그대로 사용하지 않고, 1.0 - (할인율 / 100) 계산식에 넣어 배율로 변환합니다.\nCopy sql WITH HISTORY AS ( SELECT *, (DATEDIFF(END_DATE, START_DATE) + 1) AS DAYS FROM CAR_RENTAL_COMPANY_RENTAL_HISTORY ) SELECT HIST.HISTORY_ID, ROUND(CAR.DAILY_FEE * HIST.DAYS * (1.0 - (IFNULL(DIS.DISCOUNT_RATE, 0) / 100))) AS FEE FROM HISTORY AS HIST INNER JOIN CAR_RENTAL_COMPANY_CAR AS CAR ON HIST.CAR_ID = CAR.CAR_ID LEFT JOIN CAR_RENTAL_COMPANY_DISCOUNT_PLAN AS DIS ON DIS.CAR_TYPE = CAR.CAR_TYPE AND DIS.DURATION_TYPE = ( CASE WHEN HIST.DAYS \u0026gt;= 90 THEN \u0026#39;90일 이상\u0026#39; WHEN HIST.DAYS \u0026gt;= 30 THEN \u0026#39;30일 이상\u0026#39; WHEN HIST.DAYS \u0026gt;= 7 THEN \u0026#39;7일 이상\u0026#39; ELSE \u0026#39;7일 미만\u0026#39; END) WHERE CAR.CAR_TYPE = \u0026#39;트럭\u0026#39; ORDER BY FEE DESC, HISTORY_ID DESC 특정 기간동안 대여 가능한 자동차들의 대여비용 구하기 # CAR_RENTAL_COMPANY_CAR 테이블은 대여 중인 자동차들의 정보를 담은 테이블입니다. CAR_RENTAL_COMPANY_RENTAL_HISTORY 테이블은 자동차 대여 기록 정보를 담은 테이블입니다. CAR_RENTAL_COMPANY_DISCOUNT_PLAN 테이블은 자동차 종류 별 대여 기간 종류 별 할인 정책 정보를 담은 테이블 입니다.\nCAR_RENTAL_COMPANY_CAR 테이블과 CAR_RENTAL_COMPANY_RENTAL_HISTORY 테이블과 CAR_RENTAL_COMPANY_DISCOUNT_PLAN 테이블에서 자동차 종류가 '세단' 또는 'SUV' 인 자동차 중 2022년 11월 1일부터 2022년 11월 30일까지 대여 가능하고 30일간의 대여 금액이 50만원 이상 200만원 미만인 자동차에 대해서 자동차 ID, 자동차 종류, 대여 금액(컬럼명: FEE) 리스트를 출력하는 SQL문을 작성해주세요. 결과는 대여 금액을 기준으로 내림차순 정렬하고, 대여 금액이 같은 경우 자동차 종류를 기준으로 오름차순 정렬, 자동차 종류까지 같은 경우 자동차 ID를 기준으로 내림차순 정렬해주세요.\n앞선 문제와 테이블 및 대여 금액(FEE) 계산 방법이 동일한데, 세부적인 조건이 늘어났습니다.\n첫 번째 조건인 자동차 종류를 선택하는 것은 CAR_RENTAL_COMPANY_CAR 테이블에서 WHERE 절을 통해 자동차 종류(CAR_TYPE)가 '세단' 또는 'SUV'인 것만 선택하면 됩니다.\n두 번째 조건인 2022년 11월 1일부터 2022년 11월 30일까지 대여 가능한 경우는 CAR_RENTAL_COMPANY_RENTAL_HISTORY 테이블에서 대여 시작일(START_DATE)과 대여 종료일(END_DATE)이 해당 기간에 걸치는 경우를 파악하고 대상 자동차 ID(CAR_ID)를 WHERE 절의 AND 조건으로 추가해 제외합니다.\n세 번째 조건은 대여 금액(FEE)을 계산한 후, WHERE 절을 통해 50만원 이상 200만원 미만인 경우를 선택합니다. FEE \u0026gt;= (50 * 10000) AND FEE \u0026lt; (200 * 10000) 조건을 적용할 수도 있지만, 상대적으로 길이가 짧은 BETWEEN 절로 표현했습니다. 대여 금액(FEE)은 소수점까지 떨어지지는 않기 때문에 -1 하여 미만(\u0026lt;)과 동일하게 표현할 수 있습니다.\nCopy sql WITH HISTORY AS ( SELECT *, (DATEDIFF(END_DATE, START_DATE) + 1) AS DAYS FROM CAR_RENTAL_COMPANY_RENTAL_HISTORY ), DISCOUNT_PLAN AS ( SELECT CAR_TYPE, DURATION_TYPE, (1.0 - (DISCOUNT_RATE / 100)) AS DISCOUNT_RATE FROM CAR_RENTAL_COMPANY_DISCOUNT_PLAN ), CAR_IN_USE AS ( SELECT DISTINCT CAR_ID FROM HISTORY WHERE START_DATE \u0026lt;= \u0026#39;2022-11-30\u0026#39; AND END_DATE \u0026gt;= \u0026#39;2022-11-01\u0026#39; ), CAR AS ( SELECT CAR.CAR_ID, CAR.CAR_TYPE, ROUND(CAR.DAILY_FEE * 30 * IFNULL(DIS.DISCOUNT_RATE, 1.0)) AS FEE FROM CAR_RENTAL_COMPANY_CAR AS CAR LEFT JOIN DISCOUNT_PLAN AS DIS ON CAR.CAR_TYPE = DIS.CAR_TYPE AND DIS.DURATION_TYPE = \u0026#39;30일 이상\u0026#39; WHERE CAR.CAR_TYPE IN (\u0026#39;세단\u0026#39;,\u0026#39;SUV\u0026#39;) AND CAR.CAR_ID NOT IN (SELECT CAR_ID FROM CAR_IN_USE) ) SELECT CAR_ID, CAR_TYPE, FEE FROM CAR WHERE FEE BETWEEN (50 * 10000) AND (200 * 10000 - 1) ORDER BY FEE DESC, CAR_TYPE ASC, CAR_ID DESC 개발자 스킬 테이블 # FrontEnd 개발자 찾기 # SKILLCODES 테이블은 개발자들이 사용하는 프로그래밍 언어에 대한 정보를 담은 테이블입니다. DEVELOPERS 테이블은 개발자들의 프로그래밍 스킬 정보를 담은 테이블입니다.\nDEVELOPERS 테이블에서 Front End 스킬을 가진 개발자의 정보를 조회하려 합니다. 조건에 맞는 개발자의 ID, 이메일, 이름, 성을 조회하는 SQL 문을 작성해 주세요. 결과는 ID를 기준으로 오름차순 정렬해 주세요.\n두 개의 테이블에서 정수 타입인 스킬 코드 열을 비트 연산하여 공통된 경우를 JOIN 하는 문제입니다.\nSKILLCODES 테이블에서 스킬의 범주(CATEGORY)가 'Front End'에 해당하는 항목만 선택합니다. 해당 테이블의 스킬의 코드(CODE)는 2진수로 표현했을 때 각 비트마다 하나의 스킬에 대응됩니다.\nDEVELOPERS 테이블에도 마찬가지로 개발자에 대한 스킬 코드(SKILL_CODE)가 있습니다. 두 개 테이블의 스킬 코드를 \u0026amp; 연산자를 활용해 비트 연산하면 공통된 비트만 얻을 수 있습니다. 이것이 SKILLCODES 테이블의 스킬의 코드(CODE)와 같다면 해당 스킬을 보유하고 있다고 판단할 수 있습니다.\n'Front End' 스킬을 가진 개발자의 ID(ID)만 따로 추출하고, DEVELOPERS 테이블에서 해당 ID 에 해당하는 항목들만 선택하여 조회합니다.\nCopy sql WITH TARGET AS ( SELECT DISTINCT DEV.ID FROM DEVELOPERS AS DEV INNER JOIN SKILLCODES AS SKL ON (DEV.SKILL_CODE \u0026amp; SKL.CODE) = SKL.CODE WHERE SKL.CATEGORY = \u0026#39;Front End\u0026#39; ) SELECT ID, EMAIL, FIRST_NAME, LAST_NAME FROM DEVELOPERS WHERE ID IN (SELECT ID FROM TARGET) ORDER BY ID; 언어별 개발자 분류하기 # SKILLCODES 테이블은 개발자들이 사용하는 프로그래밍 언어에 대한 정보를 담은 테이블입니다. DEVELOPERS 테이블은 개발자들의 프로그래밍 스킬 정보를 담은 테이블입니다.\nDEVELOPERS 테이블에서 GRADE별 개발자의 정보를 조회하려 합니다. GRADE는 다음과 같이 정해집니다.\nA : Front End 스킬과 Python 스킬을 함께 가지고 있는 개발자 B : C# 스킬을 가진 개발자 C : 그 외의 Front End 개발자 GRADE가 존재하는 개발자의 GRADE, ID, EMAIL을 조회하는 SQL 문을 작성해 주세요. 결과는 GRADE와 ID를 기준으로 오름차순 정렬해 주세요.\n앞선 문제와, 테이블 및 스킬 코드를 비트 연산하여 개발자가 스킬을 가졌는지 판단하는 방법이 동일한데, 세부적인 조건이 늘어났습니다.\nCASE 조건문을 통해 'A', 'B', 'C' 세 가지 등급으로 개발자를 나누는데, CASE 안에서 직접적으로 조건을 표현하면 길어질 수도 있습니다. 또한, 'Front End' 스킬을 보유한 경우에 대해서는 'A', 'C' 두 가지 등급에서 참조하기 때문에 중복 선언도 피하고 싶습니다.\n따라서, 원하는 스킬을 보유한 경우를 HAS_SKILLS 라는 임시 테이블로 구성하고, 해당 테이블을 DEVELOPERS 테이블에 INNER JOIN 하여 미리 계산한 조건에 대한 Boolean 값을 CASE 안에서 가져다 사용합니다. 조건에 해당되지 않는 나머지 개발자에 대한 등급은 NULL 로 분류하고, 등급이 NULL 인 경우만 제외하여 조회합니다.\nCopy sql WITH HAS_SKILLS AS ( SELECT DEV.ID, (SUM(SKL.CATEGORY = \u0026#39;Front End\u0026#39;) \u0026gt; 0) AS HAS_FRONT, (SUM(SKL.NAME = \u0026#39;Python\u0026#39;) \u0026gt; 0) AS HAS_PYTHON, (SUM(SKL.NAME = \u0026#39;C#\u0026#39;) \u0026gt; 0) AS HAS_CSHARP FROM DEVELOPERS AS DEV LEFT JOIN SKILLCODES AS SKL ON (DEV.SKILL_CODE \u0026amp; SKL.CODE) = SKL.CODE GROUP BY ID ), WITH_GRADE AS ( SELECT (CASE WHEN SKL.HAS_FRONT AND SKL.HAS_PYTHON THEN \u0026#39;A\u0026#39; WHEN SKL.HAS_CSHARP THEN \u0026#39;B\u0026#39; WHEN SKL.HAS_FRONT THEN \u0026#39;C\u0026#39; ELSE NULL END) AS GRADE, DEV.ID, DEV.EMAIL FROM DEVELOPERS AS DEV INNER JOIN HAS_SKILLS AS SKL ON DEV.ID = SKL.ID ) SELECT * FROM WITH_GRADE WHERE GRADE IS NOT NULL ORDER BY GRADE, ID; 인사 테이블 # 연간 평가점수에 해당하는 평가 등급 및 성과금 조회하기 # HR_DEPARTMENT 테이블은 회사의 부서 정보를 담은 테이블입니다. HR_EMPLOYEES 테이블은 회사의 사원 정보를 담은 테이블입니다. HR_GRADE 테이블은 2022년 사원의 평가 정보를 담은 테이블입니다.\nHR_DEPARTMENT, HR_EMPLOYEES, HR_GRADE 테이블을 이용해 사원별 성과금 정보를 조회하려합니다. 평가 점수별 등급과 등급에 따른 성과금 정보가 아래와 같을 때, 사번, 성명, 평가 등급, 성과금을 조회하는 SQL문을 작성해주세요.\n평가등급의 컬럼명은 GRADE 로, 성과금의 컬럼명은 BONUS 로 해주세요. 결과는 사번 기준으로 오름차순 정렬해주세요.\nHR_GRADE 테이블의 평가 점수(SCORE)를 기준으로 평가등급(GRADE)과 성과금(BONUS)을 도출하는 문제입니다.\n사원 평가는 반기마다 발생하는데 2022년 내에 두 차례 발생했기 때문에, 사번(EMP_NO) 그룹별로 평가 점수(SCORE)를 AVG 집계합니다. 평균 평가 점수(AVG(SCORE))를 CASE 조건문에 넣어 평가등급(GRADE)과 성과금(BONUS)을 분류합니다.\n분류한 성과금은 연봉에 대한 배율이기 때문에, 최종 답안을 낼 때는 HR_EMPLOYEES 테이블의 연봉(SAL)에 배율을 곱해서 실제 성과금(BONUS)을 계산합니다.\nCopy sql WITH SCORE AS ( SELECT EMP_NO, AVG(SCORE) AS SCORE FROM HR_GRADE GROUP BY EMP_NO ), GRADE AS ( SELECT EMP_NO, (CASE WHEN SCORE \u0026gt;= 96 THEN \u0026#39;S\u0026#39; WHEN SCORE \u0026gt;= 90 THEN \u0026#39;A\u0026#39; WHEN SCORE \u0026gt;= 80 THEN \u0026#39;B\u0026#39; ELSE \u0026#39;C\u0026#39; END) AS GRADE, (CASE WHEN SCORE \u0026gt;= 96 THEN 0.2 WHEN SCORE \u0026gt;= 90 THEN 0.15 WHEN SCORE \u0026gt;= 80 THEN 0.1 ELSE 0.0 END) AS BONUS FROM SCORE ) SELECT EMP.EMP_NO, EMP.EMP_NAME, GRD.GRADE, ROUND(EMP.SAL * GRD.BONUS) AS BONUS FROM HR_EMPLOYEES AS EMP INNER JOIN GRADE AS GRD ON EMP.EMP_NO = GRD.EMP_NO ORDER BY EMP_NO 대장균 개체 테이블 # 특정 세대의 대장균 찾기 # ECOLI_DATA 테이블은 실험실에서 배양한 대장균들의 정보를 담은 테이블입니다.\n3세대의 대장균의 ID(ID) 를 출력하는 SQL 문을 작성해주세요. 이때 결과는 대장균의 ID 에 대해 오름차순 정렬해주세요.\n세대별로 JOIN 연산을 중첩해서 사용하여 3세대 대장균의 ID(ID)를 도출하는 문제입니다.\n1세대 대장균(GEN1)은 부모 개체의 ID(PARENT_ID)가 NULL 인 경우입니다.\n2세대 대장균(GEN2)은 1세대(GEN1) 대장균 개체의 ID(ID)가 부모 개체의 ID(PARENT_ID)인 경우입니다. 두 항목을 INNER JOIN 하여 공통된 경우만 2세대로 판단합니다.\n3세대 대장균은 2세대 대장균(GEN2)을 구한 것과 동일한 방식으로 INNER JOIN 연산합니다. 해당하는 3세대 대장균의 ID(ID)를 출력합니다.\nCopy sql WITH GEN1 AS ( SELECT DISTINCT ID FROM ECOLI_DATA WHERE PARENT_ID IS NULL ), GEN2 AS ( SELECT DISTINCT ECOLI.ID FROM ECOLI_DATA AS ECOLI INNER JOIN GEN1 AS PARENT ON ECOLI.PARENT_ID = PARENT.ID ) SELECT DISTINCT ECOLI.ID FROM ECOLI_DATA AS ECOLI INNER JOIN GEN2 AS PARENT ON ECOLI.PARENT_ID = PARENT.ID ORDER BY ID; 멸종위기의 대장균 찾기 # ECOLI_DATA 테이블은 실험실에서 배양한 대장균들의 정보를 담은 테이블입니다.\n각 세대별 자식이 없는 개체의 수(COUNT)와 세대(GENERATION)를 출력하는 SQL문을 작성해주세요. 이때 결과는 세대에 대해 오름차순 정렬해주세요. 단, 모든 세대에는 자식이 없는 개체가 적어도 1개체는 존재합니다.\n재귀 쿼리를 사용해 이전 세대의 대장균 ID(ID)와 ECOLI_DATA 테이블의 부모 개체 ID(PARENT_ID)가 동일한 경우를 반복해서 탐색합니다. 다음 세대가 없을 때까지 조회한 모든 결과를 결합하고, 대장균의 ID(ID)가 어떠한 부모 개체의 ID(PARENT_ID)에도 포함되지 않는 경우만 선택하여, 세대(GENERATION)별 개체의 수(COUNT)를 집계하는 문제입니다.\n앞선 문제처럼 n세대까지 JOIN 연산을 중첩하려다가 도저히 아닌 것 같아서 찾아봤는데, 재귀 쿼리를 사용하여 해결할 수 있는 문제였습니다. 평소에 재귀 쿼리를 사용해보지 않아 GEN_DATA 를 도출하는 과정은 검색 결과를 참고했습니다.\nGEN_DATA 에서 자식이 없는 개체만 선택하고 GENERATION 그룹별로 COUNT 집계한 결과를 조회합니다.\nCopy sql WITH RECURSIVE GEN_DATA AS( SELECT ID, 1 AS GENERATION FROM ECOLI_DATA WHERE PARENT_ID IS NULL UNION ALL SELECT ECOLI.ID, PARENT.GENERATION+1 AS GENERATION FROM ECOLI_DATA AS ECOLI INNER JOIN GEN_DATA AS PARENT ON ECOLI.PARENT_ID = PARENT.ID ) SELECT COUNT(PARENT.GENERATION) AS COUNT, PARENT.GENERATION FROM GEN_DATA AS PARENT WHERE PARENT.ID NOT IN ( SELECT DISTINCT PARENT_ID FROM ECOLI_DATA WHERE PARENT_ID IS NOT NULL) GROUP BY GENERATION ORDER BY GENERATION; "},{"id":13,"href":"/blog/spark-study-4/","title":"Apache Spark - DataFrame과 Dataset API 활용하기","section":"Posts","content":"Spark Structure # 정형화 API에 대해 알아보기에 앞서, 정형적 모델 이전의 RDD 프로그래밍 API 모델을 확인해본다.\nRDD # RDD는 Spark 1.x 버전에 있던 저수준의 DSL을 의미하고, 스파크에서 가장 기본적인 추상적인 부분이다. RDD에는 세 가지의 핵심으로 특성이 있다.\n의존성\n어떤 입력을 필요로 하고 RDD가 어떻게 만들어지는지 Spark에게 가르쳐 주는 의존성이 필요하다.\n파티션\nExecutor들에 작업을 분산해 파티션별로 병렬 연산할 수 있는 능력을 부여한다. 지역성 정보를 사용하여 각 Executor가 가까이 있는 Executor에게 우선적으로 작업을 보낸다.\n연산 함수\n파티션에 저장되는 데이터를 Iterator[T] 형태로 만들어준다. 하지만, Iterator[T] 데이터 타입이 파이썬 RDD에서 기본 객체로만 인식이 가능해 불투명했다. Spark가 함수에서 연산이나 표현식을 검사하지 못해 객체를 바이트 뭉치로 직렬화해 쓰는 것밖에 못했다. 이로 인해 연산 순서를 재정렬해 효과적인 질의 계획으로 바꾸기가 어려웠다.\nSpark DSL # Spark 2.x는 RDD의 한계를 극복하기 위해 고수준의 DSL을 도입했다. Spark DSL은 다음과 같은 네 가지 특징이 있다.\n도메인 특화 언어\nSpark DSL은 분산 데이터 처리와 분석에 최적화된 명령어와 함수를 제공하여, 대규모 데이터셋에 대한 복잡한 연산을 간결하게 표현할 수 있다.\n다중 언어 지원\nScala 언어 뿐 아니라, Java, Python, R 등 다양한 언어에서 Spark DSL의 기능을 사용할 수 있게 지원한다.\n함수형 프로그래밍 지원\n람다 함수, 고차 함수 등 함수형 프로그래밍 기법을 활용하여 Transformation 및 Action을 간결하게 구현할 수 있다.\nSQL 통합\nSpark SQ DSL을 통해 SQL 쿼리와 유사한 구문으로 DataFrame 및 Dataset을 조작할 수 있다.\n고수준 DSL을 통한 Spark 구조를 갖추면서 더 나은 성능과 공간 효율성 등 많은 이득을 얻을 수 있었다. DataFrame API나 Dataset API를 다루면서 표현성, 단순성, 구성 용이성, 통일성 등의 장점도 가지게 되었다.\n이름별로 모든 나이들을 모아서 그룹화하고, 나이의 평균을 구하는 예제를 저수준의 RDD API로 구현한다고 하면 다음과 같을 수 있다.\nCopy python dataRDD = sc.parallelize([ (\u0026#34;Brooke\u0026#34;, 20), (\u0026#34;Denny\u0026#34;, 31), (\u0026#34;Jules\u0026#34;, 30), (\u0026#34;TD\u0026#34;, 35), (\u0026#34;Brooke\u0026#34;, 25)]) agesRDD = (dataRDD .map(lambda x: (x[0], (x[1], 1))) .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) .map(lambda x: x[0], x[1][0]/x[1][1])) 해당 코드를 Spark에게 쿼리를 계산하는 과정을 직접적으로 지시하여 의도가 전달되지 않는다. 동일한 질의를 Python의 고수준 DSL 연산자들과 DataFrame API를 사용하면 다음과 같다.\nCopy python from pyspark.sql import SparkSession from pyspark.sql.functions import avg # SparkSession 객체 생성 data_df = spark.createDataFrame( [(\u0026#34;Brooke\u0026#34;, 20), (\u0026#34;Denny\u0026#34;, 31), (\u0026#34;Jules\u0026#34;, 30), (\u0026#34;TD\u0026#34;, 35), (\u0026#34;Brooke\u0026#34;, 25)]) avg_df = data_df.groupBy(\u0026#34;name\u0026#34;).agg(avg(\u0026#34;age\u0026#34;)) avg_df.show() 고수준 DSL은 표현력이 높고 저수준 DSL보다 간단하다. Spark는 groupBy, avg 등의 연산자들을 통해 사용자의 의도를 이해하고 효과적인 실행을 위해 연산자들을 최적화하거나 적절하게 재배열할 수 있다.\n단순히 간단하기만 할 뿐 아니라 고수준 DSL은 언어 간에 일관성을 갖고 있다. 예를 들어 이름별로 나이의 평균을 집계하는 코드는 아래와 같다. 겉보기에도 똑같고 실제로 하는 일도 동일하다.\nCopy python # 파이썬 예제 avg_df = data_df.groupBy(\u0026#34;name\u0026#34;).agg(avg(\u0026#34;age\u0026#34;)) Copy kotlin // 스칼라 예제 val avgDf = dataDf.groupBy(\u0026#34;name\u0026#34;).agg(avg(\u0026#34;age\u0026#34;)) DataFrame API # pandas의 DataFrame에 영향을 받은 Spark DataFrame은 칼럼과 스키마를 가진 분산된 테이블처럼 동작하며, 각 칼럼은 정수, 문자열, 배열, 날짜 등 특정한 데이터 타입을 가질 수 있다.\n기본 데이터 타입 # 데이터 타입은 Spark Application에서 선언하거나, 스키마에서 정의할 수 있다. 먼저, Scala와 Python의 기본적인 데이터 타입은 아래와 같다.\n데이터 타입 스칼라에서 할당되는 값 파이썬에서 할당되는 값 ByteType Byte int ShortType Short int IntegerType Integer int LongType Long int FloatType Float float DoubleType Double float StringType String str BooleanType Boolean bool DecimalType java.math.BigDecimal decimal.Decimal 정형화 타입과 복합 타입 # 복합 데이터 분석을 위해서는 기본적인 데이터 타입을 사용하지 않는다. 대상 데이터는 맵, 배열, 구조체 등 자체적 구조를 갖고 있기 때문에, 이를 다루기 위한 타입을 지원한다.\n데이터 타입 스칼라에서 할당되는 값 파이썬에서 할당되는 값 BinaryType Array[Byte] bytearray TimestampType java.sqlTimestamp datetime.datetime DateType java.sql.Date datetime.date ArrayType scala.collection.Seq list, tuple, array 등 MapType scala.collection.Map dict StructType org.apache.spark.sql.Row list 또는 tuple StructField 해당 필드와 맞는 값의 타입 해당 필드와 맞는 값의 타입 Schema # 스키마는 DataFrame의 칼럼명과 데이터 타입을 정의한 것이다. 보통 외부 데이터 소스에서 구조화된 데이터를 읽어 들일 때 사용한다. 미리 스키마를 정의할 경우 두 가지 장점이 있다.\nSpark가 스키마를 추측하기 위해 파일을 읽어들이는 과정을 방지한다. 파일이 큰 경우, 비용과 시간을 절약할 수 있다. 데이터가 스키마와 맞지 않는 경우, 조기에 발견할 수 있다. 스키마 정의 # 스키마를 정의하는 방법은 두 가지가 있다.\n프로그래밍 스타일로 정의하는 것 Copy kotlin // 스칼라 예제 import org.apache.spark.sql.types._ val schema = StructType(Array( StructField(\u0026#34;author\u0026#34;, StringType, false), StructField(\u0026#34;title\u0026#34;, StringType, false), StructField(\u0026#34;pages\u0026#34;, IntegerType, false))) Copy python # 파이썬 예제 from pyspark.sql.types import * schema = StructType([ StructField(\u0026#34;author\u0026#34;, StringType(), False), StructField(\u0026#34;title\u0026#34;, StringType(), False), StructField(\u0026#34;pages\u0026#34;, IntegerType(), False)]) DDL(Data Definition Language)을 사용하는 것 Copy python schema = \u0026#34;author STRING, title, STRING, pages INT\u0026#34; 스키마 활용 (Python) # databricks/LearningSparkV2/chapter3 에서 스키마 활용 예제를 가져온다.\nCopy python # src/example_schema.py from pyspark.sql.types import * from pyspark.sql import SparkSession from pyspark.sql.functions import * # 프로그래밍 스타일로 스키마를 정의한다. schema = StructType([ StructField(\u0026#34;Id\u0026#34;, IntegerType(), False), StructField(\u0026#34;First\u0026#34;, StringType(), False), StructField(\u0026#34;Last\u0026#34;, StringType(), False), StructField(\u0026#34;Url\u0026#34;, StringType(), False), StructField(\u0026#34;Published\u0026#34;, StringType(), False), StructField(\u0026#34;Hits\u0026#34;, IntegerType(), False), StructField(\u0026#34;Campaigns\u0026#34;, ArrayType(StringType()), False)]) # DDL을 사용해서 스키마를 정의할 수도 있다. # schema = \u0026#34;\u0026#39;Id\u0026#39; INT, \u0026#39;First\u0026#39;, STRING, \u0026#39;Last\u0026#39; STRING, \u0026#39;Url\u0026#39; STRING, \u0026#34; \\ # \u0026#34;\u0026#39;Published\u0026#39; STRING, \u0026#39;Hits\u0026#39; INT, \u0026#39;Campaigns\u0026#39; ARRAY\u0026lt;STRING\u0026gt;\u0026#34; # 예제 데이터를 생성한다. data = [ [1, \u0026#34;Jules\u0026#34;, \u0026#34;Damji\u0026#34;, \u0026#34;https://tinyurl.1\u0026#34;, \u0026#34;1/4/2016\u0026#34;, 4535, [\u0026#34;twitter\u0026#34;, \u0026#34;LinkedIn\u0026#34;]], [2, \u0026#34;Brooke\u0026#34;,\u0026#34;Wenig\u0026#34;,\u0026#34;https://tinyurl.2\u0026#34;, \u0026#34;5/5/2018\u0026#34;, 8908, [\u0026#34;twitter\u0026#34;, \u0026#34;LinkedIn\u0026#34;]], [3, \u0026#34;Denny\u0026#34;, \u0026#34;Lee\u0026#34;, \u0026#34;https://tinyurl.3\u0026#34;,\u0026#34;6/7/2019\u0026#34;,7659, [\u0026#34;web\u0026#34;, \u0026#34;twitter\u0026#34;, \u0026#34;FB\u0026#34;, \u0026#34;LinkedIn\u0026#34;]], [4, \u0026#34;Tathagata\u0026#34;, \u0026#34;Das\u0026#34;,\u0026#34;https://tinyurl.4\u0026#34;, \u0026#34;5/12/2018\u0026#34;, 10568, [\u0026#34;twitter\u0026#34;, \u0026#34;FB\u0026#34;]], [5, \u0026#34;Matei\u0026#34;,\u0026#34;Zaharia\u0026#34;, \u0026#34;https://tinyurl.5\u0026#34;, \u0026#34;5/14/2014\u0026#34;, 40578, [\u0026#34;web\u0026#34;, \u0026#34;twitter\u0026#34;, \u0026#34;FB\u0026#34;, \u0026#34;LinkedIn\u0026#34;]], [6, \u0026#34;Reynold\u0026#34;, \u0026#34;Xin\u0026#34;, \u0026#34;https://tinyurl.6\u0026#34;, \u0026#34;3/2/2015\u0026#34;, 25568, [\u0026#34;twitter\u0026#34;, \u0026#34;LinkedIn\u0026#34;]]] if __name__ == \u0026#34;__main__\u0026#34;: spark = (SparkSession .builder .appName(\u0026#34;Example-3_6\u0026#34;) .getOrCreate()) # 위에서 정의한 스키마로 DataFrame을 생성하고 상위 행을 출력한다. blogs_df = spark.createDataFrame(data, schema) blogs_df.show() # DataFrame 처리에 사용된 스키마를 출력한다. print(blogs_df.printSchema()) spark.stop() 예제 데이터에 대해 프로그래밍 스타일과 DDL을 사용하는, 두 가지 스타일로 스키마를 정의할 수 있다. DataFrame 생성 시 스키마를 전달하고, printSchema() 를 실행하여 어떤 스키마가 적용되었는지 출력해 볼 수 있다.\nspark-submit 에 예제 파일을 전달하면 아래와 같은 결과를 확인할 수 있다.\nCopy bash (spark) % spark-submit src/example_schema.py +---+---------+-------+-----------------+---------+-----+--------------------+ | Id| First| Last| Url|Published| Hits| Campaigns| +---+---------+-------+-----------------+---------+-----+--------------------+ | 1| Jules| Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]| | 2| Brooke| Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]| | 3| Denny| Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...| | 4|Tathagata| Das|https://tinyurl.4|5/12/2018|10568| [twitter, FB]| | 5| Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...| | 6| Reynold| Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]| +---+---------+-------+-----------------+---------+-----+--------------------+ root |-- Id: integer (nullable = false) |-- First: string (nullable = false) |-- Last: string (nullable = false) |-- Url: string (nullable = false) |-- Published: string (nullable = false) |-- Hits: integer (nullable = false) |-- Campaigns: array (nullable = false) | |-- element: string (containsNull = true) DataFrame에 할당된 스키마를 다른 곳에서 사용하고 싶다면, blogs_df.schema 와 같이 호출하여 스키마 객체를 반환할 수 있다. 스키마 객체는 스키마를 정의할 때 사용했던 것과 동일한 pyspark.sql.types.StructType 타입이다.\nScala를 사용하는 경우에도 Python과 동일하게 정의한 스키마를 JSON 파일을 읽는데 적용한다면 아래와 같이 표현할 수 있다.\nCopy kotlin // 스칼라 예제 val blogsDF = spark.read.schema(schema).json(jsonFile) Column # 칼럼은 pandas의 DataFrame과 유사하게 어떤 특정한 타입의 필드를 나타내는 개념이다. RDBMS를 다루는 것처럼 관계형 표현이나 계산식 형태의 표현식으로 칼럼 단위의 값들에 연산을 수행할 수 있다.\n칼럼명에 대해 expr(\u0026quot;columnName * 5\u0026quot;) 같은 단순한 표현식으로 연산을 수행할 수 있다. 파이썬에서 expr() 은 pyspark.sql.functions 패키지에서 가져올 수 있다.\n표현식 활용 (Python) # 스키마 활용 예제에서 만든 blogs_df 객체를 사용한다.\nCopy python # src/example_schema.py from pyspark.sql.types import * from pyspark.sql import SparkSession from pyspark.sql.functions import * if __name__ == \u0026#34;__main__\u0026#34;: # SparkSession 및 blogs_df 객체 생성 # 표현식을 사용해 값을 계산하고 결과를 출력한다. 모두 동일한 결과를 보여준다. blogs_df.select(expr(\u0026#34;Hits\u0026#34;) * 2).show(2) blogs_df.select(col(\u0026#34;Hits\u0026#34;) * 2).show(2) blogs_df.select(expr(\u0026#34;Hits * 2\u0026#34;)).show(2) # 블로그 우수 방문자를 계산하고 결과를 출력한다. blogs_df.withColumn(\u0026#34;Big Hitters\u0026#34;, (expr(\u0026#34;Hits \u0026gt; 10000\u0026#34;))).show() spark-submit 에 예제 파일을 전달하면 아래와 같은 결과를 확인할 수 있다.\nCopy bash (spark) % spark-submit src/example_schema.py +----------+ |(Hits * 2)| +----------+ | 9070| | 17816| +----------+ only showing top 2 rows +---+---------+-------+-----------------+---------+-----+--------------------+-----------+ | Id| First| Last| Url|Published| Hits| Campaigns|Big Hitters| +---+---------+-------+-----------------+---------+-----+--------------------+-----------+ | 1| Jules| Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]| false| | 2| Brooke| Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]| false| | 3| Denny| Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...| false| | 4|Tathagata| Das|https://tinyurl.4|5/12/2018|10568| [twitter, FB]| true| | 5| Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...| true| | 6| Reynold| Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]| true| +---+---------+-------+-----------------+---------+-----+--------------------+-----------+ 첫 번째 표현식으로 계산한 결과는 모두 동일하여 하나만 출력했다. expr() 또는 col() 표현식으로 칼럼 연산을 수행할 수 있다.\nwithColumn() 을 호출하면 새로운 칼럼을 추가할 수 있다. 기존의 \u0026quot;Hits\u0026quot; 칼럼에 표현식을 사용해 블로그 우수 방문자를 분류하고, \u0026quot;Big Hitters\u0026quot; 라는 새로운 칼럼을 붙여서 출력했다.\nScala에서는 col() 대신에 칼럼명 앞에 $ 를 붙여서 Column 타입으로 변환할 수도 있다.\nCopy kotlin // \u0026#34;Id\u0026#34; 칼럼값에 따라 역순으로 정렬한다. blogsDF.sort(col(.desc).show() blogsDF.sort($\u0026#34;Id\u0026#34;.desc).show() Row # Spark에서 하나의 행은 하나 이상의 칼럼을 갖고 있는 Row 객체로 표현된다. Row 객체에 속하는 칼럼들은 동일한 타입일 수도 있고 다른 타입일 수도 있다. Row는 순서가 있는 필드 집합 객체이므로 0부터 시작하는 인덱스로 접근한다.\nCopy python # 파이썬 예제 from pyspark.sql import Row blog_row = Row(6, \u0026#34;Reynold\u0026#34;, \u0026#34;Xin\u0026#34;, \u0026#34;https://tinyurl.6\u0026#34;, 255568, \u0026#34;3/2/2015\u0026#34;, [\u0026#34;twitter\u0026#34;, \u0026#34;LinedIn\u0026#34;]) # 인덱스로 개별 값에 접근한다. blog_row[1] \u0026#39;Reynold\u0026#39; Row 객체들을 DataFrame으로 만들 수 있다.\nCopy python # 파이썬 예제 rows = [Row(\u0026#34;Matei Zaharia\u0026#34;, \u0026#34;CA\u0026#34;), Row(\u0026#34;Reynold Xin\u0026#34;, \u0026#34;CA\u0026#34;)] authors_df = spark.createDataFrame(rows, [\u0026#34;Authors\u0026#34;, \u0026#34;State\u0026#34;]) authors_df.show() DataFrame 작업 # 읽기/쓰기 # 데이터 소스에서 DataFrame으로 로드하기 위해 DataFrameReader 를 사용할 수 있다. JSON, CSV, Parquet, 텍스트, Avro, ORC 같은 다양한 포맷의 데이터 소스를 지원한다. 반대로 특정 포맷으로 DataFrame을 내보낼 때는 DataFrameWriter 를 사용할 수 있다.\nPython과 Scala에서 spark.read.csv() 함수로 CSV 파일을 읽을 수 있다.\nCopy python # 파이썬 예제 sf_fire_file = \u0026#34;data/sf-fire-calls.csv\u0026#34; fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema) Copy kotlin // 스칼라 예제 val sfFireFile = \u0026#34;data/sf-fire-calls.csv\u0026#34; val fireDF = spark.read.schema(fireSchema).option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;).csv(sfFireFile) DataFrame을 외부 데이터 소스에 내보내려면 DataFrame 객체가 가진 write() 메서드를 사용할 수 있다. 기본 포맷으로 인기있는 포맷은 칼럼 지향적인 Parquet 포맷이다. Parquet에는 스키마가 메타데이터에 들어있어 수동으로 스키마를 적용할 필요가 없다.\nCopy python # 파이썬 예제 fire_df.write.format(\u0026#34;parquet\u0026#34;).save(parquet_path) Copy kotlin // 스칼라 예제 fireDF.write.format(\u0026#34;parquet\u0026#34;).save(parquetPath) 프로젝션/필터 # 프로젝션은 필터를 이용해 특정 관계 상태와 매치되는 행들만 반환하는 방법이다. 프로젝션은 select(), 필터는 filter() 또는 where() 메서드로 표현된다.\nCopy python # 파이썬 예제 few_fire_df = (fire_df .select(\u0026#34;IncidentNumber\u0026#34;, \u0026#34;AvailableDtTm\u0026#34;, \u0026#34;CallType\u0026#34;) .where(col(\u0026#34;CallType\u0026#34;) != \u0026#34;Medical Incident\u0026#34;)) few_fire_df.show(5, truncate=False) 칼럼 변경 # 칼럼의 이름을 변경하거나 추가 또는 삭제하는 경우가 있다. 컬럼명을 변경할 때는 withColumnRenamed() 함수를 사용할 수 있다. 아래 예제는 \u0026quot;Delay\u0026quot; 칼럼의 명칭을 \u0026quot;ResponseDelayedinMins\u0026quot; 라고 변경한다.\nCopy python # 파이썬 예제 new_fire_df = fire_df.withColumnRenamed(\u0026#34;Delay\u0026#34;, \u0026#34;ResponseDelayedinMins\u0026#34;) (new_fire_df .select(\u0026#34;ResponseDelayedinMins\u0026#34;) .where(col(\u0026#34;ResponseDelayedinMins\u0026#34;) \u0026gt; 5) .show(5, False)) 기존 칼럼을 가공해 새로운 칼럼을 만들 때는 withColumn() 메서드를 사용할 수 있다. 이때, spark.sql.functions 패키지에 있는 to_timestamp() 또는 to_date() 같은 함수들을 같이 사용할 수 있다. 가공된 칼럼을 추가한 후 필요하지 않은 칼럼을 제거하려면 drop() 메서드를 사용할 수 있다.\nCopy python # 파이썬 예제 fire_ts_df = (new_fire_df .withColumn(\u0026#34;IncidentDate\u0026#34;, to_timestamp(col(\u0026#34;CallDate\u0026#34;), \u0026#34;MM/dd/yyyy\u0026#34;)).drop(\u0026#34;CallDate\u0026#34;) .withColumn(\u0026#34;OnWatchDate\u0026#34;, to_timestamp(col(\u0026#34;WatchDate\u0026#34;), \u0026#34;MM/dd/yyyy\u0026#34;)).drop(\u0026#34;WatchDate\u0026#34;) .withColumn(\u0026#34;AvailableDtTs\u0026#34;, to_timestamp(col(\u0026#34;AvailableDtTm\u0026#34;), \u0026#34;MM/dd/yyyy hh:mm:ss\u0026#34;)).drop(\u0026#34;AvailableDtTm\u0026#34;)) (fire_ts_df .select(\u0026#34;IncidentDate\u0026#34;, \u0026#34;OnWatchDate\u0026#34;, \u0026#34;AvailableDtTs\u0026#34;) .show(5, False)) 집계 연산 # groupBy(), orderBy(), count() 와 같은 Transformation 또는 Action을 사용하여 칼럼명을 가지고 집계할 수 있다. 아래 예제는 \u0026quot;CallType\u0026quot; 칼럼을 기준으로 행 개수를 세는 연산을 표현한다. 내림차순으로 정렬하여 가장 일반적인 신고 타입(CallType)을 확인할 수 있다.\nCopy python # 파이썬 예제 (fire_ts_df .select(\u0026#34;CallType\u0026#34;) .where(col(\u0026#34;CallType\u0026#34;).isNotNull()) .groupBy(\u0026#34;CallType\u0026#34;) .count() .orderBy(\u0026#34;count\u0026#34;, ascending=False) .show(n=10, truncate=False)) 집계 함수로는 min(), max(), sum(), avg() 등의 통계 함수들을 지원한다.\nCopy python # 파이썬 예제 import pyspark.sql.functions as F (fire_ts_df .select(F.sum(\u0026#34;NumAlarms\u0026#34;), F.avg(\u0026#34;ResponseDelayedinMins\u0026#34;), F.min(\u0026#34;ResponseDelayedinMins\u0026#34;), F.max(\u0026#34;ResponseDelayedinMins\u0026#34;)) .show()) Dataset API # Dataset는 정적 타입 API와 동적 타입 API의 두 가지 특성을 모두 가진다.\nDataset # DataFrame은 Dataset[Row] 로 표현할 수 있다. Row는 서로 다른 타입의 값을 저장할 수 있는 JVM 객체다. 반면에 Dataset는 엄격하게 타입이 정해진 JVM 객체의 집합으로, Java의 클래스와 유사하다.\nDataset는 Java와 Scala에서만 사용할 수 있고, Python과 R에서는 DataFrame만 사용할 수 있다. 이것은 Python과 R이 컴파일 시 타입의 안전을 보장하는 언어가 아니기 때문이다. 반대로 Java는 컴파일 시점에 타입 안정성을 제공하기 때문에 Dataset만 사용할 수 있다. Scala는 DataFrame을 Dataset[Row] 로 표현하며, Dataset[T] 도 같이 사용할 수 있다.\nCase Class # DataFrame에서 스키마로 데이터 타입을 정의한느 것처럼, Scala에서 Dataset를 만들 때 스키마를 지정하기 위해 케이스 클래스를 사용할 수 있다. Java에서는 JavaBean 클래스를 쓸 수 있다.\n예제로, IoT 디바이스에서 JSON 파일을 읽어 들일 때 케이스 클래스를 아래와 같이 정의한다.\nCopy kotlin // 스칼라 예제 case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) val ds = spark.read.json(\u0026#34;iot_devices.json\u0026#34;).as[DeviceIoTData] Dataset는 DataFrame과 같은 연산이 가능하다. 예제로, filter() 에 함수를 인자로 전달하는 질의는 아래와 같다.\nCopy kotlin // 스칼라 예제 val filterTempDS = ds.filter(d =\u0026gt; d.temp \u0026gt; 30 \u0026amp;\u0026amp; d.humidity \u0026gt; 70) DataFrame vs Dataset # DataFrame과 Dataset을 사용 중 오류가 발생하는 시점을 정리하면 아래 표와 같다. Dataset가 DataFrame과 다른점은 컴파일 시점에 엄격한 타입 체크를 한다는 것이다. 반대로, SQL과 유사한 질의를 쓰는 관계형 변환을 필요로 한다면 DataFrame을 사용한다.\nSQL DataFrame Dataset 문법 오류 실행 시점 컴파일 시점 컴파일 시점 분석 오류 실행 시점 실행 시점 컴파일 시점 Spark SQL # Spark SQL은 고수준 정형화 기능들이 구축되도록 하는 방대한 엔진으로 진화해 왔다. Spark SQL 엔진은 다음과 같은 일을 한다.\n스파크 컴포넌트들을 통합하고 DataFrame/Dataset 관련 작업을 단순화할 수 있도록 추상화를 한다. 정형화된 파일 포맷(JSON, CSV 등)을 읽고 쓰며 데이터를 임시 테이블로 변환한다. 빠른 데이터 탐색을 위한 대화형 Spark SQL 쉘을 제공한다. JDBC/ODBC 커넥터를 통해 외부의 도구들과 연결할 수 있는 중간 역할을 한다. JVM을 위한 최적화된 코드를 생성한다. Catalyst Optimizer # Spark SQL 엔진의 핵심은 Catalyst Optimizer다. Catalyst Optimizer는 두 가지 목적으로 설계되었다.\nSpark SQL에 최적화 기법을 쉽게 추가한다. 개발자가 최적화 프로그램을 확장할 수 있도록 한다. 예시로, 데이터 소스별 규칙을 추가하거나 새로운 데이터 유형을 지원하는 것 등이 있다. Catalyst Optimizer는 연산 쿼리를 받아 실행 계획으로 변환한다. 그 과정은 아래 그림과 같이 4단계의 과정을 거친다.\n분석\n제공된 코드가 유효하고 오류가 없는지 확인한다. 칼럼, 데이터 타입, 함수, 테이블, 데이터베이스 이름 목록을 갖고 있는 Catalog 객체를 참조한다. 분석 단계를 성공적으로 통과하면 Spark에서 이해하고 해결할 수 있는 요소만이 포함되어 있다는 의미를 가진다.\n논리적 최적화\n표준적인 규칙을 기반으로 최적화 접근 방식을 적용하여 효율성을 향상시킨다. 최적화를 위한 여러 계획들을 수립하는데, 예를 들면 조건절 하부 배치, 칼럼 걸러내기, 부울 표현식 단순화 등이 포함된다. 논리 계획은 물리 계획 수립의 입력 데이터가 된다.\n물리 계획 수립\n논리 계획을 바탕으로 대응되는 물리적 연산자를 사용해 최적화된 물리 계획을 생성한다. CPU, 메모리, I/O 활용을 포함한 컴퓨팅 리소스 비용을 기반으로 실행 전략을 평가한다. 리소스 가용성을 기반으로 가장 비용이 적게 드는 계획을 선택한다.\n코드 생성\n물리 계획을 Java 바이트 코드로 변환한다. 최신 컴파일러 기술을 활용해 최적화된 바이트 코드를 생성한다. Spark가 JIT(Just-In-Time) 컴파일러처럼 작동하여 런타임 성능을 최적화하고 실행 속도를 크게 향상시킨다.\nReferences # https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ https://itwiki.kr/w/아파치_스파크_DSL https://github.com/databricks/LearningSparkV2 https://www.databricks.com/spark/getting-started-with-apache-spark/datasets https://www.databricks.com/glossary/catalyst-optimizer https://blog.det.life/apache-spark-sql-engine-and-query-planning-37cafb2b98f6 "},{"id":14,"href":"/blog/spark-study-3/","title":"Apache Spark - 스파크 애플리케이션 구조와 RDD 이해하기","section":"Posts","content":"Spark Application # Spark Application은 Driver Process 하나와 일련의 일련의 Executors로 구성된다.\nDriver Process # Driver Process는 main() 함수를 실행하고 클러스터 내 노드에서 세 가지 작업을 담당한다.\nSpark Application 관련 정보를 유지한다. 사용자의 프로그램이나 입력에 대응한다. Executor 작업을 분석, 배포, 예약한다. Executor # Executor는 Driver가 할당한 작업을 실제로 실행하는 역할을 하는데, 두 가지 작업을 담당한다.\nDriver가 할당한 Task를 실행한다. Task의 상태와 결과를 Driver 노드에 보고한다. Cluster Manager # 실물 시스템을 제어하고 Spark Application에 리소스를 할당하는 작업은 Cluster Manager가 맡는다. Spark Application의 실행 과정에서 Cluster Manager는 Application이 실행되는 물리적인 머신을 관리한다. Spark Application은 클러스터에서 독립적인 프로세스로 실행되며, SparkContext 객체에 의해 조정된다.\nSparkContext는 여러 유형의 Cluster Manager(Standalone, YARN, Kubernetes)에 연결될 수 있으며, Application 간에 리소스를 할당한다. Spark가 연결되어 클러스터의 노드에서 Executor가 확보되면, SparkContext에 전달된 Application 코드가 Executor에게 전달된다.\nJob # Spark Driver는 Spark Application을 하나 이상의 Spark Job으로 변환한다. 각 Job은 DAG로 변환되며, DAG 그래프에서 각각의 노드는 하나 이상의 Spark Stage에 해당한다.\nStage # 어떤 작업이 연속적으로 또는 병렬적으로 수행되는지에 맞춰 Stage에 해당하는 DAG 노드가 생성된다. Spark 연산은 하나의 Stage 안에서 실행되지 않고 여러 Stage로 나뉘어 실행된다.\nTask # 각 Stage는 최소 실행 단위이며 Executor들 위에서 실행되는 Spark Task들로 이루어진다. 각 Task는 개별적인 CPU 코어에 할당되어 개별적인 파티션을 갖고 작업하기 때문에, 철저하게 병렬 처리가 이루어진다.\nRDD(Resilient Distributed Data) # RDD는 탄력적인 분산 데이터셋이란 의미로, 분산 데이터를 병렬로 처리하고 장애가 발생할 경우 스스로 복구될 수 있는 내성을 가지고 있다. RDD는 Spark에서 정의한 분산 데이터 모델로, 여러 서버에 나누어 저장되어 각 서버에서 저장된 데이터를 동시에 병렬로 처리할 수 있다.\nRDD 특징 # RDD는 5가지 특징을 가지고 있다.\nDistributed Collection\n데이터는 클러스터에 흩어져 있지만 하나의 파일인 것처럼 사용이 가능한다. 즉, 여러 군데의 데이터를 하나의 객체로 사용할 수 있다.\nResilient \u0026amp; Immutable\n데이터는 탄력적이고 불변하는 성질이 있다. RDD의 변환 과정은 DAG로 그릴 수 있기 때문에 문제가 생길 경우 쉽게 이전의 RDD로 돌아갈 수 있다. 연산 중 문제가 생겨도 다시 복원해서 연산하면 되기 때문에 탄력적인 성질을 가진다고 볼 수 있다. 또한, 여러 노드 중 하나의 노드에서 장애가 발생해도 복원이 가능하기 때문에 불변하다는 성질을 가진다고도 볼 수 있다.\nType-Safe\nRDD는 컴파일 시 타입을 판별할 수 있다. Integer RDD, String RDD, Double RDD 등으로 미리 판단할 수 있기 때문에 문제를 일찍 발견할 수 있다.\nStructured \u0026amp; Unstructured Data\n정형 데이터인 테이블, RDB, DataFrame과 비정형 데이터인 텍스트, 로그, 자연어 등을 모두 담을 수 있다.\nLazy Evaluation\n분산 데이터의 Spark 연산은 Transformation과 Action으로 구분된다. Action을 할 때까지 Transformation을 실행하지 않는다. Action을 하게 되면 Transformation을 실행하는 게으른 연산 방식을 가진다.\nTransformation # Transformation은 불변성의 특징을 가진 원본 데이터를 수정하지 않고 하나의 Spark DataFrame을 새로운 DataFrame으로 변형(Transform)한다. select() 나 filter() 같은 연산은 원본 DataFrame을 수정하지 않는다.\nTransformation은 즉시 계산되지 않고 Lineage라 불리는 형태로 기록된다. 기록된 Lineage는 더 효율적으로 연산할 수 있도록 Transformation들끼리 재배열하거나 합치도록 최적화된다. Lazy Evaluation은 Action이 실행되는 시점이나 데이터에 실제 접근하는 시점까지 실제 실행을 미루는 전략이다.\nLazy Evaluation이 일련의 Transformation들을 최적화한다면, Lineage는 데이터 불변성 및 장애에 대한 내구성을 제공한다. Lineage에는 Transformation들이 기록되어 있고 실행 전까지 DataFrame이 변하지 않기 때문에, 단순히 기록된 Lineage를 재실행하는 것만으로 원래 상태를 다시 만들어낼 수 있다.\nTransformation은 Narrow Transformation과 Wide Transformation으로 구분된다.\nNarrow Transformation # Narrow Transformation은 하나의 입력 파티션을 연산하여 하나의 출력 파티션을 내놓는 경우다. 입력 파티션에 대한 연산이 독립적으로 이루어지며, 연산의 결과인 출력 파티션은 입력 파티션의 데이터에만 의존한다. 즉, 다른 파티션의 데이터를 참조할 필요가 없다는 것을 의미한다. filter() 와 contains() 등의 연산이 여기에 해당된다.\nNarrow Transformation은 실행 비용이 상대적으로 낮고, 성능이 좋아 빠른 처리가 가능하다.\nWide Transformation # Wide Transformation은 입력 데이터의 여러 파티션 간에 데이터가 재분배되어야 하는 경우다. 다른 파티션으로부터 데이터를 읽어들여서 Shuffle(데이터 재분배)하는 과정이 필요하며, groupBy() 나 orderBy() 등의 연산이 여기에 해당된다.\nWide Transformation은 네트워크를 통한 대량의 데이터 이동을 발생시켜 실행 시간이 오래 걸리고 리소스 사용량이 많다.\nAction # Action은 RDD로 결과 값을 계산하고, 연산 결과를 반환하거나 외부 스토리지(HDFS 등)에 저장한다. count() 나 show() 함수는 연산 결과를 반환하거나 출력하고, saveAsTextFile() 과 같은 함수로 연산 결과를 스토리지에 저장할 수 있다.\nAction을 호출할 때마다 RDD가 처음부터 계산되는데, 반복적인 연산에 의한 비효율성을 피하기 위해 cache() 와 persist() 를 사용해 데이터를 메모리에 보관할 수 있다.\nWeb UI # 스파크는 클러스터 상태와 리소스 사용을 모니터링하기 위해 Web UI를 제공한다. 기본적으로 4040 포트를 사용하는데 다음과 같은 내용을 볼 수 있다.\n스케줄러의 Stage와 Task 목록 RDD 크기와 메모리 사용의 요약 환경 정보 실행 중인 Executor 정보 모든 스파크 SQL 쿼리 아래는 AWS 문서에서 제공하는 화면이다. Web UI를 통해 Job, Stage, Task들이 어떻게 구성되는지 DAG 형태로 시각화해서 볼 수 있다. Stage 안에서 각각의 Task는 파란 박스로 표시되는데, 아래 예시에서 Stage 2는 2개의 Task로 구성되어 있음을 알 수 있다. Task가 여러 개라면 모두 병렬로 실행된다.\nspark-submit # databricks/LearningSparkV2/chapter2 에서 각 주별로 학생들이 어떤 색깔의 M\u0026amp;M을 좋아하는지 알려주는 스파크 프로그램을 작성한 예제 mnmcount.py 를 가져온다. 동일한 위치의 data/ 경로에서 M\u0026amp;M 데이터셋 mnm_dataset.csv 을 확인할 수 있다.\nM\u0026amp;M 개수 집계 (Python) # Copy python # src/mnmcount.py from pyspark.sql import SparkSession import sys if __name__ == \u0026#34;__main__\u0026#34;: if len(sys.argv) != 2: print(\u0026#34;Usage: mnmcount \u0026lt;file\u0026gt;\u0026#34;, file=sys.stderr) sys.exit(-1) # SparkSession 객체를 만든다. spark = (SparkSession .builder .appName(\u0026#34;PythonMnMCount\u0026#34;) .getOrCreate()) # 인자에서 M\u0026amp;M 데이터가 들어있는 파일 이름을 얻는다. mnm_file = sys.argv[1] # 데이터가 CSV 형식이며 헤더가 있음을 알리고 스키마를 추론하도록 한다. mnm_df = (spark.read.format(\u0026#34;csv\u0026#34;) .option(\u0026#34;header\u0026#34;, \u0026#34;true\u0026#34;) .option(\u0026#34;inferSchema\u0026#34;, \u0026#34;true\u0026#34;) .load(mnm_file)) mnm_df.show(n=5, truncate=False) # State, Color, Count 필드를 선택하고 State, Color를 기준으로 Count를 sum 집계한다. # select, groupBy, sum, orderBy 메서드를 연결하여 연속적으로 호출한다. count_mnm_df = (mnm_df.select(\u0026#34;State\u0026#34;, \u0026#34;Color\u0026#34;, \u0026#34;Count\u0026#34;) .groupBy(\u0026#34;State\u0026#34;, \u0026#34;Color\u0026#34;) .sum(\u0026#34;Count\u0026#34;) .orderBy(\u0026#34;sum(Count)\u0026#34;, ascending=False)) # 상위 60개 결과를 보여주고, 모든 행 개수를 count 집계해 출력한다. count_mnm_df.show(n=60, truncate=False) print(\u0026#34;Total Rows = %d\u0026#34; % (count_mnm_df.count())) # 위 집계 과정에서 중간에 where 메서드를 추가해 캘리포니아(CA) 주에 대해서만 집계한다. ca_count_mnm_df = (mnm_df.select(\u0026#34;*\u0026#34;) .where(mnm_df.State == \u0026#39;CA\u0026#39;) .groupBy(\u0026#34;State\u0026#34;, \u0026#34;Color\u0026#34;) .sum(\u0026#34;Count\u0026#34;) .orderBy(\u0026#34;sum(Count)\u0026#34;, ascending=False)) # 상위 10개 결과를 보여준다. ca_count_mnm_df.show(n=10, truncate=False) # SparkSession을 멈춘다. spark.stop() Application 실행 # spark-submit 스크립트에 파이썬 코드를 첫 번째 인자로, CSV 파일을 두 번째 인자로 전달한다.\n실행 과정에서 불필요한 INFO 로그를 무시하고 싶다면, $SPARK_HOME/conf/ 경로에서 log4j2.properties.template 파일의 이름을 log4j2.properties 로 변경하고 파일 내용에서 rootLogger.level = info 부분의 값을 warn 으로 변경하면 된다.\nCopy bash (spark) % $SPARK_HOME/bin/spark-submit src/mnmcount.py data/mnm_dataset.csv +-----+------+-----+ |State|Color |Count| +-----+------+-----+ |TX |Red |20 | |NV |Blue |66 | |CO |Blue |79 | |OR |Blue |71 | |WA |Yellow|93 | +-----+------+-----+ only showing top 5 rows +-----+------+----------+ |State|Color |sum(Count)| +-----+------+----------+ |CA |Yellow|100956 | |WA |Green |96486 | |CA |Brown |95762 | |TX |Green |95753 | |TX |Red |95404 | |CO |Yellow|95038 | |NM |Red |94699 | |OR |Orange|94514 | |WY |Green |94339 | |NV |Orange|93929 | |TX |Yellow|93819 | |CO |Green |93724 | |CO |Brown |93692 | |CA |Green |93505 | |NM |Brown |93447 | |CO |Blue |93412 | |WA |Red |93332 | |WA |Brown |93082 | |WA |Yellow|92920 | |NM |Yellow|92747 | |NV |Brown |92478 | |TX |Orange|92315 | |AZ |Brown |92287 | |AZ |Green |91882 | |WY |Red |91768 | |AZ |Orange|91684 | |CA |Red |91527 | |WA |Orange|91521 | |NV |Yellow|91390 | |UT |Orange|91341 | |NV |Green |91331 | |NM |Orange|91251 | |NM |Green |91160 | |WY |Blue |91002 | |UT |Red |90995 | |CO |Orange|90971 | |AZ |Yellow|90946 | |TX |Brown |90736 | |OR |Blue |90526 | |CA |Orange|90311 | |OR |Red |90286 | |NM |Blue |90150 | |AZ |Red |90042 | |NV |Blue |90003 | |UT |Blue |89977 | |AZ |Blue |89971 | |WA |Blue |89886 | |OR |Green |89578 | |CO |Red |89465 | |NV |Red |89346 | |UT |Yellow|89264 | |OR |Brown |89136 | |CA |Blue |89123 | |UT |Brown |88973 | |TX |Blue |88466 | |UT |Green |88392 | |OR |Yellow|88129 | |WY |Orange|87956 | |WY |Yellow|87800 | |WY |Brown |86110 | +-----+------+----------+ Total Rows = 60 +-----+------+----------+ |State|Color |sum(Count)| +-----+------+----------+ |CA |Yellow|100956 | |CA |Brown |95762 | |CA |Green |93505 | |CA |Red |91527 | |CA |Orange|90311 | |CA |Blue |89123 | +-----+------+----------+ 처음에는 mnm_dataset.csv 의 상위 5개 행을 보여주고, 이어서 각 주별, 색깔별 합계를 출력한다. 그리고, 캘리포니아(CA)에 대한 결과만 별도로 출력한다.\nReferences # https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ https://www.databricks.com/en/glossary/what-are-spark-applications https://spark.apache.org/docs/latest/cluster-overview.html https://velog.io/@dbgpwl34/Spark-스파크-애플리케이션의-아키텍처-스파크-애플리케이션의-생애-주기 https://spark.apache.org/docs/latest/rdd-programming-guide.html https://6mini.github.io/data%20engineering/2021/12/12/rdd/ https://mengu.tistory.com/27 https://sunrise-min.tistory.com/entry/Apache-Spark-RDD https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-ui.html https://github.com/databricks/LearningSparkV2 "},{"id":15,"href":"/blog/spark-study-2/","title":"Apache Spark - 로컬 환경에서 설치하고 PySpark 실행하기","section":"Posts","content":"Spark Installation # Apple Silicon 환경에서 스파크 설치를 진행합니다.\n각 섹션의 이미지를 클릭하면 설치 페이지 또는 관련 문서로 이동합니다.\nSpark 설치 # 아파치 스파크 다운로드 페이지로 가서 최신 버전 4.0.0 및 \u0026quot;Pre-built for Apache Hadoop\u0026quot; 옵션을 선택하면 해당 버전의 다운로드 링크 spark-4.0.0-bin-hadoop3.tgz 가 나타난다. 해당 링크로 이동하면 아래와 같이 Hadoop 관련 바이너리 파일이 포함된 압축 파일의 설치 경로를 확인할 수 있다.\n브라우저 또는 curl, wget 등 명령어를 통해 압축 파일을 내려받을 수 있다. Copy bash wget https://dlcdn.apache.org/spark/spark-4.0.0/spark-4.0.0-bin-hadoop3.tgz 압축 해제 프로그램을 사용하거나, 터미널에서 아래 명령어를 입력하여 압축 해제한다. Copy bash tar zxvf spark-4.0.0-bin-hadoop3.tgz Spark 경로에 접근하기 위해 환경변수를 설정한다. Copy bash vi ~/.zshrc vi 편집기로 .zshrc 파일에 Spark 경로를 등록한다. SPARK_HOME 은 압축 해제한 Spark 경로를 입력한다. Copy bash export SPARK_HOME=/Users/{username}/spark-4.0.0 export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin 변경 사항을 적용하기 위해 터미널을 재시작하거나 아래 명령어를 실행한다. Copy text source ~/.zshrc 주의할 점은 Spark를 실행하기 전에 Java와 Hadoop이 설치되어 있어야 한다. 보통은 Java 또는 Hadoop 버전에 맞춰서 Spark를 설치하지만, 어떤 것도 설치되어 있지 않기 때문에 스파크 버전에 맞춰서 Java와 Hadoop을 설치한다.\nHadoop은 다운로드할 때 지정한 것과 같은 3.4 버전을 설치하고, Java는 Spark 4.0.0에서 요구하는 최소 버전인 OpenJDK 17 버전을 설치한다. 이미 설치되어 있다면 Spark 실행 섹션으로 넘어간다.\nSpark runs on Java 17/21, Scala 2.13, Python 3.9+, and R 3.5+ (Deprecated)\nJava 설치 # Homebrew가 설치되었다는 전제 하에 OpenJDK 17 버전을 설치한다. Copy bash brew install openjdk@17 설치가 완료되면, 시스템에서 JDK를 찾을 수 있도록 심볼릭 링크로 연결한다. Copy bash sudo ln -sfn /opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk-17.jdk 환경변수에 OpenJDK 11의 bin 디렉터리를 추가한다. vi 편집기 등으로 직접 수정할 수도 있다. Copy bash echo \u0026#39;export PATH=/opt/homebrew/opt/openjdk@17/bin:$PATH\u0026#39; \u0026gt;\u0026gt; ~/.zshrc 변경 사항을 적용하기 위해 터미널을 재시작하거나 아래 명령어를 실행한다. Copy bash source ~/.zshrc OpenJDK 11 버전이 정상적으로 설치되었는지 확인하기 위해 아래 명령어를 입력한다. Copy bash % java -version openjdk version \u0026#34;17.0.15\u0026#34; 2025-04-15 OpenJDK Runtime Environment Homebrew (build 17.0.15+0) OpenJDK 64-Bit Server VM Homebrew (build 17.0.15+0, mixed mode, sharing) Hadoop 설치 # Homebrew로 설치할 수도 있지만, Hadoop 3.4.0 버전을 맞추기 위해 압축 파일을 직접 내려받는다. 다운로드 버튼을 클릭하거나, curl, wget 등 명령어로 내려받을 수 있다. (ARM 아키텍처에서 설치할 때는 파일명에 aarch64 가 포함되어야 한다) Copy bash wget https://archive.apache.org/dist/hadoop/common/hadoop-3.4.0/hadoop-3.4.0-aarch64.tar.gz 압축 해제 프로그램을 사용하거나, 터미널에서 아래 명령어를 입력하여 압축 해제한다. Copy bash tar zxvf hadoop-3.4.0-aarch64.tar.gz -C ~/ Hadoop 명령어에 접근하기 위해 환경변수를 설정한다. Copy bash vi ~/.zshrc vi 편집기로 .zshrc 파일에 Hadoop 경로를 등록한다. HADOOP_HOME 은 압축 해제한 Hadoop 경로를 입력한다. Copy bash export HADOOP_HOME=/Users/{username}/hadoop-3.4.0 export PATH=$PATH:$HADOOP_HOME/bin 변경 사항을 적용하기 위해 터미널을 재시작하거나 아래 명령어를 실행한다. Copy text source ~/.zshrc Hadoop 환경 설정 파일을 수정한다. 파일들은 $HADOOP_HOME/etc/hadoop/ 경로에 있다. hadoop-env.sh\nCopy bash export JAVA_HOME=/Library/Java/JavaVirtualMachines/openjdk-17.jdk/Contents/Home core-site.xml\nCopy xml \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.defaultFS\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://localhost:9000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; hdfs-site.xml\nCopy xml \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;1\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; mapred-site.xml\nCopy xml \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.framework.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;yarn\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; yarn-site.xml\nCopy xml \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; HDFS은 자체적으로 SSH를 사용한다. SSH 키를 생성하고 본인 계정에 인증한다. Copy bash ssh-keygen -t rsa cat ~/.ssh/id_rsa.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys ssh localhost HDFS 실행 # 네임노드를 포맷하고 HDFS을 구성하는 모든 데몬을 실행한다.\nCopy bash hdfs namenode -format $HADOOP_HOME/sbin/start-dfs.sh 먼저, 네임노드를 포맷하면 아래와 같은 로그가 발생한다. 호스트명으로 localhost 대신 다른 명칭을 사용하는데, 이로 인해 오류가 발생했다.\nCopy bash % sudo hdfs namenode -format Password: 2025-06-28 18:33:03,038 INFO namenode.NameNode: STARTUP_MSG: /************************************************************ STARTUP_MSG: Starting NameNode STARTUP_MSG: host = minyeamer/127.0.0.1 STARTUP_MSG: args = [-format] STARTUP_MSG: version = 3.4.0 STARTUP_MSG: classpath = /Users... 정상적으로 HDFS이 실행된다면 아래와 같은 메시지를 조회할 수 있다.\nCopy bash % $HADOOP_HOME/sbin/start-dfs.sh Starting namenodes on [minyeamer] Starting datanodes minyeamer: datanode is running as process 21771. Stop it first and ensure /tmp/hadoop-cuz-datanode.pid file is empty before retry. Starting secondary namenodes [minyeamer] minyeamer: secondarynamenode is running as process 21906. Stop it first and ensure /tmp/hadoop-cuz-secondarynamenode.pid file is empty before retry. 2025-06-28 18:51:34,493 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable jps 명령어를 입력하면 실행중인 노드를 확인할 수 있다.\nCopy bash % jps 21906 SecondaryNameNode 23016 Jps 22216 NameNode 21771 DataNode HDFS을 종료하고 싶다면 start-dfs.sh 와 동일한 경로에서 stop-all.sh 스크립트를 실행하면 된다.\nCopy bash % $HADOOP_HOME/sbin/stop-all.sh WARNING: Stopping all Apache Hadoop daemons as cuz in 10 seconds. WARNING: Use CTRL-C to abort. Stopping namenodes on [minyeamer] Stopping datanodes Stopping secondary namenodes [minyeamer] Stopping nodemanagers Stopping resourcemanager HDFS 실행 중 오류 처리 # localhost 가 아닌 minyeamer 라는 호스트명을 사용하는데, start-dfs.sh 실행 시 아래와 같은 에러 메시지가 발생했다. localhost 명칭을 사용한다면 해당 과정은 무시하고 Spark 실행 섹션으로 넘어간다.\nCopy bash minyeamer: ssh: Could not resolve hostname minyeamer: nodename nor servname provided, or not known 첫 번째 에러는 SSH 연결 시 호스트명을 인식할 수 없다는 문제로, /etc/hosts 에 minyeamer 호스트명과 127.0.0.1 IP 주소가 매칭되지 않아서 발생한 문제다. 아래와 같이 추가할 수 있다.\nCopy text 127.0.0.1 localhost minyeamer 255.255.255.255 broadcasthost ::1 localhost minyeamer 해당 호스트명으로 SSH 접속을 시도하면 정상적으로 접속할 수 있다.\nCopy bash ssh minyeamer 또한, Hadoop 설정 파일도 일부 수정해주어야 한다. $HADOOP_HOME/etc/hadoop/ 경로의 workers 파일에는 localhost 한줄만 적혀있을 건데, minyeamer 호스트명으로 변경한다. 또한, 동일한 경로의 core-site.xml 파일의 hdfs://localhost:9000 부분도 변경해야 한다.\n그리고 나서 다시 start-dfs.sh 를 실행했는데 다른 에러가 발생했다.\nCopy bash minyeamer: ERROR: Cannot set priority of namenode process 19072 이것만으로는 오류를 파악하기 어려워서 $HADOOP_HOME/logs/ 경로 아래 hadoop-*-namenode-*.log 형식의 로그 파일을 확인했다.\n로그 파일에서 에러가 발생한 부분에 아래와 같은 에러 메시지를 확인할 수 있었다.\nCopy bash 2025-06-28 18:44:47,867 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode. java.net.BindException: Problem binding to [minyeamer:9000] java.net.BindException: Address already in use; For more details see: http://wiki.apache.org/hadoop/BindException 9000번 포트가 사용되고 있다는 건데, 확인해보니 localhost:cslistener 라는 이름의 프로세스가 실행 중에 있었다.\nCopy bash % lsof -i :9000 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME python3.1 1943 ... 아마 localhost 호스트명으로 HDFS을 실행시켰을 때 프로세스가 중지되지 않고 남아있는 것 같아 강제로 중지했다.\nCopy bash kill -9 1943 다시 네임노드를 포맷하고 HDFS을 실행하니 앞에서 보았던 정상적인 메시지를 확인할 수 있었다.\nCopy bash hdfs namenode -format $HADOOP_HOME/sbin/start-dfs.sh Spark 실행 # spark-shell 을 실행하면 정상적으로 동작하는 것을 확인할 수 있다.\nCopy bash (main) cuz@minyeamer ~ % spark-shell WARNING: Using incubator modules: jdk.incubator.vector Using Spark\u0026#39;s default log4j profile: org/apache/spark/log4j2-defaults.properties 25/06/28 19:45:07 WARN Utils: Your hostname, minyeamer, resolves to a loopback address: 127.0.0.1; using 192.168.x.x instead (on interface en0) 25/06/28 19:45:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address Using Spark\u0026#39;s default log4j profile: org/apache/spark/log4j2-defaults.properties Setting default log level to \u0026#34;WARN\u0026#34;. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ \u0026#39;_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 4.0.0 /_/ Using Scala version 2.13.16 (OpenJDK 64-Bit Server VM, Java 17.0.15) Type in expressions to have them evaluated. Type :help for more information. Spark context Web UI available at http://localhost:4040 Spark context available as \u0026#39;sc\u0026#39; (master = local[*], app id = local-1751107510852). Spark session available as \u0026#39;spark\u0026#39;. scala\u0026gt; 메시지에서 알려주는대로 http://localhost:4040 경로에 접근하니까 아래와 같은 웹 UI 화면을 조회할 수 있었다.\nSpark 디렉터리 구조 # Spark 경로 아래에는 다음과 같은 디렉터리 또는 파일이 존재한다.\nCopy bash % ls ~/spark-4.0.0 bin/\tconf/\tdata/\texamples/\thive-jackson/\tjars/\tkubernetes/\tlicenses/ python/\tR/\tsbin/\tyarn/\tLICENSE\tNOTICE\tREADME.md\tRELEASE READMD.md 스파크 셸을 어떻게 사용하는지에 대한 안내 및 스파크 문서의 링크와 설정 가이드 등이 기록되어 있다.\nbin/ spark-shell 을 포함한 대부분의 스크립트가 위치한다.\nsbin/ 다양한 배포 모드에서 클러스터의 스파크 컴포넌트들을 시작하고 중지하기 위한 관리 목적이다.\nkubernetes/ 쿠버네티스 클러스터에서 쓰는 스파크를 위해, 도커 이미지 제작을 위한 Dockerfile들을 담고 있다.\ndata/ MLlib, 정형화 프로그래밍, GraphX 등에서 입력으로 사용되는 .txt 파일이 있다.\nexamples/ Java, Python, R, Scala에 대한 예제들을 제공한다.\nPySpark Installation # PyPi 설치 # Copy bash pip install pyspark 명령어를 입력해 PyPi 저장소에서 PySpark 라이브러리를 설치할 수 있다. 다운로드한 파일과 동일하게 25년 5월 23일 릴리즈된 4.0.0 버전을 설치한다. 다른 버전을 설치하고 싶다면 pip install pyspark=3.0.0 과 같이 입력할 수 있다.\nSQL, ML, MLlib 등 추가적인 라이브러리를 같이 설치하려면 pip install pyspark[sql,ml,mllib] 와 같이 입력할 수 있다.\n별도의 가상환경에서 PySpark를 설치하는 것을 추천한다. 4.0.0 버전 기준으로 Python 3.9 버전부터 지원한다. 개인적으로는 Python 3.10 버전을 사용한다.\nCopy bash (spark) % pip install pyspark Collecting pyspark Downloading pyspark-4.0.0.tar.gz (434.1 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 434.1/434.1 MB 3.9 MB/s eta 0:00:00 ... Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9.9 pyspark-4.0.0 pyspark 실행 # spark-shell 은 Scala 쉘을 실행한다. Scala가 아닌 Python을 사용하고 싶다면 pyspark 쉘을 실행할 수 있다.\nCopy bash (spark) % pyspark Python 3.10.18 | packaged by conda-forge | (main, Jun 4 2025, 14:46:00) [Clang 18.1.8 ] on darwin Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. WARNING: Using incubator modules: jdk.incubator.vector Using Spark\u0026#39;s default log4j profile: org/apache/spark/log4j2-defaults.properties 25/06/28 21:35:50 WARN Utils: Your hostname, minyeamer, resolves to a loopback address: 127.0.0.1; using 192.168.x.x instead (on interface en0) 25/06/28 21:35:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address Using Spark\u0026#39;s default log4j profile: org/apache/spark/log4j2-defaults.properties Setting default log level to \u0026#34;WARN\u0026#34;. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 25/06/28 21:35:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ \u0026#39;_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 4.0.0 /_/ Using Python version 3.10.18 (main, Jun 4 2025 14:46:00) Spark context Web UI available at http://localhost:4040 Spark context available as \u0026#39;sc\u0026#39; (master = local[*], app id = local-1751114150819). SparkSession available as \u0026#39;spark\u0026#39;. \u0026gt;\u0026gt;\u0026gt; Python 쉘에서는 대화형으로 Python API를 사용할 수 있다. 현재 경로에 있는 README.md 파일을 첫 번째 10줄만 읽어보았다.\nCopy python \u0026gt;\u0026gt;\u0026gt; strings = spark.read.text(\u0026#34;README.md\u0026#34;) \u0026gt;\u0026gt;\u0026gt; strings.show(10, truncate=False) +--------------------------------------------------------------------------------------------------+ |value | +--------------------------------------------------------------------------------------------------+ |# Apache Spark | | | |Spark is a unified analytics engine for large-scale data processing. It provides | |high-level APIs in Scala, Java, Python, and R (Deprecated), and an optimized engine that | |supports general computation graphs for data analysis. It also supports a | |rich set of higher-level tools including Spark SQL for SQL and DataFrames, | |pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing,| |and Structured Streaming for stream processing. | | | |- Official version: \u0026lt;https://spark.apache.org/\u0026gt; | +--------------------------------------------------------------------------------------------------+ only showing top 10 rows \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; strings.count() 125 쉘을 나가고 싶다면 Ctrl+D 를 눌러 나갈 수도 있고, quit() 함수를 실행해 종료할 수도 있다.\nCopy python \u0026gt;\u0026gt;\u0026gt; quit() "},{"id":16,"href":"/blog/spark-study-1/","title":"Apache Spark - 스파크의 기본 개념과 아키텍처","section":"Posts","content":"Study Overview # 러닝 스파크 2nd 개정판 과정을 따릅니다.\n목적 # 대용량 데이터 처리를 위한 아파치 스파크를 이론적으로 학습 책에서 대상으로 하는 스파크 3.x 버전과 25년 5월 출시된 Spark 4.0 버전을 비교 각 챕터에서 배운 것으로 실습할만한 것이 있다면 추가로 시도하기 실습은 PySpark API를 사용하며, 최신화된 PySpark 4.0.0 문서를 참조 챕터 # 아파치 스파크 소개: 통합 분석 엔진 아파치 스파크 다운로드 및 시작 아파치 스파크의 정형화 API 스파크 SQL과 데이터 프레임: 내장 데이터 소스 소개 스파크 SQL과 데이터 프레임: 외부 데이터 소스와 소통하기 스파크 SQL과 데이터세트 스파크 애플리케이션의 최적화 및 튜닝 정형화 스트리밍 아파치 스파크를 통한 안정적인 데이터 레이크 구축 MLlib을 사용한 머신러닝 아파치 스파크로 머신러닝 파이프라인 관리, 배포 및 확장 에필로그: 아파치 스파크 3.0 Spark Overview # 스파크의 시작 # RDBMS 같은 전통적인 저장 시스템으로는 구글이 방대한 규모의 인터넷 문서를 다룰 수 없어 구글 파일 시스템(GFS), 맵리듀스(MapReduce), 빅테이블(BigTable) 등을 만들어 냈다. GFS는 클러스터 환경에서 분산 파일시스템을 제공하고, 빅테이블은 GFS를 기반으로 대규모 데이터 저장 수단을 제공한다. 맵리듀스는 함수형 프로그래밍 기반으로 대규모 데이터 분산 처리를 구현했다. 클러스터의 워커 노드들이 분산된 데이터에 연산을 하고(Map), 그 결과를 하나로 합쳐(Reduce) 최종 결과를 생성해낸다. 이러한 접근 방식은 네트워크 트래픽을 크게 감소시키면서 로컬 디스크에 대한 I/O를 극대화한다.\nGFS는 하둡 파일 시스템과 맵리듀스 구현에 영향을 주었다. HDFS의 맵리듀스에는 몇 가지 단점이 있었다. 첫째, 운영이 복잡해 관리가 쉽지 않았다. 둘째, 배치 처리를 위한 맵리듀스 API의 기본 설정 코드가 너무 많이 필요했다. 셋째, 맵리듀스 태스크가 필요해질 때마다 중간 과정의 데이터를 로컬 디스크에 써야 했다. 반복적인 I/O 작업에 의해 거대한 맵리듀스 작업에 며칠이 걸리기도 했다.\nUC 버클리 연구원들은 동적이고 반복적인 작업에서 비효율적인 맵리듀스를 개선하여 단순하고 빠르고 쉬운 스파크를 만들기로 했다. 구체적으로는 더 높은 장애 내구성을 갖고, 병렬성을 높이면서, 맵리듀스 연산을 위한 중간 결과를 메모리에 저장하고, 간편한 API를 다양한 언어로 제공하고자 했다.\n아파치 스파크란? # 아파치 스파크는 데이터 센터나 클라우드에서 대규모 분산 데이터 처리를 위한 통합형 엔진이다. 중간 연산을 메모리에 저장하고 머신러닝, SQL, 스트리밍 처리, 그래프 처리 등을 간편하게 API로 지원한다.\n스파크의 설계 철학에는 속도, 사용 편리성, 모듈성, 확장성이 있다.\n속도 스파크는 하드웨어 산업의 발전으로 메모리 성능 향상에 많은 이득을 얻었는데, 모든 중간 결과를 메모리에 저장해 I/O 작업을 제한하고 속도를 향상시켰다. 또한, 질의 연산을 DAG로 구성해 효율적인 연산 그래프를 만들고 병렬 수행을 지원한다.\n사용 편리성 데이터프레임이나 데이터세트 같이 고수준으로 추상화된 자료 구조를 사용해 단순성을 실현시켰다. 다양한 언어로 연산을 지원하여 사용자들이 편한 언어로 빅데이터를 처리할 수 있다.\n모듈성 문서화가 잘된 API를 제공하며, 스파크 SQL이나 정형화 스트리밍 등의 핵심 컴포넌트를 하나의 엔진 안에서 연동된 상태로 사용할 수 있다.\n확장성 스파크는 저장보다는 빠른 병렬 연산 엔진에 초점을 맞춰, 수많은 데이터 소스에서 데이터를 읽어 들여 메모리에서 처리하는 것이 가능하다. 서드파티 패키지 목록에는 다양한 외부 데이터 소스가 포함되어 있다.\n아파치 컴포넌트 # 다양한 워크로드를 위해 스파크 SQL, 스파크 MLlib, 스파크 정형화 스트리밍, GraphX를 제공한다. 자바, R, 스칼라, SQL, 파이썬 중 어느 것으로 스파크 코드를 작성해도 바이트 코드로 변환되어 워커 노드의 JVM에서 실행된다.\n스파크 SQL RDBMS 테이블이나 CSV와 같은 구조화된 데이터 파일 포맷에서 데이터를 읽어 들여 영구적이거나 임시적인 테이블을 생성한다. SQL 계통의 질의를 써서 데이터를 데이터프레임으로 읽어 들일 수 있다.\n스파크 MLlib 범용 머신러닝 알고리즘들이 들어 있다. 특성을 추출 및 가공하고 학습/검증 파이프라인을 구축하는 기능을 지원하며, 경사 하강법 최적화를 포함한 저수준 ML 기능을 포함한다.\n스파크 정형화 스트리밍 실시간으로 연결하고 반응하기 위한 데이터 모델은 스트림을 연속적으로 증가하는 테이블이자, 끝에 새로운 레코드가 추가되는 형태이다. 단순히 정형화 테이블로 보고 쿼리를 날리면 된다. 정형화 스트리밍 모델의 하부에는 스파크 SQL 엔진이 장애 복구와 지연 데이터의 모든 측면을 관리한다.\nGraphX 그래프를 조작하고 그래프 병렬 연산을 수행하기 위한 라이브러리다. 분석, 연결 탐색 등 표준적인 그래프 알고리즘과 커뮤니티 사용자들이 기여한 알고리즘을 포함한다.\nSpark Architecture # Spark Driver # SparkSession 객체를 초기화하는 책임을 가진 Spark Application의 일부이다. Spark Driver는 여러 가지 역할을 한다.\nCluster Manager와 통신하며 Spark Executor들을 위해 필요한 자원을 요청한다. 모든 스파크 작업을 DAG 연산 형태로 변환해 스케줄링한다. 각 실행 단위를 태스크로 나누어 Spark Executor들에게 분배한다. 자원이 한번 할당되면 그 다음부터는 Driver가 Executor와 직접 통신한다. SparkSession # 스파크 2.0에서 모든 스파크 연산과 데이터에 대한 통합 연결 채널이 만들어졌다.\nSparkContext, SQLContext, HiveContext, SparkConf, StreamingContext 등이 합쳐졌다. 일원화된 연결 채널을 통해 JVM 실행 파라미터들을 만들고 데이터프레임이나 데이터세트를 정의한다. 데이터 소스에서 데이터를 읽고 메타데이터에 접근해 스파크 SQL 질의를 실행할 수 있다. SparkSession은 모든 스파크 기능을 한 군데에서 접근할 수 있는 시작점을 제공한다.\npyspark.sql.SparkSession 문서를 참조해 SparkSession 생성\nCopy python spark = ( SparkSession.builder .master(\u0026#34;local\u0026#34;) # 원격 접속의 경우 .remote(\u0026#34;sc://localhost\u0026#34;) .appName(\u0026#34;LearnSpark\u0026#34;) .config(\u0026#34;spark.sql.shuffle.partitions\u0026#34;, 6) .getOrCreate() ) Cluster Manager # Spark Application이 실행되는 클러스터에서 자원을 관리 및 할당하는 책임을 가진다. Standalone, Hadoop YARN, Apache Mesos, Kubernetes 네 종류의 Cluster Manager를 지원한다.\nSpark Executor # 클러스터의 각 워커 노드에서 동작하며, Driver와 통신하며 Task를 실행하는 역할을 한다. 대부분의 배포 모드에서 노드당 하나의 Executor만 실행한다.\n배포 모드 # 스파크가 여러 환경에서 돌아갈 수 있도록 다양한 배포 모드를 지원한다. 추상화되어 있어 Cluster Manager는 실행 환경에 대한 정보가 필요없고, YARN이나 Kubernetes 같은 인기 있는 환경에 배포가 가능하다.\nMode Spark Driver Spark Executor Cluster Manager Local 단일 서버 같은 머신에서 단일 JVM 위에서 실행 Driver와 동일한 JVM 위에서 동작 동일한 호스트에서 실행 Standalone Cluster의 아무 노드에서나 실행 Cluster의 각 노드가 자체적인 Executor를 실행 Cluster의 아무 호스트에나 할당 YARN(Client) Cluster 외부의 Client에서 동작 YARM의 노드 매니저의 컨테이너 YARN의 리소스 매니저가 노드 매니저에 컨테이너 할당 YARN(Cluster) YARN 애플리케이션 마스터에서 동작 YARN(Client)와 동일 YARN(Client)와 동일 Kubernetes Kubernetes Pod에서 동작 각 워커가 자신의 Pod 내에서 실행 Kubernetes 마스터 분산 데이터 # 물리적인 데이터는 HDFS나 클라우드 저장소에 존재한다. 데이터는 파티션으로 물리적인 수준에서 분산되고, 스파크는 파티션을 추상화하여 메모리의 데이터프레임 객체를 바라본다.\n파티셔닝은 효과적인 병렬 처리를 가능하게 해준다. 데이터를 조각내 청크나 파티션 단위로 분산해 Spark Executor가 네트워크 사용을 최소화하고 가까이 있는 데이터만 처리한다.\n스파크 활용사례 # 데이터 사이언스 # 데이터 사이언티스트들은 데이터를 정제하고 패턴을 발견하기 위해 데이터를 살펴본다. 대부분은 SQL에 능하고, NumPy나 pandas 같은 라이브러리를 편하게 사용한다. 모델 구축을 위해 분류, 회귀, 클러스터링 알고리즘을 어떻게 사용할지도 알아야 한다.\n스파크는 MLlib은 모델 파이프라인을 구축할 수 있는 일반적인 머신러닝 알고리즘들을 제공한다. 또한, 스파크 SQL로 일회성 데이터 탐색을 가능하게 해준다.\n데이터 엔지니어링 # 클러스터링 모델은 독립적으로 존재하지 않고 아파치 카프카 같은 스트리밍 엔진과 연계해 동작한다. 데이터 파이프라인은 다양한 소스에서 오는 원본 데이터를 최종 단계로까지 변형해주며, 그런 데이터는 NoSQL이나 RDBMS 등에 저장된다.\n스파크의 정형화 스트리밍 API를 써서 실시간 또는 정적인 데이터 소스에 대한 ETL 파이프라인을 구축할 수 있게 해준다. 또한, 스파크가 연산을 쉽게 병렬화 해주어 고수준 언어에만 집중해 ETL을 수행할 수 있게 지원한다.\n스파크 사용 사례 # 클러스터 전체에 걸쳐 분산된 대규모 데이터세트의 병렬 처리 데이터 탐색이나 시각화를 위한 일회성이나 대화형 질의 수행 MLlib을 이용해 머신러닝 모델을 구축, 훈련, 평가 "},{"id":17,"href":"/blog/airflow-study-7/","title":"Apache Airflow - 외부 시스템 연동 (Connection, Hook, PostgreSQL)","section":"Posts","content":""},{"id":18,"href":"/blog/airflow-study-6/","title":"Apache Airflow - REST API 연동과 Custom Operator 구현","section":"Posts","content":""},{"id":19,"href":"/blog/airflow-study-5/","title":"Apache Airflow - DAG 흐름 제어 (Trigger Rule, TriggerDagRun, TaskGroup)","section":"Posts","content":""},{"id":20,"href":"/blog/airflow-study-4/","title":"Apache Airflow - 조건부 실행과 알림 (Branch, Email Operator)","section":"Posts","content":""},{"id":21,"href":"/blog/airflow-study-3/","title":"Apache Airflow - 데이터 전달과 템플릿 활용 (Jinja, XCom, Variable)","section":"Posts","content":""},{"id":22,"href":"/blog/airflow-study-2/","title":"Apache Airflow - 기본 Operator 이해하기 (Bash, Python)","section":"Posts","content":""},{"id":23,"href":"/blog/airflow-study-1/","title":"Apache Airflow - 설치하기 (Docker Compose)","section":"Posts","content":""},{"id":24,"href":"/blog/10000-recipe/","title":"[Python] 만개의 레시피 데이터 수집","section":"Posts","content":"최근 레시피 생성을 목적으로 한 사이드 프로젝트에 참여하게 되었는데\n모델 학습을 위한 만개의 레시피 데이터 크롤링을 진행해보았습니다.\n스키마 구성 # 기존엔 레시피 명칭과 음식 재료 정보만을 수집할 계획이었지만,\n만개의 레시피의 각 페이지를 살펴보면서 추가적으로 가져갈만한 데이터가 있음을 확인하여\n우선적으로 테이블 관계 및 스키마를 구성해보았습니다.\n초기에 만개의 레시피와 공공데이터를 데이터 소스로 삼았기 때문에,\n만개의 레시피에 대한 DB _10000, 공공데이터에 대한 DB food로 구성했습니다.\n_10000 DB 내 테이블은 만개의 레시피 내 각각의 페이지에서 가져온 데이터로 구성되며,\n크게 카테고리, 레시피, 사용자 단위로 구분할 수 있습니다.\n만개의 레시피 데이터 수집 # 크롤링에서 데이터 요청 및 가공을 위해 정의된 유틸리티 함수들이 있는데,\n별도로 코드를 보여주지는 않고 해당 함수가 호출될 때 간단히 어떤 동작을 하는지만 전달드립니다.\n카테고리 추출 # 만개의 레시피 카테고리는 레시피 검색 페이지에서 간단하게 추출할 수 있으므로,\n개발자 도구 또는 requests에 대한 응답에서 카테고리에 해당하는 부분을 가져옵니다.\n여기서 get_headers() 함수는 User-Agent 등 기본적인 브라우저 정보가 담긴 헤더를 반환합니다.\nCopy python url = \u0026#34;https://www.10000recipe.com/recipe/list.html\u0026#34; headers = get_headers(url, referer=url) headers[\u0026#34;upgrade-insecure-requests\u0026#34;] = \u0026#39;1\u0026#39; response = session.get(url, params=params, headers=headers) source = BeautifulSoup(response.text, \u0026#34;html.parser\u0026#34;) cate_list = source.select_one(\u0026#34;div.cate_list\u0026#34;) pattern = \u0026#34;javascript:goSearchRecipe([\\d\\w()\u0026#39;,]+)\u0026#34; raw_cat = [(re_get(pattern, cat.attrs[\u0026#34;href\u0026#34;]),cat.text) for cat in cate_list.select(\u0026#34;a\u0026#34;) if \u0026#34;href\u0026#34; in cat.attrs] cat_map = lambda catType, catId, catName: {\u0026#34;categoryId\u0026#34;:catId, \u0026#34;categoryType\u0026#34;:catType, \u0026#34;categoryName\u0026#34;:catName} categories = [cat_map(*literal_eval(data), name) for data, name in raw_cat] categories = pd.DataFrame(categories) categories = categories[categories[\u0026#34;categoryId\u0026#34;]!=\u0026#39;\u0026#39;] categories.head() 데이터 수집 결과 아래와 같은 구조의 데이터를 획득할 수 있습니다.\ncategoryId categoryType categoryName 63 cat4 밑반찬 56 cat4 메인반찬 54 cat4 국/탕 55 cat4 찌개 60 cat4 디저트 레시피 목록 추출 # 레시피 검색 페이지는 검색어, 정렬 기준, 페이지, 카테고리를 쿼리로 받습니다.\n레시피 목록을 추출하는데 검색어나 카테고리는 필요하지 않고 동일한 정렬 기준에서 수집하기 때문에\n데이터 수집 시에는 페이지에 반복문을 적용하여 데이터가 존재하는 범위를 가져올 것입니다.\nCopy python ORDER_MAP = {\u0026#34;정확순\u0026#34;:\u0026#34;accuracy\u0026#34;, \u0026#34;최신순\u0026#34;:\u0026#34;date\u0026#34;, \u0026#34;추천순\u0026#34;:\u0026#34;reco\u0026#34;} get_params = lambda **kwargs: {k:v for k,v in kwargs.items() if v} uri = \u0026#34;https://www.10000recipe.com/recipe/\u0026#34; def fetch(session: requests.Session, query=str(), sortType=\u0026#34;추천순\u0026#34;, page=1, cat1=str(), cat2=str(), cat3=str(), cat4=str(), **kwargs) -\u0026gt; List[str]: url = uri+\u0026#34;list.html\u0026#34; params = get_params(q=query, order=ORDER_MAP[sortType], page=page, cat1=cat1, cat2=cat2, cat3=cat3, cat4=cat4) headers = get_headers(url, referer=url) headers[\u0026#34;upgrade-insecure-requests\u0026#34;] = \u0026#39;1\u0026#39; response = session.get(url, params=params, headers=headers) return parse(response.text, **kwargs) def parse(response: str, **kwargs) -\u0026gt; List[str]: source = BeautifulSoup(response, \u0026#39;html.parser\u0026#39;) uris = source.select(\u0026#34;a.common_sp_link\u0026#34;) ids = [uri.attrs[\u0026#34;href\u0026#34;].split(\u0026#39;/\u0026#39;)[-1] for uri in uris if \u0026#34;href\u0026#34; in uri.attrs] return ids 데이터 수집 결과로는 문자열 타입의 레시피 ID 목록을 획득할 수 있습니다.\n레시피 정보 추출 # 레시피 ID로 접근할 수 있는 레시피 상세 정보 페이지에서\n레시피 정보에 대한 데이터를 추출합니다. 소스코드 내에서 레시피 정보가 JSON 형식으로 존재하기 때문에\n일일히 HTML 태그를 파싱할 필요 없이 데이터를 한번에 JSON 오브젝트로 가져올 수 있습니다.\n데이터를 가공하는 map_recipe() 함수 내에서\ncast_int()는 데이터를 정수형으로 변환할 때 에러가 발생하면 기본값 0을 반환하는 함수이고,\nhier_get()은 중첩 딕셔너리의에 단계별 키 목록에 대한 값을 안전하게 가져오기 위한 함수입니다.\nCopy python uri = \u0026#34;https://www.10000recipe.com/recipe/\u0026#34; def fetch(session: requests.Session, recipeId: str, **kwargs) -\u0026gt; Dict: url = uri+recipeId # https://www.10000recipe.com/recipe/6997297 headers = get_headers(url, referer=uri+\u0026#34;list.html\u0026#34;) headers[\u0026#34;upgrade-insecure-requests\u0026#34;] = \u0026#39;1\u0026#39; response = session.get(url, headers=headers) return parse(response.text, recipeId, **kwargs) def parse(response: str, recipeId: str, **kwargs) -\u0026gt; Dict: source = BeautifulSoup(response, \u0026#39;html.parser\u0026#39;) raw_json = source.select_one(\u0026#34;script[type=\\\u0026#34;application/ld+json\\\u0026#34;]\u0026#34;).text try: data = json.loads(raw_json) except: data = literal_eval(raw_json) return map_recipe(data, recipeId, source, **kwargs) def map_recipe(data: Dict, recipeId: str, source=None, **kwargs) -\u0026gt; Dict: recipe_info = {\u0026#34;recipeId\u0026#34;: recipeId} recipe_info[\u0026#34;name\u0026#34;] = data.get(\u0026#34;name\u0026#34;) recipe_info[\u0026#34;author\u0026#34;] = hier_get(data, [\u0026#34;author\u0026#34;,\u0026#34;name\u0026#34;]) recipe_info[\u0026#34;ratingValue\u0026#34;] = cast_int(hier_get(data, [\u0026#34;aggregateRating\u0026#34;,\u0026#34;ratingValue\u0026#34;])) recipe_info[\u0026#34;reviewCount\u0026#34;] = cast_int(hier_get(data, [\u0026#34;aggregateRating\u0026#34;,\u0026#34;reviewCount\u0026#34;])) recipe_info[\u0026#34;totalTime\u0026#34;] = data.get(\u0026#34;totalTime\u0026#34;) recipe_info[\u0026#34;recipeYield\u0026#34;] = data.get(\u0026#34;recipeYield\u0026#34;) try: recipe_info[\u0026#34;recipeIngredient\u0026#34;] = \u0026#39;,\u0026#39;.join(data[\u0026#34;recipeIngredient\u0026#34;]) except: recipe_info[\u0026#34;recipeIngredient\u0026#34;] = extract_ingredient(source, **kwargs) recipe_info[\u0026#34;recipeInstructions\u0026#34;] = \u0026#39;\\n\u0026#39;.join( [step.get(\u0026#34;text\u0026#34;,str()) for step in data.get(\u0026#34;recipeInstructions\u0026#34;,list()) if isinstance(step, dict)]) recipe_info[\u0026#34;createDate\u0026#34;] = data.get(\u0026#34;datePublished\u0026#34;) return recipe_info def extract_ingredient(source: Tag, **kwargs) -\u0026gt; str: cont_ingre = source.select_one(\u0026#34;div.cont_ingre\u0026#34;) if cont_ingre: return [ingre.split() for ingre in cont_ingre.select_one(\u0026#34;dd\u0026#34;).text.split(\u0026#39;,\u0026#39;)] else: return str() 데이터 수집 결과 아래와 같이 정리된 딕셔너리를 얻을 수 있습니다.\nCopy python { \u0026#34;recipeId\u0026#34;: \u0026#34;6997297\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;두부짜조\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;호이호이\u0026#34;, \u0026#34;ratingValue\u0026#34;: 5, \u0026#34;reviewCount\u0026#34;: 1, \u0026#34;totalTime\u0026#34;: \u0026#34;PT20M\u0026#34;, \u0026#34;recipeYield\u0026#34;: \u0026#34;1 servings\u0026#34;, \u0026#34;recipeIngredient\u0026#34;: \u0026#34;두부 30g,라이스페이퍼 2장,돼지고기 5g,...\u0026#34;, \u0026#34;recipeInstructions\u0026#34;: \u0026#34;부위는 상관없지만 저는 저렴하고...\u0026#34;, \u0026#34;createDate\u0026#34;: \u0026#34;2023-02-19T13:37:04+09:00\u0026#34; } 실질적으로 활용할 데이터는 레시피명 name과 재료명인 recipeIngredient이며,\n평점, 리뷰 수, 조리순서 등도 추가적인 분석을 통해 활용성을 기대해볼 수 있습니다.\n요리 후기 추출 # 동일한 레시피 상세 정보 페이지에서 요리 후기에 대한 데이터를 추출할 수 있습니다.\n단, 요리 후기는 JSON 형식으로 정리되어 있지 않기 때문에\nHTML 소스를 파싱하여 대상 문자열을 추출해야 합니다.\n데이터를 가공하는 map_review() 함수 내에서\nre_get()은 정규표현식 패턴에 매칭되는 문자열을 추출하는 함수이고,\nselect_text()는 BeautifulSoup 태그에서\nCSS Selector로 안전하게 문자열을 추출하는 함수입니다.\nCopy python GENDER = {\u0026#34;info_name_m\u0026#34;:\u0026#34;M\u0026#34;, \u0026#34;info_name_f\u0026#34;:\u0026#34;F\u0026#34;} uri = \u0026#34;https://www.10000recipe.com/recipe/\u0026#34; rid_ptn = \u0026#34;replyReviewDiv_(\\d+)\u0026#34; uid_ptn = \u0026#34;/profile/review.html\\?uid=([\\d\\w]+)\u0026#34; date_ptn = \u0026#34;(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\u0026#34; def fetch(session: requests.Session, recipeId: str, **kwargs) -\u0026gt; List[Dict]: url = uri+recipeId headers = get_headers(url, referer=uri+\u0026#34;list.html\u0026#34;) headers[\u0026#34;upgrade-insecure-requests\u0026#34;] = \u0026#39;1\u0026#39; response = session.get(url, headers=headers) return parse(response.text, recipeId, **kwargs) def parse(response: str, recipeId: str, **kwargs) -\u0026gt; List[Dict]: source = BeautifulSoup(response, \u0026#39;html.parser\u0026#39;) reply_divs = source.select(\u0026#34;div.view_reply\u0026#34;) review_div = [div for div in reply_divs if div.select_one(\u0026#34;div.reply_tit\u0026#34;).text.strip().startswith(\u0026#34;요리 후기\u0026#34;)] if review_div: review_list = review_div[0].select(\u0026#34;div.reply_list\u0026#34;) return [map_review(review, recipeId, **kwargs) for review in review_list] else: return list() def map_review(data: Tag, recipeId: str, **kwargs) -\u0026gt; Dict: review_info = dict() review_info[\u0026#34;reviewId\u0026#34;] = re_get(rid_ptn, data.select(\u0026#34;div\u0026#34;)[-1].attrs.get(\u0026#34;id\u0026#34;)) review_info[\u0026#34;recipeId\u0026#34;] = recipeId review_info[\u0026#34;userId\u0026#34;] = re_get(uid_ptn, data.select_one(\u0026#34;a\u0026#34;).attrs.get(\u0026#34;href\u0026#34;)) review_info[\u0026#34;contents\u0026#34;] = select_text(data, \u0026#34;p.reply_list_cont\u0026#34;) detail = data.select_one(\u0026#34;h4.media-heading\u0026#34;) if detail: review_info[\u0026#34;userName\u0026#34;] = select_text(detail, \u0026#34;b\u0026#34;) gender = detail.select_one(\u0026#34;b\u0026#34;).attrs.get(\u0026#34;class\u0026#34;) review_info[\u0026#34;userGender\u0026#34;] = GENDER.get(gender[0]) if gender else None review_info[\u0026#34;createDate\u0026#34;] = re_get(date_ptn, detail.text) return review_info 데이터 수집 결과 아래와 같이 정리된 딕셔너리를 얻을 수 있습니다.\n여기서 요리 후기와 별도로 사용자 명칭과 성별을 추출할 수 있습니다.\nCopy python { \u0026#34;reviewId\u0026#34;: \u0026#34;395018\u0026#34;, \u0026#34;recipeId\u0026#34;: \u0026#34;6843136\u0026#34;, \u0026#34;userId\u0026#34;: \u0026#34;58031746\u0026#34;, \u0026#34;contents\u0026#34;: \u0026#34;정말 간단한데 중불로하니 좀 태워먹었... 맛은 있네욬ㅋㅋㅋㅋㅋ다음엔 중불이랑 약불 사이로 함 더해바야겠어욬ㅋㅋㅋㄱㅋㅋ감삼둥..♡♡\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;나찡as\u0026#34;, \u0026#34;userGender\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;createDate\u0026#34;: \u0026#34;2020-11-09 17:14:02\u0026#34; } 댓글 추출 # 레시피 상세 정보 페이지에서 댓글은 미리보기만이 제공되며\n전체 댓글을 확인하기 위해서는 별도의 페이지에 접속해야 합니다.\n해당 페이지의 출력 결과에서도 요리 후기와 같은 방식으로\nHTML 소스를 파싱하여 대상 문자열을 추출해야 합니다.\nCopy python GENDER = {\u0026#34;info_name_m\u0026#34;:\u0026#34;M\u0026#34;, \u0026#34;info_name_f\u0026#34;:\u0026#34;F\u0026#34;} uri = \u0026#34;https://www.10000recipe.com/recipe/\u0026#34; cid_ptn = \u0026#34;replyCommentDiv_(\\d+)\u0026#34; uid_ptn = \u0026#34;/profile/recipe_comment.html\\?uid=([\\d\\w]+)\u0026#34; date_ptn = \u0026#34;(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2})\u0026#34; def fetch(session: requests.Session, recipeId: str, page=1, **kwargs) -\u0026gt; List[Dict]: url = uri+\u0026#34;ajax.html\u0026#34; params = dict(q_mode=\u0026#34;getListComment\u0026#34;, seq=recipeId, page=page) headers = get_headers(url, referer=uri+recipeId) headers[\u0026#34;upgrade-insecure-requests\u0026#34;] = \u0026#39;1\u0026#39; response = session.get(url, params=params, headers=headers) return parse(response.text, recipeId, **kwargs) def parse(response: str, recipeId: str, **kwargs) -\u0026gt; List[Dict]: source = BeautifulSoup(response, \u0026#39;html.parser\u0026#39;) comment_list = source.select(\u0026#34;div.reply_list\u0026#34;) return [map_comment(comment, recipeId, **kwargs) for comment in comment_list] def map_comment(data: Tag, recipeId: str, **kwargs) -\u0026gt; Dict: comment_info = dict() comment_info[\u0026#34;commentId\u0026#34;] = re_get(cid_ptn, data.select(\u0026#34;div\u0026#34;)[-1].attrs.get(\u0026#34;id\u0026#34;)) comment_info[\u0026#34;recipeId\u0026#34;] = recipeId comment_info[\u0026#34;userId\u0026#34;] = re_get(uid_ptn, data.select_one(\u0026#34;a\u0026#34;).attrs.get(\u0026#34;href\u0026#34;)) comment_info[\u0026#34;contents\u0026#34;] = select_text(data, \u0026#34;div.media-body\u0026#34;).split(\u0026#39;|\u0026#39;)[-1] detail = data.select_one(\u0026#34;h4.media-heading\u0026#34;) if detail: comment_info[\u0026#34;userName\u0026#34;] = select_text(detail, \u0026#34;b\u0026#34;) gender = detail.select_one(\u0026#34;b\u0026#34;).attrs.get(\u0026#34;class\u0026#34;) comment_info[\u0026#34;userGender\u0026#34;] = GENDER.get(gender[0]) if gender else None comment_info[\u0026#34;createDate\u0026#34;] = re_get(date_ptn, detail.text) return comment_info review = fetch(session, \u0026#34;6843136\u0026#34;) review[0] 데이터 수집 결과 아래와 같이 정리된 딕셔너리를 얻을 수 있습니다.\n데이터 구조는 요리 후기와 동일합니다.\nCopy python { \u0026#34;commentId\u0026#34;: \u0026#34;39693405\u0026#34;, \u0026#34;recipeId\u0026#34;: \u0026#34;6843136\u0026#34;, \u0026#34;userId\u0026#34;: \u0026#34;89382542\u0026#34;, \u0026#34;contents\u0026#34;: \u0026#34;신고그러네여..재료양이..ㅜ\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;휘아여\u0026#34;, \u0026#34;userGender\u0026#34;: \u0026#34;F\u0026#34;, \u0026#34;createDate\u0026#34;: \u0026#34;2022-03-18 00:02\u0026#34; } "},{"id":25,"href":"/blog/smartstore-login-3/","title":"[Python] requests로 네이버 스마트스토어센터 로그인 구현하기 (3)","section":"Posts","content":"앞선 네이버 로그인 구현 과정을 통해 네이버 로그인에 대해 이해하고\n스마트스토어센터 로그인 결과로 얻을 수 있는 쿠키 값의 일부를 획득했습니다.\nCopy python cookies = { \u0026#34;NNB\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;nid_inf\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_AUT\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_JKL\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_CHK\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NSI\u0026#34;: \u0026#34;...\u0026#34;, } 하지만, 스마트스토어센터에서 데이터를 가져오기 위해 필요한 쿠키 값은\nCBI_SES, CBI_CHK, NSI 세 가지 값이기 때문에\n지금까지는 준비 과정에 불과했다고 할 수 있습니다.\n이번 게시글에서는 스마트스토어센터 로그인 과정을 이해하고\n직접 구현해보면서 SmartstoreLogin 클래스를 완성해보겠습니다.\n스마트스토어센터 로그인 이해 # 지금까지 스마트스토어센터의 두 가지 로그인 방식 중\n네이버 로그인 방식으로 로그인을 수행하기 위해,\n실제 네이버 로그인에 대한 이해 및 구현을 진행했습니다.\n요청 내역 탐색 시 주의사항 # 새 창에서 띄워지는 네이버 로그인 페이지는\n로그인이 완료되면 닫혀버리기 때문에 네트워크 요청 내역을 확인하기 어렵습니다.\n이 경우 개발자 도구 Sources 탭에서 Event Listener Breakpoints 메뉴 아래\nWindow \u0026gt; window.close 부분을 선택하면 창이 닫히는 순간에 중단시킬 수 있습니다.\n네이버 로그인과의 차이점 # 스마트스토어센터 로그인에서의 네이버 로그인은 기존 방식과 다소의 차이점이 존재합니다.\n아래는 스마트스토어센터 로그인 POST 요청에서 확인할 수 있는 데이터입니다.\nCopy json { \u0026#34;localechange\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;dynamicKey\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;logintp\u0026#34;: \u0026#34;oauth2\u0026#34;, \u0026#34;encpw\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;enctp\u0026#34;: 1, \u0026#34;svctype\u0026#34;: 64, \u0026#34;smart_LEVEL\u0026#34;: 1, \u0026#34;bvsd\u0026#34;: { \u0026#34;uuid\u0026#34;:\u0026#34;...\u0026#34;, \u0026#34;encData\u0026#34;:\u0026#34;...\u0026#34; }, \u0026#34;encnm\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;locale\u0026#34;: \u0026#34;ko_KR\u0026#34;, \u0026#34;client_id\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://nid.naver.com/oauth2.0/authorize?response_type=code\u0026amp;state=...\u0026amp;client_id=...\u0026amp;redirect_uri=https%3A%2F%2Faccounts.commerce.naver.com%2Foauth%2Fcallback\u0026amp;locale=ko_KR\u0026amp;inapp_view=\u0026amp;oauth_os=\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;pw\u0026#34;: \u0026#34;\u0026#34; } 기존의 네이버 로그인 데이터와 비교했을 때 3개의 값이 추가되었음을 알 수 있습니다.\nlogintp의 경우 \u0026quot;oauth2\u0026quot;로 고정된 값으로 보이지만,\nurl 내 state와 client_id는 지금까지의 과정에서는 얻을 수 없었던\n새로운 값으로 로그인을 위해 추가적인 동작이 필요해 보입니다.\nOAuth URL 가져오기 # state와 client_id의 경우 네이버 로그인 페이지를 불러오는 과정에서\n이미 전달되는 값이기 때문에 해당 페이지 안에서는 출처를 찾을 수 없었습니다.\n따라서 네이버 로그인 페이지로 이동하기 위해 거치는 스마트스토어센터 로그인 페이지에서\n네이버 로그인 페이지를 띄우는 과정에 집중하여 두 값이 발생하는 지점을 찾아보았고,\ngraphql 주소로 보낸 POST 요청에 대한 응답으로 url에 해당하는 authUrl 값을 받는 것을 확인했습니다.\n이렇게 구한 client_id 및 url 값을 로그인 데이터에 담아 요청을 보낼 경우\n일반적인 네이버 로그인 결과로 얻을 수 있는 NID_AUT 등의 쿠키 값을 획득할 수 있습니다.\nGraphQL 로그인 분석 # 스마트스토어센터 로그인은 네이버 로그인에서 그치지 않고\nCBI_SES, CBI_CHK, NSI 쿠키 값을 추가로 얻어야 합니다.\n이 중에서 CBI_SES를 응답 파일 내에서 검색했을 때 graphql 주소에 대한 응답으로\nCBI_SES와 CBI_CHK 값을 반환하는 것을 알 수 있었습니다.\n해당 주소는 앞서 인증 주소를 가져오는 과정에서 보았던 것인데\n당시 snsLoginBegin라는 명칭의 쿼리와는 다른 snsLoginCallback 쿼리를 사용하여\n추가적인 로그인을 수행하는 것임을 짐작할 수 있습니다.\n변수로 전달되는 state의 경우 앞에서 구한 것과 동일한 값이지만,\ncode는 아직까지 본 적 없는 값입니다.\n하지만, code는 어떠한 응답 파일 내에서도 출처를 찾아볼 수 없고,\ncode의 값 자체를 검색했을 때 oauth_token이라는 키와 동일한 값을 사용한다는 것 말고는\n별다른 단서를 찾을 수 없었습니다.\n이 경우 네이버 로그인 후에 연속적으로 진행되는 다른 요청 내역을 직접 들여다봐야 했고,\n다행히 바로 아래의 주소에 대한 응답 내역에서 oauth_token 값을 받아볼 수 있었습니다.\nCopy html \u0026lt;html\u0026gt; \u0026lt;script language=javascript nonce=\u0026#34;4SzeR1mCGzDbnzr3s5rjQ1Li\u0026#34;\u0026gt; location.replace(\u0026#34;https://nid.naver.com/login/noauth/allow_oauth.nhn?oauth_token=...\u0026amp;with_pin\u0026amp;step=agree_term\u0026amp;inapp_view=\u0026amp;oauth_os=\u0026#34;); \u0026lt;/script\u0026gt; \u0026lt;/html\u0026gt; oauth_token의 값을 code에 넣어서 state와 함께 graphql 주소에 요청할 경우\n응답 헤더의 Set-Cookie에서 볼 수 있는 CBI_SES와 CBI_CHK를 받게 됩니다.\n2단계 인증 분석 # 스마트스토어센터는 최초 로그인 시 반드시 2단계 인증을 거쳐야 합니다.\n마지막 남은 NSI 값 또한 해당 2단계 인증을 거쳐야 얻을 수 있을 것이라 걱정했지만,\n다행히 2단계 인증을 거치지 않아도 네트워크 응답 내역에서 NSI를 확인할 수 있었습니다.\nPOST 요청이지만 전달되는 데이터는 아래와 같이 단순했기에\n추가적인 분석 없이 마지막 NSI 값을 획득했습니다.\nCopy json {\u0026#34;url\u0026#34;: \u0026#34;https://sell.smartstore.naver.com/#/home/dashboard\u0026#34;} 스마트스토어센터 로그인 구현 # 지금까지의 과정을 통해 스마트스토어센터에서 데이터를 가져오기 위해 필요한 CBI_SES, CBI_CHK, NSI 값을 획득하는 방법을 파악했습니다.\n이를 SmartstoreLogin 클래스의 메소드로 구현해보겠습니다.\n네이버 로그인 구현 # 기존의 네이버 로그인 기능에 OAuth URL을 가져오는 부분을 추가시킨\nnid_login() 및 fetch_oauth_url() 메소드를 정의합니다.\nCopy python SMARTSTORE_URL = \u0026#34;https://sell.smartstore.naver.com/\u0026#34; SLOGIN_URL = \u0026#34;https://accounts.commerce.naver.com\u0026#34; GRAPHQL_DATA = str({ \u0026#34;operationName\u0026#34;: \u0026#34;snsLoginBegin\u0026#34;, \u0026#34;variables\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;login\u0026#34;, \u0026#34;snsCd\u0026#34;: \u0026#34;naver\u0026#34;, \u0026#34;svcUrl\u0026#34;: \u0026#34;https://sell.smartstore.naver.com/#/login-callback\u0026#34;}, \u0026#34;query\u0026#34;: \u0026#34;mutation snsLoginBegin($mode: String!, $snsCd: String!, $svcUrl: String!, \\ $oneTimeLoginSessionKey: String, $userInfos: [UserInfoEntry!]) {\\n snsBegin(\\n \\ snsLoginBeginRequest: {mode: $mode, snsCd: $snsCd, svcUrl: $svcUrl, oneTimeLoginSessionKey: \\ $oneTimeLoginSessionKey, userInfos: $userInfos}\\n ) {\\n authUrl\\n __typename\\n }\\n}\\n\u0026#34; }).replace(\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;) class SmartstoreLogin(NaverLogin): def fetch_oauth_url(self): referer = f\u0026#34;{SLOGIN_URL}/login?url={SMARTSTORE_URL}#/login-callback\u0026#34; headers = self.get_headers(host=SLOGIN_URL, referer=referer) response = self.post(urljoin(SLOGIN_URL, \u0026#34;graphql\u0026#34;), data=GRAPHQL_DATA, headers=headers) self.oauth_url = json.loads(response.text)[\u0026#34;data\u0026#34;][\u0026#34;snsBegin\u0026#34;][\u0026#34;authUrl\u0026#34;] self.oauth_params = {k:v.pop() for k,v in parse_qs(urlparse(self.oauth_url).query).items()} if \u0026#34;auth_type\u0026#34; in self.oauth_params: self.oauth_params.pop(\u0026#34;auth_type\u0026#34;) self.oauth_params = dict(self.oauth_params, **{\u0026#34;locale\u0026#34;:\u0026#34;ko_KR\u0026#34;,\u0026#34;inapp_view\u0026#34;:\u0026#39;\u0026#39;,\u0026#34;oauth_os\u0026#34;:\u0026#39;\u0026#39;}) graphql 주소에 대한 요청 데이터를 그대로 구현한 것이 GRAPHQL_DATA이며,\n그 결과로 OAuth URL을 얻을 수 있습니다.\nOAuth URL의 파라미터는 향후 GraphQL 인증 과정에서 재활용되기 때문에\noauth_params 변수에 저장해둡니다.\nCopy python LOGIN_URL = \u0026#34;https://nid.naver.com/nidlogin.login\u0026#34; SLOGIN_DATA = lambda dynamicKey, encpw, bvsd, encnm, client_id: \\ dict(LOGIN_DATA(dynamicKey, encpw, bvsd, encnm), **{\u0026#34;logintp\u0026#34;:\u0026#34;oauth2\u0026#34;,\u0026#34;svctype\u0026#34;:\u0026#34;64\u0026#34;,\u0026#34;client_id\u0026#34;:client_id}) class SmartstoreLogin(NaverLogin): def nid_login(self): self.fetch_keys() self.set_encpw() self.set_bvsd() self.fetch_oauth_url() data = SLOGIN_DATA(self.dynamicKey, self.encpw, self.bvsd, self.encnm, self.oauth_params.get(\u0026#34;client_id\u0026#34;)) headers = self.get_headers(LOGIN_URL, referer=self.oauth_url) headers[\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/x-www-form-urlencoded\u0026#34; headers[\u0026#34;Upgrade-Insecure-Requests\u0026#34;] = \u0026#34;1\u0026#34; response = self.post(LOGIN_URL, data=data, headers=headers) 네이버 로그인 과정에서는 bvsd를 생성한 후 OAuth URL을 추가로 가져오고\nclient_id를 기존의 로그인 데이터 내에 포함시켜 POST 요청을 보냅니다.\n해당 메소드의 결과로 NID_AUT, NID_JKL, NID_SES를 부여받을 수 있습니다.\nOAuth 로그인 구현 # OAuth 로그인은 네이버 로그인과 GraphQL 인증으로 구성됩니다.\n현시점에서 GraphQL 인증에 필요한 것은 oauth_token 뿐이기 때문에\n앞선 네이버 로그인 과정에서 획득한 주소로부터 oauth_token을 가져오는 메소드 fetch_oauth_token()과\n전체적인 OAuth 로그인 과정을 구현한 oauth_login() 메소드를 정의합니다.\nCopy python OAUTH_URL = \u0026#34;https://nid.naver.com/oauth2.0/authorize\u0026#34; class SmartstoreLogin(NaverLogin): def fetch_oauth_token(self): headers = self.get_headers(LOGIN_URL, referer=LOGIN_URL, cookies=self.get_cookies()) response = self.get(OAUTH_URL, headers=headers, params=self.oauth_params) if re.search(\u0026#34;(?\u0026lt;=oauth_token\\=)(.*?)(?=\u0026amp;)\u0026#34;, response.text): self.oauth_token = re.search(\u0026#34;(?\u0026lt;=oauth_token\\=)(.*?)(?=\u0026amp;)\u0026#34;, response.text).group() Copy python OAUTH_DATA = lambda code, state: str({ \u0026#34;operationName\u0026#34;:\u0026#34;snsLoginCallback\u0026#34;, \u0026#34;variables\u0026#34;: { \u0026#34;code\u0026#34;: code, \u0026#34;state\u0026#34;: state}, \u0026#34;query\u0026#34;:\u0026#34;mutation snsLoginCallback($code: String!, $state: String!) \\ {\\n snsCallback(snsLoginCallbackRequest: {code: $code, state: $state}) \\ {\\n statCd\\n loginStatus\\n nextUrl\\n sessionKey\\n snsCd\\n \\ idNo\\n realnm\\n age\\n email\\n __typename\\n }\\n}\\n\u0026#34; }).replace(\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;) class SmartstoreLogin(NaverLogin): def oauth_login(self): self.nid_login() self.fetch_oauth_token() code, state = self.oauth_token, self.oauth_params.get(\u0026#34;state\u0026#34;) referer = SLOGIN_URL+f\u0026#34;/oauth/callback?code={code}\u0026amp;state={state}\u0026#34; headers = self.get_headers(host=SLOGIN_URL, referer=referer, cookies=self.get_cookies()) response = self.post(urljoin(SLOGIN_URL, \u0026#34;graphql\u0026#34;), data=OAUTH_DATA(code, state), headers=headers) 2단계 인증 구현 # 2단계 인증을 직접 수행할 필요는 없습니다.\nNSI 쿠키 값을 할당받을 수 있는 주소로 POST 요청을 보내는\ntwo_factor_login() 메소드를 정의합니다.\nCopy python TWOLOGIN_URL = SMARTSTORE_URL+\u0026#34;api/login?url=https%3A%2F%2Fsell.smartstore.naver.com%2F%23%2Fhome%2Fdashboard\u0026#34; TWOLOGIN_DATA = {\u0026#34;url\u0026#34;: \u0026#34;https://sell.smartstore.naver.com/#/home/dashboard\u0026#34;} class SmartstoreLogin(NaverLogin): def two_factor_login(self): headers = self.get_headers(SMARTSTORE_URL, referer=SMARTSTORE_URL, cookies=self.get_cookies()) headers[\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/json;charset=UTF-8\u0026#34; headers[\u0026#34;x-current-state\u0026#34;] = \u0026#34;https://sell.smartstore.naver.com/#/login-callback\u0026#34; headers[\u0026#34;x-current-statename\u0026#34;] = \u0026#34;login-callback\u0026#34; headers[\u0026#34;x-to-statename\u0026#34;] = \u0026#34;login-callback\u0026#34; response = self.post(TWOLOGIN_URL, data=TWOLOGIN_DATA, headers=headers) 로그인 메소드 구현 # SmartstoreLogin 객체를 사용할 때는 login() 메소드를 활용합니다.\nCopy python class SmartstoreLogin(NaverLogin): def login(self): email_pattern = re.compile(\u0026#34;[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\u0026#34;) self.seller_login() if email_pattern.search(self.userid) else self.oauth_login() self.two_factor_login() 향후 판매자 계정으로 로그인 하는 경우를 고려해\nuserid가 이메일인 경우 seller_login() 이라는 미구현된 메소드를 실행하도록 정의했습니다.\n일반적인 네이버 아이디를 사용할 경우엔 OAuth 로그인과 2단계 인증을 거쳐\n처음 목적으로 했던 아래의 모든 쿠키 값을 획득하게 됩니다.\nCopy python cookies = { \u0026#34;NNB\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;nid_inf\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_AUT\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_JKL\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_CHK\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NSI\u0026#34;: \u0026#34;...\u0026#34;, } 해당 쿠키를 가진 SmartstoreLogin 객체를 세션 객체로 활용한다면\n스마트스토어센터 내 어떤 데이터라도 파이썬 requests 모듈로 가져올 수 있게 됩니다.\n"},{"id":26,"href":"/blog/smartstore-login-2/","title":"[Python] requests로 네이버 스마트스토어센터 로그인 구현하기 (2)","section":"Posts","content":"이번 게시글에서는 스마트스토어센터 페이지에서 데이터를 수집하는 자동화 프로그램을 제작하기 위한\n첫 번째 과정으로 네이버 로그인을 구현할 것입니다.\n앞선 게시글에서 데이터를 수집하는 방식에 대해 알아보면서\n로그인이 필요한 페이지에 접근하기 다음과 같은 쿠키 값이 필요함을 확인했습니다.\nCopy python cookies = { \u0026#34;NNB\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;nid_inf\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_AUT\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_JKL\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_CHK\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NSI\u0026#34;: \u0026#34;...\u0026#34;, } 위 키값들은 앞으로 로그인 프로세스를 파악하는 과정에서 중요하게 활용됩니다.\n네이버 로그인 이해 # 네이버 스마트스토어센터 로그인 과정에서 진행되는 네이버 로그인은\n일반적인 네이버 로그인과는 다른 과정으로 진행됩니다.\n따라서 우선 일반적인 네이버 로그인 과정을 알아보겠습니다.\n해당 파트는 아래 게시글을 참고해 작성되었습니다.\n파이썬#76 - 파이썬 크롤링 requests 로 네이버 로그인 하기\n네이버 로그인 요청 분석 # 네이버 로그인 과정을 분석하기 위해서는 우선 네이버 로그인을 요청을 시도하여\n전달되는 값을 확인해야 합니다.\n네이버 로그인 페이지에서 로그인을 수행하는 과정에서\n발견할 수 있는 POST 요청을 살펴보면 다음과 같은 데이터가 전달됨을 발견할 수 있습니다.\n암호화된 값을 생략하고 키로 전달되는 내용을 확인하면 다음과 같습니다.\nCopy json { \u0026#34;localechange\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;dynamicKey\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;encpw\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;enctp\u0026#34;: 1, \u0026#34;svctype\u0026#34;: 1, \u0026#34;smart_LEVEL\u0026#34;: 1, \u0026#34;bvsd\u0026#34;: { \u0026#34;uuid\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;encData\u0026#34;: \u0026#34;...\u0026#34; }, \u0026#34;encnm\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;locale\u0026#34;: \u0026#34;ko_KR\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://www.naver.com\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;pw\u0026#34;: \u0026#34;\u0026#34; } 공백이나 고정된 값을 가진 키를 제외하면 결과적으로\ndynamicKey, encpw, bvsd, encnm를 밝혀내는 것이 중요할 것이라 판단됩니다.\n네이버 로그인 폼 분석 # 키의 명칭만으로는 무엇을 의미하는지 알 수 없기 때문에\n로그인 페이지 소스에서 키명칭을 검색하였고 네이버 로그인 폼에서 하나의 단서를 찾을 수 있었습니다.\ndynamicKey의 경우 로그인 폼에 동적으로 부여되는 값임을 알 수 있습니다.\n하지만 나머지 encpw, bvsd, encnm의 값은 비어있기 때문에\n다른 자바스크립트 응답을 분석해야 합니다.\n네이버 로그인 RSA 암호화 # encpw 값에 대한 단서를 찾기 위해 전체 검색을 수행했을 때\ncommon_202201.js 내부에서 RSA 암호화 처리를 통해 값을 생성함을 알 수 있습니다.\n그 중에서 가장 처음 단계로 실행될 것이라 추측되는 것이 아래 confirmSubmit() 함수입니다.\n해당 함수는 아이디와 비밀번호의 여부를 체크하고 encryptIdPw() 함수의 결과를 반환합니다.\n바로 밑에서 확인할 수 있는 encryptIdPw() 함수의 내용은 다음과 같습니다.\nCopy js function encryptIdPw() { var id = $(\u0026#34;id\u0026#34;); var pw = $(\u0026#34;pw\u0026#34;); var encpw = $(\u0026#34;encpw\u0026#34;); var rsa = new RSAKey; if (keySplit(session_keys)) { rsa.setPublic(evalue, nvalue); try{ encpw.value = rsa.encrypt( getLenChar(sessionkey) + sessionkey + getLenChar(id.value) + id.value + getLenChar(pw.value) + pw.value); } catch(e) { return false; } $(\u0026#39;enctp\u0026#39;).value = 1; id.value = \u0026#34;\u0026#34;; pw.value = \u0026#34;\u0026#34;; return true; } else { getKeyByRuntimeInclude(); return false; } return false; } 해당 함수는 session_keys라는 값을 처리하고 RSA 암호화한 결과를\nencpw의 값으로 대체하는 것을 알 수 있습니다.\n마찬가지로 해당 명칭을 검색했을 때\nsession_keys는 Ajax 통신의 응답 결과를 받아오는 것을 확인할 수 있습니다.\n하지만 네이버 로그인 페이지에서 svctype=262144를 추가적인 파라미터로 입력할 경우\n접근할 수 있는 모바일 로그인 페이지에서 해당 값을 확인할 수 있었습니다.\n다시 encryptIdPw() 함수로 돌아가서 session_keys를 처리하기 위해\nkeySplit() 함수를 찾아보았습니다.\nCopy js function keySplit(a) { keys = a.split(\u0026#34;,\u0026#34;); if (!a || !keys[0] || !keys[1] || !keys[2] || !keys[3]) { return false; } sessionkey = keys[0]; keyname = keys[1]; evalue = keys[2]; nvalue = keys[3]; $(\u0026#34;encnm\u0026#34;).value = keyname; return true } 모바일 페이지에서 볼 수 있는 session_keys 값은 콤마를 기준으로\n4개의 값으로 구분되어 있었는데 해당 함수에서는 각각을\nsessionKey, encnm, evalue, nvalue으로 분리했습니다.\n여기서 encnm 값을 우선적으로 가져올 수 있었고,\n다음으로 encpw 값을 찾기 위해 RSA 암호화 부분을 탐색해봅니다.\nCopy js rsa.setPublic(evalue, nvalue); encpw.value = rsa.encrypt( getLenChar(sessionkey) + sessionkey + getLenChar(id.value) + id.value + getLenChar(pw.value) + pw.value); session_keys에서 분리된 evalue와 nvalue로 RSA 공개키를 생성하고\n마찬가지로 session_keys에 포함된 sessionKey 및 아이디, 비밀번호의 조합을\n암호화한 결과가 encpw임을 확인할 수 있습니다.\n파이썬에서는 공개키 생성을 rsa.PublicKey() 함수로 수행할 수 있으며\nrsa.encrypt() 함수로 RSA 암호화를 진행할 수 있습니다.\n해당 과정은 아래와 같이 구현됩니다.\nCopy python publicKey = rsa.PublicKey(int(nvalue,16), int(evalue,16)) value = \u0026#39;\u0026#39;.join([chr(len(key))+key for key in [sessionKey, id, pw]]) encpw = rsa.encrypt(value.encode(), publicKey).hex() 여기까지의 과정으로 dynamicKey, encpw, encnm의 값을 얻을 수 있습니다.\nbvsd 값 생성하기 # 마지막으로 필요한 bvsd 값에 대한 단서는 응답 문서 내에서\nbvsd.1.3.8.min.js란 명칭으로 알기 쉽게 확인할 수 있지만\n그 내용은 가독성 면에서 쉽게 해석하기 어려웠습니다.\n다른 자료를 참고했을 때 bvsd는 브라우저가 정상적인지 여부를 파악하기 위한 값으로\n해당 값이 없을 경우 로그인 과정에서 캡차를 발생시킨다는 것을 알 수 있었습니다.\nbvsd.1.3.8.min.js에서 주목할 부분은 uuid 및 encData를 생성하는 부분인데\n아래 코드에서 encData는 o라는 값을 인코딩하는 것으로 추측됩니다.\no 값을 코드 내에서 찾아보니 아래와 같이 디바이스의 마우스 상태 등을\n기록한 값임을 확인할 수 있었습니다.\nCopy js o = { a: n, b: \u0026#34;1.3.8\u0026#34;, c: (0, m[\u0026#34;default\u0026#34;])(), d: r, e: this._deviceOrientation.get(), f: this._deviceMotion.get(), g: this._mouse.get(), j: this._fpDuration || y.NOT_YET, h: this._fpHash || \u0026#34;\u0026#34;, i: this._fpComponent || [] }; 하지만 각각의 값을 해석하고 생성하는 것은 쉽지 않았기에\n이미 완성된 코드를 참고하여 set_bvsd() 메소드를 정의했습니다.\nencData의 인코딩에는 lzstring 모듈의\nLZString.compressToEncodedURIComponent() 함수를 활용했습니다.\nCopy python from lzstring import LZString import uuid ENC_DATA = lambda uuid, userid, passwd: str({ \u0026#34;a\u0026#34;: f\u0026#34;{uuid}-4\u0026#34;, \u0026#34;b\u0026#34;: \u0026#34;1.3.4\u0026#34;, \u0026#34;d\u0026#34;: [{ \u0026#34;i\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;b\u0026#34;: {\u0026#34;a\u0026#34;: [\u0026#34;0\u0026#34;, userid]}, \u0026#34;d\u0026#34;: userid, \u0026#34;e\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;f\u0026#34;: \u0026#34;false\u0026#34; }, { \u0026#34;i\u0026#34;: passwd, \u0026#34;e\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;f\u0026#34;: \u0026#34;false\u0026#34; }], \u0026#34;h\u0026#34;: \u0026#34;1f\u0026#34;, \u0026#34;i\u0026#34;: {\u0026#34;a\u0026#34;: \u0026#34;Mozilla/5.0\u0026#34;} }).replace(\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;) class NaverLogin(LoginSpider): def set_bvsd(self): uuid4 = str(uuid.uuid4()) encData = LZString.compressToEncodedURIComponent(ENC_DATA(uuid4, self.userid, self.passwd)) self.bvsd = str({\u0026#34;uuid\u0026#34;:uuid4, \u0026#34;encData\u0026#34;:encData}).replace(\u0026#39;\\\u0026#39;\u0026#39;,\u0026#39;\\\u0026#34;\u0026#39;) 네이버 로그인 구현 # 지금까지의 과정을 통해 네이버 로그인에 필요한\ndynamicKey, encpw, bvsd, encnm 값을 생성하는 법을 파악했습니다.\n이를 NaverLogin 클래스의 메소드로 구현해보겠습니다.\nRSA 암호화 구현 # 먼저 dynamicKey와 함께 encpw, encmn 생성에 필요한\nsession_keys를 가져오기 위한 메소드 fetch_keys()와,\nRSA 암호화를 통해 encpw 값을 구하는 set_encpw() 메소드를 정의합니다.\nCopy python from bs4 import BeautifulSoup import rsa LOGIN_URL = \u0026#34;https://nid.naver.com/nidlogin.login\u0026#34; class NaverLogin(LoginSpider): def fetch_keys(self): response = self.get(LOGIN_URL, headers=self.get_headers(host=LOGIN_URL), params={\u0026#34;svctype\u0026#34;:\u0026#34;262144\u0026#34;}) source = BeautifulSoup(response.text, \u0026#39;lxml\u0026#39;) keys = source.find(\u0026#34;input\u0026#34;, {\u0026#34;id\u0026#34;:\u0026#34;session_keys\u0026#34;}).attrs.get(\u0026#34;value\u0026#34;) self.sessionKey, self.encnm, n, e = keys.split(\u0026#34;,\u0026#34;) self.dynamicKey = source.find(\u0026#34;input\u0026#34;, {\u0026#34;id\u0026#34;:\u0026#34;dynamicKey\u0026#34;}).attrs.get(\u0026#34;value\u0026#34;) self.publicKey = rsa.PublicKey(int(n,16), int(e,16)) session_keys의 경우 모바일 로그인 페이지에서만 가져올 수 있기 때문에\nsvctype=262144를 GET 요청의 파라미터로 전달해 모바일 로그인 페이지를 가져옵니다.\nnvalue와 evalue는 별도의 변수로 저장하지 않고\npublicKey를 생성해 클래스 변수로 저장합니다.\nCopy python class NaverLogin(LoginSpider): def set_encpw(self): value = \u0026#34;\u0026#34;.join([chr(len(key))+key for key in [self.sessionKey, self.userid, self.passwd]]) self.encpw = rsa.encrypt(value.encode(), self.publicKey).hex() 앞에서 가져온 sessionKey와 함께 미리 초기화된 네이버 아이디 및 비밀번호를\n조합 및 암호화하여 encpw를 생성합니다.\nPOST 요청 구현 # 미리 정의한 set_bvsd() 메소드를 포함해 모든 준비 과정이 마무리되었습니다.\n클래스 변수로 저장된 암호화된 값들을 데이터에 담아 POST 로그인 요청을 보내는\nlogin() 메소드는 다음과 같이 정의할 수 있습니다.\nCopy python NAVER_URL = \u0026#34;https://www.naver.com\u0026#34; LOGIN_DATA = lambda dynamicKey, encpw, bvsd, encnm: { \u0026#34;localechange\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;dynamicKey\u0026#34;: dynamicKey, \u0026#34;encpw\u0026#34;: encpw, \u0026#34;enctp\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;svctype\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;smart_LEVEL\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;bvsd\u0026#34;: bvsd, \u0026#34;encnm\u0026#34;: encnm, \u0026#34;locale\u0026#34;: \u0026#34;ko_KR\u0026#34;, \u0026#34;url\u0026#34;: quote_plus(NAVER_URL), \u0026#34;id\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;pw\u0026#34;: \u0026#34;\u0026#34;, } class NaverLogin(LoginSpider): def login(self): self.fetch_keys() self.set_encpw() self.set_bvsd() data = LOGIN_DATA(self.dynamicKey, self.encpw, self.bvsd, self.encnm) headers = self.get_headers(LOGIN_URL, referer=LOGIN_URL) headers[\u0026#34;Content-Type\u0026#34;] = \u0026#34;application/x-www-form-urlencoded\u0026#34; headers[\u0026#34;Upgrade-Insecure-Requests\u0026#34;] = \u0026#34;1\u0026#34; self.post(LOGIN_URL, data=data, headers=headers) POST 요청 시 전달되었던 데이터와 동일한 값을 반환하는 LOGIN_DATA 함수를 생성하고\n암호화된 값을 전달해 최종적인 POST 데이터를 만들었습니다.\n해당 데이터로 요청을 보낼 경우 정상적인 응답을 받게 되고\nNaverLogin 세션 객체의 쿠키 값을 확인하면 아래와 같은 결과를 확인할 수 있습니다.\nCopy python naver = NaverLogin(\u0026#34;userid\u0026#34;, \u0026#34;passwd\u0026#34;) naver.login() naver.get_cookies() ====================================== \u0026#39;NID_AUT=...; NID_JKL=...; NID_SES=...; nid_inf=1228467713\u0026#39; 또한 해당 결과는 개발자 도구에서도 응답 헤더의 set-cookie 값에서 찾아볼 수 있습니다.\n지금까지의 과정으로 네이버 로그인 과정을 거쳤을 때,\n게시글의 서두에서 언급한 쿠키 값의 목록 중에서 일부 값을 획득할 수 있습니다.\nCopy python cookies = { \u0026#34;NNB\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;nid_inf\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_AUT\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_JKL\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_CHK\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NSI\u0026#34;: \u0026#34;...\u0026#34;, } 이 중에서 NNB의 경우 네이버 페이지 접속 시 기본적으로 부여되는 값이기 때문에 무시하고\nNID_AUT, NID_JKL, NID_SES가 채워졌습니다.\n나머지 값들은 스마트스토어센터 로그인 과정에서 얻을 수 있기 때문에\n다음 게시글에서 다뤄보도록 하겠습니다.\n"},{"id":27,"href":"/blog/smartstore-login-1/","title":"[Python] requests로 네이버 스마트스토어센터 로그인 구현하기 (1)","section":"Posts","content":"네이버 스마트스토어센터에서는 매출 향상에 도움을 주는 유용한 통계 데이터를 제공해줍니다.\n쇼핑몰 데이터를 분석하는 입장에서 무료로 제공되는 이런 데이터는 큰 도움이 되지만,\n대부분이 엑셀 파일 다운로드를 지원하지 않고 빈번하게 수치가 바뀌는 데이터를 각각의 메뉴에서 매번 확인하기도 어렵습니다.\n이런 데이터를 자동화 프로그램으로 수집 및 적재할 수 있다면 업무 효율을 크게 향상시킬 수 있을 것입니다.\n이번 게시글에서는 실제 네이버 스마트스토어 로그인 구현에 앞서\n데이터 수집에 대한 간단한 설명을 진행하고 네이버 로그인 구현의 바탕이 되는 클래스와 메소드를 정의합니다.\n데이터 수집 개요 # 네이버 웹사이트에서 데이터를 수집할 때 활용할 수 있는 방안은 2가지가 있습니다.\n첫 번째는 CSS Selector 또는 XPath를 활용해 웹사이트 특정 위치의 값을 가져오는 것,\n두 번째는 API에 요청을 보내 JSON 형태의 데이터를 가져오는 것입니다.\n특정 위치의 값을 가져오는 첫 번째 방식은 UI에 의존적이어서 코드의 지속성을 보장하기 어렵고\n원하는 데이터와 관련없는 웹 소스 전체를 불러오기 때문에 속도 면에서도 단점이 있습니다.\n따라서, API를 제공하는 경우 두 번째 방식을 이용하는 것이 효율적입니다.\n데이터 수집 시나리오 # 네이버 쇼핑에서 표시되는 상품의 순위는 검색인기도를 기준으로 결정됩니다.\n키워드별 상위권 상품의 검색인기도를 가져오는 것을 예시로 데이터 수집을 진행해보겠습니다.\n위 이미지에서 왼쪽 부분은 실제 UI, 오른쪽 부분은 HTML 소스 입니다.\n해당 소스에서 데이터를 가져온다면 div.popularity-product \u0026gt; div.box-border 위치에서\ndd 태그를 순서대로 지정해서 각각의 종합, 적합도, 인기도 값을 가져올 수 있습니다.\n해당 데이터를 분석에 활용하기 위해서는 인기도 수치를 구성하는 클릭수, 판매실적 등도 필요하기 때문에\n상세보기 페이지를 확인해야하고 결과적으로 하나의 상품에 대한 데이터를 보기 위해 두 개의 페이지를 방문해야 합니다.\n하지만 네이버의 대부분의 웹페이지는 API를 기반으로 가져온 데이터로 구성되기 때문에\n해당 API를 활용할 수 있다면 더욱 효율적인 데이터 수집이 가능합니다.\n서버에서 가져오는 데이터를 확인할 때는 주로 개발자 도구의 네트워크 탭을 활용합니다.\n웹페이지 로드 시 가져오는 문서를 확인하다보면 위 이미지와 같이 목표로 하는 데이터를 보내주는 API를 발견할 수 있습니다.\n새 탭에서 해당 API 주소를 요청하면 위 이미지 내 오른쪽 부분과 같은 JSON 형식의 데이터를 받을 수 있습니다.\n실제 UI에서 가져오고자 하는 종합, 적합도, 인기도 수치도 해당 데이터에서 확인할 수 있습니다.\n여기에는 추가로 클릭수, 판매실적 등에 대한 수치 데이터도 포함되어 있기 때문에\n해당 API를 활용하면 다수의 페이지에 요청을 보낼 수고도 줄어들게 됩니다.\n로그인이 필요한 페이지의 데이터 가져오기 # 여기까지는 간단해보이지만 네이버 스마트스토어센터 데이터를 requests 모듈로 가져오는데는\n하나의 추가적인 문제가 존재합니다.\n단순한 GET 요청일지라도 로그인 정보를 갖고 있지 않다면 데이터를 받을 수 없습니다.\n스마트스토어센터에 로그인하지 않은 상태에서 위 API 주소로 요청을 보내게 된다면\n아래와 같은 에러 메시지를 받아볼 수 있습니다.\nCopy json { \u0026#34;error\u0026#34;: \u0026#34;Full authentication is required to access this resource\u0026#34; } 이 문제에 대한 해결방법은 헤더에 있습니다.\n개발자 도구 네트워크 탭에서 하나의 문서를 클릭하고 Headers 탭에서 스크롤을 내리면\n아래와 같은 Request Headers 정보를 확인할 수 있습니다.\n서버와 클라이언트 간 네트워크 요청 시 서버는 클라이언트의 정보를 확인할 목적으로\n클라이언트에 쿠키라는 암호화된 인증 정보를 남깁니다.\n클라이언트가 해당 정보를 헤더에 담아 요청을 보내는 경우에만 서버가 올바른 응답을 전달합니다.\nrequests 모듈에서는 이러한 과정을 다음과 같이 구현할 수 있습니다.\nCopy python headers = {\u0026#34;cookie\u0026#34;: \u0026#34;...\u0026#34;} response = requests.get(url, headers=headers) 하지만 일반적인 쿠키 값은 30분의 유통기한이 있기 때문에, 매번 쿠키 값을 갱신해야 하는데\n자동화 프로그램을 돌리기 전에 직접 로그인해서 쿠키 값을 갱신하는 것은 바람직하지 못합니다.\n결과적으로 로그인이 필요한 스마트스토어 페이지의 데이터를 가져오기 위해서는\n자동화된 로그인 과정을 거쳐서 쿠키 값을 갱신할 필요가 있습니다.\n쿠키 확인하기 # 클라이언트에서 요청하는 헤더 내역에서 확인할 수 있는 정보는 표현하면 다음과 같습니다.\nCopy json { \u0026#34;NNB\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;nid_inf\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_AUT\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NID_JKL\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_SES\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;CBI_CHK\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;NSI\u0026#34;: \u0026#34;...\u0026#34; } 이는 앞으로 스마트스토어센터 로그인을 구현하는데서 반드시 확인해야할 목록입니다.\n지금은 이 값들이 어떤 의미를 가지고 어디서 발생하는 값인지 알 수는 없지만,\n서버로부터 해당 값들을 받아오는 것에 집중하여 로그인 프로세스를 파악하고\n로그인 진행 과정을 쿠키 값을 통해 시각적으로 점검할 것입니다.\n스마트스토어센터 로그인 개요 # 스마트스토어센터 로그인을 구현하기 위해 로그인 페이지를 탐색할 필요가 있습니다.\n메인 페이지에서 로그인하기 버튼을 클릭했을 때 이동하는 로그인 페이지에서 실제 로그인이 이루어집니다.\n스마트스토어센터 로그인에는 판매자 아이디로 로그인하는 방식과\n네이버 아이디로 로그인하는 방식이 있습니다.\n우선적으로 네이버 아이디로 로그인하는 방식을 알아보겠습니다.\n네이버 로그인을 구현하는 것에 관해선 좋은 선례가 있어 많은 부분을 참고했습니다.\n해당 내용은 아래 링크를 참고할 수 있습니다.\n파이썬#76 - 파이썬 크롤링 requests 로 네이버 로그인 하기\n클래스 정의 # 네이버 로그인 기능은 자동화 프로그램에서 지속적으로 활용될 것이기 때문에\n별도의 클래스에서 메소드로 구현할 필요가 있습니다.\n먼저 requests 모듈의 Session 클래스를 상속받는 NaverLogin 클래스를 정의합니다.\nNaverLogin은 네이버 ID와 비밀번호를 초기화하는 단순한 기능만을 구현했지만\nrequests.Session 클래스를 상속받았기 때문에\n웹페이지 요청과 관련된 다양한 기능을 가지고 있습니다.\nCopy python class NaverLogin(requests.Session): def __init__(self, userid: str, passwd: str, **kwargs): super().__init__(**kwargs) self.userid = userid self.passwd = passwd 그리고 NaverLogin을 상속받는 SmartstoreLogin 클래스를 정의합니다.\n일반적인 네이버 로그인과 스마트스토어센터에서 진행되는 네이버 로그인이 다르기 때문에\nNaverLogin 메소드의 일부를 변경할 필요가 있을 것입니다.\nCopy python class SmartstoreLogin(NaverLogin): def __init__(self, userid=str(), passwd=str(), **kwargs): super().__init__(userid, passwd, **kwargs) 추가적으로 로그인 페이지 요청 과정에서 빈번하게 정의해야 하는 매개변수 생성을\n간단하게 할 수 있는 메소드를 정의하겠습니다.\n헤더 생성 메소드 정의 # requests 모듈은 기본적으로 헤더를 갖고 있지 않는데\n이 상태로 다수의 웹페이지에 요청을 보낸다면 로봇으로 간주당해 차단당할 것입니다.\n임의의 웹페이지에 요청을 보낼 때 확인할 수 있는 요청 헤더 HEADERS를 기본 바탕으로,\n웹페이지 별로 최적화된 헤더를 생성하는 get_headers() 메소드를 정의합니다.\nCopy python HEADERS = { \u0026#34;Accept\u0026#34;: \u0026#34;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\u0026#34;, \u0026#34;Accept-Encoding\u0026#34;: \u0026#34;gzip, deflate, br\u0026#34;, \u0026#34;Accept-Language\u0026#34;: \u0026#34;ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\u0026#34;, \u0026#34;Connection\u0026#34;: \u0026#34;keep-alive\u0026#34;, \u0026#34;sec-ch-ua\u0026#34;: \u0026#39;\u0026#34;Chromium\u0026#34;;v=\u0026#34;106\u0026#34;, \u0026#34;Google Chrome\u0026#34;;v=\u0026#34;106\u0026#34;, \u0026#34;Not;A=Brand\u0026#34;;v=\u0026#34;99\u0026#34;\u0026#39;, \u0026#34;sec-ch-ua-mobile\u0026#34;: \u0026#34;?0\u0026#34;, \u0026#34;sec-ch-ua-platform\u0026#34;: \u0026#39;\u0026#34;Windows\u0026#34;\u0026#39;, \u0026#34;Sec-Fetch-Dest\u0026#34;: \u0026#34;empty\u0026#34;, \u0026#34;Sec-Fetch-Mode\u0026#34;: \u0026#34;cors\u0026#34;, \u0026#34;Sec-Fetch-Site\u0026#34;: \u0026#34;same-origin\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\u0026#34;, } Copy python from urllib.parse import urlparse class NaverLogin(requests.Session): def get_headers(self, authority=str(), referer=str(), cookies=str(), host=str(), **kwargs) -\u0026gt; Dict[str,Any]: headers = HEADERS.copy() if authority: headers[\u0026#34;Authority\u0026#34;] = urlparse(authority).hostname if host: headers[\u0026#34;Host\u0026#34;] = urlparse(host).hostname if referer: headers[\u0026#34;Referer\u0026#34;] = referer if cookies: headers[\u0026#34;Cookie\u0026#34;] = cookies return dict(headers, **kwargs) 호스트명을 의미하는 Authority 또는 Host, 리다이렉트 전 경로를 의미하는 Referer,\n그리고 쿠키를 의미하는 Cookie 등의 값은 수시로 변하기 때문에 별도의 입력으로 지정합니다.\n쿠키 생성 메소드 정의 # 헤더와 함께 활용되는 쿠키는 헤더와 마찬가지로 웹페이지 요청 시 빈번히 활용되는데\nrequests 모듈의 쿠키 자료형인 RequestsCookieJar를 헤더에 직접 포함시킬 수 없기 때문에,\n쿠키를 적절한 형태의 문자열로 변환하는 get_cookies() 메소드를 정의합니다.\nCopy python from requests.cookies import RequestsCookieJar class NaverLogin(requests.Session): def get_cookies(self, **kwargs) -\u0026gt; str: return self.parse_cookies(dict(self.cookies, **kwargs)) def parse_cookies(self, cookies: RequestsCookieJar) -\u0026gt; str: return \u0026#34;; \u0026#34;.join([str(key)+\u0026#34;=\u0026#34;+str(value) for key,value in cookies.items()]) 마치며 # 이번 게시글에서는 두 가지 데이터 수집 방식을 예시를 통해 알아보았고\n스마트스토어센터 로그인의 바탕이 되는 클래스와 메소드를 정의했습니다.\n다음 게시글에서는 네이버 로그인을 본격적으로 구현해보겠습니다.\n"},{"id":28,"href":"/blog/hugo-blog-old-3/","title":"Hugo 블로그 만들기 [2022년] (3) - 테마 커스터마이징","section":"Posts","content":"Hugo 블로그 만들기 (3) - 테마 커스터마이징 # 블로그를 구성할 때 기술적, 시간적 한계 때문에 이미 만들어진 테마를 사용하게 됩니다.\n제가 Hugo 블로그를 만들 때도 이러한 문제 때문에 PaperMod 테마를 사용했지만,\n블로그를 보다보면 만족스럽지 못한 부분이 발견됩니다.\n이번 포스트에서는 제가 PaperMod 테마를 커스터마이징한 과정을 안내해드리겠습니다.\nArchive, Search 추가하기 # PaperMod 테마를 가져오면서 가장 신경쓰였던 부분은\n메인 메뉴가 Categories, Tags 두 개 뿐이었단 점입니다.\nArchive는 그렇다쳐도 Search 기능은 빼먹을 수 없는 부분이라 생각하기 때문에,\nHugo 및 PaperMod 내 이슈를 참고하여 관련된 내용을 탐색했습니다.\n다행히 PaperMod 테마에서 해당 기능을 연결하지 않았을 뿐,\n기능에 대한 레이아웃은 존재하기 때문에 content/ 디렉토리 아래 다음과 같은 파일을 추가했습니다.\nCopy yaml # content/archive.md --- title: \u0026#34;Archive\u0026#34; layout: \u0026#34;archives\u0026#34; url: \u0026#34;/archive\u0026#34; summary: \u0026#34;archive\u0026#34; --- Copy yaml # content/search.md --- title: \u0026#34;Search\u0026#34; layout: \u0026#34;search\u0026#34; url: \u0026#34;/search\u0026#34; summary: \u0026#34;search\u0026#34; --- 추가로, 설정에서도 해당 파일을 인식해야되기 때문에 다음과 같은 설정을 추가했습니다.\npost/ 외에 다른 디렉토리를 등록하고 싶은 경우에도 해당 키값을 활용할 수 있습니다.\nCopy yaml params: mainsections: [\u0026#34;page\u0026#34;, \u0026#34;post\u0026#34;, \u0026#34;archive\u0026#34;, \u0026#34;search\u0026#34;] 마지막으로, 메인 메뉴에서 해당 링크로 이동하기 위한 바로가기를 추가했습니다.\n여기에는 카테고리, 태그 등이 있을건데 weight 값을 통해 적절하게 위치를 조정할 수 있습니다.\nCopy yaml menu: main: - identifier: archive name: Archive url: /archive/ weight: 10 - identifier: search name: Search url: /search/ weight: 20 위와 같은 과정을 통해 Archive, Search 기능을 추가했습니다.\n검색 엔진 등록하기 # 검색 엔진에 등록하기 위한 과정은 해당 영상을 참고해주시기 바랍니다.\n저는 위 과정에서 블로그 내에 추가해야 할 Site Verification Tag를 추가하는 법을 전달드리겠습니다.\nPaperMod 테마에서는 아래처럼 해당 부분이 만들어져 있기 때문에 크게 걱정할 필요는 없습니다.\n아래는 layouts/partials/ 내에 head.html 파일에서 가져왔습니다.\nCopy html {{- if site.Params.analytics.google.SiteVerificationTag }} \u0026lt;meta name=\u0026#34;google-site-verification\u0026#34; content=\u0026#34;{{ site.Params.analytics.google.SiteVerificationTag }}\u0026#34;\u0026gt; {{- end }} {{- if site.Params.analytics.yandex.SiteVerificationTag }} \u0026lt;meta name=\u0026#34;yandex-verification\u0026#34; content=\u0026#34;{{ site.Params.analytics.yandex.SiteVerificationTag }}\u0026#34;\u0026gt; {{- end }} {{- if site.Params.analytics.bing.SiteVerificationTag }} \u0026lt;meta name=\u0026#34;msvalidate.01\u0026#34; content=\u0026#34;{{ site.Params.analytics.bing.SiteVerificationTag }}\u0026#34;\u0026gt; {{- end }} {{- if site.Params.analytics.naver.SiteVerificationTag }} \u0026lt;meta name=\u0026#34;naver-site-verification\u0026#34; content=\u0026#34;{{ site.Params.analytics.naver.SiteVerificationTag }}\u0026#34;\u0026gt; {{- end }} 구글, 네이버 외에 Bing, Yandex를 지원하며 저는 다음과 같이 구글과 네이버만 설정했습니다.\nCopy yaml params: analytics: google: SiteVerificationTag: \u0026lt;YOUR-VERIFICATION-TAG\u0026gt; naver: SiteVerificationTag: \u0026lt;YOUR-VERIFICATION-TAG\u0026gt; 번외로 Google Tag 등 head에 추가로 입력할 부분이 있다면,\n동일한 위치에 extend_head.html을 사용할 수 있습니다.\n아래는 제가 extend_head.html 내에 Google Tag를 위한 스크립트를 추가한 부분입니다.\nCopy html {{- if site.GoogleAnalytics }} {{- /* Google tag (gtag.js) */}} \u0026lt;script async src=\u0026#34;https://www.googletagmanager.com/gtag/js?id={{ site.GoogleAnalytics }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\u0026#39;js\u0026#39;, new Date()); gtag(\u0026#39;config\u0026#39;, \u0026#39;{{ site.GoogleAnalytics }}\u0026#39;); \u0026lt;/script\u0026gt; {{- end }} KaTex 추가하기 # KaTex는 웹에서 수식을 표현하기 위한 방식입니다.\n제 과거 게시글엔 KaTex 표기법을 사용한 것이 존재하는데 이것이 제대로 표시되지 않는 문제를 발견했습니다.\n저는 공식 문서 대신 Stack Overflow 등을 참고해 아래 코드를 extend_head.html에 추가했는데,\n아쉽게도 출처는 남겨두지 못했습니다.\nCopy html \u0026lt;script\u0026gt; MathJax = { tex: { inlineMath: [[\u0026#39;$\u0026#39;, \u0026#39;$\u0026#39;], [\u0026#39;\\\\(\u0026#39;, \u0026#39;\\\\)\u0026#39;]], displayMath: [[\u0026#39;$$\u0026#39;,\u0026#39;$$\u0026#39;], [\u0026#39;\\\\[\u0026#39;, \u0026#39;\\\\]\u0026#39;]], processEscapes: true, processEnvironments: true }, options: { skipHtmlTags: [\u0026#39;script\u0026#39;, \u0026#39;noscript\u0026#39;, \u0026#39;style\u0026#39;, \u0026#39;textarea\u0026#39;, \u0026#39;pre\u0026#39;] } }; window.addEventListener(\u0026#39;load\u0026#39;, (event) =\u0026gt; { document.querySelectorAll(\u0026#34;mjx-container\u0026#34;).forEach(function(x){ x.parentElement.classList += \u0026#39;has-jax\u0026#39;}) }); \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://polyfill.io/v3/polyfill.min.js?features=es6\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; id=\u0026#34;MathJax-script\u0026#34; async src=\u0026#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Cover 간편하게 지정하기 # 저는 Github 저장소 내에 업로드한 이미지 주소를 속성값에 연결해 블로그 이미지를 표시하는데,\n게시글을 작성할 때마다 지정하게 되는 Cover 이미지의 경우 매번 전체 링크를 지정하는게 불편했습니다.\n대표적으로 해당 게시글의 Cover 이미지 주소는 다음과 같습니다.\nCopy html https://github.com/minyeamer/til/blob/main/.media/covers/hugo-logo.png?raw=true 저는 여기서 hugo-logo.png를 제외한 앞뒤의 요소가 불필요하다는 것을 인식했고\n설정 파일에 다음과 같이 prefix, suffix라는 키값으로 지정하게 처리했습니다.\nCopy yaml params: cover: prefix: \u0026#34;https://github.com/minyeamer/til/blob/main/.media/covers/\u0026#34; suffix: \u0026#34;?raw=true\u0026#34; 그리고 해당 설정을 적용시키기 위해 실질적으로 Cover 이미지를 표시하는 layouts/partials/ 아래 cover.html 파일을 수정했습니다.\n주석으로 지정된 부분이 원본이며, image 키값의 앞뒤로 prefix와 suffix를 덧붙였습니다.\nCopy html \u0026lt;!-- {{- if $addLink }}\u0026lt;a href=\u0026#34;{{ (.Params.cover.image) | absURL }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34;\u0026gt;{{ end -}} \u0026lt;img loading=\u0026#34;lazy\u0026#34; src=\u0026#34;{{ (.Params.cover.image) | absURL }}\u0026#34; alt=\u0026#34;{{ $alt }}\u0026#34;\u0026gt; --\u0026gt; {{- if $addLink }}\u0026lt;a href=\u0026#34;{{ if site.Params.cover.prefix }}{{ site.Params.cover.prefix }}{{ end }}{{ .Params.cover.image }}{{ if site.Params.cover.suffix }}{{ site.Params.cover.suffix }}{{ end }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34;\u0026gt;{{ end -}} \u0026lt;img loading=\u0026#34;lazy\u0026#34; src=\u0026#34;{{ if site.Params.cover.prefix }}{{ site.Params.cover.prefix }}{{ end }}{{ .Params.cover.image }}{{ if site.Params.cover.suffix }}{{ site.Params.cover.suffix }}{{ end }}\u0026#34; alt=\u0026#34;{{ $alt }}\u0026#34;\u0026gt; 기타 설정 # 너비 설정 # 초기에 PaperMod 테마를 사용할 때 너비가 좁아 불편한 느낌이 있었습니다.\n해당 설정은 css 파일로 지정할 것이라 생각했고,\nassets/css/core/ 경로에 있는 theme-vars.css 파일을 발견해 다음과 같이 수정했습니다.\n기존 720px에서 900px로 늘어나 쾌적하게 블로그를 볼 수 있게 되었습니다.\nCopy css :root { --main-width: 900px; 새 탭에서 링크 열기 # 다음으로 관심을 가진 건 깃허브에서 매번 불편하게 생각했던 링크 오픈 방식인데,\n개인적으로는 현재 탭이 아닌 새 탭에서 열리는 방식을 선호하기 때문에 해당 부분의 수정이 필요했습니다.\n다행히 Hugo 이슈 내용 중 다음과 같은 답변을 참고해 파일을 추가했습니다.\n아래는 layouts/_default/_markup/ 경로에 추가한 render-link.html 파일입니다.\nCopy html \u0026lt;a href=\u0026#34;{{ .Destination | safeURL }}\u0026#34;{{ with .Title}} title=\u0026#34;{{ . }}\u0026#34;{{ end }}{{ if strings.HasPrefix .Destination \u0026#34;http\u0026#34; }} target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34;{{ end }}\u0026gt;{{ .Text | safeHTML }}\u0026lt;/a\u0026gt; 포스트 수정 # 마지막으로 포스트 수정 버튼에 문제를 인식했습니다.\n물론, 모든 포스트는 로컬에서 작성하고 수정하지만, 오류가 발생하는 버튼을 그냥 놔둘 수는 없습니다.\nGo에 대해 잘 알지 못해 최선의 기능이라고 생각하지는 않지만,\n검색을 통해 발견한 replace 함수를 사용해 기존 경로에서 오류를 일으키는 부분을 제거했습니다.\nCopy html {{- if or .Params.editPost.URL site.Params.editPost.URL -}} {{- $fileUrlPath := path.Join .File.Path }} {{- if or .Params.author site.Params.author (.Param \u0026#34;ShowReadingTime\u0026#34;) (not .Date.IsZero) .IsTranslated }}\u0026amp;nbsp;|\u0026amp;nbsp;{{- end -}} \u0026lt;a href=\u0026#39;{{ .Params.editPost.URL | default site.Params.editPost.URL }}{{ if .Params.editPost.appendFilePath | default ( site.Params.editPost.appendFilePath | default false ) }}/{{ replace $fileUrlPath site.Params.editPost.ignoreFilePath \u0026#34;\u0026#34; 1 }}{{ end }}\u0026#39; rel=\u0026#34;noopener noreferrer\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt; {{- .Params.editPost.Text | default (site.Params.editPost.Text | default (i18n \u0026#34;edit_post\u0026#34; | default \u0026#34;Edit\u0026#34;)) -}} \u0026lt;/a\u0026gt; {{- end }} 개선사항 # 현재 PaperMod 테마의 카테고리는 아래 그림처럼 태그와 동일한 리스트 템플릿을 사용하는데,\n개인적으로는 트리 형태의 계층식 카테고리를 선호합니다.\n언제나처럼 PaperMod 이슈를 탐색하던 중 해당 이슈를 발견했는데,\n아래 그림처럼 제가 머릿속에 그리던 방식을 그대로 표현하여 큰 관심을 가졌습니다.\n해당 기능을 구현한 분께 메일을 보내 참고 자료를 얻었지만,\n아직까진 시간적 여유가 부족해 해당 작업을 처리하지 못한 상태입니다.\n향후 개선되기를 희망하는 부분입니다.\n마치며 # Hugo 블로그 만들기 시리즈의 마지막으로 커스터마이징 과정을 소개했습니다.\n커스터마이징은 그때그때 필요하다고 생각하는 부분을 수정하는 것이기 때문에\n본인의 입맛에 맛는 블로그를 만들기 위해서는 테마의 구조를 이해해야 합니다.\n아직 Go에 대해서도 잘 몰라 검색을 통해 요령껏 찾아내는 수준이지만,\nGo에 익숙해지게 된다면 동적 TOC 등 기능의 개선을 기대해 볼 수 있을 것입니다.\n해당 게시글을 통해 Hugo 블로그 만들기에 도움이 되었으면 좋겠습니다.\n참고 자료 # EP09. 구글, 네이버 검색엔진 등록하기 KaTex Simple way to open in a new tab [Feature][Discussion] Tree-style category list page "},{"id":29,"href":"/blog/hugo-blog-old-2/","title":"Hugo 블로그 만들기 [2022년] (2) - Utterances 댓글 적용","section":"Posts","content":"Hugo 블로그는 기본적으로 댓글 기능을 제공하지는 않습니다.\n제가 사용하는 PaperMod 테마에서는 서드파티 서비스인 Disqus를 위한 레이아웃이 존재하지만,\n저는 기본적인 블로그 운영을 Github 플랫폼 내에서 구성하고 싶기 때문에 다른 기능을 사용해보려 합니다.\n이번 포스트에서는 Utterances 댓글 기능을 추가하는 방법을 안내해드리겠습니다.\nUtterances 설치하기 # Utterances는 Github issues 기반으로 댓글을 관리하는 기능입니다.\n무료 플랜에서 광고가 붙는 Disqus와 다르게 별도의 유료 플랜이 없어 간편하게 사용할 수 있습니다.\nUtterances 설치는 단순히 레이아웃 상에서 댓글이 위치할 곳에 자바스크립트 코드를 삽입하면 됩니다.\n하지만, 선행적으로 해당 링크를 통해 Utterances와 연동시킬 저장소를 등록해야 합니다.\n무료 플랜 선택 후 Utterances를 적용할 저장소를 선택하게 되는데\n모든 저장소를 지정해도 되지만, 저는 댓글을 관리할 저장소만 지정하겠습니다.\n간단하게 Utterances 적용이 완료되면 아래 공식 문서 페이지로 이동합니다.\nhttps://utteranc.es/ 공식 문서에서 저장소 이름, 이슈 맵핑 방식 등을 지정하면 해당하는 스크립트가 생성됩니다.\n저는 포스트 제목이 변경될 수 있기 때문에 pathname을 기준으로 이슈를 생성하고,\n사용자 시스템 설정에 호환되는 Preferred Color Scheme 테마를 사용합니다.\nCopy html \u0026lt;script src=\u0026#34;https://utteranc.es/client.js\u0026#34; repo=\u0026#34;[ENTER REPO HERE]\u0026#34; issue-term=\u0026#34;pathname\u0026#34; theme=\u0026#34;github-light\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; 스크립트 삽입하기 # PaperMod 테마에는 layouts/partials/ 위치에 comments.html이라는 레이아웃이 존재합니다.\n테마 별로 레이아웃이 다르기 때문에 다른 테마의 경우 이슈 등을 참고하여 구조를 파악할 필요가 있습니다.\nCopy html {{- /* Comments area start */ -}} {{- /* to add comments read =\u0026gt; https://gohugo.io/content-management/comments/ */ -}} {{- if $.Site.Params.utteranc.enable -}} \u0026lt;script src=\u0026#34;https://utteranc.es/client.js\u0026#34; repo=\u0026#34;{{ .Site.Params.utteranc.repo }}\u0026#34; issue-term=\u0026#34;{{ .Site.Params.utteranc.issueTerm }}\u0026#34; {{- if $.Site.Params.utteranc.label -}}label=\u0026#34;{{ .Site.Params.utteranc.label }}\u0026#34;{{- end }} theme=\u0026#34;{{ .Site.Params.utteranc.theme }}\u0026#34; crossorigin=\u0026#34;{{ .Site.Params.utteranc.crossorigin }}\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; {{- end }} {{- /* Comments area end */ -}} 단순하게 레이아웃에 스크립트를 붙여넣어도 되지만,\n향후 속성값을 변경하기 위해 불필요하게 테마를 수정하는 경우를 방지하기 위해\n설정 파일을 통해 동적으로 속성값을 집어넣도록 설정했습니다.\nHugo HTML 코드 내에 이중 중괄호({{ }})는 Go 템플릿을 코딩하는 부분으로,\n아래와 같은 설정 파일을 읽어서 각각의 키에 해당하는 값을 할당합니다.\n이에 대한 자세한 사용법은 Hugo 공식 문서를 참조할 수 있습니다.\nCopy yaml params: utteranc: enable: true repo: \u0026#34;minyeamer/minyeamer.github.io\u0026#34; issueTerm: \u0026#34;pathname\u0026#34; label: \u0026#34;comments\u0026#34; theme: \u0026#34;preferred-color-scheme\u0026#34; crossorigin: \u0026#34;anonymous\u0026#34; 정상적으로 스크립트가 삽입되었다면 아래와 같이 댓글을 입력하는 부분이 표시됩니다.\n댓글 기능이 정상적으로 적용되는지 확인하기 위해 실험적으로 댓글을 작성해봅니다.\n저도 과거 게시글에 댓글을 작성하여 아래와 같이 올라온 이슈를 확인했습니다.\n마치며 # Hugo 블로그를 통한 소통을 기대하여 댓글 기능을 추가해보았습니다.\n생각보다 간단하기 때문에 깃허브 블로그를 꾸미면서 댓글 기능을 희망하시는 분들이라면\nUtterances를 적극 활용해보시기를 추천드립니다.\n마지막 포스트로는 PaperMod 테마를 수정한 과정을 안내해드리겠습니다.\nHugo 테마끼리 공통적인 부분이 있기 때문에 다른 테마를 사용하시더라도 도움이 될 것입니다.\n참고 자료 # Utterances Documents Introduction to Hugo Templating "},{"id":30,"href":"/blog/hugo-blog-old-1/","title":"Hugo 블로그 만들기 [2022년] (1) - Hugo 기본 구성","section":"Posts","content":"얼마 전, 티스토리 블로그에서 Jekyll 블로그로 이동했는데,\n처음 기대했던 submodule을 활용한 효율적인 저장소 연동에서 어려움을 겪고 다른 대안을 탐색하게 되었습니다.\nJekyll 블로그를 사용함에 있어서, Ruby 언어로 구성된 블로그 구조에 대해 이해하기 어려운데다가\n로컬 환경에서 Jekyll 블로그를 실행하면서 발생하는 에러를 처리하는데도 난항을 겪었는데,\n웹상에서 자동 배포가 이루어지는 과정에서 submodule인 TIL 저장소를 포스트로 인식하지 못하는 문제가 있었습니다.\nJekyll 블로그의 대안으로 Hexo 및 Hugo 프레임워크에 주목했고,\n두 제품의 장단점을 비교하여 상대적으로 배포가 빠르고 현재까지도 업데이트가 이루어지는 Hugo를 선택했습니다.\n이번 포스트에서는 제가 Hugo 블로그를 구성한 과정을 간략한 설명과 함께 안내해드리겠습니다.\n테마 선택하기 # 블로그의 모든 페이지 레이아웃을 만들 계획이 아니라면 블로그 선택에 있어 테마 선정이 필요합니다.\nHugo는 아래 페이지에서 다양한 테마를 제공하며, 태그를 통해 블로그 외에도 목적에 맞는 테마를 찾아볼 수 있습니다.\n미리보기만으로 알기 어렵다면 제작자가 제공하는 데모 사이트를 방문해볼 수 있고,\n아래 안내드릴 Hugo 설치를 통해 로컬에서 exampleSite를 확인해 볼 수도 있습니다.\nhttps://themes.gohugo.io/ Jekyll 블로그를 사용했을 당시 적용했던 Chirpy 테마는 사이드 메뉴, 계층식 카테고리, 동적 TOC 등\n제가 추구하는 모든 기능을 가지고 있었는데, Hugo에는 저의 취향을 완벽히 만족시키는 테마가 없었습니다.\n그나마 괜찮았던 LoveIt 테마의 경우 설정 곳곳에 중국어가 포함되어 있어 이해하기 어렵겠다는 생각이 들었습니다.\n결국, 저는 모든 테마를 둘러본 후 다루기 쉬워보이면서 외적으로도 괜찮았던 PaperMod 테마를 선택했습니다.\nHugo 블로그 구성하기 # 이번 Hugo 블로그 구성은 Mac 환경에서 진행되었으며, 다른 환경의 구성 방식은 제공되지 않습니다.\nHugo 설치 # Mac 사용자라면 Homebrew를 통해 쉽게 Hugo를 설치하여 사용할 수 있습니다.\n터미널에 아래 명령어를 입력해 설치가 가능합니다.\nCopy bash brew install hugo 설치가 완료되면, 버전 정보를 출력해서 정상 설치 여부를 확인합니다.\nCopy bash % hugo version hugo v0.102.2+extended darwin/arm64 BuildDate=unknown Github 저장소 생성 # Hugo는 원본 데이터 및 설정 파일이 포함될 공간과, 렌더링된 페이지가 저장될 공간이 필요합니다.\n일반적으로는 분리된 저장소를 통해 구현하지만, 앞서 Jekyll 블로그를 구성해보면서\n브랜치를 통해 하나의 저장소에서 두 개의 공간을 관리할 수 있을 것이라 판단했습니다.\n하나의 저장소를 main과 gh-pages, 두 개의 브랜치로 나누어 구성할 계획이며,\n우선적으로 \u0026lt;USERNAME\u0026gt;.github.io 명칭의 저장소를 생성합니다.\nHugo 프로젝트 생성 # 일반적인 웹 프레임워크에서 프로젝트를 시작하는 것처럼, Hugo에서도 기본 템플릿을 제공합니다. 아래 명령어를 통해 프로젝트를 생성할 수 있고, 이름은 자유롭게 지정해도 됩니다.\nCopy bash % hugo new site \u0026lt;NAME\u0026gt; 만들어진 프로젝트 구조는 다음과 같습니다.\n만들어진 테마를 사용한다면 대부분의 구성요소들이 themes/ 디렉토리 내에 위치하게 되며,\n포스트를 위한 content/, 이미지 등을 위한 static/ 디렉토리 외엔 거의 사용하지 않습니다.\nCopy bash . ├── archetypes │ └── default.md ├── config.toml ├── content ├── data ├── layouts ├── public ├── static └── themes 저장소 연동 # 테마를 불러오기에 앞서 git 설정이 필요합니다.\n프로젝트 디렉토리로 이동한 후, 아래 명령어를 통해 원격 저장소와 연동합니다.\nCopy bash % git init % git add . % git commit -m \u0026#34;feat: new site\u0026#34; % git branch -M main % git remote add origin https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git % git push -u origin main 추가적으로, 렌더링된 페이지가 저장되고 실질적인 배포가 이루어지는 브랜치를 생성합니다.\nCopy bash % git branch gh-pages main % git checkout gh-pages % git push origin gh-pages % git checkout main Hugo에서 페이지를 렌더링한 결과는 public/ 디렉토리에 저장되며, 이를 gh-pages 브랜치와 연결해야 합니다.\n기존에 존재하는 빈 디렉토리를 제거하고 gh-pages 브랜치를 main 브랜치의 submodule로 연결합니다.\nsubmodule에 대한 개념은 해당 영상을 참고해주시기 바라며, 단순하게 설명하자면 동기화 기능입니다.\nCopy bash % rm -rf public % git submodule add -b gh-pages https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git public % git add public % git add .gitmodules % git commit -m \u0026#34;feat: add submodule for github pages\u0026#34; % git push 테마 불러오기 # git 설정을 완료한 후, 미리 정해두었던 테마를 themes/ 디렉토리 내에 위치시킵니다.\n마찬가지로 submodule을 활용하며, 테마의 디렉토리명은 반드시 테마 설정에 명시된 것과 동일한 이름이어야 합니다.\n커스터마이징을 고려하면 원본 저장소가 아닌 별도로 fork한 저장소를 연결시키는게 좋습니다.\nCopy bash % git submodule add https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod % git add themes/PaperMod % git add .gitmodules % git commit -m \u0026#34;feat: import hugo theme\u0026#34; 만약 fork 저장소를 사용하면서 원본 저장소의 변경사항을 업데이트하고 싶다면,\n원본 저장소를 새로운 원격 저장소로 등록해 pull 작업을 수행합니다.\nCopy python git remote add upstream https://github.com/adityatelange/hugo-PaperMod git fetch upstream git merge upstream/master git commit -m \u0026#34;update: pull upstream\u0026#34; 아래는 PaperMod 테마의 디렉토리 구조입니다.\n테마를 수정할 일이 있다면 아래 구조를 참고해 필요한 파일에 접근해 볼 수 있습니다.\nCopy bash themes/PaperMod ├── LICENSE ├── README.md ├── assets │ ├── css │ │ ├── common │ │ ├── core │ │ ├── extended │ │ ├── hljs │ │ └── includes │ └── js ├── go.mod ├── i18n ├── images ├── layouts │ ├── 404.html │ ├── _default │ │ └── _markup │ ├── partials │ ├── robots.txt │ └── shortcodes └── theme.toml Hugo 설정 # Hugo 블로그 설정은 config 파일에서 지정할 수 있고, toml, yaml, json 형식을 지원합니다.\n테마를 사용할 경우 커스텀 키가 존재할 수 있어 별도의 문서를 참조하는게 좋습니다.\nHugo 공식 설정에 관한 문서와 PaperMod 설정에 관한 문서는 아래를 참고해주시기 바랍니다.\nhttps://gohugo.io/getting-started/configuration/ https://github.com/adityatelange/hugo-PaperMod/wiki/Installation 제 설정 파일의 경우 커스터마이징을 통해 호환되지 않는 키가 존재할 수 있지만,\n동일한 테마를 사용한다면 일부분을 참고해 도움을 받을 수 있을거라 기대합니다.\nHugo 배포 # Hugo는 hugo -t \u0026lt;THEMES\u0026gt; 명령어를 통해 로컬에서 페이지 렌더링을 진행할 수 있고,\n그 결과인 public/ 디렉토리 내 내용을 gh-pages에 push하여 배포를 수행합니다.\n배포에 앞서, 깃허브에서 제공하는 Github Pages가 gh-pages 브랜치를 참고하도록\n아래 그림과 같이 저장소 설정에서 빌드 및 배포 대상 브랜치를 지정해주어야 합니다.\n위와 같이 수동으로 배포할 경우 두 번의 push 과정을 거쳐야 합니다.\n매번 이 과정을 수행하는 것은 불편하기 때문에 쉘 스크립트를 작성하여 작업을 단순화합니다.\n해당 스크립트는 다른 포스트를 참고해 작성했습니다.\nCopy bash #!/bin/bash echo -e \u0026#34;\\033[0;32mDeploying updates to GitHub...\\033[0m\u0026#34; # Build the project. # hugo -t \u0026lt;your theme\u0026gt; hugo -t PaperMod # Go to public folder, submodule commit cd public # Add changes to git. git add . # Commit changes. msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; # Push source and build repos. git push origin gh-pages # Come back up to the project root cd .. # Commit and push to main branch git add . msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; git push origin main 스크립트 파일에 실행 권한을 부여하고 실행해 볼 수 있습니다.\nCopy bash % chmod 777 deploy.sh % ./deploy.sh 배포가 완료되면, https://.github.io 주소로 접속해 블로그를 확인할 수 있습니다.\n포스트 작성하기 # Hugo 포스트는 아래 명령어를 통해 생성할 수 있고,\n별도의 markdown 파일을 content/post/ 경로 내에 추가할 수도 있습니다.\nCopy bash % hugo new post/\u0026lt;FILENAME\u0026gt;.md Front Matter # 제목, 작성일자 등을 지정하기 위해 포스트 상단에 Front Matter라고 하는 토큰을 작성해야 합니다. Front Matter는 설정 파일과 동일하게 toml, yaml, json 형식을 지원하며,\nHugo 공식 문서 또는 PaperMod에서 안내하는 아래형식을 참고할 수 있습니다.\nCopy yaml --- title: \u0026#34;My 1st post\u0026#34; date: 2020-09-15T11:30:03+00:00 # weight: 1 # aliases: [\u0026#34;/first\u0026#34;] tags: [\u0026#34;first\u0026#34;] author: \u0026#34;Me\u0026#34; # author: [\u0026#34;Me\u0026#34;, \u0026#34;You\u0026#34;] # multiple authors showToc: true TocOpen: false draft: false hidemeta: false comments: false description: \u0026#34;Desc Text.\u0026#34; canonicalURL: \u0026#34;https://canonical.url/to/page\u0026#34; disableHLJS: true # to disable highlightjs disableShare: false disableHLJS: false hideSummary: false searchHidden: true ShowReadingTime: true ShowBreadCrumbs: true ShowPostNavLinks: true ShowWordCount: true ShowRssButtonInSectionTermList: true UseHugoToc: true cover: image: \u0026#34;\u0026lt;image path/url\u0026gt;\u0026#34; # image path/url alt: \u0026#34;\u0026lt;alt text\u0026gt;\u0026#34; # alt text caption: \u0026#34;\u0026lt;text\u0026gt;\u0026#34; # display caption under cover relative: false # when using page bundles set this to true hidden: true # only hide on current single page editPost: URL: \u0026#34;https://github.com/\u0026lt;path_to_repo\u0026gt;/content\u0026#34; Text: \u0026#34;Suggest Changes\u0026#34; # edit text appendFilePath: true # to append file path to Edit link --- 게시글 저장소 연동 # 저는 기존 TIL 저장소를 게시글로 활용할 예정이었기에,\ncontent/post/ 디렉토리를 TIL 저장소의 submodule로 대체했습니다.\nCopy bash % git submodule add https://github.com/minyeamer/til.git content/post/ % git add content/post/ % git add .gitmodules % git commit -m \u0026#34;feat: add til repository as post\u0026#34; 이렇게 설정했을 때 장점은 TIL 저장소에 변경사항이 발생했을 경우,\n아래와 같은 단 한 줄의 명령어로 블로그 저장소에서 업데이트할 수 있습니다.\n해당 명령어는 물론, 테마와 같은 다른 submodule에도 적용할 수 있습니다.\nCopy bash % git submodule update --remote 마치며 # Jekyll 블로그와 며칠간 씨름하다 Hugo로 이동해 기존의 목표를 달성할 수 있었습니다.\nChirpy 테마를 활용하지 못하는 것이 아쉽지만, PaperMod의 코드는 알기 쉽게 작성되어 있어\n시간적 여유만 있다면 커스터마이징에서 어려움이 없을 것이라 판단합니다.\n이번 포스트에서는 Hugo 블로그를 구성하고 포스트를 작성하는 과정을 전달했습니다.\n다음엔 Utterances 위젯을 활용해 댓글 기능을 추가하는 방법을 안내해드리겠습니다.\n참고 자료 # Hugo Documents PaperMod Documents 블로그 구축기 (1) Hugo + Github으로 개인 블로그 만들기 저장소 안에 저장소 - git submodule "},{"id":31,"href":"/blog/jekyll-blog/","title":"깃허브 블로그 시작하기","section":"Posts","content":"블로그를 처음 시작함에 있어서 모든 것이 준비된 호스팅 서비스의 편의성은 무시할 수 없습니다.\n저도 처음엔 코드를 직접 건드리는 자유도 높은 방식의 블로그에 진입 장벽을 느끼고\n가볍게 시작할 수 있는 티스토리를 통해 블로그에 입문했습니다.\n하지만, 개발적 지식을 학습하면서 깃허브에 마크다운 문서를 올리는 빈도가 늘어났고,\n깃허브에 올린 문서를 굳이 티스토리로 다시 옮겨 담는 것에 불편함을 느끼게 되었습니다.\n마크다운 문서를 자주 작성하고 깃허브 저장소를 학습 노트로 활용한다면,\n깃허브 블로그를 구성해보는 것이 문서를 통합적으로 관리할 수 있다는 점에서 매력적이라 생각합니다.\n현재는 막 깃허브 블로그를 꾸려서 적응해가는 단계에 불과하지만,\n웹에서 정적 파일을 수집하는 기술을 적용할 수 있다면 중복된 자료를 생성할 필요 없이\nTIL 저장소 자체를 블로그 포스트 저장소로도 활용할 수 있을 것이라 기대합니다.\n블로그를 개설하고 처음 작성하는 이번 포스트에서는 깃허브 블로그를 만든 과정을 소개해드리겠습니다.\n테마 선택 및 가져오기 # 깃허브 블로그를 생성하는데 있어 주로 사용되는 기술이 Jekyll이라는 사이트 생성 엔진 입니다.\nJekyll을 구성하는 Ruby와 쉘 스크립트 작성에 대한 이해가 있다면 더욱 자유도 높은 작업을 할 수 있지만,\n다행히 이를 모를지라도 다른 사용자들이 만든 테마를 가져와 블로그를 구성해 볼 수 있습니다. Jekyll 테마는 아래와 같은 사이트를 참조하여 마음에 드는 UI를 확인할 수 있습니다.\nhttps://jekyllthemes.io http://jekyllthemes.org 무료로 가져다 사용할 수 있는 여러 테마 중 개인적으로 마음에 드는 Chirpy 테마를 활용해 보겠습니다.\n테마 별로 적용 및 활용하는 방식에 다소 차이가 있지만,\nChirpy 같은 경우 아래 튜토리얼 사이트가 만들어져 있어 비교적 쉽게 블로그를 구성할 수 있습니다.\nhttps://chirpy.cotes.page 블로그 배포하기 # Chirpy 테마를 설치하고 배포하는 방법엔 두 가지 방식이 있습니다.\nChirpy Starter를 통해 간단하게 설치하기\n튜토리얼에서는 Jekyll을 전혀 모르는 사용자도 쉽게 테마를 활용할 수 있는 프로젝트 파일이 마련되어 있습니다.\n깃허브 저장소를 생성하는 것과 같은 단순한 버튼 클릭만으로 완성된 사이트를 배포할 수 있습니다.\nGithub에서 소스코드를 fork 받아 직접 설치하기\n스크립트를 실행하는 등 다소의 작업이 추가되지만, 블로그 커스터마이징에 유리한 방식입니다.\nJekyll을 다뤄볼 줄 안다면 직접 설치를 진행하는 것이 취향에 맞는 방식일 수 있습니다.\n저 같은 경우 Jekyll에 친숙한 편이 아니기 때문에 1번째 방법을 통해 설치를 진행했습니다.\n이때, 저장소 이름은 \u0026lt;GH_USERNAME\u0026gt;.github.io 형식으로 지정해야 하며,\n\u0026lt;GH_USERNAME\u0026gt;에는 깃허브 아이디를 입력하면 됩니다.\n위 방식으로 저장소를 생성하면 자동으로 배포가 수행되는데, Actions 탭을 통해 아래처럼 진행사항을 확인할 수 있습니다.\n빌드 및 배포가 완료되면 https://\u0026lt;저장소 이름\u0026gt; 주소를 통해 블로그 페이지에 접근할 수 있는데,\n2022년 8월 기준에서 해당 테마를 가져온 직후엔\n--- layout: home # Index page --- 텍스트만 존재하는 화면을 마주하게 됩니다.\n이것은 현재 Github Pages가 스타일이 적용되지 않는 main 브랜치를 대상으로 하고 있는 것이 원인으로,\nSettings 탭 아래 Pages 메뉴를 클릭했을 때 보이는 Branch 부분을 gh-pages로 수정하면 됩니다.\n블로그 설정하기 # 향후 블로그 호스팅 및 사이트 제목을 수정하는 등의 설정을 위해 _config.yml 파일을 수정할 필요가 있습니다.\n제가 블로그 세팅에 도움을 받은 게시글로부터 일부 항목에 대한 설명을 가져왔습니다.\n항목 값 설명 timezone Asia/Seoul 시간대를 설정하는 부분으로 서울 표준시로 설정합니다. title 블로그 제목 프로필 사진 아래 큰 글씨로 제목이 표시됩니다. tagline 프로필 설명 블로그 제목 아래에 작은 글씨로 부연설명을 넣을 수 있습니다. description SEO 구글 검색에 어떤 키워드로 내 블로그를 검색하게 할 것인가를 정의하는 부분입니다. url https://*.github.io 블로그와 연결된 url을 입력합니다. github Github ID 본인의 github 아이디를 입력합니다. twitter.username Twitter ID 트위터를 사용한다면 아이디를 입력합니다. social.name 이름 포스트 등에 작성자로 표시할 나의 이름을 입력합니다. social.email 이메일 나의 이메일 계정을 입력합니다. social.links 소셜 링크들 트위터, 페이스북 등 내가 사용하고 있는 소셜 서비스의 나의 홈 url을 입력합니다. avatar 프로필 사진 블로그 왼쪽 상단에 표시될 프로필 사진의 경로를 설정합니다. toc true 포스트 오른쪽에 목차를 표시합니다. paginate 10 한 목록에 몇 개의 글을 표시할 것인지 지정합니다. 이 부분은 저의 설정 파일 _config.yml 또는 github 내 검색을 통해 접근할 수 있는\n다른 사용자 분들의 설정 파일을 참고하면 원하는 부분을 수정하는데 도움이 될 것입니다.\n_config.yml 파일이 수정 등 저장소에 변화가 발생하면 자동으로 빌드 및 배포 과정이 수행되며,\n변경사항이 적용되는데 약간 시간이 걸릴 수 있습니다.\n포스트 작성하기 # Jekyll은 마크다운 문법으로 글을 작성할 수 있습니다.\n마크다운 문법에 익숙하지 않다면 해당 게시글을 참고해 주시기 바랍니다.\nVS Code 또는 기타 웹 편집기를 활용하면 마크다운 작성 내용을 실시간으로 렌더링해 확인할 수 있습니다.\n게시글에 대한 마크다운 파일은 _posts 디렉토리 내에 위치시키고,\nyyyy-mm-dd-제목.md의 형식으로 파일 이름을 지정해야 합니다.\n제목에 해당하는 부분은 실제 포스트 제목이 아닌, url로 활용되는 부분이기 때문에\n게시글의 내용을 짐작하게 하는 간단한 단어나 문장을 활용하는게 좋습니다.\n마크다운 파일의 상단엔 Front Matter라고 하는 Jekyll 게시글에서 허용하는 규칙을 통해\n게시글 제목, 작성일자, 카테고리, 태그 등을 지정할 수 있습니다.\n자세한 내용은 튜토리얼을 참조할 수도 있고,\n해당 게시글에 대한 raw 파일을 확인해보셔도 좋습니다.\n마치며 # 과거 깃허브 블로그를 만들려고 했을 때는 Jekyll을 직접 다뤄야 해서 쉽게 접근하지 못했는데,\n이제는 그럴 필요 없이 완성된 패키지를 가져다 쓸 수 있게 되어서 많이 편해졌다고 생각합니다.\n참고 자료 # Chirpy Documents 깃헙(GitHub) 블로그 10분안에 완성하기 Jekyll Chirpy 테마 사용하여 블로그 만들기 Github 블로그 테마적용하기(Chirpy) "},{"id":32,"href":"/blog/dacon-shop-review/","title":"DACON 쇼핑몰 리뷰 평점 분류 - KoELECTRA와 RoBERTa 앙상블로 2위 달성","section":"Posts","content":""},{"id":33,"href":"/blog/dacon-audio-mnist/","title":"DACON 음성 분류 경진대회 - Mel Spectrogram과 MFCC 앙상블로 97% 정확도 달성","section":"Posts","content":""},{"id":34,"href":"/blog/baekjoon-1197-mst/","title":"[Python] 백준 1197 - 최소 스패닝 트리 (Gold 4)","section":"Posts","content":" 1197번: 최소 스패닝 트리 그래프가 주어졌을 때, 그 그래프의 최소 스패닝 트리를 구하는 프로그램을 작성하시오. 최소 스패닝 트리는, 주어진 그래프의 모든 정점들을 연결하는 부분 그래프 중에서 그 가중치의 합이 최소인 트리를 말한다. www.acmicpc.net 해당 문제는 n개의 정점들에 대한 간선들 중에서 가장 가중치가 작은 경로의 가중치를 찾는 것이다. 정답 풀이보다는 여러가지 방식으로 시도하면서 알고리즘을 발전시키는 과정을 서술한다.\nDFS # 처음엔 노드하면 DFS와 BFS 밖에 몰랐기 때문에 당연하게 DFS로 접근했다.\n먼저 부모, 자식, 가중치, 인덱스를 변수로 가지는 Node 클래스를 선언하여 간선의 정보를 노드 내 인스턴스 변수에 저장하게 한다.\n전체 노드 중 자식 노드를 가진 노드에 한해 가중치 최솟값을 구하는 함수를 실행한다.\n해당 함수는 root에서부터 end-point까지 순회하면서 가중치 합의 최솟값을 구하는 동작을 수행한다. 함수의 결과는 따로 반환되지 않고 root 노드의 인스턴스 변수에 저장된다. 이러한 논리를 가지고 작성한 알고리즘이 글 밑에 있는 첫 번째 코드이다.\n하지만 해당 코드는 1초의 시간 제한 안에 돌아가기엔 무리가 있었다.\n첫 번째 알고리즘 # Copy python class Node: def __init__(self, index): self.index = index self.data = 2147483647 self.parent = [] self.child = [] def print_node(self): print(self.index, self.data, self.parent, self.child) def spanning_tree(nodes, check, root, parent, data): for child in parent.child: weight = data + child[1] child = nodes[child[0]] if child.child: if not check[child.index]: spanning_tree(nodes, check, root, child, weight) else: check[parent.index] = True if weight \u0026lt; root.data: root.data = weight V, E = map(int, input().split()) graph = [Node(i) for i in range(V+1)] visited = [False for _ in range(V+1)] for _ in range(E): A, B, C = map(int, input().split()) graph[A].child.append((B,C)) graph[B].parent.append((A,C)) min_weight = 2147483647 for node in graph: if node.child and not node.parent: spanning_tree(graph, visited, node, node, 0) if node.data \u0026lt; min_weight: min_weight = node.data print(min_weight) 크루스칼 알고리즘 # DFS로 안된다는 것을 깨닫고 질문글을 훑어본 후 크루스칼 알고리즘을 선택하기로 했다.\n우선 고려해야할 것은 크루스칼 알고리즘이 모든 노드를 연결시키기 위한 알고리즘이라는 것이다.\n해당 문제는 root 노드에서부터 시작하는 모든 경로를 고려해야 하는데 크루스칼 알고리즘을 사용할 경우 가장 작은 가중치로 시작하는 경로만을 선택하고 나머지를 무시하게 된다.\n이 경우 발생하는 반례가 다음과 같다. Copy text 3 3 1 2 2 1 3 3 2 3 9999 output: 10001 answer: 3 크루스칼 알고리즘에 의해 1 -\u0026gt; 2의 간선을 선택하고 1 -\u0026gt; 3의 간선을 무시할 경우 최종적으로는 1 -\u0026gt; 2 -\u0026gt; 3의 경로에 대한 가중치 10001을 결과로 얻게 된다. 이에 대한 해결책으로 생각한 것이 EtherChannel의 Active/Passive 개념이다. 앞서 시도한 DFS 기반 알고리즘에 크루스칼 알고리즘을 조합해서 모든 경로를 탐색하는데 가중치가 가장 작은 경로로 이어지는 자식 노드를 Active로, 나머지를 Passive로 분류한다. 만약 한 노드에 새로운 자식 노드가 추가되면 자식 노드들의 가중치를 비교해서 Active를 갱신하고 해당 노드의 부모 노드를 타고 올라가며 동일한 작업을 반복한다. 해당 알고리즘은 root 노드에서부터 모든 자식 노드를 탐색해야 했던 DFS 기반 알고리즘과는 반대로 자식 노드에서부터 root 노드까지의 경로만을 탐색하기 때문에 시간 초과를 피할 수 있었다.\n하지만 여러 조건들을 고려하다보니 작성자인 나조차도 알아보기 힘들정도로 코드가 많이 복잡해졌고 root 노드가 기준인데 굳이 아래서부터 위를 탐색하는 방식이 마음에 들지 않았다. 그리고 가장 큰 문제는 해당 알고리즘에도 반례가 있어서 정답이 될 수 없었다는 것이다.\n두 번째 알고리즘 # Copy python class Node: def __init__(self, index): self.index = index self.data = 0 self.root = self self.parent = self self.active = None self.passive = [] def get_branch(self): if self.active: return self.passive + [self.active] else: return [] def set_branch(self, node, data): if self.root == node.root: if data \u0026lt; node.data: node.parent = self node.data = data else: node.root = self.root node.parent = self node.data += data if not self.active: self.active = node self.data += node.data node.data = self.data else: self.passive.append(node) self.update_data() def update_data(self): branch = self.get_branch() branch.sort(key=lambda n: n.data, reverse=True) active = branch.pop() if active != self.active: self.active = active self.passive = branch self.data = self.active.data def union_root(source: Node, target: Node, data: int) -\u0026gt; None: root = source.root if target.root in [source, source.root, target]: source.set_branch(target, data) while source != root: source = source.parent source.update_data() V, E = map(int, input().split()) graph = [Node(i) for i in range(V + 1)] edge_dict = {} for _ in range(E): A, B, C = map(int, input().split()) edge_dict[(A, B)] = C edge_list = sorted(edge_dict.items(), key=lambda x: [x[1], x[0]]) for (a, b), c in edge_list: node_a, node_b = graph[a], graph[b] if node_a.parent != node_b.parent: union_root(node_a, node_b, c) weight = 2147483647 for edge_node in graph: if (edge_node.root == edge_node) and edge_node.get_branch(): if edge_node.data \u0026lt; weight: weight = edge_node.data print(weight) 프림 알고리즘 # 하루동안 고민한 끝에 크루스칼 알고리즘을 포기하고 이와 비슷하다는 프림 알고리즘을 선택하게 되었다.\n이제까지 사용했던 Node 인스턴스 내에 모든 정보를 저장하는 접근방식을 버리고 프림 알고리즘의 기본에 집중했다.\n부모 노드의 값을 자식 노드의 배열 값에 저장하는 Union-Find 알고리즘을 기반으로 그래프를 그리고 모든 노드에 대해 프림 알고리즘을 수행하여 최소 가중치를 구하는 방식을 구상했다.\n하지만 이 경우에 두 가지 문제점이 있었다.\n프림 알고리즘도 결국 모든 노드를 연결하기 위한 알고리즘이기 때문에 root에서 end-point까지 갔다 하더라도 거기서 멈추지 않고 다른 경로를 탐색하는 문제가 생긴다.\n해당 문제에 대한 해결책으로 Find 연산을 응용한 깊이 탐색 과정을 추가했다. 매 반복마다 현재 노드에 대해 Find 연산을 수행하고 재귀한 횟수 반환하여 깊이로 지정한다. 깊이가 지속적으로 증가하지 않을 경우 end-point까지 도달했다 판단하여 반복을 멈춘다. 모든 경로의 깊이가 1일 경우 1번 조건을 무시하고 다른 경로를 탐색하는 문제가 있다.\nroot 노드에서 시작했는데 다시 root 노드로 돌아올 경우 해당 노드 자체를 무시한다. 위 조건에 걸릴 경우 양의 무한대 값을 반환하여 가중치 판단 과정에서 제외시킬 수 있었다. 이렇게 많은 시행착오를 거쳤지만 하나를 해결하면 다른 빈틈이 생겨버려 포기할 수밖에 없었다. 심지어 백준에서는 heapdict 모듈을 지원하지 않아 해당 알고리즘을 활용할 수도 없었다. 언젠가 이 문제를 완벽하게 해결하기 위해 디버그 값을 남긴다. 세 번째 알고리즘 # Copy python def prim(nodes: dict, start: int) -\u0026gt; int or float: mst, keys, pi = [], heapdict(), dict() depth, total_weight = -1, 0 for n in nodes.keys(): keys[n] = float(\u0026#39;inf\u0026#39;) pi[n] = None keys[start], pi[start] = 0, start while keys: current_node, current_key = keys.popitem() current_depth = get_depth(pi, start, current_node, 0) if current_depth \u0026lt;= depth: if pi[current_node] == start: return float(\u0026#39;inf\u0026#39;) break depth = current_depth mst.append([pi[current_node], current_node, current_key]) total_weight += current_key for adjacent, weight in nodes[current_node].items(): if adjacent in keys and weight \u0026lt; keys[adjacent]: keys[adjacent] = weight pi[adjacent] = current_node return total_weight def get_depth(nodes: dict, root: int, start: int, data: int) -\u0026gt; int: if start == root: return data if nodes[start] == root: return data+1 return get_depth(nodes, root, nodes[start], data+1) V, E = map(int, input().split()) graph = defaultdict(dict) for _ in range(E): A, B, C = map(int, input().split()) graph[A][B] = C graph[B][A] = C weight_list = [] for node in graph.keys(): heapq.heappush(weight_list, prim(graph, node)) print(heapq.heappop(weight_list)) Copy text 3 3 1 2 2 1 3 3 2 3 9999 graph = {1: {2: 1, 3: 3}, 2: {1: 1, 3: 2}, 3: {2: 2, 1: 3}} mst1 = [[1, 1, 0], [1, 2, 1], [2, 3, 2]], weight: 3 mst2 = [[2, 2, 0], [2, 1, 1], [2, 3, 2]], weight: 3 mst3 = [[3, 3, 0], [3, 2, 2], [2, 1, 1]], weight: 3 output: 3 answer: 3 6 8 1 3 -1 1 5 3 1 6 2 2 5 5 2 6 6 3 4 9 3 5 -1 5 6 -1 graph = {1: {3: -1, 5: 3, 6: 2}, 3: {1: -1, 4: 9, 5: -1}, 5: {1: 3, 2: 5, 3: -1, 6: -1}, 6: {1: 2, 2: 6, 5: -1}, 2: {5: 5, 6: 6}, 4: {3: 9}} mst1 = [[1, 1, 0], [1, 3, -1], [3, 5, -1], [5, 6, -1], [5, 2, 5], [3, 4, 9]], w = 11 mst2 = [[3, 3, 0], [3, 5, -1], [5, 6, -1], [3, 1, -1], [5, 2, 5], [3, 4, 9]], w = 11 mst3 = [[5, 5, 0], [5, 6, -1], [5, 3, -1], [3, 1, -1], [5, 2, 5], [3, 4, 9]] 11 mst4 = [[6, 6, 0], [6, 5, -1], [5, 3, -1], [3, 1, -1], [5, 2, 5], [3, 4, 9]] 11 mst5 = [[2, 2, 0], [2, 5, 5], [5, 6, -1], [5, 3, -1], [3, 1, -1], [3, 4, 9]] 11 mst6 = [[4, 4, 0], [4, 3, 9], [3, 5, -1], [5, 6, -1], [3, 1, -1], [5, 2, 5]] 11 output: 11 answer: -3 3 3 1 2 2 1 3 3 2 3 9999 graph = {1: {2: 2, 3: 3}, 2: {1: 2, 3: 9999}, 3: {1: 3, 2: 9999}} mst1 = [[1, 1, 0], [1, 2, 2], [1, 3, 3]], weight = 5 mst2 = [[2, 2, 0], [2, 1, 2], [1, 3, 3]], weight = 5 mst3 = [[3, 3, 0], [3, 1, 3], [1, 2, 2]], weight = 5 output: 5 answer: 3 결론 # 해당 문제에 대한 정답을 찾아본 결과 프림 알고리즘을 heapdict 없이 구현한 알고리즘을 보았는데 노드에 대한 방문 여부를 판단하여 경로를 구하는 방식이었다.\n백준에서는 해당 문제가 통과되었지만 위 세 개의 데이터를 넣었을 때 예상과 다른 값이 나왔다. 아마 내가 문제를 제대로 이해하지 못했거나 채점 데이터 자체가 적어서 그랬을 것이다. 결과적으로 다른 사람이 작성한 정답을 보게 됐지만 완전히 납득하지는 못했다.\n정답 알고리즘 # Copy python V, E = map(int, input().split()) graph = [[] for _ in range(V+1)] visited = [False for _ in range(V+1)] heap = [[0, 1]] for _ in range(E): A, B, C = map(int, input().split()) graph[A].append([C, B]) graph[B].append([C, A]) total_weight = 0 node_cnt = 0 while heap: if node_cnt == V: break weight, node = heapq.heappop(heap) if not visited[node]: visited[node] = True total_weight += weight node_cnt += 1 for i in graph[node]: heapq.heappush(heap, i) print(total_weight) "},{"id":35,"href":"/blog/algorithm-basics/","title":"파이썬 알고리즘 스터디 노트 - Set, Heap, DFS, BFS 완벽 정리","section":"Posts","content":"자료구조 # Set # 백준 1107번(리모컨) 문제를 풀 때 사용 해당 문제는 특정 길이의 문자열에 대해 가능한 모든 조합을 탐색해야 하는데 시간복잡도를 줄이기 위해 중복이 없는 집합을 사용 빈집합은 set() 명령어로 간단하게 정의 Set은 Dictionary와 동일한 Hash Table 기반이기 때문에 x in s 연산의 시간복잡도가 O(1) 리스트의 x in s 연산 시간복잡도가 O(n) 인 것과는 큰 차이 Set을 활용한 코드 일부 # Copy python buttons = set([str(i) for i in range(10)]) channels = {N,} diff = {abs(int_N-100)} if M \u0026gt; 0: buttons -= set(list(input().split())) channels = set() for i in range(1, count+1): product = itertools.product(buttons, repeat=i) channels|= set(map(\u0026#39;\u0026#39;.join, product)) min_chan, max_chan = \u0026#39;0\u0026#39;, \u0026#39;1\u0026#39; for _ in range(count-1): min_chan += max(buttons) for _ in range(count): max_chan += min(buttons) if set(max_chan) \u0026amp; buttons == set(max_chan): channels.add(max_chan) if set(min_chan) \u0026amp; buttons == set(min_chan): channels.add(min_chan) Dictionary # 백준 1620번(나는야 포켓몬 마스터 이다솜) 문제를 풀 때 사용 해당 문제는 문자열 또는 인덱스를 입력했을 때 대칭되는 값을 출력해야 하는데 처음엔 시간복잡도가 O(n) 인 List 의 index(x) 를 사용하여 시간초과가 발생 문자열과 인덱스의 관계를 Dictionary로 구현해 탐색 시간복잡도를 O(1) 로 개선 Coutner # 백준 10816번(숫자카드 2) 문제를 풀 때 사용 해당 문제는 숫자 카드의 값을 입력했을 때 해당 카드의 개수를 출력해야 하는데 처음엔 시간복잡도가 O(n) 인 List 의 count(x) 를 사용하여 시간초과가 발생 전체 카드에 대한 Counter를 정의하여 탐색 시간복잡도를 O(1) 로 개선 Heap # 백준 7662번(이중 우선순위 큐) 문제를 풀 때 사용 해당 문제는 최솟값과 최댓값 삭제 기능을 모두 가지고 있는 이중 우선순위 큐를 구현하는 것 처음엔 List 의 pop(x), index(x), max(x)/min(x) 를 혼합하여 사용한 것 때문에 O(n^3) 이상의 시간복잡도를 만들어서 시간초과가 발생 두번째 시도에선 List 의 pop(x) 와 heapq 모듈의 heappop(x) 를 사용해 시간복잡도를 O(1) 로 개선 하지만, Heap은 이진트리 기반으로 리스트와는 구조가 다르기 때문에 인덱스로 참조 시 에러가 발생 세번째 시도에선 단일 큐를 Max Heap과 Min Heap으로 나누고 각각에서 heappop(x), heappush(x) 를 수행 하지만, Max Heap 또는 Min Heap에서 삭제된 값이 반대쪽 Heap에서 남아있는 경우가 있어 에러가 발생 해당 에러에 대한 해결책으로 Max Heap과 Min Heap을 동기화를 시키는 방법도 있지만, 값이 유효한지 판단하는 Dictionary 를 구현해 값에 대한 참/거짓 여부를 참조하는 방법을 이용 해당 Dictionary를 heappop(x) 사용 시 한 번, 최대/최솟값 출력 시 한 번씩 참조해 에러 해결 Heap을 활용한 코드 일부 # Copy python if cmd == \u0026#39;I\u0026#39;: n = int(n) heapq.heappush(min_Q, n) heapq.heappush(max_Q, -n) try: valid[n] += 1 except KeyError: valid[n] = 1 ins += 1 elif cmd == \u0026#39;D\u0026#39;: try: if n == \u0026#39;1\u0026#39;: max_pop = -heapq.heappop(max_Q) while not valid[max_pop]: max_pop = -heapq.heappop(max_Q) valid[max_pop] -= 1 ins -= 1 elif n == \u0026#39;-1\u0026#39;: min_pop = heapq.heappop(min_Q) while not valid[min_pop]: min_pop = heapq.heappop(min_Q) valid[min_pop] -= 1 ins -= 1 except IndexError: min_Q, max_Q = [], [] continue Copy python max_pop, min_pop = 0, 0 while True: max_pop = -heapq.heappop(max_Q) if valid[max_pop]: break while True: min_pop = heapq.heappop(min_Q) if valid[min_pop]: break print(max_pop, min_pop) 조합/순열 # Combination # 백준 1010번(다리 놓기) 문제를 풀 때 사용 해당 문제는 강에 다리를 놓는 경우의 수를 출력해야 하는데 math 모듈의 comb 함수를 이용해 경우의 수를 계산 Permutation # 백준 1107번(리모컨) 문제를 풀 때 사용 해당 문제에서 특정 길이의 문자열에 대해 가능한 모든 조합을 나열하는데, 순서를 고려하고 중복을 허용하기 위해 중복 순열(Product)을 사용 Permutation을 활용한 코드 일부 # Copy python buttons = set([str(i) for i in range(10)]) ... for i in range(1, count+1): product = itertools.product(buttons, repeat=i) channels|= set(map(\u0026#39;\u0026#39;.join, product)) 탐색 # Binary Search # 백준 1654번(랜선 자르기) 문제를 풀 때 사용 해당 문제는 서로 다른 길이의 선들을 동일한 길이로 가장 길게 잘라야 되는데 처음엔 가장 긴 선부터 가장 짧은 선까지의 범위 내에서 완전탐색을 진행하여 시간초과가 발생 완전탐색을 이분탐색으로 대체하여 시간복잡도 개선 Binary Search를 활용한 코드 일부 # Copy python while mn \u0026lt; mx: md = (mx + mn) // 2 count = 0 for i in range(K): count += k[i] // md if count \u0026lt; N: mx = md else: mn = md + 1 그래프 # DFS/BFS # 백준 1260번(DFS와 BFS) 문제를 풀 때 사용 해당 문제는 DFS와 BFS로 탐색했을 때의 결과를 출력하는 기본적인 문제 DFS는 깊이 를 우선적으로 탐색, BFS는 너비 를 우선적으로 탐색 DFS는 경로의 특징을 저장할 때 사용, BFS는 최단거리를 구할 때 사용 DFS는 스택 또는 재귀함수 로 구현, BFS는 큐(데크) 를 이용해서 구현 DFS/BFS를 활용한 코드 일부 # Copy python def dfs(nodes, visited, node): visited[node] = True next_nodes = nodes[node] while next_nodes: next_node = heapq.heappop(next_nodes) if not visited[next_node]: print(next_node, end=\u0026#39; \u0026#39;) dfs(nodes, visited, next_node) Copy python def bfs(nodes, visited, root): queue = deque() visited[root] = True queue.append(root) while queue: node = queue.popleft() visited[node] = True print(node, end=\u0026#39; \u0026#39;) next_nodes = nodes[node] while next_nodes: next_node = heapq.heappop(next_nodes) if not visited[next_node]: visited[next_node] = True queue.append(next_node) Union-Find Algorithm # 두 노드가 같은 그래프에 속하는지 판별하는 알고리즘 노드를 합치는 Union 연산과 루트 노드를 찾는 Find 연산으로 이루어짐 배열에 나열된 모든 노드들은 기본적으로 자기 자신의 값을 가짐 노드를 합칠 때 자식 노드의 배열 값에 부모 노드의 배열 값을 넣음 파이썬 구현 코드 # Copy python def find(graph: list, x: int) -\u0026gt; int: if graph[x] == x: return x graph[x] = find(graph, graph[x]) def union(graph: list, x: int, y: int) -\u0026gt; None: x = find(graph, x) y = find(graph, y) if x == y: return graph[y] = x Kruskal's Algorithm # 가장 적은 비용으로 모든 노드를 연결하기 위해 사용하는 알고리즘 (최소 비용 신장 트리) 모든 간선 정보를 오름차순으로 정렬한 뒤 비용이 적은 간선부터 그래프에 포함 참고자료: https://blog.naver.com/ndb796/221230994142 파이썬 구현 코드 # Copy python class Edge: def __init__(self, a: int, b: int, cost: int): self.parent = a self.child = b self.cost = cost def get_parent(graph: list, x: int) -\u0026gt; int: if graph[x] == x: return x graph[x] = get_parent(graph, graph[x]) def union_parent(graph: list, a: int, b: int) -\u0026gt; None: a = get_parent(graph, a) b = get_parent(graph, b) if a \u0026lt; b: graph[b] = a else: graph[a] = b def find(graph: list, a: int, b: int) -\u0026gt; int: a = get_parent(graph, a) b = get_parent(graph, b) if a == b: return True else: return False def sort_edge(edge_list: list) -\u0026gt; list: return sorted(edge_list, key=lambda x: [x.cost, x.parent, x.child]) def union_edge(graph: list, edge_list: list) -\u0026gt; int: cost = 0 for edge in edge_list: if not find(graph, edge.parent, edge.child): cost += edge.cost union_parent(graph, edge.parent, edge.child) return cost Prim's Algorithm # 시작 정점을 선택한 후, 정점에 인접한 간선 중 최소 비용의 간선을 연결하여 최소 신장 트리(MST)를 확장해가는 방식 Kruskal's Algorithm 이 비용이 가장 작은 간선부터 다음 간선을 선택하는데 반해, Prim's Algorithm 은 특정 정점에서부터 다음 정점을 갱신해나가며 비용이 작은 간선을 선택 Prim's Algorithm의 시간 복잡도는 최악의 경우 O(E log E) (while 구문에서 모든 간선에 대해 반복하고, 최소 힙 구조를 사용) 참고자료: www.fun-coding.org/Chapter20-prim-live.html 파이썬 구현 코드 # Copy python def prim(edge_list: list, start_node: int) -\u0026gt; list: mst = list() adjacent_edge_list = defaultdict(list) for weight, n1, n2 in edge_list: adjacent_edge_list[n1].append((weight, n1, n2)) adjacent_edge_list[n1].append((weight, n2, n1)) connected_nodes = {start_node} candidate_edge_list = adjacent_edge_list[start_node] heapq.heapify(candidate_edge_list) while candidate_edge_list: weight, n1, n2 = heapq.heappop(candidate_edge_list) if n2 not in connected_nodes: connected_nodes.add(n2) mst.append((weight, n1, n2)) for edge in adjacent_edge_list[n2]: if edge[2] not in connected_nodes: heapq.heappush(candidate_edge_list, edge) return mst Prim's Algorithm 개선 # 간선이 아닌 노드를 중심 으로 우선순위 큐를 적용 노드마다 Key 값을 가지고 있고, Key 값을 우선순위 큐에 넣음 Key 값이 0인 정점의 인접한 정점들에 대해 Key 값과 연결된 비용을 비교하여 Key 값이 작으면 해당 정점의 Key 값을 갱신 개선된 Prim's Algorithm의 시간 복잡도는 O(E log V) 해당 알고리즘을 구현하기 위해 heapdict 라이브러리 사용 (기존의 heap 내용을 업데이트하면 알아서 최소 힙의 구조로 업데이트됨) 파이썬 구현 코드 # Copy python from heapdict import heapdict def prim(graph: dict, start_node: int) -\u0026gt; (list, int): mst, keys, pi, total_weight = list(), heapdict(), dict(), 0 for node in graph.keys(): keys[node] = float(\u0026#39;inf\u0026#39;) pi[node] = None keys[start_node], pi[start_node] = 0, start_node while keys: current_node, current_key = keys.popitem() mst.append([pi[current_node], current_node, current_key]) total_weight += current_key for adjacent, weight in graph[current_node].items(): if adjacent in keys and weight \u0026lt; keys[adjacent]: keys[adjacent] = weight pi[adjacent] = current_node return mst, total_weight 연산자 오버로딩 # 연산자 오버로딩은 인스턴스 객체끼리 서로 연산을 할 수 있게 기존 연산자의 기능을 중복으로 정의하는 것 연산자 오버로딩의 예시 Method Operator Example __add__(self, other) + (Binomial) A + B, A += B __pos__(self) + (Unary) +A _sub__(self, other) - (Binomial) A - B, A -= B __neg__(self) - (Unary) -A __mul__(self, other) * A * B, A *= B __truediv__(self, other) / A / B, A /= B __floordiv__(self, other) // A // B, A //= B __mod__(self, other) % A % B, A %= B __pow__(self, other) pow(), ** pow(A, B), A ** B __eq__(self, other) == A == B __lt__(self, other) \u0026lt; A \u0026lt; B __gt__(self, other) \u0026gt; A \u0026gt; B __lshift__(self, other) \u0026lt;\u0026lt; A \u0026lt;\u0026lt; B __rshift__(self, other) \u0026gt;\u0026gt; A \u0026gt;\u0026gt; B __and__(self, other) \u0026amp; A \u0026amp; B, A \u0026amp;= B __xor__(self, other) ^ A ^ B, A ^= B __or__(self, other) | A |B, A |= B __invert__(self) ~ ~A __abs__(self) abs() abs(A) "}]