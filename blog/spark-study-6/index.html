<!doctype html><html lang=ko-kr dir=ltr><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=author content><meta name=keywords content="Apache Spark,Data Sources,Parquet,JSON,CSV,Avro,Spark SQL,PySpark,데이터 엔지니어링,스파크,Study"><meta name=description content="Apache Spark의 다양한 데이터 소스를 다루며, Parquet, JSON, CSV, Avro 등 형식의 데이터 읽기/쓰기를 단계별로 안내합니다. 빅데이터 처리를 위한 실용적 …"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><link rel=icon href=https://minyeamer.github.io/images/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://minyeamer.github.io/images/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://minyeamer.github.io/images/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://minyeamer.github.io/images/favicons/apple-touch-icon.png><link rel=mask-icon href=https://minyeamer.github.io/images/favicons/apple-touch-icon.png><meta name=google-site-verification content="u1tWcHmHUZWfFT1cHaku6sqU-bK40N3WLR-C-4VUWN0"><meta name=naver-site-verification content="6eaf8e9da1a6104780f056f1a7797fe5a3a5a0da"><meta property="og:title" content="Apache Spark - 다양한 데이터 소스 읽기/쓰기 (Parquet, JSON, CSV, Avro)"><meta property="og:description" content="Apache Spark의 다양한 데이터 소스를 다루며, Parquet, JSON, CSV, Avro 등 형식의 데이터 읽기/쓰기를 단계별로 안내합니다. 빅데이터 처리를 위한 실용적 …"><meta property="og:type" content="article"><meta property="og:url" content="https://minyeamer.github.io/blog/spark-study-6/"><meta property="og:image" content="https://dl.dropboxusercontent.com/scl/fi/iafnblb6k95kbw7bwn2xj/spark-00-cover.webp?rlkey=6995tacnu3mvr7s31akl5sca6&amp;raw=1"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-10T23:56:54+09:00"><meta property="article:modified_time" content="2025-07-10T23:56:54+09:00"><meta property="og:see_also" content="https://minyeamer.github.io/blog/spark-study-8/"><meta property="og:see_also" content="https://minyeamer.github.io/blog/spark-study-7/"><meta property="og:see_also" content="https://minyeamer.github.io/blog/spark-study-5/"><meta property="og:see_also" content="https://minyeamer.github.io/blog/spark-study-4/"><meta property="og:see_also" content="https://minyeamer.github.io/blog/spark-study-3/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dl.dropboxusercontent.com/scl/fi/iafnblb6k95kbw7bwn2xj/spark-00-cover.webp?rlkey=6995tacnu3mvr7s31akl5sca6&amp;raw=1"><meta name=twitter:title content="Apache Spark - 다양한 데이터 소스 읽기/쓰기 (Parquet, JSON, CSV, Avro)"><meta name=twitter:description content="Apache Spark의 다양한 데이터 소스를 다루며, Parquet, JSON, CSV, Avro 등 형식의 데이터 읽기/쓰기를 단계별로 안내합니다. 빅데이터 처리를 위한 실용적 …"><meta name=twitter:site content="@https://x.com/minyeamer"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://minyeamer.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Apache Spark - 다양한 데이터 소스 읽기/쓰기 (Parquet, JSON, CSV, Avro)","item":"https://minyeamer.github.io/blog/spark-study-6/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Apache Spark - 다양한 데이터 소스 읽기/쓰기 (Parquet, JSON, CSV, Avro)","name":"Apache Spark - 다양한 데이터 소스 읽기\/쓰기 (Parquet, JSON, CSV, Avro)","description":"Apache Spark의 다양한 데이터 소스를 다루며, Parquet, JSON, CSV, Avro 등 형식의 데이터 읽기/쓰기를 단계별로 안내합니다. 빅데이터 처리를 위한 실용적인 기법을 배우세요.\n","keywords":["Apache Spark, Data Sources, Parquet, JSON, CSV, Avro, Spark SQL, PySpark, 데이터 엔지니어링, 스파크, Study"],"articleBody":" Apache Spark 배우기 1. 스파크의 기본 개념과 아키텍처 2. 로컬 환경에서 설치하고 PySpark 실행하기 3. 스파크 애플리케이션 구조와 RDD 이해하기 4. DataFrame과 Dataset API 활용하기 5. 스파크 SQL과 테이블/뷰 관리 6. 다양한 데이터 소스 읽기/쓰기 (Parquet, JSON, CSV, Avro) 7. 외부 데이터베이스 연동 (PostgreSQL, MySQL) 8. 사용자 정의 함수(UDF)와 고차 함수 활용하기 숨기기 목록 보기 6/8 Data Source API #DataFrameReader #DataFrameReader는 데이터 소스에서 DataFrame으로 데이터를 읽는 방식이다. 아래와 같이 권장되는 사용 패턴이 있다.\nCopy python DataFrameReader .format(args) # 데이터 소스 형식 .option(\"key\", \"value\") # 키/값 쌍으로 연결되는 옵션 .schema(args) # DDL 문자열 또는 StructType .load() # 데이터 소스의 경로데이터 소스 형식에는 인수로 \"parquet\", \"csv\", \"txt\", \"json\", \"jdbc\", \"orc\", \"avro\" 등이 전달된다. 기본값은 \"parquet\" 또는 spark.sql.sources.default 에 지정된 항목이 설정된다.\nJSON이나 CSV 형식은 option() 함수에서 스키마를 유추하는 inferSchema 옵션을 적용할 수 있지만, 스키마를 제공하면 로드 속도가 빨라진다.\nSparkSession 인스턴스를 통해서 DataFrame에 액세스할 경우 read() 또는 readStream() 을 사용할 수 있다. read() 는 정적 데이터 소스에서 DataFrame을 읽어 오며, readStream() 은 스트리밍 소스에서 인스턴스를 반환한다.\nDataFrameWriter #DataFrameWriter는 데이터 소스에 데이터를 저장하거나 쓰는 작업을 수행한다. 권장되는 사용 형식은 다음과 같다.\nCopy python DataFrameWriter .format(args) # 데이터 소스 형식 .option(args) # 키/값 쌍으로 연결되는 옵션 .bucketBy(args) # 버킷 개수 및 버킷 기준 칼럼 이름 .partitionBy(args) # 데이터 소스의 경로 .save(path) # 저장할 테이블DataFrame에서 인스턴스에 액세스할 경우 write() 또는 writeStream() 을 사용할 수 있다.\nData Sources #Parquet #스파크의 기본 데이터 소스인 Parquet는 다양한 I/O 최적화를 제공하는 오픈소스 칼럼 기반 파일 형식이다. 압축을 통해 저장 공간을 절약하고 데이터 칼럼에 대한 빠른 액세스를 허용한다.\nParquet 파일은 데이터 파일, 메타데이터, 여러 압축 파일 및 일부 상태 파일이 포함된 디렉터리 구조가 저장된다. 메타데이터에는 파일 형식의 버전, 스키마, 경로 등의 칼럼 데이터가 포함된다.\ndatabricks/LearningSparkV2의 databricks-datasets/learning-spark-v2/flights/summary-data/parquet 경로에서 2010-summary.parquet/ 디렉터리를 가져온다.\nParquet 파일의 디렉터리에는 다음과 같은 파일 집합이 포함된다.\nCopy bash % ls -la data/flights/summary-data/parquet/2010-summary.parquet/ -rwxr-xr-x@ ... 0 ... _SUCCESS -rwxr-xr-x@ ... 3921 ... part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquetParquet 파일을 DataFrame으로 읽으려면 형식과 경로를 지정하기만 하면 된다. spark.sql.sources.default 설정을 하지 않았다면 .format(\"parquet\") 함수는 생략해도 된다.\nCopy python file = \"data/flights/summary-data/parquet/2010-summary.parquet\" df = spark.read.format(\"parquet\").load(file) df.show(5) Copy bash +-----------------+-------------------+-----+ |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| +-----------------+-------------------+-----+ | United States| Romania| 1| | United States| Ireland| 264| | United States| India| 69| | Egypt| United States| 24| |Equatorial Guinea| United States| 1| +-----------------+-------------------+-----+Parquet 파일을 Spark SQL 테이블로 읽으려면 아래와 같은 스파크 SQL을 사용할 수 있다.\nCopy sql CREATE OR REPLACE TEMPORARY VIEW delay_flights USING parquet OPTIONS ( path \"data/flights/summary-data/parquet/2010-summary.parquet\")메타데이터가 궁금해서 parquet-tools 라이브러리를 설치하고, inspect 명령어로 part-XXXX 압축 파일을 조회했다. 아래와 같이 출력되었는데, 3개의 칼럼 DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count 에 대한 데이터 타입 등의 정보가 상세히 적혀 있다. 스파크는 해당 데이터 타입을 읽기 때문에, 위 DataFrame에 printSchema() 출력한 결과는 아래 칼럼별 데이터 타입과 같다.\nCopy bash % parquet-tools inspect data/flights/summary-data/parquet/2010-summary.parquet/part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet ############ file meta data ############ created_by: parquet-mr (build 32c46643845ea8a705c35d4ec8fc654cc8ff816d) num_columns: 3 num_rows: 255 num_row_groups: 1 format_version: 1.0 serialized_size: 658 ############ Columns ############ DEST_COUNTRY_NAME ORIGIN_COUNTRY_NAME count ############ Column(DEST_COUNTRY_NAME) ############ name: DEST_COUNTRY_NAME path: DEST_COUNTRY_NAME max_definition_level: 1 max_repetition_level: 0 physical_type: BYTE_ARRAY logical_type: String converted_type (legacy): UTF8 compression: GZIP (space_saved: 37%) ############ Column(ORIGIN_COUNTRY_NAME) ############ name: ORIGIN_COUNTRY_NAME path: ORIGIN_COUNTRY_NAME max_definition_level: 1 max_repetition_level: 0 physical_type: BYTE_ARRAY logical_type: String converted_type (legacy): UTF8 compression: GZIP (space_saved: 39%) ############ Column(count) ############ name: count path: count max_definition_level: 1 max_repetition_level: 0 physical_type: INT64 logical_type: None converted_type (legacy): NONE compression: GZIP (space_saved: 53%)DataFrame을 DataFrameWriter를 사용해 Parquet 파일로 저장할 때는 아래와 같은 함수를 사용할 수 있다.\nCopy python (df.write.format(\"parquet\") .mode(\"overwrite\") .option(\"compression\", \"snappy\") .save(\"/tmp/data/parquet/df_parquet\"))압축 방식으로 snappy 를 사용하여 snappy 압축 파일이 생성된다. DataFrame을 직접 저장하면 아래와 같은 파일이 생성된다.\nCopy bash % ls -la /tmp/data/parquet/df_parquet/ -rw-r--r--@ ... 8 ... ._SUCCESS.crc -rw-r--r--@ ... 52 ... .part-00000-9828b287-9956-40cb-9c33-d59bea52e5be-c000.snappy.parquet.crc -rw-r--r--@ ... 0 ... _SUCCESS -rw-r--r--@ ... 5442 ... part-00000-9828b287-9956-40cb-9c33-d59bea52e5be-c000.snappy.parquetJSON #JSON 데이터 형식은 XML에 비해 읽기 쉽고, 구문을 분석하기 쉬운 형식이다. 단일 라인 모드와 다중 라인 모드를 모두 지원한다. 단일 라인 모드에서는 각 라인이 단일 JSON 개체를 나타내지만, 다중 라인 모드에서는 전체 라인 객체가 단일 JSON 개체를 구성한다. option() 함수에서 multiLine 옵션에 ture 또는 false 를 설정할 수 있다.\n단일 라인 모드의 JSON 데이터는 아래와 같이 구성된다.\nCopy bash % head -n 5 data/flights/summary-data/json/2010-summary.json {\"ORIGIN_COUNTRY_NAME\":\"Romania\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":1} {\"ORIGIN_COUNTRY_NAME\":\"Ireland\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":264} {\"ORIGIN_COUNTRY_NAME\":\"India\",\"DEST_COUNTRY_NAME\":\"United States\",\"count\":69} {\"ORIGIN_COUNTRY_NAME\":\"United States\",\"DEST_COUNTRY_NAME\":\"Egypt\",\"count\":24} {\"ORIGIN_COUNTRY_NAME\":\"United States\",\"DEST_COUNTRY_NAME\":\"Equatorial Guinea\",\"count\":1}JSON 파일을 DataFrame으로 읽으려면 아래처럼 .format(\"json\") 을 지정한다.\nCopy python file = \"data/flights/summary-data/json/*\" df = spark.read.format(\"json\").load(file) df.show(5) Copy bash +-----------------+-------------------+-----+ |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| +-----------------+-------------------+-----+ | United States| Romania| 15| | United States| Croatia| 1| | United States| Ireland| 344| | Egypt| United States| 15| | United States| India| 62| +-----------------+-------------------+-----+JSON 데이터 소스에 대해 DataFrameReader 및 DataFrameWriter 에 대한 일반적인 옵션은 아래와 같다.\ncompression : 압축 코덱을 쓰기 시에 사용할 수 있다. bzip2, gzip, snappy 등이 값으로 전달된다. dateFormat : Java의 DateTimeFormatter에서 제공하는 모든 형식을 사용할 수 있다. (yyyy-MM-dd 등) multiLine : true 를 지정하면 다중 라인 모드를 사용한다. 기본값은 false 이다. allowUnquotedFileNames : 따옴표로 묶이지 않은 JSON 필드 이름을 허용한다. 기본값은 false 이다. CSV #쉼표로 각 데이터 또는 필드를 구분하며, 쉼표로 구분된 각 줄은 레코드를 나타낸다. 쉼표가 데이터의 일부인 경우, 값을 쌍따옴표로 감싸주거나, 다른 구분 기호를 사용하여 필드를 분리할 수 있다.\n일반적인 CSV 데이터는 아래와 같이 구성된다.\nCopy bash % head -n 5 data/flights/summary-data/csv/2010-summary.csv DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count United States,Romania,1 United States,Ireland,264 United States,India,69 Egypt,United States,24CSV 파일을 DataFrame으로 읽으려면 아래처럼 .format(\"csv\") 을 지정한다. 위 파일과 같이 헤더가 있는 경우 header 옵션에 true 를 설정한다. nullValue 옵션을 사용해 null 데이터를 특정 값으로 교체할 수 있다.\nCopy python file = \"data/flights/summary-data/csv/*\" df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file) df.show(5) Copy bash +-----------------+-------------------+-----+ |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| +-----------------+-------------------+-----+ | United States| Romania| 1| | United States| Ireland| 264| | United States| India| 69| | Egypt| United States| 24| |Equatorial Guinea| United States| 1| +-----------------+-------------------+-----+CSV 데이터 소스에 대해 DataFrameReader 및 DataFrameWriter 에 대한 일반적인 옵션은 아래와 같다.\ncompression : 압축 코덱을 쓰기 시에 사용할 수 있다. bzip2, gzip, snappy 등이 값으로 전달된다. dateFormat : Java의 DateTimeFormatter에서 제공하는 모든 형식을 사용할 수 있다. (yyyy-MM-dd 등) multiLine : true 를 지정하면 다중 라인 모드를 사용한다. 기본값은 false 이다. interSchema : true 를 지정하면 스파크가 칼럼 데이터 유형을 결정한다. 기본값은 false 이다. sep : 칼럼을 구분하기 위한 문자이며, 기본 구분 기호는 쉼표(,)다. escape : 따옴표를 이스케이프할 때 사용하는 문자이며, 기본값은 / 다. header : 첫 번째 줄이 칼럼명을 나타내는 헤더일 경우 true 를 지정하고, 기본값은 false 이다. Avro #스파크 2.4에 내장된 데이터 소스인 Avro 형식은 특히 아파치 카프카에서 메시지를 직렬화 및 역직렬화할 때 사용된다. JSON에 대한 직접 매핑, 속도와 효율성 등 많은 이점을 제공한다.\n스파크 공식 문서의 Apache Avro Data Source Guide에 따르면, spark-avro 모듈은 외부 모듈로 spark-submit 또는 spark-shell 에 포함되어 있지 않다. 따라서, 아래와 같이 --packages 를 사용하여 종속성을 추가할 수 있다.\nCopy bash ./bin/spark-shell --packages org.apache.spark:spark-avro_2.13:4.0.0 ...SparkSession 인스턴스를 사용할 경우, spark.jars.packages 설정에 spark-avro_2.13 종속성을 추가한다.\nCopy python from pyspark.sql import SparkSession spark = (SparkSession .builder .appName(\"SparkAvroExampleApp\") .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.13:4.0.0\") .getOrCreate())Avro 파일을 DataFrame으로 읽으려면 아래처럼 .format(\"avro\") 을 지정한다.\nCopy python file = \"data/flights/summary-data/avro/*\" df = spark.read.format(\"avro\").load(file) df.show(5) Copy bash +-----------------+-------------------+-----+ |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| +-----------------+-------------------+-----+ | United States| Romania| 1| | United States| Ireland| 264| | United States| India| 69| | Egypt| United States| 24| |Equatorial Guinea| United States| 1| +-----------------+-------------------+-----+Avro 데이터 소스에 대해 DataFrameReader 및 DataFrameWriter 에 대한 일반적인 옵션은 아래와 같다.\navroSchema : JSON 형식으로 제공할 수 있는 Avro 스키마이다. Avro 데이터나 카탈리스트 데이터와 일치하지 않으면 읽기/쓰기 작업이 실패한다. recordName : Avro 사양에 필요한 쓰기 결과의 최상위 레코드명이다. recrodNamespace : 쓰기 결과에 네임스페이스를 기록한다. ignoreExtension : 확장자가 .avro 인지 여부에 관계없이 모든 파일을 읽어들인다. compression : 쓰기에 사용할 압축 코덱을 지정할 수 있다. Image #스파크 2.4에서 머신러닝 프레임워크를 지원하기 위해 새로운 데이터 소스인 이미지 파일을 도입했다.\ndatabricks/LearningSparkV2의 databricks-datasets/learning-spark-v2 경로에서 cctvVideos/ 디렉터리를 가져온다. 해당 디렉터리의 구조는 다음과 같다.\nCopy text cctvVideos/ ├── README.md └── train_images/ ├── label=0/ │ ├── Browse2frame0000.jpg │ ├── Browse2frame0001.jpg │ ├── Browse2frame0002.jpg │ ├── ... | └── Walk2frame0042.jpg └── label=1/ ├── Fight_Chaseframe0012.jpg ├── Fight_Chaseframe0013.jpg ├── Fight_Chaseframe0014.jpg ├── ... └── Rest_WiggleOnFloorframe0050.jpg이미지 파일은 아래와 같이 DataFrameReader 함수로 읽을 수 있다. 이미지 파일을 읽을 때 numpy 라이브러리가 필요하다.\nCopy python from pyspark.ml import image image_dir = \"data/cctvVideos/train_images\" images_df = spark.read.format(\"image\").load(image_dir) images_df.printSchema() Copy bash root |-- image: struct (nullable = true) | |-- origin: string (nullable = true) | |-- height: integer (nullable = true) | |-- width: integer (nullable = true) | |-- nChannels: integer (nullable = true) | |-- mode: integer (nullable = true) | |-- data: binary (nullable = true) |-- label: integer (nullable = true)이미지의 높이 및 너비와 같은 정보는 아래와 같이 조회할 수 있다.\nCopy python images_df.select(\"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\", \"label\").show(5) Copy bash +------+-----+---------+----+-----+ |height|width|nChannels|mode|label| +------+-----+---------+----+-----+ | 288| 384| 3| 16| 0| | 288| 384| 3| 16| 1| | 288| 384| 3| 16| 0| | 288| 384| 3| 16| 0| | 288| 384| 3| 16| 0| +------+-----+---------+----+-----+Binary File #이진 파일을 읽으려면 데이터 소스 형식을 binaryFile 로 지정해야 한다. DataFrameReader는 이진 파일을 원본 내용과 메타데이터를 포함하는 단일 DataFrame 행으로 변환한다.\npathGlobFilter 를 사용하면 지정된 전역 패턴과 일치하는 경로로 파일을 로드할 수 있다. 아래 코드는 디렉터리에서 모든 .jpg 파일을 읽는다.\nCopy python path = \"data/cctvVideos/train_images\" binary_files_df = spark.read.format(\"binaryFile\").option(\"pathGlobFilter\", \"*.jpg\").load(path) binary_files_df.show(5) Copy bash +--------------------+-------------------+------+--------------------+-----+ | path| modificationTime|length| content|label| +--------------------+-------------------+------+--------------------+-----+ |file:/Users/cuz/D...|2025-01-28 13:30:40| 55037|[FF D8 FF E0 00 1...| 0| |file:/Users/cuz/D...|2025-01-28 13:30:40| 54634|[FF D8 FF E0 00 1...| 1| |file:/Users/cuz/D...|2025-01-28 13:30:40| 54624|[FF D8 FF E0 00 1...| 0| |file:/Users/cuz/D...|2025-01-28 13:30:40| 54505|[FF D8 FF E0 00 1...| 0| |file:/Users/cuz/D...|2025-01-28 13:30:40| 54475|[FF D8 FF E0 00 1...| 0| +--------------------+-------------------+------+--------------------+-----+References # https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ https://github.com/databricks/LearningSparkV2/tree/master/databricks-datasets/learning-spark-v2/flights/summary-data https://spark.apache.org/docs/latest/api/python/tutorial/sql/python_data_source.html https://spark.apache.org/docs/latest/sql-data-sources.html https://spark.apache.org/docs/latest/sql-data-sources-avro.html ","wordCount":"1493","inLanguage":"ko","image":"https:\/\/dl.dropboxusercontent.com\/scl\/fi\/iafnblb6k95kbw7bwn2xj\/spark-00-cover.webp?rlkey=6995tacnu3mvr7s31akl5sca6\u0026raw=1","datePublished":"2025-07-10T23:56:54+09:00","dateModified":"2025-07-10T23:56:54+09:00","author":{"@type":"Person","name":"minyeamer"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://minyeamer.github.io/blog/spark-study-6/"},"publisher":{"@type":"Person","name":"Minystory","logo":{"@type":"ImageObject","url":"https://minyeamer.github.io/images/favicons/favicon.ico"}}}</script><script>(function(){const e=256+768*1.3;window.innerWidth>e&&localStorage.getItem("menu-expanded")==="false"&&document.documentElement.classList.add("menu-initial-collapsed")})()</script><title>Apache Spark - 다양한 데이터 소스 읽기/쓰기 (Parquet, JSON, CSV, Avro)</title><link rel=icon href=/images/favicon.ico><link rel=manifest href=/data/manifest.json><link rel=canonical href=https://minyeamer.github.io/blog/spark-study-6/><link rel=stylesheet href=/main.min.6051a89cb99914270b8e92de78f55e2ee0c234cddc563590e40e0a85eb6ce78f.css integrity="sha256-YFGonLmZFCcLjpLeePVeLuDCNM3cVjWQ5A4Khets548=" crossorigin=anonymous><script async src="https://www.googletagmanager.com/gtag/js?id=G-BJ8Z9RMBPJ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BJ8Z9RMBPJ")</script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.1/css/all.min.css crossorigin=anonymous><script>(function(){const e=console.warn;console.warn=function(...n){const t=(new Error).stack||"";if(t.includes("highlight.min.js")||t.includes("highlightjs-line-numbers"))return;e.apply(console,n)}})()</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlightjs-line-numbers.js/2.9.0/highlightjs-line-numbers.min.js crossorigin=anonymous></script><script>window.siteI18n=window.siteI18n||{},window.siteI18n.defaultLang="en"</script><script>window.siteI18n.dataUrl="/data/i18n.min.a69d97976d04e681499d74d71f0019de3d6ed9f29a55bc606a9459d1454b3b65.json"</script><script>window.siteI18n.initData=function(){return window.siteI18n.data?Promise.resolve(window.siteI18n.data):fetch(window.siteI18n.dataUrl).then(e=>{if(!e.ok)throw new Error(`HTTP error! status: ${e.status}`);return e.json()}).then(e=>(window.siteI18n.data=e,window.siteI18n.data))},window.siteI18n.getLanguage=function(e={}){const s="site-language",t=localStorage.getItem(s);if(t)return t;const n=(navigator.language||navigator.userLanguage).split("-")[0];return!e||e[n]?n:window.siteI18n.defaultLang},window.siteI18n.translate=function(e,t=""){if(window.siteI18n.data){const n=window.siteI18n.getLanguage(),t=window.siteI18n.data[n];if(t&&e in t)return t[e]}return t},void window.siteI18n.initData().catch(()=>{})</script><script defer src=/js/core/i18n.min.c076e5ad51c454cca138121312827bfde8c5526a930ce57cea2207f501fea0d7.js integrity="sha256-wHblrVHEVMyhOBITEoJ7/ejFUmqTDOV86iIH9QH+oNc=" crossorigin=anonymous></script><script defer src=/js/partials/scroll-progress.min.841ade7e507a5f6d59c4e7bf2fe2b2ca034070677ff7957eec55610a024dd776.js integrity="sha256-hBreflB6X21ZxOe/L+KyygNAcGd/95V+7FVhCgJN13Y=" crossorigin=anonymous></script><script defer src=/js/partials/menu-toggle.min.7ee0f34ad37a564fe94803318b77c9128244ae7747bf68a99f5c639cd14ee1bc.js integrity="sha256-fuDzStN6Vk/pSAMxi3fJEoJErndHv2ipn1xjnNFO4bw=" crossorigin=anonymous></script><script defer src=/js/partials/toc-toggle.min.245e40eb40fe121e595f13897569556fa17b72530600a710a00a5d636dbf140d.js integrity="sha256-JF5A60D+Eh5ZXxOJdWlVb6F7clMGAKcQoApdY22/FA0=" crossorigin=anonymous></script><script defer src=/js/core/set-theme.min.986446861d7c48b101b4a6f53724b7fc97d6efec2a87906c6630cb9e503897c4.js integrity="sha256-mGRGhh18SLEBtKb1NyS3/JfW7+wqh5BsZjDLnlA4l8Q=" crossorigin=anonymous></script><script defer src=/js/shortcodes/copy-code.min.e18ffc44cf6beb1757c467945bb2897b0e1cb2ee52f08b460ee3ee57f0db6d60.js integrity="sha256-4Y/8RM9r6xdXxGeUW7KJew4csu5S8ItGDuPuV/DbbWA=" crossorigin=anonymous></script><script defer src=/js/partials/toc-highlight.min.7917ead4ecb7378779bfbf3f988251ac76bd55ca7f12fd5919870777dbcd6095.js integrity="sha256-eRfq1Oy3N4d5v78/mIJRrHa9Vcp/Ev1ZGYcHd9vNYJU=" crossorigin=anonymous></script><script defer src=/js/partials/reading-time.min.9dae1dd14e75a79711e28a5864bf1fe636869815c32110f680ff8e1d61bfdf28.js integrity="sha256-na4d0U51p5cR4opYZL8f5jaGmBXDIRD2gP+OHWG/3yg=" crossorigin=anonymous></script><script defer src=/js/shortcodes/image-zoom.min.bb446c50cbec91f1b4fdfbb9f695d753ef4811f43de0a9411201c8466ffe204a.js integrity="sha256-u0RsUMvskfG0/fu59pXXU+9IEfQ94KlBEgHIRm/+IEo=" crossorigin=anonymous></script><script defer src=/js/shortcodes/data-table.min.d20aaf7f506ee989f91746098231f979e58b8435513de0d4f05eefb9c45743e7.js integrity="sha256-0gqvf1Bu6Yn5F0YJgjH5eeWLhDVRPeDU8F7vucRXQ+c=" crossorigin=anonymous></script><script>window.siteSearch=window.siteSearch||{}</script><script>window.siteSearch.contentUrl="/data/content.min.0d9c35258206161520526aeccdea6826417931b6c7dda3db171fe4099b3a26f1.json"</script><script>window.siteSearch.categoriesUrl="/data/categories.min.a110e9708cdf79e3dc8d271224586cc9deffc0232514190c37e24dc74470851c.json"</script><script>window.siteSearch.tagsUrl="/data/tags.min.d80e1fb9a5142d159b8e7e3b9adea10216e811adf810f15fbe9bd79b2b6c83d8.json"</script><script defer src=/fuse.min.js></script><script defer src=/js/search/init.min.ee793e2de351fef268c6642567b4af23bd3763c2cb74f767e7240807e394417d.js integrity="sha256-7nk+LeNR/vJoxmQlZ7SvI703Y8LLdPdn5yQIB+OUQX0=" crossorigin=anonymous></script><script defer src=/js/search/input.min.d9380107ad7925e0b0ba621d64e1648e8119523b6f8cff46cdf240117921a06b.js integrity="sha256-2TgBB615JeCwumIdZOFkjoEZUjtvjP9GzfJAEXkhoGs=" crossorigin=anonymous></script><script defer src=/js/search/list.min.19c27643078f3be0fb15443fbb4903ffe3234c85373d568e3de7478ac45b16e5.js integrity="sha256-GcJ2QwePO+D7FUQ/u0kD/+MjTIU3PVaOPedHisRbFuU=" crossorigin=anonymous></script></head><body class="site-kind-page site-type-posts site-layout-post" dir=ltr><div class=scroll-progress><div class=scroll-progress-bar></div></div><label class=image-overlay-wrap for=image-overlay-toggle><input class="hidden toggle" type=checkbox id=image-overlay-toggle><div class=image-overlay></div></label><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><div class=menu-overlay></div><main class=container><aside class=site-menu aria-label=메뉴 data-i18n-id=menu.aside.tooltip data-i18n-attrs=aira-label><div class=menu-content><nav><div class=menu-profile><div class=profile-image-wrap><a href=https://minyeamer.github.io/><img src=/images/profile/menu.jpg alt=Profile class=profile-image decoding=async></a></div><h2 class=profile-title><a class="flex align-center" href=https://minyeamer.github.io/><span>Minystory</span></a></h2></div><div class=menu-links><a href=https://github.com/minyeamer target=_blank id=github-link title=GitHub><i class="fa-brands fa-github"></i>
</a><a href=/categories/ id=categories-link title=Categories><i class="fa-solid fa-folder"></i>
</a><a href=/tags/ id=tags-link title=Tags><i class="fa-solid fa-tags"></i>
</a><button class=dark-mode-toggle id=theme-toggle-button aria-label="Toggle color scheme">
<i class="fa-solid fa-circle-half-stroke"></i></button></div><div class=menu-search id=search-input data-hotkeys=s/ role=button tabindex=0 aria-label=검색 data-i18n-id=search.action.label data-i18n-attrs=aira-label><i class="fa-solid fa-magnifying-glass search-icon"></i>
<span class=search-placeholder data-i18n-id=search.input.label data-i18n-text='{"`/`": "$code"}'><code>/</code> 를 눌러 검색하세요</span></div><div class=menu-categories><input type=checkbox class="hidden toggle" id=categories-control checked>
<label for=categories-control class="categories-label categories-toggle"><a href=/categories/ class=categories-root><i class="fa-solid fa-folder"></i>
<span data-i18n-id=categories.root.label data-i18n-text>전체
</span><span class=category-count>(41)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li><input type=checkbox class="hidden toggle" id=cat-algorithm>
<label for=cat-algorithm class="categories-label categories-toggle"><a href="/search/?category1=Algorithm"><i class="fa-solid fa-folder"></i>Algorithm<span class=category-count>(4)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Algorithm&category2=Graph"><i class="fa-solid fa-file"></i>Graph<span class=category-count>(1)</span></a></li><li class=categories-label><a href="/search/?category1=Algorithm&category2=Python"><i class="fa-solid fa-file"></i>Python<span class=category-count>(2)</span></a></li><li class=categories-label><a href="/search/?category1=Algorithm&category2=SQL"><i class="fa-solid fa-file"></i>SQL<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-cloud>
<label for=cat-cloud class="categories-label categories-toggle"><a href="/search/?category1=Cloud"><i class="fa-solid fa-folder"></i>Cloud<span class=category-count>(2)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Cloud&category2=Kubernetes"><i class="fa-solid fa-file"></i>Kubernetes<span class=category-count>(2)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-data-analysis>
<label for=cat-data-analysis class="categories-label categories-toggle"><a href="/search/?category1=Data%20Analysis"><i class="fa-solid fa-folder"></i>Data Analysis<span class=category-count>(3)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Data%20Analysis&category2=Dacon"><i class="fa-solid fa-file"></i>Dacon<span class=category-count>(3)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-data-engineering>
<label for=cat-data-engineering class="categories-label categories-toggle"><a href="/search/?category1=Data%20Engineering"><i class="fa-solid fa-folder"></i>Data Engineering<span class=category-count>(19)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Data%20Engineering&category2=Apache%20Airflow"><i class="fa-solid fa-file"></i>Apache Airflow<span class=category-count>(7)</span></a></li><li class=categories-label><a href="/search/?category1=Data%20Engineering&category2=Apache%20Spark"><i class="fa-solid fa-file"></i>Apache Spark<span class=category-count>(8)</span></a></li><li class=categories-label><a href="/search/?category1=Data%20Engineering&category2=Crawling"><i class="fa-solid fa-file"></i>Crawling<span class=category-count>(4)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-frontend>
<label for=cat-frontend class="categories-label categories-toggle"><a href="/search/?category1=Frontend"><i class="fa-solid fa-folder"></i>Frontend<span class=category-count>(10)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Frontend&category2=Blog"><i class="fa-solid fa-file"></i>Blog<span class=category-count>(10)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-linux>
<label for=cat-linux class="categories-label categories-toggle"><a href="/search/?category1=Linux"><i class="fa-solid fa-folder"></i>Linux<span class=category-count>(1)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Linux&category2=Ubuntu"><i class="fa-solid fa-file"></i>Ubuntu<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-project>
<label for=cat-project class="categories-label categories-toggle"><a href="/search/?category1=Project"><i class="fa-solid fa-folder"></i>Project<span class=category-count>(2)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Project&category2=Open%20Source"><i class="fa-solid fa-file"></i>Open Source<span class=category-count>(1)</span></a></li><li class=categories-label><a href="/search/?category1=Project&category2=Tools"><i class="fa-solid fa-file"></i>Tools<span class=category-count>(1)</span></a></li></ul></li></ul></div><div class=recent-posts><div class=recent-post-label><i class="fa-solid fa-clock"></i>
<span data-i18n-id=menu.recent.posts data-i18n-text>최신글</span></div><ul class=recent-post-list><li class=recent-post-item><a href=/blog/hugo-seotax-1/ title="Hugo 서택스(SeoTax) 테마 제작기 - 동적 렌더링으로 확장된 검색"><div class=recent-post-title>Hugo 서택스(SeoTax) 테마 제작기 - 동적 렌더링으로 확장된 검색</div><div class=recent-post-date><time datetime=2026-01-25>2026.01.25</time></div></a></li><li class=recent-post-item><a href=/blog/hugo-blog-5/ title="Hugo Disqus 댓글 연동 - 본문 레이아웃 개선하기 (헤더/푸터)"><div class=recent-post-title>Hugo Disqus 댓글 연동 - 본문 레이아웃 개선하기 (헤더/푸터)</div><div class=recent-post-date><time datetime=2025-12-15>2025.12.15</time></div></a></li><li class=recent-post-item><a href=/blog/hugo-blog-4/ title="Hugo 검색 기능 구현 - Fuse.js로 검색 인덱스 최적화하기"><div class=recent-post-title>Hugo 검색 기능 구현 - Fuse.js로 검색 인덱스 최적화하기</div><div class=recent-post-date><time datetime=2025-12-14>2025.12.14</time></div></a></li><li class=recent-post-item><a href=/blog/hugo-blog-3/ title="Hugo 분류(Taxonomies) 커스터마이징 - 태그/카테고리 템플릿 구현"><div class=recent-post-title>Hugo 분류(Taxonomies) 커스터마이징 - 태그/카테고리 템플릿 구현</div><div class=recent-post-date><time datetime=2025-11-22>2025.11.22</time></div></a></li><li class=recent-post-item><a href=/blog/hugo-blog-2/ title="Hugo Book 테마 커스터마이징 - 메뉴/목차/헤더 레이아웃 개선"><div class=recent-post-title>Hugo Book 테마 커스터마이징 - 메뉴/목차/헤더 레이아웃 개선</div><div class=recent-post-date><time datetime=2025-11-04>2025.11.04</time></div></a></li></ul></div></nav><script>(function(){var e=document.querySelector("aside .menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=main-wrap><header class=site-header><div class="flex align-center justify-between"><label for=menu-control aria-label="메뉴 접기/펼치기" title="메뉴 접기/펼치기" data-i18n-id=menu.toggle.tooltip data-i18n-attrs=aria-label,title><i class="fa-solid fa-bars menu-icon" id=menu-icon></i></label><div class=mobile-search id=mobile-search-input role=button tabindex=0 aria-label=검색 data-i18n-id=search.action.label data-i18n-attrs=aria-label><i class="fa-solid fa-magnifying-glass search-icon"></i>
<span class=mobile-title>Minystory</span>
<i class=mobile-title-pad></i></div><label for=toc-control aria-label="목차 접기/펼치기" title="목차 접기/펼치기" data-i18n-id=toc.toggle.tooltip data-i18n-attrs=aria-label,title><i class="fa-solid fa-list menu-icon" id=toc-icon></i></label></div></header><article class="content-wrap markdown" id=content-wrap><header class=content-header><div class=content-category><a href="/search/?category1=Data%20Engineering&category2=Apache%20Spark" class=content-category-link>Data Engineering/Apache Spark</a></div><h1 class=content-title>Apache Spark - 다양한 데이터 소스 읽기/쓰기 (Parquet, JSON, CSV, Avro)</h1><div class=content-datetime><time datetime=2025-07-10T23:56:54+09:00>2025. 07. 10. 23:56
</time><span id=reading-time></span></div></header><div class=content-cover-wrap><img src="https://dl.dropboxusercontent.com/scl/fi/iafnblb6k95kbw7bwn2xj/spark-00-cover.webp?rlkey=6995tacnu3mvr7s31akl5sca6&amp;raw=1" class=content-cover alt="Cover Image" decoding=async></div><div id=series-anchor></div><div class=sc-series><div class=series-bookmark><svg width="32" height="48" fill="currentColor" viewBox="0 0 32 48" class="series-corner-image"><path fill="currentColor" d="M32 0H0v48h.163l16-16L32 47.836V0z"/></svg></div><div class=series-header><h2 class=series-title>Apache Spark 배우기</h2></div><input type=checkbox id=series-toggle class=series-toggle-input hidden><div class=series-content><ol class=series-list><li class=series-item><span class=series-item-index>1.</span>
<a href=/blog/spark-study-1/#series-anchor>스파크의 기본 개념과 아키텍처</a></li><li class=series-item><span class=series-item-index>2.</span>
<a href=/blog/spark-study-2/#series-anchor>로컬 환경에서 설치하고 PySpark 실행하기</a></li><li class=series-item><span class=series-item-index>3.</span>
<a href=/blog/spark-study-3/#series-anchor>스파크 애플리케이션 구조와 RDD 이해하기</a></li><li class=series-item><span class=series-item-index>4.</span>
<a href=/blog/spark-study-4/#series-anchor>DataFrame과 Dataset API 활용하기</a></li><li class=series-item><span class=series-item-index>5.</span>
<a href=/blog/spark-study-5/#series-anchor>스파크 SQL과 테이블/뷰 관리</a></li><li class="series-item active"><span class=series-item-index>6.</span>
<a href=/blog/spark-study-6/#series-anchor>다양한 데이터 소스 읽기/쓰기 (Parquet, JSON, CSV, Avro)</a></li><li class=series-item><span class=series-item-index>7.</span>
<a href=/blog/spark-study-7/#series-anchor>외부 데이터베이스 연동 (PostgreSQL, MySQL)</a></li><li class=series-item><span class=series-item-index>8.</span>
<a href=/blog/spark-study-8/#series-anchor>사용자 정의 함수(UDF)와 고차 함수 활용하기</a></li></ol></div><div class=series-footer><label for=series-toggle class=series-toggle-label><span class=series-toggle-icon><i class="fas fa-caret-up"></i></span>
<span class=series-toggle-text-hide data-i18n-id=series.hide.label data-i18n-text>숨기기</span>
<span class=series-toggle-text-show data-i18n-id=series.show.label data-i18n-text>목록 보기</span></label><div class=series-nav><span class=series-nav-info>6/8</span><div class=series-nav-buttons><a href=/blog/spark-study-5/#series-anchor class=series-nav-button><i class="fas fa-chevron-left"></i></a>
<a href=/blog/spark-study-7/#series-anchor class=series-nav-button><i class="fas fa-chevron-right"></i></a></div></div></div></div><script>(function(){const e=document.getElementById("series-toggle"),t="series-expanded",n=localStorage.getItem(t)??"true";n==="true"&&(e.checked=!0),e.addEventListener("change",function(){localStorage.setItem(t,this.checked)})})()</script><h2 id=data-source-api>Data Source API
<a class=anchor href=#data-source-api>#</a></h2><h3 id=dataframereader>DataFrameReader
<a class=anchor href=#dataframereader>#</a></h3><p>DataFrameReader는 데이터 소스에서 DataFrame으로 데이터를 읽는 방식이다. 아래와 같이 권장되는 사용 패턴이 있다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>DataFrameReader</span>
</span></span><span class=line><span class=cl>	<span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>args</span><span class=p>)</span> <span class=c1># 데이터 소스 형식</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;key&#34;</span><span class=p>,</span> <span class=s2>&#34;value&#34;</span><span class=p>)</span> <span class=c1># 키/값 쌍으로 연결되는 옵션</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>schema</span><span class=p>(</span><span class=n>args</span><span class=p>)</span> <span class=c1># DDL 문자열 또는 StructType</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>load</span><span class=p>()</span> <span class=c1># 데이터 소스의 경로</span></span></span></code></pre></div></div><p>데이터 소스 형식에는 인수로 "parquet", "csv", "txt", "json", "jdbc", "orc", "avro" 등이 전달된다.
기본값은 "parquet" 또는 <code>spark.sql.sources.default</code> 에 지정된 항목이 설정된다.</p><p>JSON이나 CSV 형식은 <code>option()</code> 함수에서 스키마를 유추하는 <code>inferSchema</code> 옵션을 적용할 수 있지만,
스키마를 제공하면 로드 속도가 빨라진다.</p><p>SparkSession 인스턴스를 통해서 DataFrame에 액세스할 경우 <code>read()</code> 또는 <code>readStream()</code> 을 사용할 수 있다.
<code>read()</code> 는 정적 데이터 소스에서 DataFrame을 읽어 오며, <code>readStream()</code> 은 스트리밍 소스에서 인스턴스를 반환한다.</p><h3 id=dataframewriter>DataFrameWriter
<a class=anchor href=#dataframewriter>#</a></h3><p>DataFrameWriter는 데이터 소스에 데이터를 저장하거나 쓰는 작업을 수행한다. 권장되는 사용 형식은 다음과 같다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>DataFrameWriter</span>
</span></span><span class=line><span class=cl>	<span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>args</span><span class=p>)</span> <span class=c1># 데이터 소스 형식</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=n>args</span><span class=p>)</span> <span class=c1># 키/값 쌍으로 연결되는 옵션</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>bucketBy</span><span class=p>(</span><span class=n>args</span><span class=p>)</span> <span class=c1># 버킷 개수 및 버킷 기준 칼럼 이름</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>partitionBy</span><span class=p>(</span><span class=n>args</span><span class=p>)</span> <span class=c1># 데이터 소스의 경로</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>path</span><span class=p>)</span> <span class=c1># 저장할 테이블</span></span></span></code></pre></div></div><p>DataFrame에서 인스턴스에 액세스할 경우 <code>write()</code> 또는 <code>writeStream()</code> 을 사용할 수 있다.</p><h2 id=data-sources>Data Sources
<a class=anchor href=#data-sources>#</a></h2><h3 id=parquet>Parquet
<a class=anchor href=#parquet>#</a></h3><p>스파크의 기본 데이터 소스인 Parquet는 다양한 I/O 최적화를 제공하는 오픈소스 칼럼 기반 파일 형식이다.
압축을 통해 저장 공간을 절약하고 데이터 칼럼에 대한 빠른 액세스를 허용한다.</p><p>Parquet 파일은 데이터 파일, 메타데이터, 여러 압축 파일 및 일부 상태 파일이 포함된 디렉터리 구조가 저장된다.
메타데이터에는 파일 형식의 버전, 스키마, 경로 등의 칼럼 데이터가 포함된다.</p><p><a href=https://github.com/databricks/LearningSparkV2/tree/master/databricks-datasets/learning-spark-v2/flights/summary-data target=_blank rel="noopener noreferrer">databricks/LearningSparkV2</a>의
<code>databricks-datasets/learning-spark-v2/flights/summary-data/parquet</code> 경로에서
<code>2010-summary.parquet/</code> 디렉터리를 가져온다.</p><p>Parquet 파일의 디렉터리에는 다음과 같은 파일 집합이 포함된다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>% ls -la data/flights/summary-data/parquet/2010-summary.parquet/
</span></span><span class=line><span class=cl>-rwxr-xr-x@ ...     <span class=m>0</span> ... _SUCCESS
</span></span><span class=line><span class=cl>-rwxr-xr-x@ ...  <span class=m>3921</span> ... part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet</span></span></code></pre></div></div><p>Parquet 파일을 DataFrame으로 읽으려면 형식과 경로를 지정하기만 하면 된다.
<code>spark.sql.sources.default</code> 설정을 하지 않았다면 <code>.format("parquet")</code> 함수는 생략해도 된다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>file</span> <span class=o>=</span> <span class=s2>&#34;data/flights/summary-data/parquet/2010-summary.parquet&#34;</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>&#34;parquet&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>file</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span></span></span></code></pre></div></div><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>+-----------------+-------------------+-----+
</span></span><span class=line><span class=cl><span class=p>|</span>DEST_COUNTRY_NAME<span class=p>|</span>ORIGIN_COUNTRY_NAME<span class=p>|</span>count<span class=p>|</span>
</span></span><span class=line><span class=cl>+-----------------+-------------------+-----+
</span></span><span class=line><span class=cl><span class=p>|</span>    United States<span class=p>|</span>            Romania<span class=p>|</span>    1<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>    United States<span class=p>|</span>            Ireland<span class=p>|</span>  264<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>    United States<span class=p>|</span>              India<span class=p>|</span>   69<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>            Egypt<span class=p>|</span>      United States<span class=p>|</span>   24<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>Equatorial Guinea<span class=p>|</span>      United States<span class=p>|</span>    1<span class=p>|</span>
</span></span><span class=line><span class=cl>+-----------------+-------------------+-----+</span></span></code></pre></div></div><p>Parquet 파일을 Spark SQL 테이블로 읽으려면 아래와 같은 스파크 SQL을 사용할 수 있다.</p><div class=sc-codeblock data-lang=sql><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">sql</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>CREATE</span><span class=w> </span><span class=k>OR</span><span class=w> </span><span class=k>REPLACE</span><span class=w> </span><span class=k>TEMPORARY</span><span class=w> </span><span class=k>VIEW</span><span class=w> </span><span class=n>delay_flights</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>	</span><span class=k>USING</span><span class=w> </span><span class=n>parquet</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=k>OPTIONS</span><span class=w> </span><span class=p>(</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    	</span><span class=n>path</span><span class=w> </span><span class=s2>&#34;data/flights/summary-data/parquet/2010-summary.parquet&#34;</span><span class=p>)</span></span></span></code></pre></div></div><p>메타데이터가 궁금해서 <code>parquet-tools</code> 라이브러리를 설치하고,
<code>inspect</code> 명령어로 <code>part-XXXX</code> 압축 파일을 조회했다. 아래와 같이 출력되었는데,
3개의 칼럼 <code>DEST_COUNTRY_NAME</code>, <code>ORIGIN_COUNTRY_NAME</code>, <code>count</code> 에 대한
데이터 타입 등의 정보가 상세히 적혀 있다. 스파크는 해당 데이터 타입을 읽기 때문에,
위 DataFrame에 <code>printSchema()</code> 출력한 결과는 아래 칼럼별 데이터 타입과 같다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>% parquet-tools inspect data/flights/summary-data/parquet/2010-summary.parquet/part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>############ file meta data ############</span>
</span></span><span class=line><span class=cl>created_by: parquet-mr <span class=o>(</span>build 32c46643845ea8a705c35d4ec8fc654cc8ff816d<span class=o>)</span>
</span></span><span class=line><span class=cl>num_columns: <span class=m>3</span>
</span></span><span class=line><span class=cl>num_rows: <span class=m>255</span>
</span></span><span class=line><span class=cl>num_row_groups: <span class=m>1</span>
</span></span><span class=line><span class=cl>format_version: 1.0
</span></span><span class=line><span class=cl>serialized_size: <span class=m>658</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>############ Columns ############</span>
</span></span><span class=line><span class=cl>DEST_COUNTRY_NAME
</span></span><span class=line><span class=cl>ORIGIN_COUNTRY_NAME
</span></span><span class=line><span class=cl>count
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>############ Column(DEST_COUNTRY_NAME) ############</span>
</span></span><span class=line><span class=cl>name: DEST_COUNTRY_NAME
</span></span><span class=line><span class=cl>path: DEST_COUNTRY_NAME
</span></span><span class=line><span class=cl>max_definition_level: <span class=m>1</span>
</span></span><span class=line><span class=cl>max_repetition_level: <span class=m>0</span>
</span></span><span class=line><span class=cl>physical_type: BYTE_ARRAY
</span></span><span class=line><span class=cl>logical_type: String
</span></span><span class=line><span class=cl>converted_type <span class=o>(</span>legacy<span class=o>)</span>: UTF8
</span></span><span class=line><span class=cl>compression: GZIP <span class=o>(</span>space_saved: 37%<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>############ Column(ORIGIN_COUNTRY_NAME) ############</span>
</span></span><span class=line><span class=cl>name: ORIGIN_COUNTRY_NAME
</span></span><span class=line><span class=cl>path: ORIGIN_COUNTRY_NAME
</span></span><span class=line><span class=cl>max_definition_level: <span class=m>1</span>
</span></span><span class=line><span class=cl>max_repetition_level: <span class=m>0</span>
</span></span><span class=line><span class=cl>physical_type: BYTE_ARRAY
</span></span><span class=line><span class=cl>logical_type: String
</span></span><span class=line><span class=cl>converted_type <span class=o>(</span>legacy<span class=o>)</span>: UTF8
</span></span><span class=line><span class=cl>compression: GZIP <span class=o>(</span>space_saved: 39%<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>############ Column(count) ############</span>
</span></span><span class=line><span class=cl>name: count
</span></span><span class=line><span class=cl>path: count
</span></span><span class=line><span class=cl>max_definition_level: <span class=m>1</span>
</span></span><span class=line><span class=cl>max_repetition_level: <span class=m>0</span>
</span></span><span class=line><span class=cl>physical_type: INT64
</span></span><span class=line><span class=cl>logical_type: None
</span></span><span class=line><span class=cl>converted_type <span class=o>(</span>legacy<span class=o>)</span>: NONE
</span></span><span class=line><span class=cl>compression: GZIP <span class=o>(</span>space_saved: 53%<span class=o>)</span></span></span></code></pre></div></div><p>DataFrame을 DataFrameWriter를 사용해 Parquet 파일로 저장할 때는 아래와 같은 함수를 사용할 수 있다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>&#34;parquet&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=o>.</span><span class=n>mode</span><span class=p>(</span><span class=s2>&#34;overwrite&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;compression&#34;</span><span class=p>,</span> <span class=s2>&#34;snappy&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>&#34;/tmp/data/parquet/df_parquet&#34;</span><span class=p>))</span></span></span></code></pre></div></div><p>압축 방식으로 <code>snappy</code> 를 사용하여 <code>snappy</code> 압축 파일이 생성된다.
DataFrame을 직접 저장하면 아래와 같은 파일이 생성된다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>% ls -la /tmp/data/parquet/df_parquet/
</span></span><span class=line><span class=cl>-rw-r--r--@ ...     <span class=m>8</span> ... ._SUCCESS.crc
</span></span><span class=line><span class=cl>-rw-r--r--@ ...    <span class=m>52</span> ... .part-00000-9828b287-9956-40cb-9c33-d59bea52e5be-c000.snappy.parquet.crc
</span></span><span class=line><span class=cl>-rw-r--r--@ ...     <span class=m>0</span> ... _SUCCESS
</span></span><span class=line><span class=cl>-rw-r--r--@ ...  <span class=m>5442</span> ... part-00000-9828b287-9956-40cb-9c33-d59bea52e5be-c000.snappy.parquet</span></span></code></pre></div></div><h3 id=json>JSON
<a class=anchor href=#json>#</a></h3><p>JSON 데이터 형식은 XML에 비해 읽기 쉽고, 구문을 분석하기 쉬운 형식이다.
단일 라인 모드와 다중 라인 모드를 모두 지원한다. 단일 라인 모드에서는 각 라인이 단일 JSON 개체를 나타내지만,
다중 라인 모드에서는 전체 라인 객체가 단일 JSON 개체를 구성한다. <code>option()</code> 함수에서
<code>multiLine</code> 옵션에 <code>ture</code> 또는 <code>false</code> 를 설정할 수 있다.</p><p>단일 라인 모드의 JSON 데이터는 아래와 같이 구성된다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>% head -n <span class=m>5</span> data/flights/summary-data/json/2010-summary.json
</span></span><span class=line><span class=cl><span class=o>{</span><span class=s2>&#34;ORIGIN_COUNTRY_NAME&#34;</span>:<span class=s2>&#34;Romania&#34;</span>,<span class=s2>&#34;DEST_COUNTRY_NAME&#34;</span>:<span class=s2>&#34;United States&#34;</span>,<span class=s2>&#34;count&#34;</span>:1<span class=o>}</span>
</span></span><span class=line><span class=cl><span class=o>{</span><span class=s2>&#34;ORIGIN_COUNTRY_NAME&#34;</span>:<span class=s2>&#34;Ireland&#34;</span>,<span class=s2>&#34;DEST_COUNTRY_NAME&#34;</span>:<span class=s2>&#34;United States&#34;</span>,<span class=s2>&#34;count&#34;</span>:264<span class=o>}</span>
</span></span><span class=line><span class=cl><span class=o>{</span><span class=s2>&#34;ORIGIN_COUNTRY_NAME&#34;</span>:<span class=s2>&#34;India&#34;</span>,<span class=s2>&#34;DEST_COUNTRY_NAME&#34;</span>:<span class=s2>&#34;United States&#34;</span>,<span class=s2>&#34;count&#34;</span>:69<span class=o>}</span>
</span></span><span class=line><span class=cl><span class=o>{</span><span class=s2>&#34;ORIGIN_COUNTRY_NAME&#34;</span>:<span class=s2>&#34;United States&#34;</span>,<span class=s2>&#34;DEST_COUNTRY_NAME&#34;</span>:<span class=s2>&#34;Egypt&#34;</span>,<span class=s2>&#34;count&#34;</span>:24<span class=o>}</span>
</span></span><span class=line><span class=cl><span class=o>{</span><span class=s2>&#34;ORIGIN_COUNTRY_NAME&#34;</span>:<span class=s2>&#34;United States&#34;</span>,<span class=s2>&#34;DEST_COUNTRY_NAME&#34;</span>:<span class=s2>&#34;Equatorial Guinea&#34;</span>,<span class=s2>&#34;count&#34;</span>:1<span class=o>}</span></span></span></code></pre></div></div><p>JSON 파일을 DataFrame으로 읽으려면 아래처럼 <code>.format("json")</code> 을 지정한다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>file</span> <span class=o>=</span> <span class=s2>&#34;data/flights/summary-data/json/*&#34;</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>&#34;json&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>file</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span></span></span></code></pre></div></div><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>+-----------------+-------------------+-----+
</span></span><span class=line><span class=cl><span class=p>|</span>DEST_COUNTRY_NAME<span class=p>|</span>ORIGIN_COUNTRY_NAME<span class=p>|</span>count<span class=p>|</span>
</span></span><span class=line><span class=cl>+-----------------+-------------------+-----+
</span></span><span class=line><span class=cl><span class=p>|</span>    United States<span class=p>|</span>            Romania<span class=p>|</span>   15<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>    United States<span class=p>|</span>            Croatia<span class=p>|</span>    1<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>    United States<span class=p>|</span>            Ireland<span class=p>|</span>  344<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>            Egypt<span class=p>|</span>      United States<span class=p>|</span>   15<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>    United States<span class=p>|</span>              India<span class=p>|</span>   62<span class=p>|</span>
</span></span><span class=line><span class=cl>+-----------------+-------------------+-----+</span></span></code></pre></div></div><p>JSON 데이터 소스에 대해 DataFrameReader 및 DataFrameWriter 에 대한 일반적인 옵션은 아래와 같다.</p><ul><li><code>compression</code> : 압축 코덱을 쓰기 시에 사용할 수 있다. <code>bzip2</code>, <code>gzip</code>, <code>snappy</code> 등이 값으로 전달된다.</li><li><code>dateFormat</code> : Java의 DateTimeFormatter에서 제공하는 모든 형식을 사용할 수 있다. (yyyy-MM-dd 등)</li><li><code>multiLine</code> : <code>true</code> 를 지정하면 다중 라인 모드를 사용한다. 기본값은 <code>false</code> 이다.</li><li><code>allowUnquotedFileNames</code> : 따옴표로 묶이지 않은 JSON 필드 이름을 허용한다. 기본값은 <code>false</code> 이다.</li></ul><h3 id=csv>CSV
<a class=anchor href=#csv>#</a></h3><p>쉼표로 각 데이터 또는 필드를 구분하며, 쉼표로 구분된 각 줄은 레코드를 나타낸다.
쉼표가 데이터의 일부인 경우, 값을 쌍따옴표로 감싸주거나, 다른 구분 기호를 사용하여 필드를 분리할 수 있다.</p><p>일반적인 CSV 데이터는 아래와 같이 구성된다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>% head -n <span class=m>5</span> data/flights/summary-data/csv/2010-summary.csv          
</span></span><span class=line><span class=cl>DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count
</span></span><span class=line><span class=cl>United States,Romania,1
</span></span><span class=line><span class=cl>United States,Ireland,264
</span></span><span class=line><span class=cl>United States,India,69
</span></span><span class=line><span class=cl>Egypt,United States,24</span></span></code></pre></div></div><p>CSV 파일을 DataFrame으로 읽으려면 아래처럼 <code>.format("csv")</code> 을 지정한다.
위 파일과 같이 헤더가 있는 경우 <code>header</code> 옵션에 <code>true</code> 를 설정한다.
<code>nullValue</code> 옵션을 사용해 <code>null</code> 데이터를 특정 값으로 교체할 수 있다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>file</span> <span class=o>=</span> <span class=s2>&#34;data/flights/summary-data/csv/*&#34;</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>&#34;csv&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;header&#34;</span><span class=p>,</span> <span class=s2>&#34;true&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>file</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span></span></span></code></pre></div></div><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>+-----------------+-------------------+-----+
</span></span><span class=line><span class=cl><span class=p>|</span>DEST_COUNTRY_NAME<span class=p>|</span>ORIGIN_COUNTRY_NAME<span class=p>|</span>count<span class=p>|</span>
</span></span><span class=line><span class=cl>+-----------------+-------------------+-----+
</span></span><span class=line><span class=cl><span class=p>|</span>    United States<span class=p>|</span>            Romania<span class=p>|</span>    1<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>    United States<span class=p>|</span>            Ireland<span class=p>|</span>  264<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>    United States<span class=p>|</span>              India<span class=p>|</span>   69<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>            Egypt<span class=p>|</span>      United States<span class=p>|</span>   24<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>Equatorial Guinea<span class=p>|</span>      United States<span class=p>|</span>    1<span class=p>|</span>
</span></span><span class=line><span class=cl>+-----------------+-------------------+-----+</span></span></code></pre></div></div><p>CSV 데이터 소스에 대해 DataFrameReader 및 DataFrameWriter 에 대한 일반적인 옵션은 아래와 같다.</p><ul><li><code>compression</code> : 압축 코덱을 쓰기 시에 사용할 수 있다. <code>bzip2</code>, <code>gzip</code>, <code>snappy</code> 등이 값으로 전달된다.</li><li><code>dateFormat</code> : Java의 DateTimeFormatter에서 제공하는 모든 형식을 사용할 수 있다. (yyyy-MM-dd 등)</li><li><code>multiLine</code> : <code>true</code> 를 지정하면 다중 라인 모드를 사용한다. 기본값은 <code>false</code> 이다.</li><li><code>interSchema</code> : <code>true</code> 를 지정하면 스파크가 칼럼 데이터 유형을 결정한다. 기본값은 <code>false</code> 이다.</li><li><code>sep</code> : 칼럼을 구분하기 위한 문자이며, 기본 구분 기호는 쉼표(,)다.</li><li><code>escape</code> : 따옴표를 이스케이프할 때 사용하는 문자이며, 기본값은 / 다.</li><li><code>header</code> : 첫 번째 줄이 칼럼명을 나타내는 헤더일 경우 <code>true</code> 를 지정하고, 기본값은 <code>false</code> 이다.</li></ul><h3 id=avro>Avro
<a class=anchor href=#avro>#</a></h3><p>스파크 2.4에 내장된 데이터 소스인 Avro 형식은 특히 아파치 카프카에서
메시지를 직렬화 및 역직렬화할 때 사용된다. JSON에 대한 직접 매핑, 속도와 효율성 등 많은 이점을 제공한다.</p><p>스파크 공식 문서의 <a href=https://spark.apache.org/docs/latest/sql-data-sources-avro.html target=_blank rel="noopener noreferrer">Apache Avro Data Source Guide</a>에 따르면,
<code>spark-avro</code> 모듈은 외부 모듈로 <code>spark-submit</code> 또는 <code>spark-shell</code> 에 포함되어 있지 않다.
따라서, 아래와 같이 <code>--packages</code> 를 사용하여 종속성을 추가할 수 있다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>./bin/spark-shell --packages org.apache.spark:spark-avro_2.13:4.0.0 ...</span></span></code></pre></div></div><p>SparkSession 인스턴스를 사용할 경우, <code>spark.jars.packages</code> 설정에 <code>spark-avro_2.13</code> 종속성을 추가한다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>SparkSession</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>spark</span> <span class=o>=</span> <span class=p>(</span><span class=n>SparkSession</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>builder</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>&#34;SparkAvroExampleApp&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>&#34;spark.jars.packages&#34;</span><span class=p>,</span> <span class=s2>&#34;org.apache.spark:spark-avro_2.13:4.0.0&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>())</span></span></span></code></pre></div></div><p>Avro 파일을 DataFrame으로 읽으려면 아래처럼 <code>.format("avro")</code> 을 지정한다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>file</span> <span class=o>=</span> <span class=s2>&#34;data/flights/summary-data/avro/*&#34;</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>&#34;avro&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>file</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span></span></span></code></pre></div></div><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>+-----------------+-------------------+-----+
</span></span><span class=line><span class=cl><span class=p>|</span>DEST_COUNTRY_NAME<span class=p>|</span>ORIGIN_COUNTRY_NAME<span class=p>|</span>count<span class=p>|</span>
</span></span><span class=line><span class=cl>+-----------------+-------------------+-----+
</span></span><span class=line><span class=cl><span class=p>|</span>    United States<span class=p>|</span>            Romania<span class=p>|</span>    1<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>    United States<span class=p>|</span>            Ireland<span class=p>|</span>  264<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>    United States<span class=p>|</span>              India<span class=p>|</span>   69<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>            Egypt<span class=p>|</span>      United States<span class=p>|</span>   24<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>Equatorial Guinea<span class=p>|</span>      United States<span class=p>|</span>    1<span class=p>|</span>
</span></span><span class=line><span class=cl>+-----------------+-------------------+-----+</span></span></code></pre></div></div><p>Avro 데이터 소스에 대해 DataFrameReader 및 DataFrameWriter 에 대한 일반적인 옵션은 아래와 같다.</p><ul><li><code>avroSchema</code> : JSON 형식으로 제공할 수 있는 Avro 스키마이다. Avro 데이터나 카탈리스트 데이터와 일치하지 않으면 읽기/쓰기 작업이 실패한다.</li><li><code>recordName</code> : Avro 사양에 필요한 쓰기 결과의 최상위 레코드명이다.</li><li><code>recrodNamespace</code> : 쓰기 결과에 네임스페이스를 기록한다.</li><li><code>ignoreExtension</code> : 확장자가 <code>.avro</code> 인지 여부에 관계없이 모든 파일을 읽어들인다.</li><li><code>compression</code> : 쓰기에 사용할 압축 코덱을 지정할 수 있다.</li></ul><h3 id=image>Image
<a class=anchor href=#image>#</a></h3><p>스파크 2.4에서 머신러닝 프레임워크를 지원하기 위해 새로운 데이터 소스인 이미지 파일을 도입했다.</p><p><a href=https://github.com/databricks/LearningSparkV2/tree/master/databricks-datasets/learning-spark-v2 target=_blank rel="noopener noreferrer">databricks/LearningSparkV2</a>의
<code>databricks-datasets/learning-spark-v2</code> 경로에서 <code>cctvVideos/</code> 디렉터리를 가져온다.
해당 디렉터리의 구조는 다음과 같다.</p><div class=sc-codeblock data-lang><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">text</span></div><pre tabindex=0><code>cctvVideos/
├── README.md
└── train_images/
    ├── label=0/
    │   ├── Browse2frame0000.jpg
    │   ├── Browse2frame0001.jpg
    │   ├── Browse2frame0002.jpg
    │   ├── ...
    |   └── Walk2frame0042.jpg
    └── label=1/
        ├── Fight_Chaseframe0012.jpg
        ├── Fight_Chaseframe0013.jpg
        ├── Fight_Chaseframe0014.jpg
        ├── ...
        └── Rest_WiggleOnFloorframe0050.jpg</code></pre></div><p>이미지 파일은 아래와 같이 DataFrameReader 함수로 읽을 수 있다.
이미지 파일을 읽을 때 <code>numpy</code> 라이브러리가 필요하다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.ml</span> <span class=kn>import</span> <span class=n>image</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>image_dir</span> <span class=o>=</span> <span class=s2>&#34;data/cctvVideos/train_images&#34;</span>
</span></span><span class=line><span class=cl><span class=n>images_df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>&#34;image&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>image_dir</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>images_df</span><span class=o>.</span><span class=n>printSchema</span><span class=p>()</span></span></span></code></pre></div></div><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root
</span></span><span class=line><span class=cl> <span class=p>|</span>-- image: struct <span class=o>(</span><span class=nv>nullable</span> <span class=o>=</span> <span class=nb>true</span><span class=o>)</span>
</span></span><span class=line><span class=cl> <span class=p>|</span>    <span class=p>|</span>-- origin: string <span class=o>(</span><span class=nv>nullable</span> <span class=o>=</span> <span class=nb>true</span><span class=o>)</span>
</span></span><span class=line><span class=cl> <span class=p>|</span>    <span class=p>|</span>-- height: integer <span class=o>(</span><span class=nv>nullable</span> <span class=o>=</span> <span class=nb>true</span><span class=o>)</span>
</span></span><span class=line><span class=cl> <span class=p>|</span>    <span class=p>|</span>-- width: integer <span class=o>(</span><span class=nv>nullable</span> <span class=o>=</span> <span class=nb>true</span><span class=o>)</span>
</span></span><span class=line><span class=cl> <span class=p>|</span>    <span class=p>|</span>-- nChannels: integer <span class=o>(</span><span class=nv>nullable</span> <span class=o>=</span> <span class=nb>true</span><span class=o>)</span>
</span></span><span class=line><span class=cl> <span class=p>|</span>    <span class=p>|</span>-- mode: integer <span class=o>(</span><span class=nv>nullable</span> <span class=o>=</span> <span class=nb>true</span><span class=o>)</span>
</span></span><span class=line><span class=cl> <span class=p>|</span>    <span class=p>|</span>-- data: binary <span class=o>(</span><span class=nv>nullable</span> <span class=o>=</span> <span class=nb>true</span><span class=o>)</span>
</span></span><span class=line><span class=cl> <span class=p>|</span>-- label: integer <span class=o>(</span><span class=nv>nullable</span> <span class=o>=</span> <span class=nb>true</span><span class=o>)</span></span></span></code></pre></div></div><p>이미지의 높이 및 너비와 같은 정보는 아래와 같이 조회할 수 있다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>images_df</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=s2>&#34;image.height&#34;</span><span class=p>,</span> <span class=s2>&#34;image.width&#34;</span><span class=p>,</span> <span class=s2>&#34;image.nChannels&#34;</span><span class=p>,</span> <span class=s2>&#34;image.mode&#34;</span><span class=p>,</span> <span class=s2>&#34;label&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span></span></span></code></pre></div></div><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>+------+-----+---------+----+-----+
</span></span><span class=line><span class=cl><span class=p>|</span>height<span class=p>|</span>width<span class=p>|</span>nChannels<span class=p>|</span>mode<span class=p>|</span>label<span class=p>|</span>
</span></span><span class=line><span class=cl>+------+-----+---------+----+-----+
</span></span><span class=line><span class=cl><span class=p>|</span>   288<span class=p>|</span>  384<span class=p>|</span>        3<span class=p>|</span>  16<span class=p>|</span>    0<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>   288<span class=p>|</span>  384<span class=p>|</span>        3<span class=p>|</span>  16<span class=p>|</span>    1<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>   288<span class=p>|</span>  384<span class=p>|</span>        3<span class=p>|</span>  16<span class=p>|</span>    0<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>   288<span class=p>|</span>  384<span class=p>|</span>        3<span class=p>|</span>  16<span class=p>|</span>    0<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>   288<span class=p>|</span>  384<span class=p>|</span>        3<span class=p>|</span>  16<span class=p>|</span>    0<span class=p>|</span>
</span></span><span class=line><span class=cl>+------+-----+---------+----+-----+</span></span></code></pre></div></div><h3 id=binary-file>Binary File
<a class=anchor href=#binary-file>#</a></h3><p>이진 파일을 읽으려면 데이터 소스 형식을 <code>binaryFile</code> 로 지정해야 한다.
DataFrameReader는 이진 파일을 원본 내용과 메타데이터를 포함하는 단일 DataFrame 행으로 변환한다.</p><p><code>pathGlobFilter</code> 를 사용하면 지정된 전역 패턴과 일치하는 경로로 파일을 로드할 수 있다.
아래 코드는 디렉터리에서 모든 <code>.jpg</code> 파일을 읽는다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>path</span> <span class=o>=</span> <span class=s2>&#34;data/cctvVideos/train_images&#34;</span>
</span></span><span class=line><span class=cl><span class=n>binary_files_df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>&#34;binaryFile&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;pathGlobFilter&#34;</span><span class=p>,</span> <span class=s2>&#34;*.jpg&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>path</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>binary_files_df</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span></span></span></code></pre></div></div><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>+--------------------+-------------------+------+--------------------+-----+
</span></span><span class=line><span class=cl><span class=p>|</span>                path<span class=p>|</span>   modificationTime<span class=p>|</span>length<span class=p>|</span>             content<span class=p>|</span>label<span class=p>|</span>
</span></span><span class=line><span class=cl>+--------------------+-------------------+------+--------------------+-----+
</span></span><span class=line><span class=cl><span class=p>|</span>file:/Users/cuz/D...<span class=p>|</span>2025-01-28 13:30:40<span class=p>|</span> 55037<span class=p>|</span><span class=o>[</span>FF D8 FF E0 <span class=m>00</span> 1...<span class=p>|</span>    0<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>file:/Users/cuz/D...<span class=p>|</span>2025-01-28 13:30:40<span class=p>|</span> 54634<span class=p>|</span><span class=o>[</span>FF D8 FF E0 <span class=m>00</span> 1...<span class=p>|</span>    1<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>file:/Users/cuz/D...<span class=p>|</span>2025-01-28 13:30:40<span class=p>|</span> 54624<span class=p>|</span><span class=o>[</span>FF D8 FF E0 <span class=m>00</span> 1...<span class=p>|</span>    0<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>file:/Users/cuz/D...<span class=p>|</span>2025-01-28 13:30:40<span class=p>|</span> 54505<span class=p>|</span><span class=o>[</span>FF D8 FF E0 <span class=m>00</span> 1...<span class=p>|</span>    0<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>file:/Users/cuz/D...<span class=p>|</span>2025-01-28 13:30:40<span class=p>|</span> 54475<span class=p>|</span><span class=o>[</span>FF D8 FF E0 <span class=m>00</span> 1...<span class=p>|</span>    0<span class=p>|</span>
</span></span><span class=line><span class=cl>+--------------------+-------------------+------+--------------------+-----+</span></span></code></pre></div></div><h2 id=references>References
<a class=anchor href=#references>#</a></h2><ul><li><a href=https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ target=_blank rel="noopener noreferrer">https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/</a></li><li><a href=https://github.com/databricks/LearningSparkV2/tree/master/databricks-datasets/learning-spark-v2/flights/summary-data target=_blank rel="noopener noreferrer">https://github.com/databricks/LearningSparkV2/tree/master/databricks-datasets/learning-spark-v2/flights/summary-data</a></li><li><a href=https://spark.apache.org/docs/latest/api/python/tutorial/sql/python_data_source.html target=_blank rel="noopener noreferrer">https://spark.apache.org/docs/latest/api/python/tutorial/sql/python_data_source.html</a></li><li><a href=https://spark.apache.org/docs/latest/sql-data-sources.html target=_blank rel="noopener noreferrer">https://spark.apache.org/docs/latest/sql-data-sources.html</a></li><li><a href=https://spark.apache.org/docs/latest/sql-data-sources-avro.html target=_blank rel="noopener noreferrer">https://spark.apache.org/docs/latest/sql-data-sources-avro.html</a></li></ul></article><footer class=site-footer><div class=tags-wrap><a href="/search/?tags=Apache%20Spark" class=tag>#Apache Spark</a>
<a href="/search/?tags=Data%20Sources" class=tag>#Data Sources</a>
<a href="/search/?tags=Parquet" class=tag>#Parquet</a>
<a href="/search/?tags=JSON" class=tag>#JSON</a>
<a href="/search/?tags=CSV" class=tag>#CSV</a>
<a href="/search/?tags=Avro" class=tag>#Avro</a>
<a href="/search/?tags=Spark%20SQL" class=tag>#Spark SQL</a>
<a href="/search/?tags=PySpark" class=tag>#PySpark</a>
<a href="/search/?tags=%eb%8d%b0%ec%9d%b4%ed%84%b0%20%ec%97%94%ec%a7%80%eb%8b%88%ec%96%b4%eb%a7%81" class=tag>#데이터 엔지니어링</a>
<a href="/search/?tags=%ec%8a%a4%ed%8c%8c%ed%81%ac" class=tag>#스파크</a>
<a href="/search/?tags=Study" class=tag>#Study</a></div><div class=prev-next-wrap><a href=/blog/spark-study-5/ class=prev-link><span class=prev-next-label><i class="fa-solid fa-backward"></i>
<span data-i18n-id=post.prev.link data-i18n-text>이전</span>
</span><span class=prev-next-title>Apache Spark - 스파크 SQL과 테이블/뷰 관리</span>
</a><a href=/blog/spark-study-7/ class=next-link><span class=prev-next-label><span data-i18n-id=post.next.link data-i18n-text>다음</span>
<i class="fa-solid fa-forward"></i>
</span><span class=prev-next-title>Apache Spark - 외부 데이터베이스 연동 (PostgreSQL, MySQL)</span></a></div><div class=comments-wrap><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://minyeamer.github.io/blog/spark-study-6/",this.page.identifier="https://minyeamer.github.io/blog/spark-study-6/"};window.relocateDisqus=()=>{let e=0;const t=20,n=setInterval(()=>{e++;const s=document.getElementById(atob(atob("WkdsemNYVnpYM1JvY21WaFpBPT0=")));var o=!1;if(s){const e=s.querySelector(atob(atob("VzNOeVl5bzlKMkZrY3lkZA==")));e&&(s.appendChild(e),o=!0)}(o||e>t)&&clearInterval(n)},1e3)},function(){var e=document,t=e.createElement("script");t.src="https://minyeamer.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)}(),window.relocateDisqus();function reloadDisqus(){window.DISQUS&&(DISQUS.reset({reload:!0,config:function(){this.page.url="https://minyeamer.github.io/blog/spark-study-6/",this.page.identifier="https://minyeamer.github.io/blog/spark-study-6/"}}),window.relocateDisqus())}</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div><div class="copyright-wrap flex justify-center"><small>Minystory - <a href=https://creativecommons.org/licenses/by/4.0/legalcode target=_blank rel="noopener noreferrer">© CC BY 4.0</a></small></div></footer><label for=menu-control class="hidden site-menu-overlay"></label></div><aside class=site-toc aria-label=목차 data-i18n-id=toc.aside.tooltip data-i18n-attrs=aira-label><div class=toc-content><nav id=TableOfContents><ul><li><a href=#data-source-api>Data Source API</a><ul><li><a href=#dataframereader>DataFrameReader</a></li><li><a href=#dataframewriter>DataFrameWriter</a></li></ul></li><li><a href=#data-sources>Data Sources</a><ul><li><a href=#parquet>Parquet</a></li><li><a href=#json>JSON</a></li><li><a href=#csv>CSV</a></li><li><a href=#avro>Avro</a></li><li><a href=#image>Image</a></li><li><a href=#binary-file>Binary File</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div><div class=hidden id=toc-config data-start=2 data-end=3></div></aside><div class=site-toolbar role=toolbar aria-label="툴바'" data-i18n-id=toolbar.wrap.tooltip data-i18n-attrs=aira-label><input type=checkbox id=toolbar-toggle class="hidden toggle"><div class=toolbar-items><div class="toolbar-button i18n-button"><i class="fa-solid fa-globe"></i>
<select class=i18n-selector id=i18n-selector aria-label="i18n `toolbar.i18n.tooltip` | default `Select language`" title="i18n `toolbar.i18n.tooltip` | default `Select language`" data-i18n-id=toolbar.i18n.tooltip data-i18n-attrs=aria-label,title><option value=ko>한국어</option><option value=en>English</option><option value=ja>日本語</option><option value=zh>中文</option><option value=es>Español</option><option value=fr>Français</option><option value=de>Deutsch</option><option value=it>Italiano</option><option value=pt>Português</option><option value=ru>Русский</option><option value=am>አማርኛ</option><option value=bg>Български</option><option value=bn>বাংলা</option><option value=cn>简体中文</option><option value=cs>Čeština</option><option value=fa>فارسی</option><option value=he>עברית</option><option value=nb>Norsk</option><option value=nl>Nederlands</option><option value=oc>Occitan</option><option value=pl>Polski</option><option value=pt-BR>Português (BR)</option><option value=sv>Svenska</option><option value=sw>Kiswahili</option><option value=tr>Türkçe</option><option value=uk>Українська</option><option value=zh-TW>繁體中文</option></select></div><button class=toolbar-button onclick='window.scrollTo({top:0,behavior:"smooth"})' aria-label="i18n `toolbar.top.tooltip` | default `Go to top`" title="i18n `toolbar.top.tooltip` | default `Go to top`" data-i18n-id=toolbar.top.tooltip data-i18n-attrs=aria-label,title>
<i class="fa fa-chevron-up"></i>
</button>
<button class=toolbar-button onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' aria-label="i18n `toolbar.bottom.tooltip` | default `Go to bottom`" title="i18n `toolbar.bottom.tooltip` | default `Go to bottom`" data-i18n-id=toolbar.bottom.tooltip data-i18n-attrs=aria-label,title>
<i class="fa fa-chevron-down"></i>
</button>
<button class=toolbar-button onclick=history.back() aria-label="i18n `toolbar.back.tooltip` | default `Go back`" title="i18n `toolbar.back.tooltip` | default `Go back`" data-i18n-id=toolbar.back.tooltip data-i18n-attrs=aria-label,title>
<i class="fa-solid fa-arrow-left"></i></button></div><label for=toolbar-toggle class="toolbar-button toolbar-toggle-label"><i class="fa-solid fa-plus icon-expand"></i>
<i class="fa-solid fa-xmark icon-collapse"></i></label></div></main></body></html>