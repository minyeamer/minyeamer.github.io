<!doctype html><html lang=ko-kr dir=ltr><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=keywords content="TIL,Deep Learning,PyTorch"><meta name=description content="PyTorch로 시작하는 딥 러닝 입문"><meta name=author content="minyeamer"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><link rel=canonical href=https://minyeamer.github.io/blog/2022-07-13/><link rel=icon href=https://minyeamer.github.io/images/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://minyeamer.github.io/images/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://minyeamer.github.io/images/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://minyeamer.github.io/images/favicons/apple-touch-icon.png><link rel=mask-icon href=https://minyeamer.github.io/images/favicons/apple-touch-icon.png><meta name=google-site-verification content="u1tWcHmHUZWfFT1cHaku6sqU-bK40N3WLR-C-4VUWN0"><meta name=naver-site-verification content="6eaf8e9da1a6104780f056f1a7797fe5a3a5a0da"><meta property="og:url" content="https://minyeamer.github.io/blog/2022-07-13/"><meta property="og:site_name" content="Minystory"><meta property="og:title" content="2022-07-13 Log"><meta property="og:description" content="PyTorch로 시작하는 딥 러닝 입문"><meta property="og:locale" content="ko_kr"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-07-13T20:00:00+09:00"><meta property="article:modified_time" content="2022-07-13T20:00:00+09:00"><meta property="article:tag" content="TIL"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="PyTorch"><title>2022-07-13 Log | Minystory</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://minyeamer.github.io/blog/2022-07-13/><link rel=stylesheet href=/book.min.2f0b8e358d607b091af6602f2ba7e898282882ad0bf2ef1e908b00058dda4781.css integrity="sha256-LwuONY1gewka9mAvK6fomCgogq0L8u8ekIsABY3aR4E=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/search-input.min.97bd2c69bb66aba393499c7ad9ff319905e14e001ef050bf45d8b47a9c6d9278.js integrity="sha256-l70sabtmq6OTSZx62f8xmQXhTgAe8FC/Rdi0epxtkng=" crossorigin=anonymous></script><link rel=preload href=/search-data.min.7cb06c5a504171147e5c23f8fa923c5413dc6769b4d635e327bb3c3acb570140.json as=fetch crossorigin><script>window.SEARCH_DATA_URL="/search-data.min.7cb06c5a504171147e5c23f8fa923c5413dc6769b4d635e327bb3c3acb570140.json"</script><script defer src=/search.min.f30f9834d4764fd9751da64098c954d01085f648ba9ca421a3c97582f8c47253.js integrity="sha256-8w+YNNR2T9l1HaZAmMlU0BCF9ki6nKQho8l1gvjEclM=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V")</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.1/css/all.min.css crossorigin=anonymous><script defer src=/scroll-progress.min.841ade7e507a5f6d59c4e7bf2fe2b2ca034070677ff7957eec55610a024dd776.js integrity="sha256-hBreflB6X21ZxOe/L+KyygNAcGd/95V+7FVhCgJN13Y=" crossorigin=anonymous></script><script defer src=/dark-mode.min.e41c6440ffd9967d6f6a419ff3ce09b862009fe1646ab265f5cb2817d2a508e3.js integrity="sha256-5BxkQP/Zln1vakGf884JuGIAn+FkarJl9csoF9KlCOM=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlightjs-line-numbers.js/2.9.0/highlightjs-line-numbers.min.js></script><script defer src=/copy-code.min.aaeef965f0b4992e55f976edaecb34a89d414e1791caa18c3f4f4376c6d8b5a8.js integrity="sha256-qu75ZfC0mS5V+Xbtrss0qJ1BTheRyqGMP09DdsbYtag=" crossorigin=anonymous></script><script defer src=/toc-highlightjs.093016f0ef312174ad862fdcf5792e88ab5442bd39beecc38d15643f71ab5c31.min integrity="sha256-CTAW8O8xIXSthi/c9XkuiKtUQr05vuzDjRVkP3GrXDE=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-post book-layout-post"><div class=scroll-progress><div class=scroll-progress-bar></div></div><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><div class=sidebar-profile><div class=profile-img-wrap><a href=https://minyeamer.github.io/><img src="https://avatars.githubusercontent.com/u/17109173?v=4" alt=Profile class=profile-img></a></div><div class=sidebar-social><a href=https://github.com/minyeamer target=_blank title=GitHub><i class="fa-brands fa-github"></i>
</a><a href=/categories/ title=Categories><i class="fa-solid fa-folder"></i>
</a><a href=/tags/ title=Tags><i class="fa-solid fa-tags"></i>
</a><button id=dark-mode-toggle class=dark-mode-toggle aria-label="Toggle dark mode">
<i class="fa-solid fa-circle-half-stroke"></i></button></div></div><h2 class=book-brand><a class="flex align-center" href=/><span>Minystory</span></a></h2><div class="book-search hidden"><div class=search-input-container><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/ onkeydown='event.key==="Enter"&&goToSearchPage()'>
<button type=button id=book-search-button class=book-search-btn onclick=goToSearchPage()>
<i class="fa-solid fa-magnifying-glass"></i></button></div><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden");function goToSearchPage(){const t=document.getElementById("book-search-input"),e=t.value.trim();e&&(window.location.href="/search/?q="+encodeURIComponent(e))}</script><div class=book-categories><input type=checkbox class="hidden toggle" id=categories-control checked>
<label for=categories-control class="categories-toggle categories-link"><a href=/categories/><i class="fa-solid fa-folder"></i>
<span>전체</span>
<span class=category-count>(127)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul class=categories-menu id=categories-menu><li><input type=checkbox class="hidden toggle" id=cat-algorithm>
<label for=cat-algorithm class="categories-toggle categories-link"><a href=/categories/algorithm/><i class="fa-solid fa-folder"></i>
Algorithm
<span class=category-count>(51)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/algorithm/baekjoon/><i class="fa-solid fa-file"></i>
Baekjoon
<span class=category-count>(31)</span></a></li><li class=categories-link><a href=/categories/algorithm/leetcode/><i class="fa-solid fa-file"></i>
LeetCode
<span class=category-count>(2)</span></a></li><li class=categories-link><a href=/categories/algorithm/programmers/><i class="fa-solid fa-file"></i>
Programmers
<span class=category-count>(17)</span></a></li><li class=categories-link><a href=/categories/algorithm/references/><i class="fa-solid fa-file"></i>
References
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-blog>
<label for=cat-blog class="categories-toggle categories-link"><a href=/categories/blog/><i class="fa-solid fa-folder"></i>
Blog
<span class=category-count>(5)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/blog/review/><i class="fa-solid fa-file"></i>
Review
<span class=category-count>(1)</span></a></li><li class=categories-link><a href=/categories/blog/tech/><i class="fa-solid fa-file"></i>
Tech
<span class=category-count>(4)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-book>
<label for=cat-book class="categories-toggle categories-link"><a href=/categories/book/><i class="fa-solid fa-folder"></i>
Book
<span class=category-count>(1)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/book/finance/><i class="fa-solid fa-file"></i>
Finance
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-data>
<label for=cat-data class="categories-toggle categories-link"><a href=/categories/data/><i class="fa-solid fa-folder"></i>
Data
<span class=category-count>(4)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/data/crawling/><i class="fa-solid fa-file"></i>
Crawling
<span class=category-count>(4)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-diary>
<label for=cat-diary class="categories-toggle categories-link"><a href=/categories/diary/><i class="fa-solid fa-folder"></i>
Diary
<span class=category-count>(3)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/diary/2022/><i class="fa-solid fa-file"></i>
2022
<span class=category-count>(2)</span></a></li><li class=categories-link><a href=/categories/diary/2023/><i class="fa-solid fa-file"></i>
2023
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-study>
<label for=cat-study class="categories-toggle categories-link"><a href=/categories/study/><i class="fa-solid fa-folder"></i>
Study
<span class=category-count>(61)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/study/2022/><i class="fa-solid fa-file"></i>
2022
<span class=category-count>(16)</span></a></li><li class=categories-link><a href=/categories/study/2023/><i class="fa-solid fa-file"></i>
2023
<span class=category-count>(10)</span></a></li><li class=categories-link><a href=/categories/study/ai-school/><i class="fa-solid fa-file"></i>
AI SCHOOL
<span class=category-count>(34)</span></a></li><li class=categories-link><a href=/categories/study/datacamp/><i class="fa-solid fa-file"></i>
DataCamp
<span class=category-count>(1)</span></a></li></ul></li></ul></div><div class=recent-posts><div class=recent-posts-header><i class="fa-solid fa-clock"></i>
<span>최신글</span></div><ul class=recent-posts-list><li class=recent-post-item><a href=/blog/2023-04-02/ title="2023-04-02 Log"><div class=recent-post-title>2023-04-02 Log</div><div class=recent-post-date><time datetime=2023-04-02>2023.04.02</time></div></a></li><li class=recent-post-item><a href=/blog/10000-recipe/ title="[Python] 만개의 레시피 데이터 수집"><div class=recent-post-title>[Python] 만개의 레시피 데이터 수집</div><div class=recent-post-date><time datetime=2023-03-26>2023.03.26</time></div></a></li><li class=recent-post-item><a href=/blog/2023-03-25/ title="2023-03-25 Log"><div class=recent-post-title>2023-03-25 Log</div><div class=recent-post-date><time datetime=2023-03-25>2023.03.25</time></div></a></li><li class=recent-post-item><a href=/blog/2023-03-21/ title="2023-03-21 Log"><div class=recent-post-title>2023-03-21 Log</div><div class=recent-post-date><time datetime=2023-03-21>2023.03.21</time></div></a></li><li class=recent-post-item><a href=/blog/2023-02-19/ title="2023-02-19 Log"><div class=recent-post-title>2023-02-19 Log</div><div class=recent-post-date><time datetime=2023-02-19>2023.02.19</time></div></a></li></ul></div></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><i class="fa-solid fa-bars book-icon" id=menu-icon></i></label><h3><a href=https://minyeamer.github.io/ class=site-title>Minystory</a></h3><label for=toc-control><i class="fa-solid fa-list book-icon" id=toc-icon></i></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#pytorch-packages>PyTorch Packages</a></li><li><a href=#tensor>Tensor</a></li></ul><ul><li><a href=#선형-회귀-구현>선형 회귀 구현</a></li><li><a href=#autograd>Autograd</a></li><li><a href=#다중-선형-회귀>다중 선형 회귀</a></li></ul><ul><li><a href=#torchtext-error>torchtext Error</a></li><li><a href=#make-vocabulary>Make Vocabulary</a></li></ul><ul><li><a href=#char-rnn>Char RNN</a></li><li><a href=#classification-with-gru>Classification with GRU</a></li></ul></nav><div class=book-nav><button class=book-nav-btn3 onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=book-nav-btn3 onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
<button class=book-nav-btn3 onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div></aside></header><article class="markdown book-article"><header class=post-header><div class=post-header-category><a href=/categories/study/2022/ class=post-header-category-link>Study/2022</a></div><h1 class=post-header-title>2022-07-13 Log</h1><div class=post-header-date><time datetime=2022-07-13T20:00:00+09:00>2022. 7. 13. 20:00</time></div></header><h1 id=pytorch-basic>PyTorch Basic
<a class=anchor href=#pytorch-basic>#</a></h1><h2 id=pytorch-packages>PyTorch Packages
<a class=anchor href=#pytorch-packages>#</a></h2><ul><li><code>torch</code>: 메인 네임스페이스</li><li><code>torch.autograd</code>: 자동 미분을 위한 함수</li><li><code>torch.nn</code>: 데이터 구조나 레이어 등 정의</li><li><code>torch.optim</code>: 경사 하강법 등 파라미터 최적화 알고리즘 구현</li><li><code>torch.utils.data</code>: 미니 배치용 유틸리티 함수</li><li><code>torch.onnx</code>: ONNX의 포맷으로 모델을 저장할 때 사용</li></ul><h2 id=tensor>Tensor
<a class=anchor href=#tensor>#</a></h2><ul><li>2D Tensor: (batch size, dim)</li><li>3D Tensor: (batch size, width, height)</li><li>3D Tensor(NLP): (batch size, length, dim)</li><li><code>torch.FloatTensor()</code>: 텐서 생성</li><li>행렬 곱셈(<code>.matmul()</code>), 원소 별 곱셈(<code>.mul()</code>, <code>*</code>)</li><li><code>dim=0</code>: 첫번째 차원(행) 제거 = 같은 열끼리 연산, <code>dim=1</code>: 두번째 차원(열) 제거 = 같은 행끼리 연산</li><li><code>view()</code>: reshape, <code>squeeze()</code>: 1인 차원 제거, <code>unsequeeze()</code>: 특정 위치에 1인 차원 추가</li><li><code>cat()</code>, <code>stack()</code>, <code>ones_like()</code>, <code>zeros_like()</code></li></ul><hr><h1 id=linear-regression>Linear Regression
<a class=anchor href=#linear-regression>#</a></h1><h2 id=선형-회귀-구현>선형 회귀 구현
<a class=anchor href=#%ec%84%a0%ed%98%95-%ed%9a%8c%ea%b7%80-%ea%b5%ac%ed%98%84>#</a></h2><ol><li>기본 셋팅 및 변수 선언: seed 설정, x_train 및 y_train 선언</li><li>가중치와 편향의 초기화: <code>W = torch.zeros(1, requires_grad=True)</code></li><li>가설 세우기: <code>hypothesis = x_train * W + b</code></li><li>비용 함수 선언: <code>cost = torch.mean((hypothesis - y_train) ** 2)</code></li><li>경사 하강법 구현:</li></ol><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>([</span><span class=n>W</span><span class=p>,</span> <span class=n>b</span><span class=p>],</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span> <span class=c1># gradient를 0으로 초기화</span>
</span></span><span class=line><span class=cl><span class=c1># 파이토치가 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값을 누적시키기 때문에 미분값을 초기화</span>
</span></span><span class=line><span class=cl><span class=n>cost</span><span class=o>.</span><span class=n>backword</span><span class=p>()</span> <span class=c1># 비용 함수를 미분하여 gradient 계산</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span> <span class=c1># W와 b를 업데이트</span></span></span></code></pre></div></div><h2 id=autograd>Autograd
<a class=anchor href=#autograd>#</a></h2><ul><li><code>requires_grad=True</code>, <code>backward()</code> 등</li></ul><h2 id=다중-선형-회귀>다중 선형 회귀
<a class=anchor href=#%eb%8b%a4%ec%a4%91-%ec%84%a0%ed%98%95-%ed%9a%8c%ea%b7%80>#</a></h2><ul><li>x의 계수를 행렬로 변환해 W와 내적</li><li>5x3 크기의 x_train과 3x1 크기의 W를 내적해 5x1 크기의 y_train 계산</li></ul><h1 id=nnmodule>nn.Module
<a class=anchor href=#nnmodule>#</a></h1><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>cost</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>mse_loss</span><span class=p>(</span><span class=n>prediction</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span></span></span></code></pre></div></div><h1 id=class>Class
<a class=anchor href=#class>#</a></h1><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>LinearRegressionModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span> <span class=c1>#</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span></span></span></code></pre></div></div><h1 id=custom-dataset>Custom Dataset
<a class=anchor href=#custom-dataset>#</a></h1><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>CustomDataset</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>Dataset</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 데이터셋의 전처리를 해주는 부분</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__len__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__getitem__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>idx</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 데이터셋에서 특정 1개의 샘플을 가져오는 함수</span></span></span></code></pre></div></div><hr><h1 id=logistic-regression>Logistic Regression
<a class=anchor href=#logistic-regression>#</a></h1><ul><li><code>hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))</code></li><li><code>cost = F.binary_cross_entropy(hypothesis, y_train)</code></li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>BinaryClassifier</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>sigmoid</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sigmoid</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>sigmoid</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=n>x</span><span class=p>))</span></span></span></code></pre></div></div><hr><h1 id=softmax-regression>Softmax Regression
<a class=anchor href=#softmax-regression>#</a></h1><ul><li><code>F.softmax(z, dim=1)</code></li><li>F.softmax() + torch.log() = F.log_softmax()</li><li>F.log_softmax() + F.nll_loss() = F.cross_entropy()</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># One-Hot Encoding</span>
</span></span><span class=line><span class=cl><span class=n>y_one_hot</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>hypothesis</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y_one_hot</span><span class=o>.</span><span class=n>scatter_</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>y_train</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>),</span> <span class=mi>1</span><span class=p>)</span></span></span></code></pre></div></div><hr><h1 id=artificial-neural-network>Artificial Neural Network
<a class=anchor href=#artificial-neural-network>#</a></h1><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>32</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>16</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span></span></span></code></pre></div></div><h1 id=convolutional-neural-network>Convolutional Neural Network
<a class=anchor href=#convolutional-neural-network>#</a></h1><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>CNN</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>CNN</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 첫번째층</span>
</span></span><span class=line><span class=cl>        <span class=c1># ImgIn shape=(?, 28, 28, 1)</span>
</span></span><span class=line><span class=cl>        <span class=c1>#    Conv     -&gt; (?, 28, 28, 32)</span>
</span></span><span class=line><span class=cl>        <span class=c1>#    Pool     -&gt; (?, 14, 14, 32)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layer1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=n>kernel_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 두번째층</span>
</span></span><span class=line><span class=cl>        <span class=c1># ImgIn shape=(?, 14, 14, 32)</span>
</span></span><span class=line><span class=cl>        <span class=c1>#    Conv      -&gt;(?, 14, 14, 64)</span>
</span></span><span class=line><span class=cl>        <span class=c1>#    Pool      -&gt;(?, 7, 7, 64)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layer2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>MaxPool2d</span><span class=p>(</span><span class=n>kernel_size</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 전결합층 7x7x64 inputs -&gt; 10 outputs</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>7</span> <span class=o>*</span> <span class=mi>7</span> <span class=o>*</span> <span class=mi>64</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 전결합층 한정으로 가중치 초기화</span>
</span></span><span class=line><span class=cl>        <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>xavier_uniform_</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=o>.</span><span class=n>weight</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer2</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=n>out</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>out</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>   <span class=c1># 전결합층을 위해서 Flatten</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span></span></span></code></pre></div></div><hr><h1 id=nlp>NLP
<a class=anchor href=#nlp>#</a></h1><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=n>embedding_layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>num_embeddings</span><span class=o>=</span><span class=nb>len</span><span class=p>(</span><span class=n>vocab</span><span class=p>),</span> 
</span></span><span class=line><span class=cl>                               <span class=n>embedding_dim</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                               <span class=n>padding_idx</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span></span></span></code></pre></div></div><h2 id=torchtext-error>torchtext Error
<a class=anchor href=#torchtext-error>#</a></h2><blockquote class=book-hint><p>cannot import name &rsquo;load_state_dict_from_url'</p></blockquote><ul><li>apply this change in <code>_download_hooks.py</code></li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=kn>from</span> <span class=nn>torch.hub</span> <span class=kn>import</span> <span class=n>load_state_dict_from_url</span>
</span></span><span class=line><span class=cl><span class=k>except</span> <span class=ne>ImportError</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=kn>from</span> <span class=nn>torch.utils.model_zoo</span> <span class=kn>import</span> <span class=n>load_url</span> <span class=k>as</span> <span class=n>load_state_dict_from_url</span></span></span></code></pre></div></div><h2 id=make-vocabulary>Make Vocabulary
<a class=anchor href=#make-vocabulary>#</a></h2><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torchtext.vocab</span> <span class=kn>import</span> <span class=n>build_vocab_from_iterator</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>vocab</span> <span class=o>=</span> <span class=n>build_vocab_from_iterator</span><span class=p>(</span><span class=n>sequences</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>vocab</span><span class=o>.</span><span class=n>get_stoi</span><span class=p>()</span> <span class=c1># 각 단어의 정수 인덱스가 저장된 딕셔너리</span></span></span></code></pre></div></div><hr><h1 id=recurrent-neural-network>Recurrent Neural Network
<a class=anchor href=#recurrent-neural-network>#</a></h1><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>nn</span><span class=o>.</span><span class=n>RNN</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>,</span> <span class=n>batch_fisrt</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>nn</span><span class=o>.</span><span class=n>LSTM</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>,</span> <span class=n>batch_fisrt</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span></span></span></code></pre></div></div><h2 id=char-rnn>Char RNN
<a class=anchor href=#char-rnn>#</a></h2><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Net</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>,</span> <span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>batch_first</span><span class=o>=</span><span class=kc>True</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>Net</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embedding_layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>num_embeddings</span><span class=o>=</span><span class=n>vocab_size</span><span class=p>,</span> <span class=c1># 워드 임베딩</span>
</span></span><span class=line><span class=cl>                                            <span class=n>embedding_dim</span><span class=o>=</span><span class=n>input_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>rnn_layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>RNN</span><span class=p>(</span><span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=c1># 입력 차원, 은닉 상태의 크기 정의</span>
</span></span><span class=line><span class=cl>                                <span class=n>batch_first</span><span class=o>=</span><span class=n>batch_first</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>)</span> <span class=c1># 출력은 원-핫 벡터의 크기를 가져야함. 또는 단어 집합의 크기만큼 가져야함.</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 1. 임베딩 층</span>
</span></span><span class=line><span class=cl>        <span class=c1># 크기변화: (배치 크기, 시퀀스 길이) =&gt; (배치 크기, 시퀀스 길이, 임베딩 차원)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embedding_layer</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 2. RNN 층</span>
</span></span><span class=line><span class=cl>        <span class=c1># 크기변화: (배치 크기, 시퀀스 길이, 임베딩 차원)</span>
</span></span><span class=line><span class=cl>        <span class=c1># =&gt; output (배치 크기, 시퀀스 길이, 은닉층 크기), hidden (1, 배치 크기, 은닉층 크기)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span><span class=p>,</span> <span class=n>hidden</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>rnn_layer</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 3. 최종 출력층</span>
</span></span><span class=line><span class=cl>        <span class=c1># 크기변화: (배치 크기, 시퀀스 길이, 은닉층 크기) =&gt; (배치 크기, 시퀀스 길이, 단어장 크기)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 4. view를 통해서 배치 차원 제거</span>
</span></span><span class=line><span class=cl>        <span class=c1># 크기변화: (배치 크기, 시퀀스 길이, 단어장 크기) =&gt; (배치 크기*시퀀스 길이, 단어장 크기)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>output</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 모델 생성</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Net</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>input_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>,</span> <span class=n>batch_first</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 손실함수 정의</span>
</span></span><span class=line><span class=cl><span class=n>loss_function</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span> <span class=c1># 소프트맥스 함수 포함이며 실제값은 원-핫 인코딩 안 해도 됨.</span>
</span></span><span class=line><span class=cl><span class=c1># 옵티마이저 정의</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>params</span><span class=o>=</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span></span></span></code></pre></div></div><h2 id=classification-with-gru>Classification with GRU
<a class=anchor href=#classification-with-gru>#</a></h2><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>GRU</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>n_layers</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>,</span> <span class=n>n_vocab</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>,</span> <span class=n>n_classes</span><span class=p>,</span> <span class=n>dropout_p</span><span class=o>=</span><span class=mf>0.2</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>GRU</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>n_layers</span> <span class=o>=</span> <span class=n>n_layers</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>hidden_dim</span> <span class=o>=</span> <span class=n>hidden_dim</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embed</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>n_vocab</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout_p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>gru</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>GRU</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_dim</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                          <span class=n>num_layers</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>n_layers</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                          <span class=n>batch_first</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>out</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>hidden_dim</span><span class=p>,</span> <span class=n>n_classes</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embed</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>h_0</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_init_state</span><span class=p>(</span><span class=n>batch_size</span><span class=o>=</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span> <span class=c1># 첫번째 히든 스테이트를 0벡터로 초기화</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>gru</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>h_0</span><span class=p>)</span> <span class=c1># GRU의 리턴값은 (배치 크기, 시퀀스 길이, 은닉 상태의 크기)</span>
</span></span><span class=line><span class=cl>        <span class=n>h_t</span> <span class=o>=</span> <span class=n>x</span><span class=p>[:,</span><span class=o>-</span><span class=mi>1</span><span class=p>,:]</span> <span class=c1># (배치 크기, 은닉 상태의 크기)의 텐서로 크기가 변경됨. 즉, 마지막 time-step의 은닉 상태만 가져온다.</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>h_t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>logit</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>out</span><span class=p>(</span><span class=n>h_t</span><span class=p>)</span> <span class=c1># (배치 크기, 은닉 상태의 크기) -&gt; (배치 크기, 출력층의 크기)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>logit</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_init_state</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>weight</span> <span class=o>=</span> <span class=nb>next</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span><span class=o>.</span><span class=n>data</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>weight</span><span class=o>.</span><span class=n>new</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>n_layers</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_dim</span><span class=p>)</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span></span></span></code></pre></div></div></article><div class=book-mobile-nav><button class=book-nav-btn3 onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=book-nav-btn3 onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
<button class=book-nav-btn3 onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class=post-tags><a href=/tags/til/ class=tag>#TIL</a>
<a href=/tags/deep-learning/ class=tag>#Deep Learning</a>
<a href=/tags/pytorch/ class=tag>#PyTorch</a></div><div class=post-navigation><a href=/blog/2022-07-17/ class="post-nav-link post-nav-prev"><span class=post-nav-direction><i class="fa-solid fa-backward"></i> PREV</span>
<span class=post-nav-title>2022-07-17 Log</span>
</a><a href=/blog/2022-07-08/ class="post-nav-link post-nav-next"><span class=post-nav-direction>NEXT <i class="fa-solid fa-forward"></i></span>
<span class=post-nav-title>2022-07-08 Log</span></a></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script><div class=book-comments><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://minyeamer.github.io/blog/2022-07-13/",this.page.identifier="https://minyeamer.github.io/blog/2022-07-13/"};(function(){var e=document,t=e.createElement("script");t.src="https://minyeamer.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})();function reloadDisqus(){window.DISQUS&&DISQUS.reset({reload:!0,config:function(){this.page.url="https://minyeamer.github.io/blog/2022-07-13/",this.page.identifier="https://minyeamer.github.io/blog/2022-07-13/"}})}</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#pytorch-packages>PyTorch Packages</a></li><li><a href=#tensor>Tensor</a></li></ul><ul><li><a href=#선형-회귀-구현>선형 회귀 구현</a></li><li><a href=#autograd>Autograd</a></li><li><a href=#다중-선형-회귀>다중 선형 회귀</a></li></ul><ul><li><a href=#torchtext-error>torchtext Error</a></li><li><a href=#make-vocabulary>Make Vocabulary</a></li></ul><ul><li><a href=#char-rnn>Char RNN</a></li><li><a href=#classification-with-gru>Classification with GRU</a></li></ul></nav><div class=book-nav><button class=book-nav-btn3 onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=book-nav-btn3 onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
<button class=book-nav-btn3 onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div></div></aside></main></body></html>