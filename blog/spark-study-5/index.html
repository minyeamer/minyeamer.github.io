<!doctype html><html lang=ko-kr dir=ltr><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=author content><meta name=keywords content="Apache Spark,Spark SQL,SQL,View,PySpark,데이터 엔지니어링,스파크,Study"><meta name=description content="Apache Spark의 Spark SQL을 다루며, SQL 쿼리 실행부터 테이블/뷰 관리, 메타데이터 캐싱까지 단계별로 안내합니다. 빅데이터 분석과 데이터 엔지니어링을 위한 강력 …"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><link rel=icon href=https://minyeamer.github.io/images/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://minyeamer.github.io/images/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://minyeamer.github.io/images/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://minyeamer.github.io/images/favicons/apple-touch-icon.png><link rel=mask-icon href=https://minyeamer.github.io/images/favicons/apple-touch-icon.png><meta name=google-site-verification content="u1tWcHmHUZWfFT1cHaku6sqU-bK40N3WLR-C-4VUWN0"><meta name=naver-site-verification content="6eaf8e9da1a6104780f056f1a7797fe5a3a5a0da"><meta property="og:title" content="Apache Spark - 스파크 SQL과 테이블/뷰 관리"><meta property="og:description" content="Apache Spark의 Spark SQL을 다루며, SQL 쿼리 실행부터 테이블/뷰 관리, 메타데이터 캐싱까지 단계별로 안내합니다. 빅데이터 분석과 데이터 엔지니어링을 위한 강력 …"><meta property="og:type" content="article"><meta property="og:url" content="https://minyeamer.github.io/blog/spark-study-5/"><meta property="og:image" content="https://dl.dropboxusercontent.com/scl/fi/iafnblb6k95kbw7bwn2xj/spark-00-cover.webp?rlkey=6995tacnu3mvr7s31akl5sca6&amp;raw=1"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-05T23:03:04+09:00"><meta property="article:modified_time" content="2025-07-05T23:03:04+09:00"><meta property="og:see_also" content="https://minyeamer.github.io/blog/spark-study-8/"><meta property="og:see_also" content="https://minyeamer.github.io/blog/spark-study-7/"><meta property="og:see_also" content="https://minyeamer.github.io/blog/spark-study-6/"><meta property="og:see_also" content="https://minyeamer.github.io/blog/spark-study-4/"><meta property="og:see_also" content="https://minyeamer.github.io/blog/spark-study-3/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dl.dropboxusercontent.com/scl/fi/iafnblb6k95kbw7bwn2xj/spark-00-cover.webp?rlkey=6995tacnu3mvr7s31akl5sca6&amp;raw=1"><meta name=twitter:title content="Apache Spark - 스파크 SQL과 테이블/뷰 관리"><meta name=twitter:description content="Apache Spark의 Spark SQL을 다루며, SQL 쿼리 실행부터 테이블/뷰 관리, 메타데이터 캐싱까지 단계별로 안내합니다. 빅데이터 분석과 데이터 엔지니어링을 위한 강력 …"><meta name=twitter:site content="@https://x.com/minyeamer"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://minyeamer.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Apache Spark - 스파크 SQL과 테이블/뷰 관리","item":"https://minyeamer.github.io/blog/spark-study-5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Apache Spark - 스파크 SQL과 테이블/뷰 관리","name":"Apache Spark - 스파크 SQL과 테이블\/뷰 관리","description":"Apache Spark의 Spark SQL을 다루며, SQL 쿼리 실행부터 테이블/뷰 관리, 메타데이터 캐싱까지 단계별로 안내합니다. 빅데이터 분석과 데이터 엔지니어링을 위한 강력한 도구를 습득하고 실무에 적용하세요.\n","keywords":["Apache Spark, Spark SQL, SQL, View, PySpark, 데이터 엔지니어링, 스파크, Study"],"articleBody":" Apache Spark 배우기 1. 스파크의 기본 개념과 아키텍처 2. 로컬 환경에서 설치하고 PySpark 실행하기 3. 스파크 애플리케이션 구조와 RDD 이해하기 4. DataFrame과 Dataset API 활용하기 5. 스파크 SQL과 테이블/뷰 관리 6. 다양한 데이터 소스 읽기/쓰기 (Parquet, JSON, CSV, Avro) 7. 외부 데이터베이스 연동 (PostgreSQL, MySQL) 8. 사용자 정의 함수(UDF)와 고차 함수 활용하기 숨기기 목록 보기 5/8 Spark SQL #스파크 SQL은 다음과 같은 특징을 갖는다.\n정형화 API가 엔진으로 제공한다. 다양한 정형 데이터(Parquet 등)를 읽거나 쓸 수 있다. 외부 BI 툴(태블로 등)의 데이터 소스나 RDBMS(MySQL 등)의 데이터를 쿼리할 수 있다. 정형 데이터에 대해 SQL 쿼리를 실행할 수 있는 대화형 쉘을 제공한다. Spark SQL 사용법 # Copy python spark.sql(\"SELECT * FROM table\")SparkSession 객체에 sql() 함수를 사용한다. 쿼리 결과로는 DataFrame 객체가 반환된다.\nSpark SQL 활용 (Python) #databricks/LearningSparkV2의 databricks-datasets/learning-spark-v2/flights 경로에서 미국 항공편 운항 지연 데이터세트 departuredelays.csv 를 가져온다. 해당 데이터를 활용해 아래와 같이 임시뷰를 생성한다.\nCopy python from pyspark.sql import SparkSession spark = (SparkSession .builder .appName(\"SparkSQLExampleApp\") .getOrCreate()) # 데이터 경로 csv_file = \"data/flights/departuredelays.csv\" # 스키마를 추론하여 데이터를 읽기 df = (spark.read.format(\"csv\") .option(\"inferSchema\", \"true\") .option(\"header\", \"true\") .load(csv_file)) # 데이터로부터 임시뷰를 생성 df.createOrReplaceTempView(\"delay_flights\")스파크 SQL을 사용해 임시뷰에 대해 SQL 쿼리를 실행할 수 있다. 스파크 SQL은 ANSI:2003과 호환되는 SQl 인터페이스를 제공한다.\nCopy python spark.sql(\"\"\" SELECT distance, origin, destination FROM delay_flights WHERE distance \u003e 1000 ORDER BY distance DESC;\"\"\").show(10) Copy bash +--------+------+-----------+ |distance|origin|destination| +--------+------+-----------+ | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| | 4330| HNL| JFK| +--------+------+-----------+ only showing top 10 rows위의 쿼리는 아래 파이썬 예제와 같이 동등한 DataFrame API로 표현할 수 있다.\nCopy python from pyspark.sql.functions import col, desc (df.select(\"distance\", \"origin\", \"destination\") .where(col(\"distance\") \u003e 1000) .orderBy(desc(\"distance\"))).show(10)SQL 쿼리로 단순 SELECT 조회뿐 아니라 현재 생성된 임시뷰 목록을 조회할 수도 있다.\nCopy python spark.sql(\"SHOW TABLES;\").show() Copy bash +---------+-------------+-----------+ |namespace| tableName|isTemporary| +---------+-------------+-----------+ | |delay_flights| true| +---------+-------------+-----------+앞에서 스키마를 추론하여 DataFrame을 읽었는데, SQL 쿼리로 어떤 스키마로 인식되었는지 확인해 보았다. DataFrame 객체의 스키마 df.schema 의 출력 결과와 동일하다.\nCopy python spark.sql(\"DESC delay_flights;\").show() Copy bash +-----------+---------+-------+ | col_name|data_type|comment| +-----------+---------+-------+ | date| int| NULL| | delay| int| NULL| | distance| int| NULL| | origin| string| NULL| |destination| string| NULL| +-----------+---------+-------+Table \u0026 View #스파크는 테이블을 위한 별도 메타스토어를 생성하지 않고 기본적으로 /user/hive/warehouse 경로에 있는 아파치 하이브 메타스토어를 사용해 테이블에 대한 모든 메타데이터를 유지한다.\n스파크는 관리형과 비관리형이라는 두 가지 유형의 테이블로 만들 수 있다.\n관리형 테이블은 스파크가 메타데이터와 파일 저장소의 데이터를 모두 관리한다. 따라서, DROP TABLE 과 같은 SQL 명령에 대해 메타데이터와 실제 데이터를 모두 삭제한다.\n반면에 비관리형 테이블은 스파크가 메타데이터만 관리하고 외부 데이터 소스에서 데이터를 직접 관리한다. 그래서, DROP TABLE 명령에도 실제 데이터는 그대로 두고 메타데이터만 삭제한다.\n테이블 생성하기 #스파크는 기본적으로 default 데이터베이스 안에 테이블을 생성한다. SparkSession을 열고 현재 접속한 데이터베이스를 조회하면 알 수 있다.\nCopy python spark.sql(\"SELECT current_database();\").show() Copy bash +----------------+ |current_schema()| +----------------+ | default| +----------------+우선, SQL 명령어를 실행하여 새로운 데이터베이스 learn_spark_db 를 생성할 수 있다. 생성한 데이터베이스를 사용하고 다시 현재 접속한 데이터베이스를 확인해 보았다.\nCopy python spark.sql(\"CREATE DATABASE learn_spark_db;\") spark.sql(\"USE learn_spark_db;\") spark.sql(\"SELECT current_database();\").show() Copy bash +----------------+ |current_schema()| +----------------+ | learn_spark_db| +----------------+관리형 테이블 생성하기 #CREATE 문을 사용하여 현재 데이터베이스 안에 관리형 테이블을 생성할 수 있다.\nCopy python table = \"managed_delay_flights\" schema = \"date STRING, delay INT, distaince INT, origin STRING, destination STRING\" spark.sql(\"CREATE TABLE {} ({});\".format(table, schema))위 SQL 쿼리는 마찬가지로 아래처럼 DataFrame API로 표현할 수도 있다. 이미 테이블을 만들었을 경우, 아래 파이썬 예제를 그대로 실행하면 TABLE_OR_VIEW_ALREADY_EXISTS 에러가 발생하므로 mode=\"overwrite\" 옵션을 넣어주어 기존 테이블을 덮어쓴다.\nCopy python csv_file = \"data/flights/departuredelays.csv\" flights_df = spark.read.csv(csv_file, schema=schema) flights_df.write.saveAsTable(table, mode=\"overwrite\")테이블을 생성하게 되면 현재 위치 아래의 spark-warehouse/{{DB명}}.db/{{테이블명}} 경로에 .parquet 파일들이 생성된다. 스파크 공식문서 중 Hive Table을 참고하면, 기본 디렉토리인 spark-warehouse 는 SparkSession을 실행할 때 spark.sql.warehouse.dir 설정을 통해 변경할 수 있다.\nCopy python warehouse_location = abspath('spark-warehouse') spark = SparkSession \\ .builder \\ .appName(\"Python Spark SQL Hive integration example\") \\ .config(\"spark.sql.warehouse.dir\", warehouse_location) \\ .enableHiveSupport() \\ .getOrCreate()정적 설정이라 세션 실행 중에는 변경할 수 없어서 세션을 종료하고 다시 실행했다. saved 경로로 변경하고 다시 관리형 테이블을 생성해보니 해당 위치에 Parquet 파일들이 만들어졌다.\nCopy python from pyspark.sql import SparkSession from pathlib import Path warehouse_location = Path(\"saved\") warehouse_location.mkdir(exist_ok=True) spark = (SparkSession .builder .appName(\"SparkSQLExampleApp\") .config(\"spark.sql.warehouse.dir\", str(warehouse_location.absolute())) .getOrCreate())비관리형 테이블 생성하기 #기존 CREATE 문을 사용하는 SQL 쿼리에서 뒤에 USING 키워드로 시작하는 CSV 파일 경로를 붙여주어 외부 데이터 소스로부터 비관리형 테이블을 생성할 수 있다.\nCopy python import os table = \"unmanaged_delay_flights\" schema = \"date STRING, delay INT, distaince INT, origin STRING, destination STRING\" csv_file = os.path.join(os.getcwd(), \"data/flights/departuredelays.csv\") spark.sql(\"CREATE TABLE {} ({}) USING csv OPTIONS (PATH '{}');\".format(table, schema, csv_file))CSV 파일의 상대경로로 SQL 쿼리를 실행해보니까 FileStreamSink: Assume no metadata directory. 경고 메시지가 발생했는데 절대경로로 바꿔주니까 해결되었다.\nSQL 쿼리를 DataFrame API로 표현하면 아래와 같다. 관리형 테이블을 만드는 구문이랑 거의 비슷한데, path 옵션으로 /tmp 경로를 지정하는데 차이가 있다.\nCopy python (flights_df .write .option(\"path\", \"/tmp/data/delay_flights\") .saveAsTable(table, mode=\"overwrite\"))뷰 생성하기 #기존 테이블을 토대로 뷰를 만들 수 있다. 전역(모든 SparkSession) 또는 세션 범위에서 생성할 수 있고, Spark Application이 종료되면 뷰는 사라진다.\n전역 임시 뷰는 SQL 쿼리에서는 GLOBAL TEMP VIEW 키워드를 추가하고, 파이썬에서는 createOrReplaceGlobalTempView() 함수를 호출하여 생성할 수 있다.\nCopy python table = \"us_origin_airport_SFO\" spark.sql(\"\"\" CREATE OR REPLACE GLOBAL TEMP VIEW {} AS SELECT date, delay, origin, destination FROM delay_flights WHERE origin = 'SFO'; \"\"\".format(table)) Copy python from pyspark.sql.functions import col table = \"us_origin_airport_SFO\" (df.select(\"date\", \"delay\", \"origin\", \"destination\") .where(col(\"origin\") == \"SFO\") .createOrReplaceGlobalTempView(table))전역 임시 뷰는 global_temp 라는 전역 임시 데이터베이스에 생성되며, global_temp. 처럼 명시하여 뷰 테이블에 접근할 수 있다. 일반 임시 뷰는 현재 데이터베이스에 생성되므로 global_temp 접두사를 붙일 필요가 없다.\nCopy python spark.sql(\"SELECT * FROM global_temp.{};\".format(table)).show(5)메타데이터 보기 #스파크는 관리형 및 비관리형 테이블에 대한 메타데이터를 관리한다. 메타데이터는 스파크 SQL의 상위 추상화 모듈인 카탈로그에 저장된다. 카탈로그에서 아래와 같이 데이터베이스, 테이블, 칼럼 목록을 조회할 수 있다.\nCopy python \u003e\u003e\u003e spark.catalog.listDatabases() [Database(name='default', catalog='spark_catalog', description='default database', locationUri='file:/Users/cuz/Documents/Github/study/spark/saved')] Copy python \u003e\u003e\u003e spark.catalog.listTables() [Table(name='delay_flights', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)] Copy python \u003e\u003e\u003e spark.catalog.listColumns(\"delay_flights\") [Column(name='date', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False, isCluster=False), Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False, isCluster=False), Column(name='distance', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False, isCluster=False), Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False), Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False, isCluster=False)]SQL 테이블 캐싱하기 #스파크 공식문서 중 CACHE TABLE을 참고하면, CACHE TABLE 쿼리를 사용하여 임시 뷰를 캐싱할 수 있다. CACHE LAZY TABLE 과 같이 LAZY 파라미터를 추가하면 테이블이 사용되는 시점까지 캐싱을 미룰 수 있다.\nCopy python spark.sql(\"CACHE TABLE delay_flights;\")테이블 캐시가 활성화 상태인지 보려면 카탈로그가 가진 isCached 함수를 참고할 수 있다. 캐시가 활성화되었다면 True, 아니면 False 를 반환한다.\nCopy python \u003e\u003e\u003e spark.catalog.isCached(\"delay_flights\") True Copy python \u003e\u003e\u003e spark.catalog.isCached(\"global_temp.us_origin_airport_sfo\") False테이블 캐시를 삭제하고 싶다면 UNCACHE TABLE 쿼리를 사용한다.\nCopy python \u003e\u003e\u003e spark.sql(\"UNCACHE TABLE delay_flights;\") \u003e\u003e\u003e spark.catalog.isCached(\"delay_flights\") False테이블을 DataFrame으로 변환하기 #SQL 쿼리로 테이블 전체를 읽을 수도 있지만, table() 함수를 사용할 수도 있다.\nCopy python spark.sql(\"SELECT * FROM delay_flights;\") Copy python spark.table(\"delay_flights\")sql() 과 table() 모두 DataFrame 객체를 반환한다.\nReferences # https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ https://github.com/databricks/LearningSparkV2/tree/master/databricks-datasets/learning-spark-v2/flights https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html https://spark.apache.org/docs/latest/sql-ref-syntax-aux-cache-cache-table.html https://spark.apache.org/docs/latest/sql-ref-syntax-aux-cache-uncache-table.html ","wordCount":"1091","inLanguage":"ko","image":"https:\/\/dl.dropboxusercontent.com\/scl\/fi\/iafnblb6k95kbw7bwn2xj\/spark-00-cover.webp?rlkey=6995tacnu3mvr7s31akl5sca6\u0026raw=1","datePublished":"2025-07-05T23:03:04+09:00","dateModified":"2025-07-05T23:03:04+09:00","author":{"@type":"Person","name":"minyeamer"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://minyeamer.github.io/blog/spark-study-5/"},"publisher":{"@type":"Person","name":"Minystory","logo":{"@type":"ImageObject","url":"https://minyeamer.github.io/images/favicons/favicon.ico"}}}</script><script>(function(){const e=256+768*1.3;window.innerWidth>e&&localStorage.getItem("menu-expanded")==="false"&&document.documentElement.classList.add("menu-initial-collapsed")})()</script><title>Apache Spark - 스파크 SQL과 테이블/뷰 관리|Minystory</title><link rel=icon href=/images/favicon.ico><link rel=manifest href=/data/manifest.json><link rel=canonical href=https://minyeamer.github.io/blog/spark-study-5/><link rel=stylesheet href=/main.min.bde103040955e62330088cfc990f437c722b645b0ee73b886f006f479c190e36.css integrity="sha256-veEDBAlV5iMwCIz8mQ9DfHIrZFsO5zuIbwBvR5wZDjY=" crossorigin=anonymous><script async src="https://www.googletagmanager.com/gtag/js?id=G-BJ8Z9RMBPJ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BJ8Z9RMBPJ")</script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.1/css/all.min.css crossorigin=anonymous><script>(function(){const e=console.warn;console.warn=function(...n){const t=(new Error).stack||"";if(t.includes("highlight.min.js")||t.includes("highlightjs-line-numbers"))return;e.apply(console,n)}})()</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlightjs-line-numbers.js/2.9.0/highlightjs-line-numbers.min.js crossorigin=anonymous></script><script defer src=/js/scroll-progress.min.841ade7e507a5f6d59c4e7bf2fe2b2ca034070677ff7957eec55610a024dd776.js integrity="sha256-hBreflB6X21ZxOe/L+KyygNAcGd/95V+7FVhCgJN13Y=" crossorigin=anonymous></script><script defer src=/js/menu-toggle.min.dd99cf31bd583912ea1bbd2c953c8c6b53d89da9846e8f05c9189de9369c97c9.js integrity="sha256-3ZnPMb1YORLqG70slTyMa1PYnamEbo8FyRid6Tacl8k=" crossorigin=anonymous></script><script defer src=/js/toc-toggle.min.48870f47596275987bbd50f04ba301b2b9469da2b3ed9d94e569774691851717.js integrity="sha256-SIcPR1lidZh7vVDwS6MBsrlGnaKz7Z2U5Wl3RpGFFxc=" crossorigin=anonymous></script><script defer src=/js/set-theme.min.986446861d7c48b101b4a6f53724b7fc97d6efec2a87906c6630cb9e503897c4.js integrity="sha256-mGRGhh18SLEBtKb1NyS3/JfW7+wqh5BsZjDLnlA4l8Q=" crossorigin=anonymous></script><script defer src=/js/copy-code.min.e18ffc44cf6beb1757c467945bb2897b0e1cb2ee52f08b460ee3ee57f0db6d60.js integrity="sha256-4Y/8RM9r6xdXxGeUW7KJew4csu5S8ItGDuPuV/DbbWA=" crossorigin=anonymous></script><script defer src=/js/toc-highlight.min.7917ead4ecb7378779bfbf3f988251ac76bd55ca7f12fd5919870777dbcd6095.js integrity="sha256-eRfq1Oy3N4d5v78/mIJRrHa9Vcp/Ev1ZGYcHd9vNYJU=" crossorigin=anonymous></script><script defer src=/js/reading-time.min.c8cf75fb9d8569d73b13b6a2bf6f52c235ff03ad7848e0386ccce2d7dcb8d9be.js integrity="sha256-yM91+52Fadc7E7aiv29SwjX/A614SOA4bMzi19y42b4=" crossorigin=anonymous></script><script defer src=/js/image-zoom.min.73ef1954212429af5df3e0386860f8f9b32748998c0ed1eb62ed800e348a2fab.js integrity="sha256-c+8ZVCEkKa9d8+A4aGD4+bMnSJmMDtHrYu2ADjSKL6s=" crossorigin=anonymous></script><script>window.siteSearch=window.siteSearch||{}</script><script>window.siteSearch.contentUrl="/data/content.min.af5361e8628f25c494b3aab53c6f0a5cd1500450dd2ef2d5c5c0c0b9b1c42f11.json"</script><script>window.siteSearch.categoriesUrl="/data/categories.min.249a3bfbb1ae8ee6e09c08bd2449462f8e161c8a98748da0169c2abe6f79598e.json"</script><script>window.siteSearch.tagsUrl="/data/tags.min.11b7d30fcd3cbb5269823c54a83c4889d580a62df2d1988344ddf158197da767.json"</script><script defer src=/fuse.min.js></script><script defer src=/js/search/init.min.3ec02e8ced439eef10e68b1663c5dab0c0ec89f63a79e154e33b0d62221bdbdb.js integrity="sha256-PsAujO1Dnu8Q5osWY8XasMDsifY6eeFU4zsNYiIb29s=" crossorigin=anonymous></script><script defer src=/js/search/input.min.a408f4459dfde324373a5993c754ed34738b88a238907346e0e5ec1a110bcf9b.js integrity="sha256-pAj0RZ394yQ3OlmTx1TtNHOLiKI4kHNG4OXsGhELz5s=" crossorigin=anonymous></script><script defer src=/js/search/list.min.985cf5b5f5b98133bffdd5b9f1d502cf1deb431146225583ca9d116ed6d737be.js integrity="sha256-mFz1tfW5gTO//dW58dUCzx3rQxFGIlWDyp0RbtbXN74=" crossorigin=anonymous></script></head><body dir=ltr class="site-kind-page site-type-posts site-layout-post"><div class=scroll-progress><div class=scroll-progress-bar></div></div><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><div class=menu-overlay></div><main class=container><aside class=site-menu aria-label=메뉴><div class=menu-content><nav><div class=menu-profile><div class=profile-image-wrap><a href=https://minyeamer.github.io/><img src=/images/profile/menu.jpg alt=Profile class=profile-image decoding=async></a></div><h2 class=profile-title><a class="flex align-center" href=https://minyeamer.github.io/><span>Minystory</span></a></h2></div><div class=menu-links><a href=https://github.com/minyeamer target=_blank id=github-link title=GitHub><i class="fa-brands fa-github"></i>
</a><a href=/categories/ id=categories-link title=Categories><i class="fa-solid fa-folder"></i>
</a><a href=/tags/ id=tags-link title=Tags><i class="fa-solid fa-tags"></i>
</a><button class=dark-mode-toggle id=theme-toggle-button aria-label="Toggle color scheme">
<i class="fa-solid fa-circle-half-stroke"></i></button></div><div class="site-search hidden"><div class=search-input-container><input type=text id=search-input placeholder=검색 aria-label=검색 maxlength=64 data-hotkeys=s/ onkeydown='event.key==="Enter"&&goToSearchPage()'>
<button type=button id=search-button class=search-button onclick=goToSearchPage()>
<i class="fa-solid fa-magnifying-glass"></i></button></div><div class="search-spinner hidden"></div><ul class=search-input-results id=search-input-results></ul></div><script>document.querySelector(".site-search").classList.remove("hidden");function goToSearchPage(){const t=document.getElementById("search-input"),e=t.value.trim();e&&(window.location.href="/search/?query="+encodeURIComponent(e))}</script><div class=menu-categories><input type=checkbox class="hidden toggle" id=categories-control checked>
<label for=categories-control class="categories-label categories-toggle"><a href=/categories/ class=categories-root><i class="fa-solid fa-folder"></i>
전체
<span class=category-count>(40)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li><input type=checkbox class="hidden toggle" id=cat-algorithm>
<label for=cat-algorithm class="categories-label categories-toggle"><a href="/search/?category1=Algorithm"><i class="fa-solid fa-folder"></i>Algorithm<span class=category-count>(4)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Algorithm&category2=Graph"><i class="fa-solid fa-file"></i>Graph<span class=category-count>(1)</span></a></li><li class=categories-label><a href="/search/?category1=Algorithm&category2=Python"><i class="fa-solid fa-file"></i>Python<span class=category-count>(2)</span></a></li><li class=categories-label><a href="/search/?category1=Algorithm&category2=SQL"><i class="fa-solid fa-file"></i>SQL<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-cloud>
<label for=cat-cloud class="categories-label categories-toggle"><a href="/search/?category1=Cloud"><i class="fa-solid fa-folder"></i>Cloud<span class=category-count>(2)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Cloud&category2=Kubernetes"><i class="fa-solid fa-file"></i>Kubernetes<span class=category-count>(2)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-data-analysis>
<label for=cat-data-analysis class="categories-label categories-toggle"><a href="/search/?category1=Data%20Analysis"><i class="fa-solid fa-folder"></i>Data Analysis<span class=category-count>(3)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Data%20Analysis&category2=Dacon"><i class="fa-solid fa-file"></i>Dacon<span class=category-count>(3)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-data-engineering>
<label for=cat-data-engineering class="categories-label categories-toggle"><a href="/search/?category1=Data%20Engineering"><i class="fa-solid fa-folder"></i>Data Engineering<span class=category-count>(19)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Data%20Engineering&category2=Apache%20Airflow"><i class="fa-solid fa-file"></i>Apache Airflow<span class=category-count>(7)</span></a></li><li class=categories-label><a href="/search/?category1=Data%20Engineering&category2=Apache%20Spark"><i class="fa-solid fa-file"></i>Apache Spark<span class=category-count>(8)</span></a></li><li class=categories-label><a href="/search/?category1=Data%20Engineering&category2=Crawling"><i class="fa-solid fa-file"></i>Crawling<span class=category-count>(4)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-frontend>
<label for=cat-frontend class="categories-label categories-toggle"><a href="/search/?category1=Frontend"><i class="fa-solid fa-folder"></i>Frontend<span class=category-count>(9)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Frontend&category2=Blog"><i class="fa-solid fa-file"></i>Blog<span class=category-count>(9)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-linux>
<label for=cat-linux class="categories-label categories-toggle"><a href="/search/?category1=Linux"><i class="fa-solid fa-folder"></i>Linux<span class=category-count>(1)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Linux&category2=Ubuntu"><i class="fa-solid fa-file"></i>Ubuntu<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-project>
<label for=cat-project class="categories-label categories-toggle"><a href="/search/?category1=Project"><i class="fa-solid fa-folder"></i>Project<span class=category-count>(2)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Project&category2=Open%20Source"><i class="fa-solid fa-file"></i>Open Source<span class=category-count>(1)</span></a></li><li class=categories-label><a href="/search/?category1=Project&category2=Tools"><i class="fa-solid fa-file"></i>Tools<span class=category-count>(1)</span></a></li></ul></li></ul></div><div class=recent-posts><div class=recent-post-label><i class="fa-solid fa-clock"></i>
<span>최신글</span></div><ul class=recent-post-list><li class=recent-post-item><a href=/blog/hugo-blog-5/ title="Hugo 블로그 만들기 (5) - 본문 레이아웃 개선 (헤더와 푸터 및 Disqus 댓글 기능 구현)"><div class=recent-post-title>Hugo 블로그 만들기 (5) - 본문 레이아웃 개선 (헤더와 푸터 및 Disqus 댓글 기능 구현)</div><div class=recent-post-date><time datetime=2025-12-15>2025.12.15</time></div></a></li><li class=recent-post-item><a href=/blog/hugo-blog-4/ title="Hugo 블로그 만들기 (4) - 검색 기능 개선 및 검색 페이지 구현 (Fuse.js)"><div class=recent-post-title>Hugo 블로그 만들기 (4) - 검색 기능 개선 및 검색 페이지 구현 (Fuse.js)</div><div class=recent-post-date><time datetime=2025-12-14>2025.12.14</time></div></a></li><li class=recent-post-item><a href=/blog/hugo-blog-3/ title="Hugo 블로그 만들기 (3) - Taxonomies로 태그/카테고리 페이지 커스터마이징"><div class=recent-post-title>Hugo 블로그 만들기 (3) - Taxonomies로 태그/카테고리 페이지 커스터마이징</div><div class=recent-post-date><time datetime=2025-11-22>2025.11.22</time></div></a></li><li class=recent-post-item><a href=/blog/hugo-blog-2/ title="Hugo 블로그 만들기 (2) - 메인 레이아웃 커스터마이징 (메뉴, 목차, 헤더)"><div class=recent-post-title>Hugo 블로그 만들기 (2) - 메인 레이아웃 커스터마이징 (메뉴, 목차, 헤더)</div><div class=recent-post-date><time datetime=2025-11-04>2025.11.04</time></div></a></li><li class=recent-post-item><a href=/blog/hugo-blog-1/ title="Hugo 블로그 만들기 (1) - 프로젝트 구성과 GitHub Pages 배포 (Submodule 활용)"><div class=recent-post-title>Hugo 블로그 만들기 (1) - 프로젝트 구성과 GitHub Pages 배포 (Submodule 활용)</div><div class=recent-post-date><time datetime=2025-11-01>2025.11.01</time></div></a></li></ul></div></nav><script>(function(){var e=document.querySelector("aside .menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=main-wrap><header class=site-header><div class="flex align-center justify-between"><label for=menu-control aria-label="메뉴 접기/펼치기" title="메뉴 접기/펼치기"><i class="fa-solid fa-bars menu-icon" id=menu-icon></i></label><h3><a href=https://minyeamer.github.io/ class=mobile-title>Minystory</a></h3><label for=toc-control aria-label="목차 접기/펼치기" title="목차 접기/펼치기"><i class="fa-solid fa-list menu-icon" id=toc-icon></i></label></div></header><article class="content-wrap markdown"><header class=content-header><div class=content-category><a href="/search/?category1=Data%20Engineering&category2=Apache%20Spark" class=content-category-link>Data Engineering/Apache Spark</a></div><h1 class=content-title>Apache Spark - 스파크 SQL과 테이블/뷰 관리</h1><div class=content-datetime><time datetime=2025-07-05T23:03:04+09:00>2025. 7. 5. 23:03
</time><span id=reading-time></span></div></header><div class=content-cover-wrap><img src="https://dl.dropboxusercontent.com/scl/fi/iafnblb6k95kbw7bwn2xj/spark-00-cover.webp?rlkey=6995tacnu3mvr7s31akl5sca6&amp;raw=1" class=content-cover alt="Cover Image" decoding=async></div><div id=series-anchor></div><div class=sc-series><div class=series-bookmark><svg width="32" height="48" fill="currentColor" viewBox="0 0 32 48" class="series-corner-image"><path fill="currentColor" d="M32 0H0v48h.163l16-16L32 47.836V0z"/></svg></div><div class=series-header><h2 class=series-title>Apache Spark 배우기</h2></div><input type=checkbox id=series-toggle class=series-toggle-input hidden><div class=series-content><ol class=series-list><li class=series-item><span class=series-item-index>1.</span>
<a href=/blog/spark-study-1/#series-anchor>스파크의 기본 개념과 아키텍처</a></li><li class=series-item><span class=series-item-index>2.</span>
<a href=/blog/spark-study-2/#series-anchor>로컬 환경에서 설치하고 PySpark 실행하기</a></li><li class=series-item><span class=series-item-index>3.</span>
<a href=/blog/spark-study-3/#series-anchor>스파크 애플리케이션 구조와 RDD 이해하기</a></li><li class=series-item><span class=series-item-index>4.</span>
<a href=/blog/spark-study-4/#series-anchor>DataFrame과 Dataset API 활용하기</a></li><li class="series-item active"><span class=series-item-index>5.</span>
<a href=/blog/spark-study-5/#series-anchor>스파크 SQL과 테이블/뷰 관리</a></li><li class=series-item><span class=series-item-index>6.</span>
<a href=/blog/spark-study-6/#series-anchor>다양한 데이터 소스 읽기/쓰기 (Parquet, JSON, CSV, Avro)</a></li><li class=series-item><span class=series-item-index>7.</span>
<a href=/blog/spark-study-7/#series-anchor>외부 데이터베이스 연동 (PostgreSQL, MySQL)</a></li><li class=series-item><span class=series-item-index>8.</span>
<a href=/blog/spark-study-8/#series-anchor>사용자 정의 함수(UDF)와 고차 함수 활용하기</a></li></ol></div><div class=series-footer><label for=series-toggle class=series-toggle-label><span class=series-toggle-icon><i class="fas fa-caret-up"></i></span>
<span class=series-toggle-text-hide>숨기기</span>
<span class=series-toggle-text-show>목록 보기</span></label><div class=series-nav><span class=series-nav-info>5/8</span><div class=series-nav-buttons><a href=/blog/spark-study-4/#series-anchor class=series-nav-button><i class="fas fa-chevron-left"></i></a>
<a href=/blog/spark-study-6/#series-anchor class=series-nav-button><i class="fas fa-chevron-right"></i></a></div></div></div></div><script>(function(){const e=document.getElementById("series-toggle"),t="series-expanded",n=localStorage.getItem(t);n==="true"&&(e.checked=!0),e.addEventListener("change",function(){localStorage.setItem(t,this.checked)})})()</script><h2 id=spark-sql>Spark SQL
<a class=anchor href=#spark-sql>#</a></h2><p>스파크 SQL은 다음과 같은 특징을 갖는다.</p><ul><li>정형화 API가 엔진으로 제공한다.</li><li>다양한 정형 데이터(Parquet 등)를 읽거나 쓸 수 있다.</li><li>외부 BI 툴(태블로 등)의 데이터 소스나 RDBMS(MySQL 등)의 데이터를 쿼리할 수 있다.</li><li>정형 데이터에 대해 SQL 쿼리를 실행할 수 있는 대화형 쉘을 제공한다.</li></ul><h3 id=spark-sql-사용법>Spark SQL 사용법
<a class=anchor href=#spark-sql-%ec%82%ac%ec%9a%a9%eb%b2%95>#</a></h3><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;SELECT * FROM table&#34;</span><span class=p>)</span></span></span></code></pre></div></div><p><code>SparkSession</code> 객체에 <code>sql()</code> 함수를 사용한다. 쿼리 결과로는 <code>DataFrame</code> 객체가 반환된다.</p><h3 id=spark-sql-활용-python>Spark SQL 활용 (Python)
<a class=anchor href=#spark-sql-%ed%99%9c%ec%9a%a9-python>#</a></h3><p><a href=https://github.com/databricks/LearningSparkV2/tree/master/databricks-datasets/learning-spark-v2/flights target=_blank rel="noopener noreferrer">databricks/LearningSparkV2</a>의
<code>databricks-datasets/learning-spark-v2/flights</code> 경로에서 미국 항공편 운항 지연 데이터세트
<code>departuredelays.csv</code> 를 가져온다. 해당 데이터를 활용해 아래와 같이 임시뷰를 생성한다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>SparkSession</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>spark</span> <span class=o>=</span> <span class=p>(</span><span class=n>SparkSession</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>builder</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>&#34;SparkSQLExampleApp&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 데이터 경로</span>
</span></span><span class=line><span class=cl><span class=n>csv_file</span> <span class=o>=</span> <span class=s2>&#34;data/flights/departuredelays.csv&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 스키마를 추론하여 데이터를 읽기</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=p>(</span><span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>&#34;csv&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;inferSchema&#34;</span><span class=p>,</span> <span class=s2>&#34;true&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;header&#34;</span><span class=p>,</span> <span class=s2>&#34;true&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>csv_file</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 데이터로부터 임시뷰를 생성</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>createOrReplaceTempView</span><span class=p>(</span><span class=s2>&#34;delay_flights&#34;</span><span class=p>)</span></span></span></code></pre></div></div><p>스파크 SQL을 사용해 임시뷰에 대해 SQL 쿼리를 실행할 수 있다.
스파크 SQL은 ANSI:2003과 호환되는 SQl 인터페이스를 제공한다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>SELECT distance, origin, destination
</span></span></span><span class=line><span class=cl><span class=s2>FROM delay_flights
</span></span></span><span class=line><span class=cl><span class=s2>WHERE distance &gt; 1000
</span></span></span><span class=line><span class=cl><span class=s2>ORDER BY distance DESC;&#34;&#34;&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span></span></span></code></pre></div></div><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>+--------+------+-----------+
</span></span><span class=line><span class=cl><span class=p>|</span>distance<span class=p>|</span>origin<span class=p>|</span>destination<span class=p>|</span>
</span></span><span class=line><span class=cl>+--------+------+-----------+
</span></span><span class=line><span class=cl><span class=p>|</span>    4330<span class=p>|</span>   HNL<span class=p>|</span>        JFK<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>    4330<span class=p>|</span>   HNL<span class=p>|</span>        JFK<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>    4330<span class=p>|</span>   HNL<span class=p>|</span>        JFK<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>    4330<span class=p>|</span>   HNL<span class=p>|</span>        JFK<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>    4330<span class=p>|</span>   HNL<span class=p>|</span>        JFK<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>    4330<span class=p>|</span>   HNL<span class=p>|</span>        JFK<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>    4330<span class=p>|</span>   HNL<span class=p>|</span>        JFK<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>    4330<span class=p>|</span>   HNL<span class=p>|</span>        JFK<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>    4330<span class=p>|</span>   HNL<span class=p>|</span>        JFK<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>    4330<span class=p>|</span>   HNL<span class=p>|</span>        JFK<span class=p>|</span>
</span></span><span class=line><span class=cl>+--------+------+-----------+
</span></span><span class=line><span class=cl>only showing top <span class=m>10</span> rows</span></span></code></pre></div></div><p>위의 쿼리는 아래 파이썬 예제와 같이 동등한 DataFrame API로 표현할 수 있다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql.functions</span> <span class=kn>import</span> <span class=n>col</span><span class=p>,</span> <span class=n>desc</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=s2>&#34;distance&#34;</span><span class=p>,</span> <span class=s2>&#34;origin&#34;</span><span class=p>,</span> <span class=s2>&#34;destination&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>&#34;distance&#34;</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>1000</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>orderBy</span><span class=p>(</span><span class=n>desc</span><span class=p>(</span><span class=s2>&#34;distance&#34;</span><span class=p>)))</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span></span></span></code></pre></div></div><p>SQL 쿼리로 단순 <code>SELECT</code> 조회뿐 아니라 현재 생성된 임시뷰 목록을 조회할 수도 있다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;SHOW TABLES;&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>()</span></span></span></code></pre></div></div><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>+---------+-------------+-----------+
</span></span><span class=line><span class=cl><span class=p>|</span>namespace<span class=p>|</span>    tableName<span class=p>|</span>isTemporary<span class=p>|</span>
</span></span><span class=line><span class=cl>+---------+-------------+-----------+
</span></span><span class=line><span class=cl><span class=p>|</span>         <span class=p>|</span>delay_flights<span class=p>|</span>       true<span class=p>|</span>
</span></span><span class=line><span class=cl>+---------+-------------+-----------+</span></span></code></pre></div></div><p>앞에서 스키마를 추론하여 DataFrame을 읽었는데, SQL 쿼리로 어떤 스키마로 인식되었는지 확인해 보았다.
DataFrame 객체의 스키마 <code>df.schema</code> 의 출력 결과와 동일하다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;DESC delay_flights;&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>()</span></span></span></code></pre></div></div><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>+-----------+---------+-------+
</span></span><span class=line><span class=cl><span class=p>|</span>   col_name<span class=p>|</span>data_type<span class=p>|</span>comment<span class=p>|</span>
</span></span><span class=line><span class=cl>+-----------+---------+-------+
</span></span><span class=line><span class=cl><span class=p>|</span>       date<span class=p>|</span>      int<span class=p>|</span>   NULL<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>      delay<span class=p>|</span>      int<span class=p>|</span>   NULL<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>   distance<span class=p>|</span>      int<span class=p>|</span>   NULL<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>     origin<span class=p>|</span>   string<span class=p>|</span>   NULL<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>destination<span class=p>|</span>   string<span class=p>|</span>   NULL<span class=p>|</span>
</span></span><span class=line><span class=cl>+-----------+---------+-------+</span></span></code></pre></div></div><h2 id=table--view>Table & View
<a class=anchor href=#table--view>#</a></h2><p>스파크는 테이블을 위한 별도 메타스토어를 생성하지 않고 기본적으로 <code>/user/hive/warehouse</code> 경로에 있는
아파치 하이브 메타스토어를 사용해 테이블에 대한 모든 메타데이터를 유지한다.</p><p>스파크는 관리형과 비관리형이라는 두 가지 유형의 테이블로 만들 수 있다.</p><p><strong>관리형 테이블</strong>은 스파크가 메타데이터와 파일 저장소의 데이터를 모두 관리한다.
따라서, <code>DROP TABLE</code> 과 같은 SQL 명령에 대해 메타데이터와 실제 데이터를 모두 삭제한다.</p><p>반면에 <strong>비관리형 테이블</strong>은 스파크가 메타데이터만 관리하고 외부 데이터 소스에서 데이터를 직접 관리한다.
그래서, <code>DROP TABLE</code> 명령에도 실제 데이터는 그대로 두고 메타데이터만 삭제한다.</p><h3 id=테이블-생성하기>테이블 생성하기
<a class=anchor href=#%ed%85%8c%ec%9d%b4%eb%b8%94-%ec%83%9d%ec%84%b1%ed%95%98%ea%b8%b0>#</a></h3><p>스파크는 기본적으로 <code>default</code> 데이터베이스 안에 테이블을 생성한다.
SparkSession을 열고 현재 접속한 데이터베이스를 조회하면 알 수 있다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;SELECT current_database();&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>()</span></span></span></code></pre></div></div><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>+----------------+
</span></span><span class=line><span class=cl><span class=p>|</span>current_schema<span class=o>()</span><span class=p>|</span>
</span></span><span class=line><span class=cl>+----------------+
</span></span><span class=line><span class=cl><span class=p>|</span>         default<span class=p>|</span>
</span></span><span class=line><span class=cl>+----------------+</span></span></code></pre></div></div><p>우선, SQL 명령어를 실행하여 새로운 데이터베이스 <code>learn_spark_db</code> 를 생성할 수 있다.
생성한 데이터베이스를 사용하고 다시 현재 접속한 데이터베이스를 확인해 보았다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;CREATE DATABASE learn_spark_db;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;USE learn_spark_db;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;SELECT current_database();&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>()</span></span></span></code></pre></div></div><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>+----------------+
</span></span><span class=line><span class=cl><span class=p>|</span>current_schema<span class=o>()</span><span class=p>|</span>
</span></span><span class=line><span class=cl>+----------------+
</span></span><span class=line><span class=cl><span class=p>|</span>  learn_spark_db<span class=p>|</span>
</span></span><span class=line><span class=cl>+----------------+</span></span></code></pre></div></div><h3 id=관리형-테이블-생성하기>관리형 테이블 생성하기
<a class=anchor href=#%ea%b4%80%eb%a6%ac%ed%98%95-%ed%85%8c%ec%9d%b4%eb%b8%94-%ec%83%9d%ec%84%b1%ed%95%98%ea%b8%b0>#</a></h3><p><code>CREATE</code> 문을 사용하여 현재 데이터베이스 안에 관리형 테이블을 생성할 수 있다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>table</span> <span class=o>=</span> <span class=s2>&#34;managed_delay_flights&#34;</span>
</span></span><span class=line><span class=cl><span class=n>schema</span> <span class=o>=</span> <span class=s2>&#34;date STRING, delay INT, distaince INT, origin STRING, destination STRING&#34;</span>
</span></span><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;CREATE TABLE </span><span class=si>{}</span><span class=s2> (</span><span class=si>{}</span><span class=s2>);&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>table</span><span class=p>,</span> <span class=n>schema</span><span class=p>))</span></span></span></code></pre></div></div><p>위 SQL 쿼리는 마찬가지로 아래처럼 DataFrame API로 표현할 수도 있다.
이미 테이블을 만들었을 경우, 아래 파이썬 예제를 그대로 실행하면 <code>TABLE_OR_VIEW_ALREADY_EXISTS</code> 에러가
발생하므로 <code>mode="overwrite"</code> 옵션을 넣어주어 기존 테이블을 덮어쓴다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>csv_file</span> <span class=o>=</span> <span class=s2>&#34;data/flights/departuredelays.csv&#34;</span>
</span></span><span class=line><span class=cl><span class=n>flights_df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>csv</span><span class=p>(</span><span class=n>csv_file</span><span class=p>,</span> <span class=n>schema</span><span class=o>=</span><span class=n>schema</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>flights_df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>saveAsTable</span><span class=p>(</span><span class=n>table</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s2>&#34;overwrite&#34;</span><span class=p>)</span></span></span></code></pre></div></div><p>테이블을 생성하게 되면 현재 위치 아래의 <code>spark-warehouse/{{DB명}}.db/{{테이블명}}</code> 경로에
<code>.parquet</code> 파일들이 생성된다. 스파크 공식문서 중
<a href=https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html target=_blank rel="noopener noreferrer">Hive Table</a>을 참고하면,
기본 디렉토리인 <code>spark-warehouse</code> 는 SparkSession을 실행할 때 <code>spark.sql.warehouse.dir</code> 설정을 통해 변경할 수 있다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>warehouse_location</span> <span class=o>=</span> <span class=n>abspath</span><span class=p>(</span><span class=s1>&#39;spark-warehouse&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>spark</span> <span class=o>=</span> <span class=n>SparkSession</span> \
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>builder</span> \
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>&#34;Python Spark SQL Hive integration example&#34;</span><span class=p>)</span> \
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>&#34;spark.sql.warehouse.dir&#34;</span><span class=p>,</span> <span class=n>warehouse_location</span><span class=p>)</span> \
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>enableHiveSupport</span><span class=p>()</span> \
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>()</span></span></span></code></pre></div></div><p>정적 설정이라 세션 실행 중에는 변경할 수 없어서 세션을 종료하고 다시 실행했다.
<code>saved</code> 경로로 변경하고 다시 관리형 테이블을 생성해보니 해당 위치에 Parquet 파일들이 만들어졌다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>SparkSession</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pathlib</span> <span class=kn>import</span> <span class=n>Path</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>warehouse_location</span> <span class=o>=</span> <span class=n>Path</span><span class=p>(</span><span class=s2>&#34;saved&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>warehouse_location</span><span class=o>.</span><span class=n>mkdir</span><span class=p>(</span><span class=n>exist_ok</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>spark</span> <span class=o>=</span> <span class=p>(</span><span class=n>SparkSession</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>builder</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>&#34;SparkSQLExampleApp&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>config</span><span class=p>(</span><span class=s2>&#34;spark.sql.warehouse.dir&#34;</span><span class=p>,</span> <span class=nb>str</span><span class=p>(</span><span class=n>warehouse_location</span><span class=o>.</span><span class=n>absolute</span><span class=p>()))</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>getOrCreate</span><span class=p>())</span></span></span></code></pre></div></div><h3 id=비관리형-테이블-생성하기>비관리형 테이블 생성하기
<a class=anchor href=#%eb%b9%84%ea%b4%80%eb%a6%ac%ed%98%95-%ed%85%8c%ec%9d%b4%eb%b8%94-%ec%83%9d%ec%84%b1%ed%95%98%ea%b8%b0>#</a></h3><p>기존 <code>CREATE</code> 문을 사용하는 SQL 쿼리에서 뒤에 <code>USING</code> 키워드로 시작하는
CSV 파일 경로를 붙여주어 외부 데이터 소스로부터 비관리형 테이블을 생성할 수 있다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>table</span> <span class=o>=</span> <span class=s2>&#34;unmanaged_delay_flights&#34;</span>
</span></span><span class=line><span class=cl><span class=n>schema</span> <span class=o>=</span> <span class=s2>&#34;date STRING, delay INT, distaince INT, origin STRING, destination STRING&#34;</span>
</span></span><span class=line><span class=cl><span class=n>csv_file</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>getcwd</span><span class=p>(),</span> <span class=s2>&#34;data/flights/departuredelays.csv&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;CREATE TABLE </span><span class=si>{}</span><span class=s2> (</span><span class=si>{}</span><span class=s2>) USING csv OPTIONS (PATH &#39;</span><span class=si>{}</span><span class=s2>&#39;);&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>table</span><span class=p>,</span> <span class=n>schema</span><span class=p>,</span> <span class=n>csv_file</span><span class=p>))</span></span></span></code></pre></div></div><p>CSV 파일의 상대경로로 SQL 쿼리를 실행해보니까 <code>FileStreamSink: Assume no metadata directory.</code>
경고 메시지가 발생했는데 절대경로로 바꿔주니까 해결되었다.</p><p>SQL 쿼리를 DataFrame API로 표현하면 아래와 같다. 관리형 테이블을 만드는 구문이랑 거의 비슷한데,
<code>path</code> 옵션으로 <code>/tmp</code> 경로를 지정하는데 차이가 있다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>(</span><span class=n>flights_df</span>
</span></span><span class=line><span class=cl>	<span class=o>.</span><span class=n>write</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;path&#34;</span><span class=p>,</span> <span class=s2>&#34;/tmp/data/delay_flights&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>saveAsTable</span><span class=p>(</span><span class=n>table</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s2>&#34;overwrite&#34;</span><span class=p>))</span></span></span></code></pre></div></div><h3 id=뷰-생성하기>뷰 생성하기
<a class=anchor href=#%eb%b7%b0-%ec%83%9d%ec%84%b1%ed%95%98%ea%b8%b0>#</a></h3><p>기존 테이블을 토대로 뷰를 만들 수 있다. 전역(모든 SparkSession) 또는 세션 범위에서 생성할 수 있고,
Spark Application이 종료되면 뷰는 사라진다.</p><p>전역 임시 뷰는 SQL 쿼리에서는 <code>GLOBAL TEMP VIEW</code> 키워드를 추가하고,
파이썬에서는 <code>createOrReplaceGlobalTempView()</code> 함수를 호출하여 생성할 수 있다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>table</span> <span class=o>=</span> <span class=s2>&#34;us_origin_airport_SFO&#34;</span>
</span></span><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>CREATE OR REPLACE GLOBAL TEMP VIEW </span><span class=si>{}</span><span class=s2> AS
</span></span></span><span class=line><span class=cl><span class=s2>SELECT date, delay, origin, destination FROM delay_flights
</span></span></span><span class=line><span class=cl><span class=s2>WHERE origin = &#39;SFO&#39;;
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>table</span><span class=p>))</span></span></span></code></pre></div></div><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql.functions</span> <span class=kn>import</span> <span class=n>col</span>
</span></span><span class=line><span class=cl><span class=n>table</span> <span class=o>=</span> <span class=s2>&#34;us_origin_airport_SFO&#34;</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=s2>&#34;date&#34;</span><span class=p>,</span> <span class=s2>&#34;delay&#34;</span><span class=p>,</span> <span class=s2>&#34;origin&#34;</span><span class=p>,</span> <span class=s2>&#34;destination&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>&#34;origin&#34;</span><span class=p>)</span> <span class=o>==</span> <span class=s2>&#34;SFO&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>createOrReplaceGlobalTempView</span><span class=p>(</span><span class=n>table</span><span class=p>))</span></span></span></code></pre></div></div><p>전역 임시 뷰는 <code>global_temp</code> 라는 전역 임시 데이터베이스에 생성되며,
<code>global_temp.&lt;view_name></code> 처럼 명시하여 뷰 테이블에 접근할 수 있다.
일반 임시 뷰는 현재 데이터베이스에 생성되므로 <code>global_temp</code> 접두사를 붙일 필요가 없다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;SELECT * FROM global_temp.</span><span class=si>{}</span><span class=s2>;&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>table</span><span class=p>))</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span></span></span></code></pre></div></div><h3 id=메타데이터-보기>메타데이터 보기
<a class=anchor href=#%eb%a9%94%ed%83%80%eb%8d%b0%ec%9d%b4%ed%84%b0-%eb%b3%b4%ea%b8%b0>#</a></h3><p>스파크는 관리형 및 비관리형 테이블에 대한 메타데이터를 관리한다.
메타데이터는 스파크 SQL의 상위 추상화 모듈인 카탈로그에 저장된다.
카탈로그에서 아래와 같이 데이터베이스, 테이블, 칼럼 목록을 조회할 수 있다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>spark</span><span class=o>.</span><span class=n>catalog</span><span class=o>.</span><span class=n>listDatabases</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=n>Database</span><span class=p>(</span><span class=n>name</span><span class=o>=</span><span class=s1>&#39;default&#39;</span><span class=p>,</span> <span class=n>catalog</span><span class=o>=</span><span class=s1>&#39;spark_catalog&#39;</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=s1>&#39;default database&#39;</span><span class=p>,</span> <span class=n>locationUri</span><span class=o>=</span><span class=s1>&#39;file:/Users/cuz/Documents/Github/study/spark/saved&#39;</span><span class=p>)]</span></span></span></code></pre></div></div><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>spark</span><span class=o>.</span><span class=n>catalog</span><span class=o>.</span><span class=n>listTables</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=n>Table</span><span class=p>(</span><span class=n>name</span><span class=o>=</span><span class=s1>&#39;delay_flights&#39;</span><span class=p>,</span> <span class=n>catalog</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>namespace</span><span class=o>=</span><span class=p>[],</span> <span class=n>description</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>tableType</span><span class=o>=</span><span class=s1>&#39;TEMPORARY&#39;</span><span class=p>,</span> <span class=n>isTemporary</span><span class=o>=</span><span class=kc>True</span><span class=p>)]</span></span></span></code></pre></div></div><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>spark</span><span class=o>.</span><span class=n>catalog</span><span class=o>.</span><span class=n>listColumns</span><span class=p>(</span><span class=s2>&#34;delay_flights&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=n>Column</span><span class=p>(</span><span class=n>name</span><span class=o>=</span><span class=s1>&#39;date&#39;</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>dataType</span><span class=o>=</span><span class=s1>&#39;int&#39;</span><span class=p>,</span> <span class=n>nullable</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>isPartition</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>isBucket</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>isCluster</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl> <span class=n>Column</span><span class=p>(</span><span class=n>name</span><span class=o>=</span><span class=s1>&#39;delay&#39;</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>dataType</span><span class=o>=</span><span class=s1>&#39;int&#39;</span><span class=p>,</span> <span class=n>nullable</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>isPartition</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>isBucket</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>isCluster</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl> <span class=n>Column</span><span class=p>(</span><span class=n>name</span><span class=o>=</span><span class=s1>&#39;distance&#39;</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>dataType</span><span class=o>=</span><span class=s1>&#39;int&#39;</span><span class=p>,</span> <span class=n>nullable</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>isPartition</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>isBucket</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>isCluster</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl> <span class=n>Column</span><span class=p>(</span><span class=n>name</span><span class=o>=</span><span class=s1>&#39;origin&#39;</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>dataType</span><span class=o>=</span><span class=s1>&#39;string&#39;</span><span class=p>,</span> <span class=n>nullable</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>isPartition</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>isBucket</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>isCluster</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl> <span class=n>Column</span><span class=p>(</span><span class=n>name</span><span class=o>=</span><span class=s1>&#39;destination&#39;</span><span class=p>,</span> <span class=n>description</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>dataType</span><span class=o>=</span><span class=s1>&#39;string&#39;</span><span class=p>,</span> <span class=n>nullable</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>isPartition</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>isBucket</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>isCluster</span><span class=o>=</span><span class=kc>False</span><span class=p>)]</span></span></span></code></pre></div></div><h3 id=sql-테이블-캐싱하기>SQL 테이블 캐싱하기
<a class=anchor href=#sql-%ed%85%8c%ec%9d%b4%eb%b8%94-%ec%ba%90%ec%8b%b1%ed%95%98%ea%b8%b0>#</a></h3><p>스파크 공식문서 중 <a href=https://spark.apache.org/docs/latest/sql-ref-syntax-aux-cache-cache-table.html target=_blank rel="noopener noreferrer">CACHE TABLE</a>을 참고하면,
<code>CACHE TABLE</code> 쿼리를 사용하여 임시 뷰를 캐싱할 수 있다.
<code>CACHE LAZY TABLE</code> 과 같이 <code>LAZY</code> 파라미터를 추가하면 테이블이 사용되는 시점까지 캐싱을 미룰 수 있다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;CACHE TABLE delay_flights;&#34;</span><span class=p>)</span></span></span></code></pre></div></div><p>테이블 캐시가 활성화 상태인지 보려면 카탈로그가 가진 <code>isCached</code> 함수를 참고할 수 있다.
캐시가 활성화되었다면 <code>True</code>, 아니면 <code>False</code> 를 반환한다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>spark</span><span class=o>.</span><span class=n>catalog</span><span class=o>.</span><span class=n>isCached</span><span class=p>(</span><span class=s2>&#34;delay_flights&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=kc>True</span></span></span></code></pre></div></div><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>spark</span><span class=o>.</span><span class=n>catalog</span><span class=o>.</span><span class=n>isCached</span><span class=p>(</span><span class=s2>&#34;global_temp.us_origin_airport_sfo&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=kc>False</span></span></span></code></pre></div></div><p>테이블 캐시를 삭제하고 싶다면 <code>UNCACHE TABLE</code> 쿼리를 사용한다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;UNCACHE TABLE delay_flights;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>spark</span><span class=o>.</span><span class=n>catalog</span><span class=o>.</span><span class=n>isCached</span><span class=p>(</span><span class=s2>&#34;delay_flights&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=kc>False</span></span></span></code></pre></div></div><h3 id=테이블을-dataframe으로-변환하기>테이블을 DataFrame으로 변환하기
<a class=anchor href=#%ed%85%8c%ec%9d%b4%eb%b8%94%ec%9d%84-dataframe%ec%9c%bc%eb%a1%9c-%eb%b3%80%ed%99%98%ed%95%98%ea%b8%b0>#</a></h3><p>SQL 쿼리로 테이블 전체를 읽을 수도 있지만, <code>table()</code> 함수를 사용할 수도 있다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=s2>&#34;SELECT * FROM delay_flights;&#34;</span><span class=p>)</span></span></span></code></pre></div></div><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>spark</span><span class=o>.</span><span class=n>table</span><span class=p>(</span><span class=s2>&#34;delay_flights&#34;</span><span class=p>)</span></span></span></code></pre></div></div><p><code>sql()</code> 과 <code>table()</code> 모두 <code>DataFrame</code> 객체를 반환한다.</p><h2 id=references>References
<a class=anchor href=#references>#</a></h2><ul><li><a href=https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ target=_blank rel="noopener noreferrer">https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/</a></li><li><a href=https://github.com/databricks/LearningSparkV2/tree/master/databricks-datasets/learning-spark-v2/flights target=_blank rel="noopener noreferrer">https://github.com/databricks/LearningSparkV2/tree/master/databricks-datasets/learning-spark-v2/flights</a></li><li><a href=https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html target=_blank rel="noopener noreferrer">https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html</a></li><li><a href=https://spark.apache.org/docs/latest/sql-ref-syntax-aux-cache-cache-table.html target=_blank rel="noopener noreferrer">https://spark.apache.org/docs/latest/sql-ref-syntax-aux-cache-cache-table.html</a></li><li><a href=https://spark.apache.org/docs/latest/sql-ref-syntax-aux-cache-uncache-table.html target=_blank rel="noopener noreferrer">https://spark.apache.org/docs/latest/sql-ref-syntax-aux-cache-uncache-table.html</a></li></ul></article><footer class=site-footer><div class=tags-wrap><a href="/search/?tags=Apache%20Spark" class=tag>#Apache Spark</a>
<a href="/search/?tags=Spark%20SQL" class=tag>#Spark SQL</a>
<a href="/search/?tags=SQL" class=tag>#SQL</a>
<a href="/search/?tags=View" class=tag>#View</a>
<a href="/search/?tags=PySpark" class=tag>#PySpark</a>
<a href="/search/?tags=%eb%8d%b0%ec%9d%b4%ed%84%b0%20%ec%97%94%ec%a7%80%eb%8b%88%ec%96%b4%eb%a7%81" class=tag>#데이터 엔지니어링</a>
<a href="/search/?tags=%ec%8a%a4%ed%8c%8c%ed%81%ac" class=tag>#스파크</a>
<a href="/search/?tags=Study" class=tag>#Study</a></div><div class=prev-next-wrap><a href=/blog/spark-study-4/ class=prev-link><span class=prev-next-label><i class="fa-solid fa-backward"></i> PREV</span>
<span class=prev-next-title>Apache Spark - DataFrame과 Dataset API 활용하기</span>
</a><a href=/blog/spark-study-6/ class=next-link><span class=prev-next-label>NEXT <i class="fa-solid fa-forward"></i></span>
<span class=prev-next-title>Apache Spark - 다양한 데이터 소스 읽기/쓰기 (Parquet, JSON, CSV, Avro)</span></a></div><div class=comments-wrap><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://minyeamer.github.io/blog/spark-study-5/",this.page.identifier="https://minyeamer.github.io/blog/spark-study-5/"};(function(){var e=document,t=e.createElement("script");t.src="https://minyeamer.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})();function reloadDisqus(){window.DISQUS&&DISQUS.reset({reload:!0,config:function(){this.page.url="https://minyeamer.github.io/blog/spark-study-5/",this.page.identifier="https://minyeamer.github.io/blog/spark-study-5/"}})}</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div><div class="copyright-wrap flex justify-center"><small>Minystory - <a href=https://creativecommons.org/licenses/by/4.0/legalcode target=_blank rel="noopener noreferrer">© CC BY 4.0</a></small></div></footer><label for=menu-control class="hidden site-menu-overlay"></label></div><aside class=site-toc aria-label=목차><div class=toc-content><nav id=TableOfContents><ul><li><a href=#spark-sql>Spark SQL</a><ul><li><a href=#spark-sql-사용법>Spark SQL 사용법</a></li><li><a href=#spark-sql-활용-python>Spark SQL 활용 (Python)</a></li></ul></li><li><a href=#table--view>Table & View</a><ul><li><a href=#테이블-생성하기>테이블 생성하기</a></li><li><a href=#관리형-테이블-생성하기>관리형 테이블 생성하기</a></li><li><a href=#비관리형-테이블-생성하기>비관리형 테이블 생성하기</a></li><li><a href=#뷰-생성하기>뷰 생성하기</a></li><li><a href=#메타데이터-보기>메타데이터 보기</a></li><li><a href=#sql-테이블-캐싱하기>SQL 테이블 캐싱하기</a></li><li><a href=#테이블을-dataframe으로-변환하기>테이블을 DataFrame으로 변환하기</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div><div class=hidden id=toc-config data-start=2 data-end=3></div></aside><div class=site-toolbar role=toolbar aria-label="툴바'"><input type=checkbox id=toolbar-toggle class="hidden toggle"><div class=toolbar-items><button class=toolbar-button onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=toolbar-button onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
</button>
<button class=toolbar-button onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div><label for=toolbar-toggle class="toolbar-button toolbar-toggle-label"><i class="fa-solid fa-plus icon-expand"></i>
<i class="fa-solid fa-xmark icon-collapse"></i></label></div></main></body></html>