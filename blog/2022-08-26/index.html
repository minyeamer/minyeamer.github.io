<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>2022-08-26 Log | Minystory</title><meta name=keywords content="TIL,NLP,Paper"><meta name=description content="Language models paper reviews"><meta name=author content="minyeamer"><link rel=canonical href=https://minyeamer.github.io/blog/2022-08-26/><meta name=google-site-verification content="u1tWcHmHUZWfFT1cHaku6sqU-bK40N3WLR-C-4VUWN0"><meta name=naver-site-verification content="6eaf8e9da1a6104780f056f1a7797fe5a3a5a0da"><link crossorigin=anonymous href=/assets/css/stylesheet.78a14cf8249250820b49e9dc59e58b846a9beea6d16d50612c68b121ddf02146.css integrity="sha256-eKFM+CSSUIILSencWeWLhGqb7qbRbVBhLGixId3wIUY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://minyeamer.github.io/img/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://minyeamer.github.io/img/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://minyeamer.github.io/img/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://minyeamer.github.io/img/favicons/apple-touch-icon.png><link rel=mask-icon href=https://minyeamer.github.io/img/favicons/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V")</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V",{anonymize_ip:!1})}</script><meta property="og:title" content="2022-08-26 Log"><meta property="og:description" content="Language models paper reviews"><meta property="og:type" content="article"><meta property="og:url" content="https://minyeamer.github.io/blog/2022-08-26/"><meta property="og:image" content="https://github.com/minyeamer/til/blob/main/.media/main/thumbnail.png?raw=true"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-08-26T20:00:00+09:00"><meta property="article:modified_time" content="2022-08-26T20:00:00+09:00"><meta property="og:site_name" content="Minystory"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://github.com/minyeamer/til/blob/main/.media/main/thumbnail.png?raw=true"><meta name=twitter:title content="2022-08-26 Log"><meta name=twitter:description content="Language models paper reviews"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://minyeamer.github.io/post/"},{"@type":"ListItem","position":2,"name":"2022-08-26 Log","item":"https://minyeamer.github.io/blog/2022-08-26/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"2022-08-26 Log","name":"2022-08-26 Log","description":"Language models paper reviews","keywords":["TIL","NLP","Paper"],"articleBody":"NNLM Word Embedding: 의미적으로 유사한 단어를 가까운 벡터 공간에 표현 one-hot vector: 단어의 개수만큼의 공간을 표현, 모든 단어간 유사도가 0에 가까움 어떤 feature vector 표현이 좋고, sequence들의 probability가 높음을 학습 chain rule에 기반해 이전 단어에 대한 다음 단어의 확률을 계산,\n단어의 개수가 많아질수록 과거로 봐야할 단어의 수가 많아지기 때문에,\nMarkov assumption을 적용해 n개의 단어만 참조 (n-grams) input이 hidden node를 거치지 않고 output으로 연결되는 skip-connection이 존재할 수 있음 (optional) NNLM의 목적은 윈도우 내에서 t번째 단어에 대해 t-1번째 단어의 확률이 극대화되는 것을 학습 Word2Vec CBOW: 주변 단어를 가지고 중심 단어를 예측, Skip-gram: 중심 단어를 가지고 주변 단어 예측 CBOW는 gradient가 각각의 주변 단어에 분산되지만,\nSkip-gram은 주변 단어의 gradient를 합산하는 방향으로 학습하여 성능이 좋음 activation function이 존재하지 않는 linear 구조 Skip-gram의 목적 함수는 t번째 단어에 대해 좌우로 n개의 단어에 대한 확률 합의 최대화 대상 단어 벡터와 특정 단어 벡터 간 내적을 하고 총합에 softmax를 취함 빈번한 단어는 한쌍으로 묶고, 빈도가 높은 단어의 학습을 제거해 계산을 간소화 negative sampling: output 단어에 대한 softmax 값을 계산하기 위해 나머지 모든 것들에 대한 내적값을 계산해야 하는데,\nk 개의 예시에 대해서만 sampling하여 계산의 효율성을 추구 FastText 형태론적 feature 정보를 한 단어의 subwork unit, 문자 단위에서 추출하는 기법 기존 방법론은 모든 단어를 각각의 vector로 표현하는 것에 한계가 있고 (OOV 문제),\n단어의 내부적 구조를 무시하여 유사한 형태의 단어를 표현하지 못함 skip-gram 기반에 문자 단위 character n-gram을 활용 기존 모델의 경우 corpus 대비 context word에 대해서만 학습해 비효율적인 연산이 많은데,\nnegative sampling을 적용해 일정한 확률값으로 뽑인 word를 참고 공통된느 형태소들에 대해 parameter sharing을 하여 embedding에 반영 (의미 공유) 단어의 시작과 끝 표현 ‘\u003c’, ‘\u003e‘을 추가하고 n개 문자 단위의 벡터를 사용해 계산하며,\nn-gram이 커지는 문제를 방지하기 위해 hasing function으로 hash값 계산 LSTM RNN에서 새로운 가중치는 기존 가중치 W - lr * W에 대한 미분값의 chain rule인데,\nhidden state가 많아질수록 새로운 weight가 기존 weight와 거의 차이가 없어짐 memory cell을 추가해 새로운 input에 대해 과거의 정보에 대해 몇 퍼센트를 기억할지 결정하고 나머지 정보를 제거 일부가 제거된 cell state는 새로운 input에 대한 hidden state와 더해지고,\n이것이 다시 hidden state와 곱해져 다음 hidden state를 생성 Seq2Seq LSTM을 활용한 효율적인 기계 번역 아키텍쳐 전통적인 RNN 기반의 기계 번역은 입력과 출력의 크기가 같다고 가정 (입력 토큰과 출력 토큰이 대응) 위 문제를 해결하기 위해 인코더에서 고정된 크기의 context vector를 추출,\n디코더가 문맥 벡터로부터 번역 결과를 추론 (인코더와 디코더는 서로 다른 파라미터를 가짐) 시작 시점에 대한 토큰 와 종료 시점에 대한 토큰 를 추가하여,\n종료 토큰을 생성할때까지 반복문을 반복 입력 문장의 순서를 거꾸로 했을 때 더 높은 정확도를 보임 (앞쪽에 위치한 단어끼리 연관성이 높음) Attention을 적용한 Seq2Seq의 경우 모든 hidden states를 디코더로 전달하여,\n필요한 hidden state를 선택 ELMo ELMo에서의 embedding vector는 bi-directional LSTM에서 가져옴 forward 및 backward의 각 단계별 hidden state를 concatenate하고,\n각각의 벡터에 대한 가중합을 통해 embedding을 생성 특정 task에 대한 가중합 embedding과 task에 대한 scale vector를 곱해 계산 forward LM의 경우 k번째 토큰의 j번째 hidden state를 사용하며,\n마지막 hidden state가 t+1 단어를 예측하는데 사용 backward LM의 경우 역방향으로 t-1 단어를 예측 bi-directional LSTM은 forward와 backward에서의 정확도를 함께 최대화 2L+1개의 표현 R에 가중합을 취해 하나의 벡터로 만드는데, 태스크에 맞는 j번째 layer에 가중칠르 두고 전반적으로 스케일을 취함 가중합을 취하는 방식에 대해선 task별로 각각 취하는 것이 가장 좋고,\n1/(L+1)로 통합하는 것이 다음, 마지막 hidden state의 가중치를 사용하는 것이 다음으로 좋음 ELMo의 조합은 downstream task 모델의 input과 output 양쪽에 붙이는게 가장 좋고,\ninput에 붙이는 것, output에 붙이는 것 순으로 좋음 GPT unlabeled text 데이터가 labeled text 데이터보다 훨씬 많기 때문에,\n사전학습을 우선 수행하고 labeled 데이터에 대해 fine-tuning을 수행하면 더욱 도움이 될 것 transformer의 decoder block을 사용하며,\nencoder-decoder 간의 masked multi-head attention 없이 단순히 쌓아올림 ELMo는 bi-directional LSTM을 사용하는 반면, GPT는 마스크된 forward를 사용해 다음 단어를 예측 일반적인 LM은 전 단계까지의 시퀀스에 대해 i번째 토큰의 likelihood를 최대화하는 것이 목적 토큰 임베딩과 포지션 임베딩을 더해 첫번째 hidden state를 만들고,\nl-1 번째 hidden state를 decoder block을 통과시켜 l번째 hidden state 생성 GPT는 입력 단어에 대해 정방향으로 예측을 수행하기 때문에 BERT처럼 masking할 필요가 없음 BERT와 달리 각각의 토큰이 생성되면, 그 다음 토큰을 생성하기 위해 해당 토큰이 사용되는 auto-regressive 방식을 가짐 GPT-3 transformer 모델에서 log loss는 scale이 커질수록 개선되고, context도 마찬가지로 scale이 커질수록 정보가 향상됨 example이 많을수록 성능이 향상되는데 scale이 클수록 차이가 도드라짐 기존엔 각각의 example에 대한 결과를 보면서 gradient를 update하면서 fine-tuning 하는데,\nGPT-3는 zero-shot일 경우 task description과 prompt를 주고,\none-shot일 경우 하나의 example, few-shot일 경우 여러 개의 example을 줌 (fine-tuning을 다시 하지 않음) fine-tuning은 새로운 데이터셋이 필요하면서, 일반화 성능이 떨어지고 과적합 문제가 발생,\nfew-shot learning은 task-specific 데이터를 사용하지 않지만 SOTA보다는 성능이 떨어짐 기존 transformer는 이전 토큰에 대해 모두 attention이 걸리는데,\nsparse transformer를 사용해 개선 학습 데이터와 테스트 데이터 간 overlap이 존재하는데, 비용이 많이 들어 해결하지 못함 문서 레벨에서 특정 표현을 반복하는 경우, 매우 긴 문장의 경우 등 특정 task에 대해 성능이 떨어짐 bidirectional이 아닌 auto-regressive한 구조적 단점 ","wordCount":"761","inLanguage":"en","datePublished":"2022-08-26T20:00:00+09:00","dateModified":"2022-08-26T20:00:00+09:00","author":{"@type":"Person","name":"minyeamer"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://minyeamer.github.io/blog/2022-08-26/"},"publisher":{"@type":"Organization","name":"Minystory","logo":{"@type":"ImageObject","url":"https://minyeamer.github.io/img/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://minyeamer.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://minyeamer.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://minyeamer.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://minyeamer.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://minyeamer.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://minyeamer.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://minyeamer.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://minyeamer.github.io/post/>Posts</a></div><h1 class=post-title>2022-08-26 Log</h1><div class=post-meta><span title='2022-08-26 20:00:00 +0900 KST'>August 26, 2022</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;761 words&nbsp;·&nbsp;minyeamer&nbsp;|&nbsp;<a href=https://github.com/minyeamer/til/edit/main/study/2022/2022-08/2022-08-26.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>&nbsp;Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#nnlmhttpsyoutubebvshjg-fz3y><a href=https://youtu.be/bvSHJG-Fz3Y>NNLM</a></a></li><li><a href=#word2vechttpsyoutubes2kepv-oxzm><a href=https://youtu.be/s2KePv-OxZM>Word2Vec</a></a></li><li><a href=#fasttexthttpsyoutube7ua21vg4kke><a href=https://youtu.be/7UA21vg4kKE>FastText</a></a></li><li><a href=#lstmhttpsyoutubebx6glbpw-a4><a href=https://youtu.be/bX6GLbpw-A4>LSTM</a></a></li><li><a href=#seq2seqhttpsyoutube4dzkm0vgg1y><a href=https://youtu.be/4DzKM0vgG1Y>Seq2Seq</a></a></li><li><a href=#elmohttpsyoutubezv8kiuwh32m><a href=https://youtu.be/zV8kIUwH32M>ELMo</a></a></li><li><a href=#gpthttpsyoutubeo_wl29aw5xm><a href=https://youtu.be/o_Wl29aW5XM>GPT</a></a></li><li><a href=#gpt-3httpsyoutubexndp3_zrr8q><a href=https://youtu.be/xNdp3_Zrr8Q>GPT-3</a></a></li></ul></nav></div></details></div><div class=post-content><h1 id=nnlmhttpsyoutubebvshjg-fz3y><a href=https://youtu.be/bvSHJG-Fz3Y target=_blank rel=noopener>NNLM</a><a hidden class=anchor aria-hidden=true href=#nnlmhttpsyoutubebvshjg-fz3y>#</a></h1><ul><li>Word Embedding: 의미적으로 유사한 단어를 가까운 벡터 공간에 표현</li><li>one-hot vector: 단어의 개수만큼의 공간을 표현, 모든 단어간 유사도가 0에 가까움</li><li>어떤 feature vector 표현이 좋고, sequence들의 probability가 높음을 학습</li><li>chain rule에 기반해 이전 단어에 대한 다음 단어의 확률을 계산,<br>단어의 개수가 많아질수록 과거로 봐야할 단어의 수가 많아지기 때문에,<br>Markov assumption을 적용해 n개의 단어만 참조 (n-grams)</li><li>input이 hidden node를 거치지 않고 output으로 연결되는 skip-connection이 존재할 수 있음 (optional)</li><li>NNLM의 목적은 윈도우 내에서 t번째 단어에 대해 t-1번째 단어의 확률이 극대화되는 것을 학습</li></ul><h1 id=word2vechttpsyoutubes2kepv-oxzm><a href=https://youtu.be/s2KePv-OxZM target=_blank rel=noopener>Word2Vec</a><a hidden class=anchor aria-hidden=true href=#word2vechttpsyoutubes2kepv-oxzm>#</a></h1><ul><li>CBOW: 주변 단어를 가지고 중심 단어를 예측, Skip-gram: 중심 단어를 가지고 주변 단어 예측</li><li>CBOW는 gradient가 각각의 주변 단어에 분산되지만,<br>Skip-gram은 주변 단어의 gradient를 합산하는 방향으로 학습하여 성능이 좋음</li><li>activation function이 존재하지 않는 linear 구조</li><li>Skip-gram의 목적 함수는 t번째 단어에 대해 좌우로 n개의 단어에 대한 확률 합의 최대화</li><li>대상 단어 벡터와 특정 단어 벡터 간 내적을 하고 총합에 softmax를 취함</li><li>빈번한 단어는 한쌍으로 묶고, 빈도가 높은 단어의 학습을 제거해 계산을 간소화</li><li>negative sampling: output 단어에 대한 softmax 값을 계산하기 위해 나머지 모든 것들에 대한 내적값을 계산해야 하는데,<br>k 개의 예시에 대해서만 sampling하여 계산의 효율성을 추구</li></ul><h1 id=fasttexthttpsyoutube7ua21vg4kke><a href=https://youtu.be/7UA21vg4kKE target=_blank rel=noopener>FastText</a><a hidden class=anchor aria-hidden=true href=#fasttexthttpsyoutube7ua21vg4kke>#</a></h1><ul><li>형태론적 feature 정보를 한 단어의 subwork unit, 문자 단위에서 추출하는 기법</li><li>기존 방법론은 모든 단어를 각각의 vector로 표현하는 것에 한계가 있고 (OOV 문제),<br>단어의 내부적 구조를 무시하여 유사한 형태의 단어를 표현하지 못함</li><li>skip-gram 기반에 문자 단위 character n-gram을 활용</li><li>기존 모델의 경우 corpus 대비 context word에 대해서만 학습해 비효율적인 연산이 많은데,<br>negative sampling을 적용해 일정한 확률값으로 뽑인 word를 참고</li><li>공통된느 형태소들에 대해 parameter sharing을 하여 embedding에 반영 (의미 공유)</li><li>단어의 시작과 끝 표현 &lsquo;&lt;&rsquo;, &lsquo;>&lsquo;을 추가하고 n개 문자 단위의 벡터를 사용해 계산하며,<br>n-gram이 커지는 문제를 방지하기 위해 hasing function으로 hash값 계산</li></ul><h1 id=lstmhttpsyoutubebx6glbpw-a4><a href=https://youtu.be/bX6GLbpw-A4 target=_blank rel=noopener>LSTM</a><a hidden class=anchor aria-hidden=true href=#lstmhttpsyoutubebx6glbpw-a4>#</a></h1><ul><li>RNN에서 새로운 가중치는 기존 가중치 W - lr * W에 대한 미분값의 chain rule인데,<br>hidden state가 많아질수록 새로운 weight가 기존 weight와 거의 차이가 없어짐</li><li>memory cell을 추가해 새로운 input에 대해 과거의 정보에 대해 몇 퍼센트를 기억할지 결정하고 나머지 정보를 제거</li><li>일부가 제거된 cell state는 새로운 input에 대한 hidden state와 더해지고,<br>이것이 다시 hidden state와 곱해져 다음 hidden state를 생성</li></ul><h1 id=seq2seqhttpsyoutube4dzkm0vgg1y><a href=https://youtu.be/4DzKM0vgG1Y target=_blank rel=noopener>Seq2Seq</a><a hidden class=anchor aria-hidden=true href=#seq2seqhttpsyoutube4dzkm0vgg1y>#</a></h1><ul><li>LSTM을 활용한 효율적인 기계 번역 아키텍쳐</li><li>전통적인 RNN 기반의 기계 번역은 입력과 출력의 크기가 같다고 가정 (입력 토큰과 출력 토큰이 대응)</li><li>위 문제를 해결하기 위해 인코더에서 고정된 크기의 context vector를 추출,<br>디코더가 문맥 벡터로부터 번역 결과를 추론 (인코더와 디코더는 서로 다른 파라미터를 가짐)</li><li>시작 시점에 대한 토큰 &lt;sos>와 종료 시점에 대한 토큰 &lt;eos>를 추가하여,<br>종료 토큰을 생성할때까지 반복문을 반복</li><li>입력 문장의 순서를 거꾸로 했을 때 더 높은 정확도를 보임 (앞쪽에 위치한 단어끼리 연관성이 높음)</li><li>Attention을 적용한 Seq2Seq의 경우 모든 hidden states를 디코더로 전달하여,<br>필요한 hidden state를 선택</li></ul><h1 id=elmohttpsyoutubezv8kiuwh32m><a href=https://youtu.be/zV8kIUwH32M target=_blank rel=noopener>ELMo</a><a hidden class=anchor aria-hidden=true href=#elmohttpsyoutubezv8kiuwh32m>#</a></h1><ul><li>ELMo에서의 embedding vector는 bi-directional LSTM에서 가져옴</li><li>forward 및 backward의 각 단계별 hidden state를 concatenate하고,<br>각각의 벡터에 대한 가중합을 통해 embedding을 생성</li><li>특정 task에 대한 가중합 embedding과 task에 대한 scale vector를 곱해 계산</li><li>forward LM의 경우 k번째 토큰의 j번째 hidden state를 사용하며,<br>마지막 hidden state가 t+1 단어를 예측하는데 사용</li><li>backward LM의 경우 역방향으로 t-1 단어를 예측</li><li>bi-directional LSTM은 forward와 backward에서의 정확도를 함께 최대화</li><li>2L+1개의 표현 R에 가중합을 취해 하나의 벡터로 만드는데, 태스크에 맞는 j번째 layer에 가중칠르 두고 전반적으로 스케일을 취함</li><li>가중합을 취하는 방식에 대해선 task별로 각각 취하는 것이 가장 좋고,<br>1/(L+1)로 통합하는 것이 다음, 마지막 hidden state의 가중치를 사용하는 것이 다음으로 좋음</li><li>ELMo의 조합은 downstream task 모델의 input과 output 양쪽에 붙이는게 가장 좋고,<br>input에 붙이는 것, output에 붙이는 것 순으로 좋음</li></ul><h1 id=gpthttpsyoutubeo_wl29aw5xm><a href=https://youtu.be/o_Wl29aW5XM target=_blank rel=noopener>GPT</a><a hidden class=anchor aria-hidden=true href=#gpthttpsyoutubeo_wl29aw5xm>#</a></h1><ul><li>unlabeled text 데이터가 labeled text 데이터보다 훨씬 많기 때문에,<br>사전학습을 우선 수행하고 labeled 데이터에 대해 fine-tuning을 수행하면 더욱 도움이 될 것</li><li>transformer의 decoder block을 사용하며,<br>encoder-decoder 간의 masked multi-head attention 없이 단순히 쌓아올림</li><li>ELMo는 bi-directional LSTM을 사용하는 반면, GPT는 마스크된 forward를 사용해 다음 단어를 예측</li><li>일반적인 LM은 전 단계까지의 시퀀스에 대해 i번째 토큰의 likelihood를 최대화하는 것이 목적</li><li>토큰 임베딩과 포지션 임베딩을 더해 첫번째 hidden state를 만들고,<br>l-1 번째 hidden state를 decoder block을 통과시켜 l번째 hidden state 생성</li><li>GPT는 입력 단어에 대해 정방향으로 예측을 수행하기 때문에 BERT처럼 masking할 필요가 없음</li><li>BERT와 달리 각각의 토큰이 생성되면, 그 다음 토큰을 생성하기 위해 해당 토큰이 사용되는 auto-regressive 방식을 가짐</li></ul><h1 id=gpt-3httpsyoutubexndp3_zrr8q><a href=https://youtu.be/xNdp3_Zrr8Q target=_blank rel=noopener>GPT-3</a><a hidden class=anchor aria-hidden=true href=#gpt-3httpsyoutubexndp3_zrr8q>#</a></h1><ul><li>transformer 모델에서 log loss는 scale이 커질수록 개선되고, context도 마찬가지로 scale이 커질수록 정보가 향상됨</li><li>example이 많을수록 성능이 향상되는데 scale이 클수록 차이가 도드라짐</li><li>기존엔 각각의 example에 대한 결과를 보면서 gradient를 update하면서 fine-tuning 하는데,<br>GPT-3는 zero-shot일 경우 task description과 prompt를 주고,<br>one-shot일 경우 하나의 example, few-shot일 경우 여러 개의 example을 줌 (fine-tuning을 다시 하지 않음)</li><li>fine-tuning은 새로운 데이터셋이 필요하면서, 일반화 성능이 떨어지고 과적합 문제가 발생,<br>few-shot learning은 task-specific 데이터를 사용하지 않지만 SOTA보다는 성능이 떨어짐</li><li>기존 transformer는 이전 토큰에 대해 모두 attention이 걸리는데,<br>sparse transformer를 사용해 개선</li><li>학습 데이터와 테스트 데이터 간 overlap이 존재하는데, 비용이 많이 들어 해결하지 못함</li><li>문서 레벨에서 특정 표현을 반복하는 경우, 매우 긴 문장의 경우 등 특정 task에 대해 성능이 떨어짐</li><li>bidirectional이 아닌 auto-regressive한 구조적 단점</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://minyeamer.github.io/tags/til/>TIL</a></li><li><a href=https://minyeamer.github.io/tags/nlp/>NLP</a></li><li><a href=https://minyeamer.github.io/tags/paper/>Paper</a></li></ul><nav class=paginav><a class=prev href=https://minyeamer.github.io/blog/jekyll-blog/><span class=title>« Prev</span><br><span>깃허브 블로그 시작하기</span></a>
<a class=next href=https://minyeamer.github.io/blog/boj-problems-1308/><span class=title>Next »</span><br><span>[백준 1308] D-Day (Python)</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 2022-08-26 Log on twitter" href="https://twitter.com/intent/tweet/?text=2022-08-26%20Log&url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-08-26%2f&hashtags=TIL%2cNLP%2cPaper"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-08-26 Log on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-08-26%2f&title=2022-08-26%20Log&summary=2022-08-26%20Log&source=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-08-26%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-08-26 Log on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-08-26%2f&title=2022-08-26%20Log"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-08-26 Log on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-08-26%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-08-26 Log on whatsapp" href="https://api.whatsapp.com/send?text=2022-08-26%20Log%20-%20https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-08-26%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-08-26 Log on telegram" href="https://telegram.me/share/url?text=2022-08-26%20Log&url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-08-26%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=minyeamer/til issue-term=pathname label=comments theme=preferred-color-scheme crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://minyeamer.github.io/>Minystory</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>