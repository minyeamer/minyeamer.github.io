<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>2022-07-17 Log | Minystory</title><meta name=keywords content="TIL,NLP,BERT"><meta name=description content="구글 BERT의 정석 1"><meta name=author content="minyeamer"><link rel=canonical href=https://minyeamer.github.io/blog/2022-07-17/><meta name=google-site-verification content="u1tWcHmHUZWfFT1cHaku6sqU-bK40N3WLR-C-4VUWN0"><meta name=naver-site-verification content="6eaf8e9da1a6104780f056f1a7797fe5a3a5a0da"><link crossorigin=anonymous href=/assets/css/stylesheet.78a14cf8249250820b49e9dc59e58b846a9beea6d16d50612c68b121ddf02146.css integrity="sha256-eKFM+CSSUIILSencWeWLhGqb7qbRbVBhLGixId3wIUY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://minyeamer.github.io/img/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://minyeamer.github.io/img/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://minyeamer.github.io/img/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://minyeamer.github.io/img/favicons/apple-touch-icon.png><link rel=mask-icon href=https://minyeamer.github.io/img/favicons/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V")</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V",{anonymize_ip:!1})}</script><meta property="og:title" content="2022-07-17 Log"><meta property="og:description" content="구글 BERT의 정석 1"><meta property="og:type" content="article"><meta property="og:url" content="https://minyeamer.github.io/blog/2022-07-17/"><meta property="og:image" content="https://minyeamer.github.io/calendar.jpg"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-07-17T20:00:00+09:00"><meta property="article:modified_time" content="2022-07-17T20:00:00+09:00"><meta property="og:site_name" content="Minystory"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://minyeamer.github.io/calendar.jpg"><meta name=twitter:title content="2022-07-17 Log"><meta name=twitter:description content="구글 BERT의 정석 1"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://minyeamer.github.io/post/"},{"@type":"ListItem","position":2,"name":"2022-07-17 Log","item":"https://minyeamer.github.io/blog/2022-07-17/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"2022-07-17 Log","name":"2022-07-17 Log","description":"구글 BERT의 정석 1","keywords":["TIL","NLP","BERT"],"articleBody":"기계번역 과제 인코더: 입력 문장의 표현 방법 학습 디코더: 인코더에서 학습한 표현 결과를 입력받아 사용자가 원하는 문장 생성 Encoder Self Attention 임베딩: 각각의 단어를 표현하는 벡터값, [문장 길이 x 임베딩 차원] 쿼리(Q), 키(K), 밸류(V) \u003e 각각의 가중치 행렬을 입력 행렬에 곱해 Q, K, V 행렬 생성 Q, K, V의 차원 [문장 길이 x 벡터의 차원] 1단계: Q와 K^T 행렬의 내적 연산, 쿼리 벡터(I)와 키 벡터(I, am, good) 사이의 유사도 계산 2단계: QK^T 행렬을 키 벡터 차원의 제곱근값($\\sqrt{d_k}$)으로 나눈 것, 안정적인 gradient 얻음 3단계: 소프트맥스 함수를 사용해 비정규화된 형태의 유사도 값을 정규화 (score 행렬) 4단계: 스코어 행렬에 V 행렬을 곱해 어텐션(Z) 행렬 계산, 어텐션 행렬은 문장의 각 단어와 벡터값 가짐\n(단어 I의 셀프 어텐션은 각 밸류 벡터값의 가중치 합으로 계산, 단어가 문장 내에 있는 다른 단어와의 연관성) Multi-Head Attention 문장 내에서 모호한 의미를 가진 단어(it)가 있을 경우,\n문장의 의미가 잘못 해석될 수 있기 때문에 멀티 헤드 어텐션을 사용한 후 그 결괏값을 더함 다수의 어텐션 행렬을 구하기 위해 서로 다른 가중치 행렬을 입력 행렬에 곱해 Q, K, V 생성 다수의 어텐션 행렬을 concatenate하고 새로운 가중치 행렬을 곱해 멀티 헤드 어텐션 결과 도출\n(concatenate 시 [어텐션 헤드 x h] 크기가 되기 때문에 원래 크기로 만들기 위해 가중치 행렬 곱함) Positional Encoding 트랜스포머는 문장 안에 있는 모든 단어를 병렬 형태로 입력 단어의 순서 정보를 제공하기 위해 문장에서 단어의 위치를 나타내는 인코딩 제공 위치 인코딩은 사인파 함수를 사용 입력 임베딩 결과에 위치 인코딩을 합한 후 멀티 헤드 어텐션에 입력 Feed Forward Network 2개의 전결합층(Dense)과 ReLU 활성화 함수로 구성 add와 norm을 추가해 서브레이어에서 멀티 헤드 어텐션의 입력값과 출력값을 서로 연결 add와 norm은 레이어 정규화(각 레이어 값이 크게 변화하는 것을 방지해 모델 학습 빠르게)와 잔차 연결 인코더 순서 입력값은 입력 임베딩으로 변환한 다음 위치 인코딩 추가, 가장 아래 있는 인코더 1의 입력값으로 공급 인코더 1은 입력값을 받아 멀티 헤드 어텐션의 서브레이어에 값을 보냄, 어텐션 행렬을 결괏값으로 출력 어텐션 행렬의 값을 다음 서브레이어인 피드포워드 네트워크에 입력, 결괏값 출력 인코더 1의 출력값을 그 위에 있는 인코더 2에 입력값으로 제공 인코더 2에서 이전과 동일한 방법 수행, 주어진 문장에 대한 인코더 표현 결과를 출력으로 제공 Decoder 이전 디코더의 입력값과 인코더의 표현(인코더의 출력값), 2개를 입력 데이터로 받음 t=1에서 디코더의 입력값은 문장의 시작을 알리는 를 입력 \u003e 타깃 문장의 첫 번째 단어(Je) 생성 t=2에서 t-1 디코더에서 생성한 단어(, Je)를 추가해 문장의 다음 단어 생성 t=3에서도 동일하게 (, Je, vais)를 입력받아 다음 단어 생성 디코더에서 토큰을 생성할 때 타깃 문장의 생성이 완료 디코더도 입력값을 바로 입력하지 않고 위치 인코딩을 추가한 값을 출력 임베딩에 더해 입력값으로 사용 Masked Multi-Head Attention 디코더에서 문장을 생성할 때 이전 단계에서 생성한 단어만 입력으로 넣기 때문에,\n아직 예측하지 않은 오른쪽의 모든 단어를 마스킹해 학습을 진행 소프트맥스 함수를 적용한 정규화 작업 전에 오른쪽의 모든 단어를 $-{\\infty}$로 마스킹 수행\n($-{\\infty}$는 학습 도중 발산하는 경우가 있기 때문에 실제로는 작은 값 $e^{-9}$으로 지정) Encoder-Decoder Attention Layer 디코더의 멀티 헤드 어텐션의 입력으로 인코더의 표현값 R과 마스크된 멀티 헤드 어텐션의 결과 M을 받을 때 상호작용 발생 쿼리, 키, 밸류 행렬을 생성할 때, M을 사용해 Q를 생성, R을 활용해 K, V를 생성\n(쿼리 행렬은 타깃 문장의 표현을 포함하기 때문에 M을 참조, 키와 밸류 행렬은 입력 문장의 표현을 참조) 쿼리, 키 행렬 간의 내적 시 타깃 단어 가 입력 문장의 모든 단어(I, am, good)와 얼마나 유사한지 계산\n(두 번째 행에서 Je에 대해, 나머지 행에서도 동일한 방법을 적용해 유사도 계산) Linear and Sofmax Layer 최상위 디코더에서 얻은 출력 값을 선형 및 소프트맥스 레이어에 전달 선형 레이어는 vocab 크기와 같은 logit 형태 logit 값을 확률값으로 변환하고, 디코더에서 가장 높은 확률값을 갖는 인덱스의 단어로 출력 디코더 순서 디코더에 대한 입력 문장을 임베딩 행렬로 변환하고 위치 인코딩 정보를 추가해 디코더 1에 입력 입력을 가져와서 마스크된 멀티 헤드 어텐션 레이어에 보내고, 출력으로 어텐션 행렬 M 반환 어텐션 행렬 M, 인코딩 표현 R을 입력받아 멀티 헤드 어텐션 레이어에 값을 입력, 새로운 어텐션 행렬 생성 인코더-디코더 어텐션 레이어에서 출력한 어텐션 행렬을 피드포워드 네트워크에 입력, 디코더의 표현으로 값 출력 디코더 1의 출력값을 다음 디코더 2의 입력값으로 사용 디코더 2는 이전과 동일한 방법 수행, 타깃 문장에 대한 디코더 표현 반환 타깃 문장의 디코더 표현을 선형 및 소프트맥스 레이어에 입력해 최종으로 예측된 단어 얻음 학습 손실 함수로 cross-entropy를 사용해 분포의 차이를 확인 옵티마이저로 Adam 사용 과적합을 방지하기 위해 각 서브레이어 출력에 dropout 적용 (임베딩 및 위치 인코딩 합을 구할 때도 포함) BERT Word2Vec: 문맥 독립 임베딩, BERT: 문맥 기반 임베딩 BERT는 인코더-디코더가 있는 트랜스포머 모델에서 인코더만 사용 BERT-base: L(인코더 레이어)=12, A(어텐션 헤드)=12, H(은닉 유닛)=768 BERT-large: L=24, A=16, H=1024 BERT-tiny(L=2, A=2, H=128), BERT-mini(L=4, A=4, H=256) 등 사전 학습 대규모 데이터셋으로 학습된 가중치를 활용해 새로운 태스크에 적용 (find-tuning) BERT는 MLM과 NSP 태스크를 이용해 거대한 말뭉치를 기반으로 사전 학습 Token Embedding 첫 번째 문장의 시작 부분에 [CLS] 토큰 추가 모든 문장 끝에 [SEP] 토큰 추가 Segment Embedding 두 문장을 구별하는데 사용 [SEP] 토큰과 별도로 두 문장을 구분하기 위해 입력 토큰($E_A,E_B$) 제공 Position Embedding 단어(토큰)의 위치에 대한 정보 제공 입력 데이터 토큰 임베딩 + 세그먼트 임베딩 + 위치 임베딩 으로 표현 WordPiece Tokenizer 하위 단어 토큰화 알고리즘 기반 Let us start pretraining the model \u003e [let, us, start, pre, ##train, ##ing, the, model] 단어가 어휘 사전에 있으면 토큰으로 사용, 없으면 하위 단어로 분할해 하위 단어가 어휘 사전에 있는지 확인 (OOV 처리에 효과적) Language Modeling 임의의 문장이 주어지고 단어를 순서대로 보면서 다음 단어를 예측하도록 모델 학습 자동 회귀 언어 모델링: 전방(좌\u003e우) 예측, 후방(좌\u003c우) 예측, 각 방향(단방향)으로 공백까지 모든 단어를 읽음 자동 인코딩 언어 모델링: 예측하면서 양방향으로 문장을 읽음 Masked Language Modeling (MLM) 주어진 입력 문장에서 전체 단어의 15%를 무작위로 마스킹, 마스크된 단어를 예측 (빈칸 채우기 태스크) [MASK] 토큰을 사전 학습시킬 경우 파인 튜닝 시 입력에 [MASK] 토큰이 없어 불일치가 발생 15% 토큰에 대해 80%만 [MASK] 토큰으로 교체, 10%는 임의의 토큰(단어)로 교체, 10%는 변경하지 않음\n(사전 학습과 파인 튜닝 태스크의 차이를 줄이기 위한 일종의 정규화 작업) 역전파를 통한 반복 학습을 거치며 최적의 가중치 학습 Whole Word Masking (WWM) WWM 방법에서는 하위 단어가 마스킹되면 해당 하위 단어와 관련된 모든 단어를 마스킹 하위 단어와 관련된 모든 단어의 마스크 비율이 15%를 초과하면 다른 단어의 마스킹을 무시 Next Sentence Prediction (NSP) BERT에 두 문장을 입력하고 두 번째 문장이 첫 번째 문장의 다음 문장인지 예측 B 문장이 A 문장에 이어지만 isNext를 반환하고, 그렇지 않으면 notNext를 반환 두 문장 사이의 관계를 파악해 질문-응답 및 유사문장탐지와 같은 downstream 태스크에서 유용 한 문서에서 연속된 두 문장을 isNext로 표시하고, 두 문서에서 각각 문장을 가져와 notNext로 표시 [CLS] 토큰 표현에 소프트맥스 함수를 사용하고 피드포워드 네트워크에 입력해 두 클래스에 대한 확률값 반환 [CLS] 토큰은 모든 토큰의 집계 표현을 보유하고 있기 때문에 문장 전체에 대한 표현을 담고 있음 사전 학습 절차 lr = 1e-4, b1 = 0.9, b2 = 0.999\n초기 모델의 큰 변화를 유도하기 위해 웜업으로 1만 스텝 학습\n(0에서 1e-4로 선형적으로 학습률 증가, 1만 스탭 후 수렴에 가까워짐에 따라 학습률을 선형적으로 감소) dropout 0.1, GELU(가우시안 오차 선형 유닛) 활성화 함수 사용 하위 단어 토큰화 알고리즘 Byte Pair Encoding (BPE) 모든 단어를 문자로 나누고 문자 시퀀스로 만듦 우선 문자 시퀀스에 있는 고유 문자를 어휘 사전에 추가 어휘 사전 크기에 도달할 때까지 가장 빈도수가 큰 기호 쌍을 반복적으로 병합해 어휘 사전에 추가 토큰화 시 어휘 사전에 존재하지 않는 단어는 하위 단어로 나눔, 사전에 없는 개별 문자는 토큰으로 교체 Byte-Level Byte Pair Encoding (BBPE) 문자 수준 시퀀스 대신 바이트 수준 시퀀스를 사용 유니코드 문자가 바이트로 변환되어 단일 문자 크기는 1~4 바이트가 됨 바이트 수준에서 빈번한 쌍을 구분해 어휘 사전을 구축 다국어 설정에서 유용, OOV 단어 처리에 효과적 WordPiece BPE랑 다르게 빈도수 대신 likelihood를 기준으로 기호 쌍을 병합 모든 기호 쌍에 대해 언어 모델의 가능도를 확인, 가능도가 가장 높은 기호 쌍을 병합 ","wordCount":"1204","inLanguage":"en","image":"https://minyeamer.github.io/calendar.jpg","datePublished":"2022-07-17T20:00:00+09:00","dateModified":"2022-07-17T20:00:00+09:00","author":{"@type":"Person","name":"minyeamer"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://minyeamer.github.io/blog/2022-07-17/"},"publisher":{"@type":"Organization","name":"Minystory","logo":{"@type":"ImageObject","url":"https://minyeamer.github.io/img/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://minyeamer.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://minyeamer.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://minyeamer.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://minyeamer.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://minyeamer.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://minyeamer.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://minyeamer.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://minyeamer.github.io/post/>Posts</a></div><h1 class=post-title>2022-07-17 Log</h1><div class=post-meta><span title='2022-07-17 20:00:00 +0900 KST'>July 17, 2022</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1204 words&nbsp;·&nbsp;minyeamer&nbsp;|&nbsp;<a href=https://github.com/minyeamer/til/edit/main/2022/2022-07/2022-07-17.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><figure class=entry-cover><img loading=lazy src="https://github.com/minyeamer/til/blob/main/.media/covers/calendar.jpg?raw=true" alt></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>&nbsp;Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#기계번역-과제>기계번역 과제</a></li><li><a href=#encoder>Encoder</a><ul><li><a href=#self-attention>Self Attention</a></li><li><a href=#multi-head-attention>Multi-Head Attention</a></li><li><a href=#positional-encoding>Positional Encoding</a></li><li><a href=#feed-forward-network>Feed Forward Network</a></li><li><a href=#인코더-순서>인코더 순서</a></li></ul></li><li><a href=#decoder>Decoder</a><ul><li><a href=#masked-multi-head-attention>Masked Multi-Head Attention</a></li><li><a href=#encoder-decoder-attention-layer>Encoder-Decoder Attention Layer</a></li><li><a href=#linear-and-sofmax-layer>Linear and Sofmax Layer</a></li><li><a href=#디코더-순서>디코더 순서</a></li><li><a href=#학습>학습</a></li></ul></li><li><a href=#bert>BERT</a><ul><li><a href=#사전-학습>사전 학습</a></li><li><a href=#token-embedding>Token Embedding</a></li><li><a href=#segment-embedding>Segment Embedding</a></li><li><a href=#position-embedding>Position Embedding</a></li><li><a href=#입력-데이터>입력 데이터</a></li><li><a href=#wordpiece-tokenizer>WordPiece Tokenizer</a></li><li><a href=#language-modeling>Language Modeling</a></li><li><a href=#masked-language-modeling-mlm>Masked Language Modeling (MLM)</a></li><li><a href=#whole-word-masking-wwm>Whole Word Masking (WWM)</a></li><li><a href=#next-sentence-prediction-nsp>Next Sentence Prediction (NSP)</a></li><li><a href=#사전-학습-절차>사전 학습 절차</a></li></ul></li><li><a href=#하위-단어-토큰화-알고리즘>하위 단어 토큰화 알고리즘</a><ul><li><a href=#byte-pair-encoding-bpe>Byte Pair Encoding (BPE)</a></li><li><a href=#byte-level-byte-pair-encoding-bbpe>Byte-Level Byte Pair Encoding (BBPE)</a></li><li><a href=#wordpiece>WordPiece</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h1 id=기계번역-과제>기계번역 과제<a hidden class=anchor aria-hidden=true href=#기계번역-과제>#</a></h1><ul><li>인코더: 입력 문장의 표현 방법 학습</li><li>디코더: 인코더에서 학습한 표현 결과를 입력받아 사용자가 원하는 문장 생성</li></ul><hr><h1 id=encoder>Encoder<a hidden class=anchor aria-hidden=true href=#encoder>#</a></h1><h2 id=self-attention>Self Attention<a hidden class=anchor aria-hidden=true href=#self-attention>#</a></h2><ul><li>임베딩: 각각의 단어를 표현하는 벡터값, [문장 길이 x 임베딩 차원]</li><li>쿼리(Q), 키(K), 밸류(V) > 각각의 가중치 행렬을 입력 행렬에 곱해 Q, K, V 행렬 생성</li><li>Q, K, V의 차원 [문장 길이 x 벡터의 차원]</li><li>1단계: Q와 K^T 행렬의 내적 연산, 쿼리 벡터(I)와 키 벡터(I, am, good) 사이의 유사도 계산</li><li>2단계: QK^T 행렬을 키 벡터 차원의 제곱근값($\sqrt{d_k}$)으로 나눈 것, 안정적인 gradient 얻음</li><li>3단계: 소프트맥스 함수를 사용해 비정규화된 형태의 유사도 값을 정규화 (score 행렬)</li><li>4단계: 스코어 행렬에 V 행렬을 곱해 어텐션(Z) 행렬 계산, 어텐션 행렬은 문장의 각 단어와 벡터값 가짐<br>(단어 I의 셀프 어텐션은 각 밸류 벡터값의 가중치 합으로 계산, 단어가 문장 내에 있는 다른 단어와의 연관성)</li></ul><h2 id=multi-head-attention>Multi-Head Attention<a hidden class=anchor aria-hidden=true href=#multi-head-attention>#</a></h2><ul><li>문장 내에서 모호한 의미를 가진 단어(it)가 있을 경우,<br>문장의 의미가 잘못 해석될 수 있기 때문에 멀티 헤드 어텐션을 사용한 후 그 결괏값을 더함</li><li>다수의 어텐션 행렬을 구하기 위해 서로 다른 가중치 행렬을 입력 행렬에 곱해 Q, K, V 생성</li><li>다수의 어텐션 행렬을 concatenate하고 새로운 가중치 행렬을 곱해 멀티 헤드 어텐션 결과 도출<br>(concatenate 시 [어텐션 헤드 x h] 크기가 되기 때문에 원래 크기로 만들기 위해 가중치 행렬 곱함)</li></ul><h2 id=positional-encoding>Positional Encoding<a hidden class=anchor aria-hidden=true href=#positional-encoding>#</a></h2><ul><li>트랜스포머는 문장 안에 있는 모든 단어를 병렬 형태로 입력</li><li>단어의 순서 정보를 제공하기 위해 문장에서 단어의 위치를 나타내는 인코딩 제공</li><li>위치 인코딩은 사인파 함수를 사용</li><li>입력 임베딩 결과에 위치 인코딩을 합한 후 멀티 헤드 어텐션에 입력</li></ul><h2 id=feed-forward-network>Feed Forward Network<a hidden class=anchor aria-hidden=true href=#feed-forward-network>#</a></h2><ul><li>2개의 전결합층(Dense)과 ReLU 활성화 함수로 구성</li><li>add와 norm을 추가해 서브레이어에서 멀티 헤드 어텐션의 입력값과 출력값을 서로 연결</li><li>add와 norm은 레이어 정규화(각 레이어 값이 크게 변화하는 것을 방지해 모델 학습 빠르게)와 잔차 연결</li></ul><h2 id=인코더-순서>인코더 순서<a hidden class=anchor aria-hidden=true href=#인코더-순서>#</a></h2><ol><li>입력값은 입력 임베딩으로 변환한 다음 위치 인코딩 추가, 가장 아래 있는 인코더 1의 입력값으로 공급</li><li>인코더 1은 입력값을 받아 멀티 헤드 어텐션의 서브레이어에 값을 보냄, 어텐션 행렬을 결괏값으로 출력</li><li>어텐션 행렬의 값을 다음 서브레이어인 피드포워드 네트워크에 입력, 결괏값 출력</li><li>인코더 1의 출력값을 그 위에 있는 인코더 2에 입력값으로 제공</li><li>인코더 2에서 이전과 동일한 방법 수행, 주어진 문장에 대한 인코더 표현 결과를 출력으로 제공</li></ol><hr><h1 id=decoder>Decoder<a hidden class=anchor aria-hidden=true href=#decoder>#</a></h1><ul><li>이전 디코더의 입력값과 인코더의 표현(인코더의 출력값), 2개를 입력 데이터로 받음</li><li>t=1에서 디코더의 입력값은 문장의 시작을 알리는 <sos>를 입력 > 타깃 문장의 첫 번째 단어(Je) 생성</li><li>t=2에서 t-1 디코더에서 생성한 단어(<sos>, Je)를 추가해 문장의 다음 단어 생성</li><li>t=3에서도 동일하게 (<sos>, Je, vais)를 입력받아 다음 단어 생성</li><li>디코더에서 <eos>토큰을 생성할 때 타깃 문장의 생성이 완료</li><li>디코더도 입력값을 바로 입력하지 않고 위치 인코딩을 추가한 값을 출력 임베딩에 더해 입력값으로 사용</li></ul><h2 id=masked-multi-head-attention>Masked Multi-Head Attention<a hidden class=anchor aria-hidden=true href=#masked-multi-head-attention>#</a></h2><ul><li>디코더에서 문장을 생성할 때 이전 단계에서 생성한 단어만 입력으로 넣기 때문에,<br>아직 예측하지 않은 오른쪽의 모든 단어를 마스킹해 학습을 진행</li><li>소프트맥스 함수를 적용한 정규화 작업 전에 <sos>오른쪽의 모든 단어를 $-{\infty}$로 마스킹 수행<br>($-{\infty}$는 학습 도중 발산하는 경우가 있기 때문에 실제로는 작은 값 $e^{-9}$으로 지정)</li></ul><h2 id=encoder-decoder-attention-layer>Encoder-Decoder Attention Layer<a hidden class=anchor aria-hidden=true href=#encoder-decoder-attention-layer>#</a></h2><ul><li>디코더의 멀티 헤드 어텐션의 입력으로 인코더의 표현값 R과 마스크된 멀티 헤드 어텐션의 결과 M을 받을 때 상호작용 발생</li><li>쿼리, 키, 밸류 행렬을 생성할 때, M을 사용해 Q를 생성, R을 활용해 K, V를 생성<br>(쿼리 행렬은 타깃 문장의 표현을 포함하기 때문에 M을 참조, 키와 밸류 행렬은 입력 문장의 표현을 참조)</li><li>쿼리, 키 행렬 간의 내적 시 타깃 단어 <sos>가 입력 문장의 모든 단어(I, am, good)와 얼마나 유사한지 계산<br>(두 번째 행에서 Je에 대해, 나머지 행에서도 동일한 방법을 적용해 유사도 계산)</li></ul><h2 id=linear-and-sofmax-layer>Linear and Sofmax Layer<a hidden class=anchor aria-hidden=true href=#linear-and-sofmax-layer>#</a></h2><ul><li>최상위 디코더에서 얻은 출력 값을 선형 및 소프트맥스 레이어에 전달</li><li>선형 레이어는 vocab 크기와 같은 logit 형태</li><li>logit 값을 확률값으로 변환하고, 디코더에서 가장 높은 확률값을 갖는 인덱스의 단어로 출력</li></ul><h2 id=디코더-순서>디코더 순서<a hidden class=anchor aria-hidden=true href=#디코더-순서>#</a></h2><ol><li>디코더에 대한 입력 문장을 임베딩 행렬로 변환하고 위치 인코딩 정보를 추가해 디코더 1에 입력</li><li>입력을 가져와서 마스크된 멀티 헤드 어텐션 레이어에 보내고, 출력으로 어텐션 행렬 M 반환</li><li>어텐션 행렬 M, 인코딩 표현 R을 입력받아 멀티 헤드 어텐션 레이어에 값을 입력, 새로운 어텐션 행렬 생성</li><li>인코더-디코더 어텐션 레이어에서 출력한 어텐션 행렬을 피드포워드 네트워크에 입력, 디코더의 표현으로 값 출력</li><li>디코더 1의 출력값을 다음 디코더 2의 입력값으로 사용</li><li>디코더 2는 이전과 동일한 방법 수행, 타깃 문장에 대한 디코더 표현 반환</li><li>타깃 문장의 디코더 표현을 선형 및 소프트맥스 레이어에 입력해 최종으로 예측된 단어 얻음</li></ol><h2 id=학습>학습<a hidden class=anchor aria-hidden=true href=#학습>#</a></h2><ul><li>손실 함수로 cross-entropy를 사용해 분포의 차이를 확인</li><li>옵티마이저로 Adam 사용</li><li>과적합을 방지하기 위해 각 서브레이어 출력에 dropout 적용 (임베딩 및 위치 인코딩 합을 구할 때도 포함)</li></ul><hr><h1 id=bert>BERT<a hidden class=anchor aria-hidden=true href=#bert>#</a></h1><ul><li>Word2Vec: 문맥 독립 임베딩, BERT: 문맥 기반 임베딩</li><li>BERT는 인코더-디코더가 있는 트랜스포머 모델에서 인코더만 사용</li><li>BERT-base: L(인코더 레이어)=12, A(어텐션 헤드)=12, H(은닉 유닛)=768</li><li>BERT-large: L=24, A=16, H=1024</li><li>BERT-tiny(L=2, A=2, H=128), BERT-mini(L=4, A=4, H=256) 등</li></ul><h2 id=사전-학습>사전 학습<a hidden class=anchor aria-hidden=true href=#사전-학습>#</a></h2><ul><li>대규모 데이터셋으로 학습된 가중치를 활용해 새로운 태스크에 적용 (find-tuning)</li><li>BERT는 MLM과 NSP 태스크를 이용해 거대한 말뭉치를 기반으로 사전 학습</li></ul><h2 id=token-embedding>Token Embedding<a hidden class=anchor aria-hidden=true href=#token-embedding>#</a></h2><ul><li>첫 번째 문장의 시작 부분에 [CLS] 토큰 추가</li><li>모든 문장 끝에 [SEP] 토큰 추가</li></ul><h2 id=segment-embedding>Segment Embedding<a hidden class=anchor aria-hidden=true href=#segment-embedding>#</a></h2><ul><li>두 문장을 구별하는데 사용</li><li>[SEP] 토큰과 별도로 두 문장을 구분하기 위해 입력 토큰($E_A,E_B$) 제공</li></ul><h2 id=position-embedding>Position Embedding<a hidden class=anchor aria-hidden=true href=#position-embedding>#</a></h2><ul><li>단어(토큰)의 위치에 대한 정보 제공</li></ul><h2 id=입력-데이터>입력 데이터<a hidden class=anchor aria-hidden=true href=#입력-데이터>#</a></h2><ul><li>토큰 임베딩 + 세그먼트 임베딩 + 위치 임베딩 으로 표현</li></ul><h2 id=wordpiece-tokenizer>WordPiece Tokenizer<a hidden class=anchor aria-hidden=true href=#wordpiece-tokenizer>#</a></h2><ul><li>하위 단어 토큰화 알고리즘 기반</li><li>Let us start pretraining the model > <code>[let, us, start, pre, ##train, ##ing, the, model]</code></li><li>단어가 어휘 사전에 있으면 토큰으로 사용, 없으면 하위 단어로 분할해 하위 단어가 어휘 사전에 있는지 확인 (OOV 처리에 효과적)</li></ul><h2 id=language-modeling>Language Modeling<a hidden class=anchor aria-hidden=true href=#language-modeling>#</a></h2><ul><li>임의의 문장이 주어지고 단어를 순서대로 보면서 다음 단어를 예측하도록 모델 학습</li><li>자동 회귀 언어 모델링: 전방(좌>우) 예측, 후방(좌&lt;우) 예측, 각 방향(단방향)으로 공백까지 모든 단어를 읽음</li><li>자동 인코딩 언어 모델링: 예측하면서 양방향으로 문장을 읽음</li></ul><h2 id=masked-language-modeling-mlm>Masked Language Modeling (MLM)<a hidden class=anchor aria-hidden=true href=#masked-language-modeling-mlm>#</a></h2><ul><li>주어진 입력 문장에서 전체 단어의 15%를 무작위로 마스킹, 마스크된 단어를 예측 (빈칸 채우기 태스크)</li><li>[MASK] 토큰을 사전 학습시킬 경우 파인 튜닝 시 입력에 [MASK] 토큰이 없어 불일치가 발생</li><li>15% 토큰에 대해 80%만 [MASK] 토큰으로 교체, 10%는 임의의 토큰(단어)로 교체, 10%는 변경하지 않음<br>(사전 학습과 파인 튜닝 태스크의 차이를 줄이기 위한 일종의 정규화 작업)</li><li>역전파를 통한 반복 학습을 거치며 최적의 가중치 학습</li></ul><h2 id=whole-word-masking-wwm>Whole Word Masking (WWM)<a hidden class=anchor aria-hidden=true href=#whole-word-masking-wwm>#</a></h2><ul><li>WWM 방법에서는 하위 단어가 마스킹되면 해당 하위 단어와 관련된 모든 단어를 마스킹</li><li>하위 단어와 관련된 모든 단어의 마스크 비율이 15%를 초과하면 다른 단어의 마스킹을 무시</li></ul><h2 id=next-sentence-prediction-nsp>Next Sentence Prediction (NSP)<a hidden class=anchor aria-hidden=true href=#next-sentence-prediction-nsp>#</a></h2><ul><li>BERT에 두 문장을 입력하고 두 번째 문장이 첫 번째 문장의 다음 문장인지 예측</li><li>B 문장이 A 문장에 이어지만 <code>isNext</code>를 반환하고, 그렇지 않으면 <code>notNext</code>를 반환</li><li>두 문장 사이의 관계를 파악해 질문-응답 및 유사문장탐지와 같은 downstream 태스크에서 유용</li><li>한 문서에서 연속된 두 문장을 <code>isNext</code>로 표시하고, 두 문서에서 각각 문장을 가져와 <code>notNext</code>로 표시</li><li>[CLS] 토큰 표현에 소프트맥스 함수를 사용하고 피드포워드 네트워크에 입력해 두 클래스에 대한 확률값 반환</li><li>[CLS] 토큰은 모든 토큰의 집계 표현을 보유하고 있기 때문에 문장 전체에 대한 표현을 담고 있음</li></ul><h2 id=사전-학습-절차>사전 학습 절차<a hidden class=anchor aria-hidden=true href=#사전-학습-절차>#</a></h2><ul><li>lr = 1e-4, b1 = 0.9, b2 = 0.999<br>초기 모델의 큰 변화를 유도하기 위해 웜업으로 1만 스텝 학습<br>(0에서 1e-4로 선형적으로 학습률 증가, 1만 스탭 후 수렴에 가까워짐에 따라 학습률을 선형적으로 감소)</li><li>dropout 0.1, GELU(가우시안 오차 선형 유닛) 활성화 함수 사용</li></ul><hr><h1 id=하위-단어-토큰화-알고리즘>하위 단어 토큰화 알고리즘<a hidden class=anchor aria-hidden=true href=#하위-단어-토큰화-알고리즘>#</a></h1><h2 id=byte-pair-encoding-bpe>Byte Pair Encoding (BPE)<a hidden class=anchor aria-hidden=true href=#byte-pair-encoding-bpe>#</a></h2><ul><li>모든 단어를 문자로 나누고 문자 시퀀스로 만듦</li><li>우선 문자 시퀀스에 있는 고유 문자를 어휘 사전에 추가</li><li>어휘 사전 크기에 도달할 때까지 가장 빈도수가 큰 기호 쌍을 반복적으로 병합해 어휘 사전에 추가</li><li>토큰화 시 어휘 사전에 존재하지 않는 단어는 하위 단어로 나눔, 사전에 없는 개별 문자는 <unk>토큰으로 교체</li></ul><h2 id=byte-level-byte-pair-encoding-bbpe>Byte-Level Byte Pair Encoding (BBPE)<a hidden class=anchor aria-hidden=true href=#byte-level-byte-pair-encoding-bbpe>#</a></h2><ul><li>문자 수준 시퀀스 대신 바이트 수준 시퀀스를 사용</li><li>유니코드 문자가 바이트로 변환되어 단일 문자 크기는 1~4 바이트가 됨</li><li>바이트 수준에서 빈번한 쌍을 구분해 어휘 사전을 구축</li><li>다국어 설정에서 유용, OOV 단어 처리에 효과적</li></ul><h2 id=wordpiece>WordPiece<a hidden class=anchor aria-hidden=true href=#wordpiece>#</a></h2><ul><li>BPE랑 다르게 빈도수 대신 likelihood를 기준으로 기호 쌍을 병합</li><li>모든 기호 쌍에 대해 언어 모델의 가능도를 확인, 가능도가 가장 높은 기호 쌍을 병합</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://minyeamer.github.io/tags/til/>TIL</a></li><li><a href=https://minyeamer.github.io/tags/nlp/>NLP</a></li><li><a href=https://minyeamer.github.io/tags/bert/>BERT</a></li></ul><nav class=paginav><a class=prev href=https://minyeamer.github.io/blog/2022-08-26/><span class=title>« Prev</span><br><span>2022-07-19 Log</span></a>
<a class=next href=https://minyeamer.github.io/blog/2022-07-13/><span class=title>Next »</span><br><span>2022-07-13 Log</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 2022-07-17 Log on twitter" href="https://twitter.com/intent/tweet/?text=2022-07-17%20Log&url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-07-17%2f&hashtags=TIL%2cNLP%2cBERT"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-07-17 Log on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-07-17%2f&title=2022-07-17%20Log&summary=2022-07-17%20Log&source=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-07-17%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-07-17 Log on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-07-17%2f&title=2022-07-17%20Log"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-07-17 Log on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-07-17%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-07-17 Log on whatsapp" href="https://api.whatsapp.com/send?text=2022-07-17%20Log%20-%20https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-07-17%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-07-17 Log on telegram" href="https://telegram.me/share/url?text=2022-07-17%20Log&url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-07-17%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=minyeamer/til issue-term=pathname label=comments theme=preferred-color-scheme crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://minyeamer.github.io/>Minystory</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>