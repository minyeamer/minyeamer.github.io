<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[AI SCHOOL 5기] 머신 러닝 실습 - 선형 회귀 | Minystory</title><meta name=keywords content="AI SCHOOL,멋쟁이사자처럼,코드라이언,Machine Learning,Linear Regression,Gradient"><meta name=description content="Linear Regression 종속 변수 y와 독립 변수 x 사이의 선형 상관 관계를 모델링하는 회귀분석 기법 정답이 있는 데이터의 추세를 잘 설명하는 선형 함수를 찾아 x에 대한 y를 예측 Linear Combination (선형 결합): 더하기와 곱하기로만 이루어진 식 단순 회귀분석: 1개의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때 다중 회귀분석: 2개 이상의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때 선형 회귀는 가장 적합한 θ들의 집합을 찾는 것이 목표 Cost Function 예측 값과 실제 값의 차이를 기반으로 모델의 성능(정확도)을 판단하기 위한 함수 Objective (MIN or MAX) 함수 안에 Cost Function이 존재 선형 회귀에서는 Mean Squre(d) Error Function (평균 제곱 오차 함수) 활용 MSE(Cost)가 최소가 되는 θ(a & b)를 찾아야하며,"><meta name=author content="minyeamer"><link rel=canonical href=https://minyeamer.github.io/blog/aischool-06-01-linear-regression/><meta name=google-site-verification content="u1tWcHmHUZWfFT1cHaku6sqU-bK40N3WLR-C-4VUWN0"><meta name=naver-site-verification content="6eaf8e9da1a6104780f056f1a7797fe5a3a5a0da"><link crossorigin=anonymous href=/assets/css/stylesheet.78a14cf8249250820b49e9dc59e58b846a9beea6d16d50612c68b121ddf02146.css integrity="sha256-eKFM+CSSUIILSencWeWLhGqb7qbRbVBhLGixId3wIUY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://minyeamer.github.io/img/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://minyeamer.github.io/img/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://minyeamer.github.io/img/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://minyeamer.github.io/img/favicons/apple-touch-icon.png><link rel=mask-icon href=https://minyeamer.github.io/img/favicons/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V")</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V",{anonymize_ip:!1})}</script><meta property="og:title" content="[AI SCHOOL 5기] 머신 러닝 실습 - 선형 회귀"><meta property="og:description" content="Linear Regression 종속 변수 y와 독립 변수 x 사이의 선형 상관 관계를 모델링하는 회귀분석 기법 정답이 있는 데이터의 추세를 잘 설명하는 선형 함수를 찾아 x에 대한 y를 예측 Linear Combination (선형 결합): 더하기와 곱하기로만 이루어진 식 단순 회귀분석: 1개의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때 다중 회귀분석: 2개 이상의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때 선형 회귀는 가장 적합한 θ들의 집합을 찾는 것이 목표 Cost Function 예측 값과 실제 값의 차이를 기반으로 모델의 성능(정확도)을 판단하기 위한 함수 Objective (MIN or MAX) 함수 안에 Cost Function이 존재 선형 회귀에서는 Mean Squre(d) Error Function (평균 제곱 오차 함수) 활용 MSE(Cost)가 최소가 되는 θ(a & b)를 찾아야하며,"><meta property="og:type" content="article"><meta property="og:url" content="https://minyeamer.github.io/blog/aischool-06-01-linear-regression/"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-04-13T16:31:00+09:00"><meta property="article:modified_time" content="2022-04-13T16:31:00+09:00"><meta property="og:site_name" content="Minystory"><meta name=twitter:card content="summary"><meta name=twitter:title content="[AI SCHOOL 5기] 머신 러닝 실습 - 선형 회귀"><meta name=twitter:description content="Linear Regression 종속 변수 y와 독립 변수 x 사이의 선형 상관 관계를 모델링하는 회귀분석 기법 정답이 있는 데이터의 추세를 잘 설명하는 선형 함수를 찾아 x에 대한 y를 예측 Linear Combination (선형 결합): 더하기와 곱하기로만 이루어진 식 단순 회귀분석: 1개의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때 다중 회귀분석: 2개 이상의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때 선형 회귀는 가장 적합한 θ들의 집합을 찾는 것이 목표 Cost Function 예측 값과 실제 값의 차이를 기반으로 모델의 성능(정확도)을 판단하기 위한 함수 Objective (MIN or MAX) 함수 안에 Cost Function이 존재 선형 회귀에서는 Mean Squre(d) Error Function (평균 제곱 오차 함수) 활용 MSE(Cost)가 최소가 되는 θ(a & b)를 찾아야하며,"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://minyeamer.github.io/post/"},{"@type":"ListItem","position":2,"name":"[AI SCHOOL 5기] 머신 러닝 실습 - 선형 회귀","item":"https://minyeamer.github.io/blog/aischool-06-01-linear-regression/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[AI SCHOOL 5기] 머신 러닝 실습 - 선형 회귀","name":"[AI SCHOOL 5기] 머신 러닝 실습 - 선형 회귀","description":"Linear Regression 종속 변수 y와 독립 변수 x 사이의 선형 상관 관계를 모델링하는 회귀분석 기법 정답이 있는 데이터의 추세를 잘 설명하는 선형 함수를 찾아 x에 대한 y를 예측 Linear Combination (선형 결합): 더하기와 곱하기로만 이루어진 식 단순 회귀분석: 1개의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때 다중 회귀분석: 2개 이상의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때 선형 회귀는 가장 적합한 θ들의 집합을 찾는 것이 목표 Cost Function 예측 값과 실제 값의 차이를 기반으로 모델의 성능(정확도)을 판단하기 위한 함수 Objective (MIN or MAX) 함수 안에 Cost Function이 존재 선형 회귀에서는 Mean Squre(d) Error Function (평균 제곱 오차 함수) 활용 MSE(Cost)가 최소가 되는 θ(a \u0026amp; b)를 찾아야하며,","keywords":["AI SCHOOL","멋쟁이사자처럼","코드라이언","Machine Learning","Linear Regression","Gradient"],"articleBody":"Linear Regression 종속 변수 y와 독립 변수 x 사이의 선형 상관 관계를 모델링하는 회귀분석 기법 정답이 있는 데이터의 추세를 잘 설명하는 선형 함수를 찾아 x에 대한 y를 예측 Linear Combination (선형 결합): 더하기와 곱하기로만 이루어진 식 단순 회귀분석: 1개의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때 다중 회귀분석: 2개 이상의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때 선형 회귀는 가장 적합한 θ들의 집합을 찾는 것이 목표 Cost Function 예측 값과 실제 값의 차이를 기반으로 모델의 성능(정확도)을 판단하기 위한 함수 Objective (MIN or MAX) 함수 안에 Cost Function이 존재 선형 회귀에서는 Mean Squre(d) Error Function (평균 제곱 오차 함수) 활용 MSE(Cost)가 최소가 되는 θ(a \u0026 b)를 찾아야하며,\n이를 위한 최적화 기법으로 Gradient Descent Algorithm (경사하강법) 활용 Mean Squre Error Function 회귀 분석을 위한 Cost Function y축 방향의 차이를 에러로 판단하는데 전체 에러를 단순하게 합칠 경우\n양 에러와 음 에러가 상쇄되어 올바른 판단을 할 수 없음 부호를 제거하기 위해 모든 에러에 제곱을 취하고 그 평균을 구한 것이 MSE MSE(Cost)가 0에 가까울수록 에러가 적다고 판단 값에 제곱을 취하기 때문에 이상치가 있으면 영향을 많이 받아 이상치를 찾아내기 쉬움 제곱 대신에 절댓값을 사용하는 MAE Function은 이상치에 영향을 덜 받음 Gradient Descent Algorithm Cost Function의 값을 최소로 만드는 θ를 찾아나가는 방법 Cost Function의 Gradient(기울기)에 상수를 곱한 값을 빼서 θ를 조정 어느 방향으로 θ를 움직이면 Cost가 작아지는지 현재 위치에서 함수를 미분하여 판단 변수(θ)를 움직이면서 전체 Cost 값이 변하지 않거나 매우 느리게 변할 때까지 접근 MSE를 미분했을 때 0이 나오는 지점을 찾아도 되지만, 빅데이터에서 x 데이터 역행렬이 오래걸림 그래프 중간에 함정처럼 페인 부분을 Local Minima라 부름 (목표점은 Global Minima) 가던 방향에서 조금 더 가는 발전된 Gradient Descent 기법을 통해 함정을 빠져나감 Local Minima도 Global Minima와 비슷하게 떨어지기 때문에 에러가 적음 $$\\text{repeat until convergence}\\ { \\theta_j:=\\theta_j-{\\alpha}\\frac{\\delta}{\\delta\\theta_j}J(\\theta_0,\\theta_1)\\quad(\\text{for}j=0\\text{and}j=1) }$$\n계산식에서 J(θ0, θ1)는 MSE를 의미하며 이를 미분한 것에 ⍺를 곱함 ⍺는 한 번 이동하는 길이를 결정하는 상수 (Step Size, 보폭, Learning Rate) ⍺와 같이 사람이 결정해야 하는 값을 Hyper-Parameter라 부름 Hyper-Parameter 사람이 결정하는 파라미터, 모델 클래스 생성 시 집어 넣는 파라미터 모델을 선택하는 것, 인공신경망의 층을 몇개로 구성할 것인지 등 Hyper-Parameter를 설정하는 것을 Model Tuning, Hyper-Parmas Tuning,\n또는 Hyper-Parameter Optimizator(HPO)라 부름 AutoML Hyper-Parameter Tuning을 컴퓨터에게 맡김 Automated FE (Feature Engineering): 결측치 채움, x열(feature) 생성 Automated MS (Model Selection) Automated HPO (Hyper Parameter Optimization) Learning Process Load Data pd.read_excel()로 엑셀 데이터 불러오기 엑셀 데이터를 np.array() 안에 넣어 Numpy Array 형태로 변경 Select Feature Numpy Array는 2차원 행렬이어야 하기 때문에 data[:, 1:2] 형식으로 열을 꺼냄 (data[:, 1]는 1차원 행렬을 반환) Training \u0026 Test Set 1 2 3 4 from sklearn import model_selection x_train, x_test, y_train, y_test = \\ model_selection.train_test_split(boston_X, boston_Y, test_size=0.3, random_state=0) Create Model 1 2 3 from sklearn import linear_model model = linear_model.LinearRegression() Model Fitting 1 model.fit(x_train, y_train) model.coef_: a에 해당하는 θ 값 model.intercept_: b에 해당하는 θ 값 (y 절편) Model Predict 1 model.predict(x_train) MSE 1 print(np.mean((model.predict(x_train) - y_train) ** 2)) 1 2 3 from sklearn.metrics import mean_squared_error print(mean_squared_error(model.predict(x_train), y_train)) Training Data와 Test Data의 MSE 차이를 원본 데이터와 함께 비교하여 Overfitting 판단 선형회귀는 성능을 기대하기 어려움 Plot Linear Model 1 2 3 4 5 6 7 8 plt.figure(figsize=(10, 10)) plt.scatter(x_test, y_test, color=\"black\") # Test data plt.scatter(x_train, y_train, color=\"red\", s=1) # Train data plt.plot(x_test, model.predict(x_test), color=\"blue\", linewidth=3) plt.show() ","wordCount":"512","inLanguage":"en","datePublished":"2022-04-13T16:31:00+09:00","dateModified":"2022-04-13T16:31:00+09:00","author":{"@type":"Person","name":"minyeamer"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://minyeamer.github.io/blog/aischool-06-01-linear-regression/"},"publisher":{"@type":"Organization","name":"Minystory","logo":{"@type":"ImageObject","url":"https://minyeamer.github.io/img/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://minyeamer.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://minyeamer.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://minyeamer.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://minyeamer.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://minyeamer.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://minyeamer.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://minyeamer.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://minyeamer.github.io/post/>Posts</a></div><h1 class=post-title>[AI SCHOOL 5기] 머신 러닝 실습 - 선형 회귀</h1><div class=post-meta><span title='2022-04-13 16:31:00 +0900 KST'>April 13, 2022</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;512 words&nbsp;·&nbsp;minyeamer&nbsp;|&nbsp;<a href=https://github.com/minyeamer/til/edit/main/study/ai-school/06-machine-learning/01-linear-regression.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>&nbsp;Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#linear-regression>Linear Regression</a><ul><li><a href=#cost-function>Cost Function</a></li><li><a href=#mean-squre-error-function>Mean Squre Error Function</a></li><li><a href=#gradient-descent-algorithm>Gradient Descent Algorithm</a></li><li><a href=#hyper-parameter>Hyper-Parameter</a></li><li><a href=#automl>AutoML</a></li></ul></li><li><a href=#learning-process>Learning Process</a><ul><li><a href=#load-data>Load Data</a></li><li><a href=#select-feature>Select Feature</a></li><li><a href=#training--test-set>Training & Test Set</a></li><li><a href=#create-model>Create Model</a></li><li><a href=#model-fitting>Model Fitting</a></li><li><a href=#model-predict>Model Predict</a></li><li><a href=#mse>MSE</a></li></ul></li><li><a href=#plot-linear-model>Plot Linear Model</a></li></ul></nav></div></details></div><div class=post-content><h1 id=linear-regression>Linear Regression<a hidden class=anchor aria-hidden=true href=#linear-regression>#</a></h1><ul><li>종속 변수 y와 독립 변수 x 사이의 선형 상관 관계를 모델링하는 회귀분석 기법</li><li>정답이 있는 데이터의 추세를 잘 설명하는 선형 함수를 찾아 x에 대한 y를 예측</li><li><strong>Linear Combination (선형 결합)</strong>: 더하기와 곱하기로만 이루어진 식</li><li><strong>단순 회귀분석</strong>: 1개의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때</li><li><strong>다중 회귀분석</strong>: 2개 이상의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때</li><li>선형 회귀는 가장 적합한 θ들의 집합을 찾는 것이 목표</li></ul><h2 id=cost-function>Cost Function<a hidden class=anchor aria-hidden=true href=#cost-function>#</a></h2><ul><li>예측 값과 실제 값의 차이를 기반으로 모델의 성능(정확도)을 판단하기 위한 함수</li><li>Objective (MIN or MAX) 함수 안에 Cost Function이 존재</li><li>선형 회귀에서는 <strong>Mean Squre(d) Error Function (평균 제곱 오차 함수)</strong> 활용</li><li>MSE(Cost)가 최소가 되는 θ(a & b)를 찾아야하며,<br>이를 위한 최적화 기법으로 <strong>Gradient Descent Algorithm (경사하강법)</strong> 활용</li></ul><h2 id=mean-squre-error-function>Mean Squre Error Function<a hidden class=anchor aria-hidden=true href=#mean-squre-error-function>#</a></h2><ul><li>회귀 분석을 위한 Cost Function</li><li>y축 방향의 차이를 에러로 판단하는데 전체 에러를 단순하게 합칠 경우<br>양 에러와 음 에러가 상쇄되어 올바른 판단을 할 수 없음</li><li>부호를 제거하기 위해 모든 에러에 제곱을 취하고 그 평균을 구한 것이 MSE</li><li>MSE(Cost)가 0에 가까울수록 에러가 적다고 판단</li><li>값에 제곱을 취하기 때문에 이상치가 있으면 영향을 많이 받아 이상치를 찾아내기 쉬움</li><li>제곱 대신에 절댓값을 사용하는 <strong>MAE Function</strong>은 이상치에 영향을 덜 받음</li></ul><h2 id=gradient-descent-algorithm>Gradient Descent Algorithm<a hidden class=anchor aria-hidden=true href=#gradient-descent-algorithm>#</a></h2><ul><li>Cost Function의 값을 최소로 만드는 θ를 찾아나가는 방법</li><li>Cost Function의 Gradient(기울기)에 상수를 곱한 값을 빼서 θ를 조정</li><li>어느 방향으로 θ를 움직이면 Cost가 작아지는지 현재 위치에서 함수를 미분하여 판단</li><li>변수(θ)를 움직이면서 전체 Cost 값이 변하지 않거나 매우 느리게 변할 때까지 접근</li><li>MSE를 미분했을 때 0이 나오는 지점을 찾아도 되지만, 빅데이터에서 x 데이터 역행렬이 오래걸림</li><li>그래프 중간에 함정처럼 페인 부분을 Local Minima라 부름 (목표점은 Global Minima)</li><li>가던 방향에서 조금 더 가는 발전된 Gradient Descent 기법을 통해 함정을 빠져나감</li><li>Local Minima도 Global Minima와 비슷하게 떨어지기 때문에 에러가 적음</li></ul><p>$$\text{repeat until convergence}\ { \theta_j:=\theta_j-{\alpha}\frac{\delta}{\delta\theta_j}J(\theta_0,\theta_1)\quad(\text{for}j=0\text{and}j=1) }$$</p><ul><li>계산식에서 J(θ0, θ1)는 MSE를 의미하며 이를 미분한 것에 ⍺를 곱함</li><li>⍺는 한 번 이동하는 길이를 결정하는 상수 (Step Size, 보폭, Learning Rate)</li><li>⍺와 같이 사람이 결정해야 하는 값을 <strong>Hyper-Parameter</strong>라 부름</li></ul><h2 id=hyper-parameter>Hyper-Parameter<a hidden class=anchor aria-hidden=true href=#hyper-parameter>#</a></h2><ul><li>사람이 결정하는 파라미터, 모델 클래스 생성 시 집어 넣는 파라미터</li><li>모델을 선택하는 것, 인공신경망의 층을 몇개로 구성할 것인지 등</li><li>Hyper-Parameter를 설정하는 것을 Model Tuning, Hyper-Parmas Tuning,<br>또는 Hyper-Parameter Optimizator(HPO)라 부름</li></ul><h2 id=automl>AutoML<a hidden class=anchor aria-hidden=true href=#automl>#</a></h2><ul><li>Hyper-Parameter Tuning을 컴퓨터에게 맡김</li><li>Automated FE (Feature Engineering): 결측치 채움, x열(feature) 생성</li><li>Automated MS (Model Selection)</li><li>Automated HPO (Hyper Parameter Optimization)</li></ul><hr><h1 id=learning-process>Learning Process<a hidden class=anchor aria-hidden=true href=#learning-process>#</a></h1><h2 id=load-data>Load Data<a hidden class=anchor aria-hidden=true href=#load-data>#</a></h2><ul><li><code>pd.read_excel()</code>로 엑셀 데이터 불러오기</li><li>엑셀 데이터를 <code>np.array()</code> 안에 넣어 Numpy Array 형태로 변경</li></ul><h2 id=select-feature>Select Feature<a hidden class=anchor aria-hidden=true href=#select-feature>#</a></h2><ul><li>Numpy Array는 2차원 행렬이어야 하기 때문에 <code>data[:, 1:2]</code> 형식으로 열을 꺼냄
(<code>data[:, 1]</code>는 1차원 행렬을 반환)</li></ul><h2 id=training--test-set>Training & Test Set<a hidden class=anchor aria-hidden=true href=#training--test-set>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>model_selection</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x_train</span><span class=p>,</span> <span class=n>x_test</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> \
</span></span><span class=line><span class=cl>   <span class=n>model_selection</span><span class=o>.</span><span class=n>train_test_split</span><span class=p>(</span><span class=n>boston_X</span><span class=p>,</span> <span class=n>boston_Y</span><span class=p>,</span> <span class=n>test_size</span><span class=o>=</span><span class=mf>0.3</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=create-model>Create Model<a hidden class=anchor aria-hidden=true href=#create-model>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>linear_model</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>linear_model</span><span class=o>.</span><span class=n>LinearRegression</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=model-fitting>Model Fitting<a hidden class=anchor aria-hidden=true href=#model-fitting>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>x_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><code>model.coef_</code>: a에 해당하는 θ 값</li><li><code>model.intercept_</code>: b에 해당하는 θ 값 (y 절편)</li></ul><h2 id=model-predict>Model Predict<a hidden class=anchor aria-hidden=true href=#model-predict>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x_train</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=mse>MSE<a hidden class=anchor aria-hidden=true href=#mse>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>((</span><span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x_train</span><span class=p>)</span> <span class=o>-</span> <span class=n>y_train</span><span class=p>)</span> <span class=o>**</span> <span class=mi>2</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>mean_squared_error</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>mean_squared_error</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x_train</span><span class=p>),</span> <span class=n>y_train</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>Training Data와 Test Data의 MSE 차이를 원본 데이터와 함께 비교하여 Overfitting 판단</li><li>선형회귀는 성능을 기대하기 어려움</li></ul><hr><h1 id=plot-linear-model>Plot Linear Model<a hidden class=anchor aria-hidden=true href=#plot-linear-model>#</a></h1><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>10</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>x_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s2>&#34;black&#34;</span><span class=p>)</span> <span class=c1># Test data</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>x_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s2>&#34;red&#34;</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> <span class=c1># Train data</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>x_test</span><span class=p>,</span> <span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>x_test</span><span class=p>),</span> <span class=n>color</span><span class=o>=</span><span class=s2>&#34;blue&#34;</span><span class=p>,</span> <span class=n>linewidth</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><div style=display:flex;justify-content:center><img src="https://github.com/minyeamer/til/blob/main/.media/study/ai-school/06-machine-learning/01-linear-regression/linear.png?raw=true" style=max-width:700px></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://minyeamer.github.io/tags/ai-school/>AI SCHOOL</a></li><li><a href=https://minyeamer.github.io/tags/%EB%A9%8B%EC%9F%81%EC%9D%B4%EC%82%AC%EC%9E%90%EC%B2%98%EB%9F%BC/>멋쟁이사자처럼</a></li><li><a href=https://minyeamer.github.io/tags/%EC%BD%94%EB%93%9C%EB%9D%BC%EC%9D%B4%EC%96%B8/>코드라이언</a></li><li><a href=https://minyeamer.github.io/tags/machine-learning/>Machine Learning</a></li><li><a href=https://minyeamer.github.io/tags/linear-regression/>Linear Regression</a></li><li><a href=https://minyeamer.github.io/tags/gradient/>Gradient</a></li></ul><nav class=paginav><a class=prev href=https://minyeamer.github.io/blog/aischool-06-00-machine-learning/><span class=title>« Prev</span><br><span>[AI SCHOOL 5기] 머신 러닝</span></a>
<a class=next href=https://minyeamer.github.io/blog/aischool-05-03-merge/><span class=title>Next »</span><br><span>[AI SCHOOL 5기] SQL 프로그래밍 실습 - Merge</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share [AI SCHOOL 5기] 머신 러닝 실습 - 선형 회귀 on twitter" href="https://twitter.com/intent/tweet/?text=%5bAI%20SCHOOL%205%ea%b8%b0%5d%20%eb%a8%b8%ec%8b%a0%20%eb%9f%ac%eb%8b%9d%20%ec%8b%a4%ec%8a%b5%20-%20%ec%84%a0%ed%98%95%20%ed%9a%8c%ea%b7%80&url=https%3a%2f%2fminyeamer.github.io%2fblog%2faischool-06-01-linear-regression%2f&hashtags=AISCHOOL%2c%eb%a9%8b%ec%9f%81%ec%9d%b4%ec%82%ac%ec%9e%90%ec%b2%98%eb%9f%bc%2c%ec%bd%94%eb%93%9c%eb%9d%bc%ec%9d%b4%ec%96%b8%2cMachineLearning%2cLinearRegression%2cGradient"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [AI SCHOOL 5기] 머신 러닝 실습 - 선형 회귀 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fminyeamer.github.io%2fblog%2faischool-06-01-linear-regression%2f&title=%5bAI%20SCHOOL%205%ea%b8%b0%5d%20%eb%a8%b8%ec%8b%a0%20%eb%9f%ac%eb%8b%9d%20%ec%8b%a4%ec%8a%b5%20-%20%ec%84%a0%ed%98%95%20%ed%9a%8c%ea%b7%80&summary=%5bAI%20SCHOOL%205%ea%b8%b0%5d%20%eb%a8%b8%ec%8b%a0%20%eb%9f%ac%eb%8b%9d%20%ec%8b%a4%ec%8a%b5%20-%20%ec%84%a0%ed%98%95%20%ed%9a%8c%ea%b7%80&source=https%3a%2f%2fminyeamer.github.io%2fblog%2faischool-06-01-linear-regression%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [AI SCHOOL 5기] 머신 러닝 실습 - 선형 회귀 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fminyeamer.github.io%2fblog%2faischool-06-01-linear-regression%2f&title=%5bAI%20SCHOOL%205%ea%b8%b0%5d%20%eb%a8%b8%ec%8b%a0%20%eb%9f%ac%eb%8b%9d%20%ec%8b%a4%ec%8a%b5%20-%20%ec%84%a0%ed%98%95%20%ed%9a%8c%ea%b7%80"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [AI SCHOOL 5기] 머신 러닝 실습 - 선형 회귀 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fminyeamer.github.io%2fblog%2faischool-06-01-linear-regression%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [AI SCHOOL 5기] 머신 러닝 실습 - 선형 회귀 on whatsapp" href="https://api.whatsapp.com/send?text=%5bAI%20SCHOOL%205%ea%b8%b0%5d%20%eb%a8%b8%ec%8b%a0%20%eb%9f%ac%eb%8b%9d%20%ec%8b%a4%ec%8a%b5%20-%20%ec%84%a0%ed%98%95%20%ed%9a%8c%ea%b7%80%20-%20https%3a%2f%2fminyeamer.github.io%2fblog%2faischool-06-01-linear-regression%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [AI SCHOOL 5기] 머신 러닝 실습 - 선형 회귀 on telegram" href="https://telegram.me/share/url?text=%5bAI%20SCHOOL%205%ea%b8%b0%5d%20%eb%a8%b8%ec%8b%a0%20%eb%9f%ac%eb%8b%9d%20%ec%8b%a4%ec%8a%b5%20-%20%ec%84%a0%ed%98%95%20%ed%9a%8c%ea%b7%80&url=https%3a%2f%2fminyeamer.github.io%2fblog%2faischool-06-01-linear-regression%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=minyeamer/til issue-term=pathname label=comments theme=preferred-color-scheme crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://minyeamer.github.io/>Minystory</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>