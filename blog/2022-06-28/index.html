<!doctype html><html lang=ko-kr dir=ltr><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=keywords content="TIL,NLP"><meta name=description content="딥 러닝을 이용한 자연어 처리 입문 1"><meta name=author content="minyeamer"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><link rel=canonical href=https://minyeamer.github.io/blog/2022-06-28/><meta name=google-site-verification content="u1tWcHmHUZWfFT1cHaku6sqU-bK40N3WLR-C-4VUWN0"><meta name=naver-site-verification content="6eaf8e9da1a6104780f056f1a7797fe5a3a5a0da"><meta property="og:url" content="https://minyeamer.github.io/blog/2022-06-28/"><meta property="og:site_name" content="Minystory"><meta property="og:title" content="2022-06-28 Log"><meta property="og:description" content="딥 러닝을 이용한 자연어 처리 입문 1"><meta property="og:locale" content="ko_kr"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-06-28T20:00:00+09:00"><meta property="article:modified_time" content="2022-06-28T20:00:00+09:00"><meta property="article:tag" content="TIL"><meta property="article:tag" content="NLP"><title>2022-06-28 Log | Minystory</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://minyeamer.github.io/blog/2022-06-28/><link rel=stylesheet href=/book.min.b806db2fcd7252f443c9368f219d473f25aaf586f28798bf9f651f41a7225060.css integrity="sha256-uAbbL81yUvRDyTaPIZ1HPyWq9Ybyh5i/n2UfQaciUGA=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/search-input.min.97bd2c69bb66aba393499c7ad9ff319905e14e001ef050bf45d8b47a9c6d9278.js integrity="sha256-l70sabtmq6OTSZx62f8xmQXhTgAe8FC/Rdi0epxtkng=" crossorigin=anonymous></script><link rel=preload href=/search-data.min.7cb06c5a504171147e5c23f8fa923c5413dc6769b4d635e327bb3c3acb570140.json as=fetch crossorigin><script>window.SEARCH_DATA_URL="/search-data.min.7cb06c5a504171147e5c23f8fa923c5413dc6769b4d635e327bb3c3acb570140.json"</script><script defer src=/search.min.f30f9834d4764fd9751da64098c954d01085f648ba9ca421a3c97582f8c47253.js integrity="sha256-8w+YNNR2T9l1HaZAmMlU0BCF9ki6nKQho8l1gvjEclM=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V")</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.1/css/all.min.css crossorigin=anonymous><script defer src=/scroll-progress.min.841ade7e507a5f6d59c4e7bf2fe2b2ca034070677ff7957eec55610a024dd776.js integrity="sha256-hBreflB6X21ZxOe/L+KyygNAcGd/95V+7FVhCgJN13Y=" crossorigin=anonymous></script><script defer src=/dark-mode.min.e41c6440ffd9967d6f6a419ff3ce09b862009fe1646ab265f5cb2817d2a508e3.js integrity="sha256-5BxkQP/Zln1vakGf884JuGIAn+FkarJl9csoF9KlCOM=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlightjs-line-numbers.js/2.9.0/highlightjs-line-numbers.min.js></script><script defer src=/copy-code.min.aaeef965f0b4992e55f976edaecb34a89d414e1791caa18c3f4f4376c6d8b5a8.js integrity="sha256-qu75ZfC0mS5V+Xbtrss0qJ1BTheRyqGMP09DdsbYtag=" crossorigin=anonymous></script><script defer src=/toc-highlightjs.093016f0ef312174ad862fdcf5792e88ab5442bd39beecc38d15643f71ab5c31.min integrity="sha256-CTAW8O8xIXSthi/c9XkuiKtUQr05vuzDjRVkP3GrXDE=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-post book-layout-post"><div class=scroll-progress><div class=scroll-progress-bar></div></div><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><div class=sidebar-profile><div class=profile-img-wrap><a href=https://minyeamer.github.io/><img src="https://avatars.githubusercontent.com/u/17109173?v=4" alt=Profile class=profile-img></a></div><div class=sidebar-social><a href=https://github.com/minyeamer target=_blank title=GitHub><i class="fa-brands fa-github"></i>
</a><a href=/categories/ title=Categories><i class="fa-solid fa-folder"></i>
</a><a href=/tags/ title=Tags><i class="fa-solid fa-tags"></i>
</a><button id=dark-mode-toggle class=dark-mode-toggle aria-label="Toggle dark mode">
<i class="fa-solid fa-circle-half-stroke"></i></button></div></div><h2 class=book-brand><a class="flex align-center" href=/><span>Minystory</span></a></h2><div class="book-search hidden"><div class=search-input-container><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/ onkeydown='event.key==="Enter"&&goToSearchPage()'>
<button type=button id=book-search-button class=book-search-btn onclick=goToSearchPage()>
<i class="fa-solid fa-magnifying-glass"></i></button></div><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden");function goToSearchPage(){const t=document.getElementById("book-search-input"),e=t.value.trim();e&&(window.location.href="/search/?q="+encodeURIComponent(e))}</script><div class=book-categories><input type=checkbox class="hidden toggle" id=categories-control checked>
<label for=categories-control class="categories-toggle categories-link"><a href=/categories/><i class="fa-solid fa-folder"></i>
<span>전체</span>
<span class=category-count>(126)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul class=categories-menu id=categories-menu><li><input type=checkbox class="hidden toggle" id=cat-algorithm>
<label for=cat-algorithm class="categories-toggle categories-link"><a href=/categories/algorithm/><i class="fa-solid fa-folder"></i>
Algorithm
<span class=category-count>(51)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/algorithm/baekjoon/><i class="fa-solid fa-file"></i>
Baekjoon
<span class=category-count>(31)</span></a></li><li class=categories-link><a href=/categories/algorithm/leetcode/><i class="fa-solid fa-file"></i>
LeetCode
<span class=category-count>(2)</span></a></li><li class=categories-link><a href=/categories/algorithm/programmers/><i class="fa-solid fa-file"></i>
Programmers
<span class=category-count>(17)</span></a></li><li class=categories-link><a href=/categories/algorithm/references/><i class="fa-solid fa-file"></i>
References
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-blog>
<label for=cat-blog class="categories-toggle categories-link"><a href=/categories/blog/><i class="fa-solid fa-folder"></i>
Blog
<span class=category-count>(5)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/blog/review/><i class="fa-solid fa-file"></i>
Review
<span class=category-count>(1)</span></a></li><li class=categories-link><a href=/categories/blog/tech/><i class="fa-solid fa-file"></i>
Tech
<span class=category-count>(4)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-book>
<label for=cat-book class="categories-toggle categories-link"><a href=/categories/book/><i class="fa-solid fa-folder"></i>
Book
<span class=category-count>(1)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/book/finance/><i class="fa-solid fa-file"></i>
Finance
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-data>
<label for=cat-data class="categories-toggle categories-link"><a href=/categories/data/><i class="fa-solid fa-folder"></i>
Data
<span class=category-count>(4)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/data/crawling/><i class="fa-solid fa-file"></i>
Crawling
<span class=category-count>(4)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-diary>
<label for=cat-diary class="categories-toggle categories-link"><a href=/categories/diary/><i class="fa-solid fa-folder"></i>
Diary
<span class=category-count>(3)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/diary/2022/><i class="fa-solid fa-file"></i>
2022
<span class=category-count>(2)</span></a></li><li class=categories-link><a href=/categories/diary/2023/><i class="fa-solid fa-file"></i>
2023
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-study>
<label for=cat-study class="categories-toggle categories-link"><a href=/categories/study/><i class="fa-solid fa-folder"></i>
Study
<span class=category-count>(61)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/study/2022/><i class="fa-solid fa-file"></i>
2022
<span class=category-count>(16)</span></a></li><li class=categories-link><a href=/categories/study/2023/><i class="fa-solid fa-file"></i>
2023
<span class=category-count>(10)</span></a></li><li class=categories-link><a href=/categories/study/ai-school/><i class="fa-solid fa-file"></i>
AI SCHOOL
<span class=category-count>(34)</span></a></li><li class=categories-link><a href=/categories/study/datacamp/><i class="fa-solid fa-file"></i>
DataCamp
<span class=category-count>(1)</span></a></li></ul></li></ul></div><div class=recent-posts><div class=recent-posts-header><i class="fa-solid fa-clock"></i>
<span>최신글</span></div><ul class=recent-posts-list><li class=recent-post-item><a href=/blog/2023-04-02/ title="2023-04-02 Log"><div class=recent-post-title>2023-04-02 Log</div><div class=recent-post-date><time datetime=2023-04-02>2023.04.02</time></div></a></li><li class=recent-post-item><a href=/blog/10000-recipe/ title="[Python] 만개의 레시피 데이터 수집"><div class=recent-post-title>[Python] 만개의 레시피 데이터 수집</div><div class=recent-post-date><time datetime=2023-03-26>2023.03.26</time></div></a></li><li class=recent-post-item><a href=/blog/2023-03-25/ title="2023-03-25 Log"><div class=recent-post-title>2023-03-25 Log</div><div class=recent-post-date><time datetime=2023-03-25>2023.03.25</time></div></a></li><li class=recent-post-item><a href=/blog/2023-03-21/ title="2023-03-21 Log"><div class=recent-post-title>2023-03-21 Log</div><div class=recent-post-date><time datetime=2023-03-21>2023.03.21</time></div></a></li><li class=recent-post-item><a href=/blog/2023-02-19/ title="2023-02-19 Log"><div class=recent-post-title>2023-02-19 Log</div><div class=recent-post-date><time datetime=2023-02-19>2023.02.19</time></div></a></li></ul></div></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><i class="fa-solid fa-bars book-icon" id=menu-icon></i></label><h3><a href=https://minyeamer.github.io/ class=site-title>Minystory</a></h3><label for=toc-control><i class="fa-solid fa-list book-icon" id=toc-icon></i></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#한국어-토큰화>한국어 토큰화</a></li><li><a href=#nltk-konlpy>NLTK, KoNLPy</a></li></ul><ul><li><a href=#lemmatization>Lemmatization</a></li><li><a href=#stemming>Stemming</a></li><li><a href=#한국어에서의-어간-추출>한국어에서의 어간 추출</a></li></ul><ul><li><a href=#조건부-확률>조건부 확률</a></li><li><a href=#문장에-대한-확률>문장에 대한 확률</a></li><li><a href=#카운트-기반-접근>카운트 기반 접근</a></li></ul><ul><li><a href=#n-gram>N-gram</a></li></ul><ul><li><a href=#branching-factor>Branching Factor</a></li></ul><ul><li><a href=#cosine-similarity>Cosine Similarity</a></li><li><a href=#euclidean-distance>Euclidean Distance</a></li><li><a href=#jaccard-similarity>Jaccard Similarity</a></li></ul><ul><li><a href=#classification-and-regression>Classification and Regression</a></li><li><a href=#learning>Learning</a></li><li><a href=#confusion-matrix>Confusion Matrix</a></li><li><a href=#overfitting-and-underfitting>Overfitting and Underfitting</a></li></ul><ul><li><a href=#perceptron>Perceptron</a></li><li><a href=#ffnn>FFNN</a></li><li><a href=#activision-function>Activision Function</a></li><li><a href=#loss-function>Loss Function</a></li><li><a href=#optimizer>Optimizer</a></li><li><a href=#overfitting-방지>Overfitting 방지</a></li><li><a href=#기울기-소실>기울기 소실</a></li><li><a href=#weight-initialization>Weight Initialization</a></li><li><a href=#batch-normalization>Batch Normalization</a></li><li><a href=#keras-api>Keras API</a></li><li><a href=#texts_to_matrix><code>texts_to_matrix()</code></a></li><li><a href=#nnlm>NNLM</a></li></ul><ul><li><a href=#rnn-parameter>RNN Parameter</a></li><li><a href=#keras-rnn>Keras RNN</a></li><li><a href=#deep-rnn>Deep RNN</a></li><li><a href=#bidirectional-rnn>Bidirectional RNN</a></li></ul><ul><li><a href=#입력-생성>입력 생성</a></li><li><a href=#simpernn>SimpeRNN</a></li><li><a href=#lstm>LSTM</a></li><li><a href=#bidirectional-lstm>Bidirectional LSTM</a></li></ul><ul><li><a href=#데이터-전처리>데이터 전처리</a></li><li><a href=#rnn-모델-설계>RNN 모델 설계</a></li><li><a href=#lstm-모델-설계>LSTM 모델 설계</a></li></ul><ul><li><a href=#char-rnn-모델-설계>Char RNN 모델 설계</a></li></ul></nav><div class=book-nav><button class=book-nav-btn3 onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=book-nav-btn3 onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
<button class=book-nav-btn3 onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div></aside></header><article class="markdown book-article"><header class=post-header><div class=post-header-category><a href=/categories/study/2022/ class=post-header-category-link>Study/2022</a></div><h1 class=post-header-title>2022-06-28 Log</h1><div class=post-header-date><time datetime=2022-06-28T20:00:00+09:00>2022. 6. 28. 20:00</time></div></header><h1 id=02-01-tokenization>02-01. Tokenization
<a class=anchor href=#02-01-tokenization>#</a></h1><ul><li>Corpus에서 token이라 불리는 단위로 나누는 작업</li><li>단어 토큰화에서 단순히 구두점이나 특수문자를 제거하는 것은 의미의 손실을 발생시킬 수 있기 때문에,<br>사용자의 목적과 일치하는 토큰화 도구를 사용할 필요가 있음</li><li><strong>구두점이나 특수 문자가 필요한 경우</strong>: Ph.D, AT&amp;T, $45.55, 01/02/06 등</li><li><strong>줄임말과 단어 내에 띄어쓰기가 있는 경우</strong>: what&rsquo;re/what are, New York, rock &rsquo;n&rsquo; roll 등</li><li>문장 토큰화에서 단순히 마침표를 기준으로 문장을 잘라내는 것은
192.168.56.31, gmail.com과 같은 경우를 고려했을 때 올바르지 않음</li></ul><h2 id=한국어-토큰화>한국어 토큰화
<a class=anchor href=#%ed%95%9c%ea%b5%ad%ec%96%b4-%ed%86%a0%ed%81%b0%ed%99%94>#</a></h2><ul><li>한국어의 경우 띄어쓰기가 가능한 단위가 어절인데,<br>&lsquo;그가&rsquo;, &lsquo;그에게&rsquo;, &lsquo;그를&rsquo;과 같이 어절이 독립적인 단어로 구성되는 것이 아니라<br>조사 등의 무언가가 붙어있는 경우가 많기 때문에 이를 전부 형태소 단위로 분리해줘야 함</li><li><strong>자립 형태소</strong>: 접사, 어미, 조사와 상관업싱 자립하여 사용할 수 있는 형태소, [체언, 수식언, 감탄사] 등</li><li><strong>의존 형태소</strong>: 다른 형태소와 결합하여 사용되는 형태소, [접사, 어미, 조사, 어간]</li><li>한국어의 경우 <strong>띄어쓰기</strong>가 지켜지지 않아도 글을 쉽게 이해할 수 있어 띄어쓰기가 잘 지켜지지 않음</li><li><strong>품사 태깅</strong>: 단어의 의미를 제대로 파악하기 위해서는 해당 단어가 어떤 품사로 쓰였는지 구분할 필요</li></ul><h2 id=nltk-konlpy>NLTK, KoNLPy
<a class=anchor href=#nltk-konlpy>#</a></h2><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>nltk.tokenize</span> <span class=kn>import</span> <span class=n>word_tokenize</span> <span class=c1># 단어 토큰화</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>nltk.tag</span> <span class=kn>import</span> <span class=n>pos_tag</span> <span class=c1># 품사 태깅</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>konlpy.tag</span> <span class=kn>import</span> <span class=n>Okt</span>
</span></span><span class=line><span class=cl><span class=n>okt</span> <span class=o>=</span> <span class=n>Okt</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>okt</span><span class=o>.</span><span class=n>porphs</span><span class=p>(</span><span class=n>sentence</span><span class=p>)</span> <span class=c1># 형태소 추출</span>
</span></span><span class=line><span class=cl><span class=n>okt</span><span class=o>.</span><span class=n>pos</span><span class=p>(</span><span class=n>sentence</span><span class=p>)</span> <span class=c1># 품사 태깅</span></span></span></code></pre></div></div><h1 id=02-02-cleaning-and-normalization>02-02. Cleaning and Normalization
<a class=anchor href=#02-02-cleaning-and-normalization>#</a></h1><ul><li><strong>Cleaning(정제)</strong>: 갖고 있는 corpus로부터 노이즈 데이터를 제거</li><li><strong>Normalization(정규화)</strong>: 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만듦</li><li>영어권 언어에서 단어의 개수를 줄이는 정규화 방법으로 <strong>대,소문자 통합</strong>을 활용</li><li><strong>노이즈 데이터</strong>: 아무 의미 없는 특수 문자 등, 분석하고자 하는 목적에 맞지 않는 불필요한 단어들</li><li>불필요한 단어를 제거하기 위해 <strong>불용어</strong>, <strong>등장 빈도가 적은 단어</strong>, <strong>길이가 짧은 단어</strong> 등을 제거</li><li>노이즈 데이터의 특징을 잡아낼 수 있다면, <strong>정규표현식</strong>을 사용해서 제거</li></ul><h1 id=02-03-stemming-and-lemmatization>02-03. Stemming and Lemmatization
<a class=anchor href=#02-03-stemming-and-lemmatization>#</a></h1><h2 id=lemmatization>Lemmatization
<a class=anchor href=#lemmatization>#</a></h2><ul><li><strong>Lemma(표제어)</strong>: 기본 사전형 단어, [am, are, is]의 뿌리 단어 be 등</li><li><strong>Stem(어간)</strong>: 단어의 의미를 담고 있는 단어의 핵심 부분, &lsquo;cats&rsquo;에서 &lsquo;cat&rsquo;</li><li><strong>Affix(접사)</strong>: 단어에 추가적인 의미를 주는 부분, &lsquo;cats&rsquo;에서 &rsquo;s&rsquo;</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>nltk.stem</span> <span class=kn>import</span> <span class=n>WordNetLemmatizer</span>
</span></span><span class=line><span class=cl><span class=n>lemmatizer</span> <span class=o>=</span> <span class=n>WordNetLemmatizer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>lemmatizer</span><span class=o>.</span><span class=n>lemmatize</span><span class=p>(</span><span class=n>word</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># am -&gt; be, having -&gt; have</span></span></span></code></pre></div></div><h2 id=stemming>Stemming
<a class=anchor href=#stemming>#</a></h2><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>nltk.stem</span> <span class=kn>import</span> <span class=n>PorterStemmer</span>
</span></span><span class=line><span class=cl><span class=n>stemmer</span> <span class=o>=</span> <span class=n>PorterStemmer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>stemmer</span><span class=o>.</span><span class=n>stem</span><span class=p>(</span><span class=n>word</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># am -&gt; am, having -&gt; hav</span></span></span></code></pre></div></div><h2 id=한국어에서의-어간-추출>한국어에서의 어간 추출
<a class=anchor href=#%ed%95%9c%ea%b5%ad%ec%96%b4%ec%97%90%ec%84%9c%ec%9d%98-%ec%96%b4%ea%b0%84-%ec%b6%94%ec%b6%9c>#</a></h2><ul><li>5언 9품사의 구조에서 용언에 해당되는 동사와 형용사는 어간과 어미의 결합으로 구성</li><li><strong>활용</strong>: 용언의 어간이 어미를 가지는 일</li><li><strong>규칙 활용</strong>: 어간이 어미를 취할 때 어간의 모습이 일정, <code>잡/어간 + 다/어미</code></li><li><strong>불규칙 활용</strong>: 어간이 어미를 취할 때 어간의 모습이 바뀌거나 특수한 어미일 경우, &lsquo;오르+아/어->올라&rsquo; 등</li></ul><h1 id=02-04-stopword>02-04. Stopword
<a class=anchor href=#02-04-stopword>#</a></h1><ul><li><strong>Stopword(불용어)</strong>: 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 기여하지 않는 단어, [조사, 접미사] 등</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>nltk.corpus</span> <span class=kn>import</span> <span class=n>stopwords</span>
</span></span><span class=line><span class=cl><span class=n>stop_words_list</span> <span class=o>=</span> <span class=n>stopwords</span><span class=o>.</span><span class=n>words</span><span class=p>(</span><span class=s1>&#39;english&#39;</span><span class=p>)</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>okt</span> <span class=o>=</span> <span class=n>Okt</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>okt</span><span class=o>.</span><span class=n>morphs</span><span class=p>(</span><span class=n>sentence</span><span class=p>)</span> <span class=c1># 조사, 접속사 등 제거</span>
</span></span><span class=line><span class=cl><span class=c1># 또는 불용어 사전을 만들어서 제거</span></span></span></code></pre></div></div><h1 id=02-05-regular-expression>02-05. Regular Expression
<a class=anchor href=#02-05-regular-expression>#</a></h1><ul><li>정규 표현식 <a href=https://wikidocs.net/21703>참고</a></li></ul><h1 id=02-06-integer-encoding>02-06. Integer Encoding
<a class=anchor href=#02-06-integer-encoding>#</a></h1><ul><li>컴퓨터는 텍스트보다 숫자를 더 잘 처리할 수 있기 때문에 텍스트를 숫자로 변경</li><li>단어를 빈도수 순으로 정렬하고 순서대로 낮은 숫자부터 정수를 부여</li><li>dictionary, Counter, nltk.FreqDist, keras.Tokenizer 등 활용</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>nltk</span> <span class=kn>import</span> <span class=n>FreqDist</span>
</span></span><span class=line><span class=cl><span class=n>FreqDist</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>hstack</span><span class=p>(</span><span class=n>preprocessed_sentences</span><span class=p>))</span> <span class=c1># np.hastack으로 문장 구분을 제거</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.preprocessing.text</span> <span class=kn>import</span> <span class=n>Tokenizer</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>Tokenizer</span><span class=p>()</span> <span class=c1># num_words 파라미터로 사용할 단어 개수 지정</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>fit_on_texts</span><span class=p>(</span><span class=n>preprocessed_sentences</span><span class=p>)</span> <span class=c1># 빈도수 기분으로 단어 집합 생성</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>word_intex</span><span class=p>)</span> <span class=c1># 정수 인덱스 확인</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>word_counts</span><span class=p>)</span> <span class=c1># 단어 빈도수 확인</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>texts_to_sequences</span><span class=p>(</span><span class=n>preprocessed_sentences</span><span class=p>))</span> <span class=c1># corpus를 인덱스로 변환</span></span></span></code></pre></div></div><h1 id=02-07-padding>02-07. Padding
<a class=anchor href=#02-07-padding>#</a></h1><ul><li>병렬 연산을 위해 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.preprocessing.sequence</span> <span class=kn>import</span> <span class=n>pad_sequences</span>
</span></span><span class=line><span class=cl><span class=n>encoded</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>texts_to_sequences</span><span class=p>(</span><span class=n>preprocessed_sentences</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>padded</span> <span class=o>=</span> <span class=n>pad_seqences</span><span class=p>(</span><span class=n>encoded</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># padding=&#39;post&#39;를 입력해야 뒤에서 부터 0을 채움</span>
</span></span><span class=line><span class=cl><span class=c1># maxlen으로 문장 길이 조절</span>
</span></span><span class=line><span class=cl><span class=c1># truncating=&#39;post&#39;를 통해 문장 길이 초과 시 뒤의 단어가 삭제되도록 설정</span></span></span></code></pre></div></div><h1 id=02-08-one-hot-encoding>02-08. One-Hot Encoding
<a class=anchor href=#02-08-one-hot-encoding>#</a></h1><ul><li><strong>Vocabulary(단어 집합)</strong>: 서로 다른 단어들의 집합, book과 books과 같은 변형 형태도 다른 단어로 간주</li><li><strong>One-Hot Encoding</strong>: 단어 집합의 크기를 벡터의 차원으로 하고,<br>표현하고 싶은 단어에 1, 다른 인텍스에 0을 부여하는 단어의 벡터 표현 방식</li><li>단어의 개수가 늘어날 수록 벡터를 저장하기 위해 필요한 공간이 계속 늘어나는 단점</li><li>단어의 유사도를 표현하지 못하는 단점 (강아지, 개, 냉장고 등)</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.utils</span> <span class=kn>import</span> <span class=n>to_categorical</span>
</span></span><span class=line><span class=cl><span class=n>encoded</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>texts_to_sequences</span><span class=p>(</span><span class=n>preprocessed_sentences</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>one_hot</span> <span class=o>=</span> <span class=n>to_categorical</span><span class=p>(</span><span class=n>encoded</span><span class=p>)</span></span></span></code></pre></div></div><hr><h1 id=03-01-language-model>03-01. Language Model
<a class=anchor href=#03-01-language-model>#</a></h1><ul><li>단어 시퀀스(문장)에 확률을 할당하는 모델, 이전 단어들이 주어졌을 때 다음 단어를 예측</li><li>단어 시퀀스 W의 확률 $P(W)=P(w_1,w_2,w_3,w_4,w_5,&mldr;,w_n)$</li><li>다음 단어 등장 확률 $P(w_n|w_1,&mldr;,w_{n-1})$</li></ul><h1 id=03-02-statistical-language-model>03-02. Statistical Language Model
<a class=anchor href=#03-02-statistical-language-model>#</a></h1><h2 id=조건부-확률>조건부 확률
<a class=anchor href=#%ec%a1%b0%ea%b1%b4%eb%b6%80-%ed%99%95%eb%a5%a0>#</a></h2><table><thead><tr><th style=text-align:center></th><th style=text-align:center>남학생(A)</th><th style=text-align:center>여학생(B)</th><th style=text-align:center>계</th></tr></thead><tbody><tr><td style=text-align:center>중학생(C)</td><td style=text-align:center>100</td><td style=text-align:center>60</td><td style=text-align:center>160</td></tr><tr><td style=text-align:center>고등학생(D)</td><td style=text-align:center>80</td><td style=text-align:center>120</td><td style=text-align:center>200</td></tr><tr><td style=text-align:center>계</td><td style=text-align:center>180</td><td style=text-align:center>180</td><td style=text-align:center>360</td></tr></tbody></table><ul><li>학생을 뽑았을 때, 고등학생이면서 남학생일 확률 $P(A \bigcap B)=80/360$</li><li>고등학생 중 한명을 뽑았을 때, 남학생일 확률 $P(A|D)=P(A \bigcap D)/P(D)=(80/360)/(200/360)$</li></ul><h2 id=문장에-대한-확률>문장에 대한 확률
<a class=anchor href=#%eb%ac%b8%ec%9e%a5%ec%97%90-%eb%8c%80%ed%95%9c-%ed%99%95%eb%a5%a0>#</a></h2><ul><li>&lsquo;An adorable little boy is spreading smiles&rsquo;의 확률<br>$P(\text{An adorable little boy is spreading smiles})=\
P(\text{An}) \times P(\text{adorable}|\text{An}) \times &mldr; \times P(\text{smiles}|\text{An adorable little boy is spreading})$</li></ul><h2 id=카운트-기반-접근>카운트 기반 접근
<a class=anchor href=#%ec%b9%b4%ec%9a%b4%ed%8a%b8-%ea%b8%b0%eb%b0%98-%ec%a0%91%ea%b7%bc>#</a></h2><ul><li>An adorable little boy가 100번 등장했을 때 그 다음에 is가 등장한 경우가 30번이라면,<br>$P(\text{is}|\text{An adorable little boy})$는 30%</li><li>카운트 기반으로 훈련할 경우 단어 시퀀스가 없어 확률이 0이 되는 경우를 방지하기 위해 방대한 양의 훈련 데이터가 필요</li><li><strong>회소 문제</strong>: 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제</li></ul><p>$$P(\text{is}|\text{An adorable little boy})= \frac{count(\text{An adorable little boy is})}{count(\text{An adorable little boy})}$$</p><h1 id=03-03-n-gram-language-model>03-03. N-gram Language Model
<a class=anchor href=#03-03-n-gram-language-model>#</a></h1><ul><li>통계적 언어 모델의 일종이지만, 모든 단어가 아닌 일부 단어만 고려하는 접근 방법 사용</li><li>An adorable little boy에서 is가 나올 확률을 boy가 나왔을 때 is가 나올 확률로 대체<br>$P(\text{is}|\text{An adorable little boy}) \approx P(\text{is}|\text{boy})$</li><li>뒤의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 발생</li><li>전체 문장을 고려한 언어 모델보다는 정확도가 떨어짐</li><li>몇 개의 단어를 볼지 n을 정하는 것은 trade-off 문제를 발생시킴, n은 최대 5를 넘게 잡아서는 안된다고 권장</li></ul><h2 id=n-gram>N-gram
<a class=anchor href=#n-gram>#</a></h2><ul><li>n개의 연속적인 단어 나열</li><li>An adorable little boy에 대해<br>unigrams: an, adorable, little, boy<br>bigrams: an adorable, adorable little, little boy</li></ul><h1 id=03-05-perplexity>03-05. Perplexity
<a class=anchor href=#03-05-perplexity>#</a></h1><ul><li><strong>Perplexity(PPL)</strong>: 헷갈리는 정도, 낮을수록 언어 모델의 성능이 좋음</li></ul><p>$$PPL(W)=P(w_1,w_2,w_3,&mldr;,w_N)^{-\frac{1}{N}}=\sqrt[N]{\frac{1}{P(w_1,w_2,w_3,&mldr;,w_N)}}$$</p><h2 id=branching-factor>Branching Factor
<a class=anchor href=#branching-factor>#</a></h2><ul><li><strong>Branching factor(분기계수)</strong>: PPL이 선택할 수 있는 가능한 경우의 수</li><li>대해 PPL이 10이 나왔을 때, 언어 모델은 테스트 데이터에 대해 다음 단어를 예측할 때 평균 10개의 단어를 고려</li><li>PPL의 값이 낮다는 것은 테스트 데이터 상에서 높은 정확도를 보이는 것일뿐, 반드시 사람이 직접 느끼기에 좋은 모델인 것은 아님</li></ul><p>$$PPL(W)=P(w_1,w_2,w_3,&mldr;,w_N)^{-\frac{1}{N}}=(\frac{1}{10}^N)^{-\frac{1}{N}}=\frac{1}{10}^{-1}=10$$</p><hr><h1 id=04-01-단어의-표현-방법>04-01. 단어의 표현 방법
<a class=anchor href=#04-01-%eb%8b%a8%ec%96%b4%ec%9d%98-%ed%91%9c%ed%98%84-%eb%b0%a9%eb%b2%95>#</a></h1><ul><li><strong>국소 표현(이산 표현)</strong>: 해당 단어 그 자체만 보고, 특정값을 맵핑하여 단어를 표현하는 방법</li><li><strong>분산 표현(연속 표현)</strong>: 그 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법</li></ul><h1 id=04-02-bag-of-wordsbow>04-02. Bag of Words(BoW)
<a class=anchor href=#04-02-bag-of-wordsbow>#</a></h1><ul><li>단어들의 순서는 고려하지 않고, 단어들의 출현 빈도에만 집중하는 텍스트 데이터 수치화 표현 방법</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>doc1</span> <span class=o>=</span> <span class=s1>&#39;정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.&#39;</span>
</span></span><span class=line><span class=cl><span class=n>vocabulary</span> <span class=p>:</span> <span class=p>{</span><span class=s1>&#39;정부&#39;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span> <span class=s1>&#39;가&#39;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s1>&#39;발표&#39;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span> <span class=s1>&#39;하는&#39;</span><span class=p>:</span> <span class=mi>3</span><span class=p>,</span> <span class=s1>&#39;물가상승률&#39;</span><span class=p>:</span> <span class=mi>4</span><span class=p>,</span> <span class=s1>&#39;과&#39;</span><span class=p>:</span> <span class=mi>5</span><span class=p>,</span> <span class=s1>&#39;소비자&#39;</span><span class=p>:</span> <span class=mi>6</span><span class=p>,</span> <span class=s1>&#39;느끼는&#39;</span><span class=p>:</span> <span class=mi>7</span><span class=p>,</span> <span class=s1>&#39;은&#39;</span><span class=p>:</span> <span class=mi>8</span><span class=p>,</span> <span class=s1>&#39;다르다&#39;</span><span class=p>:</span> <span class=mi>9</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>bag</span> <span class=n>of</span> <span class=n>words</span> <span class=n>vector</span> <span class=p>:</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.feature_extraction.text</span> <span class=kn>import</span> <span class=n>CounteVectorizer</span>
</span></span><span class=line><span class=cl><span class=n>vector</span> <span class=o>=</span> <span class=n>CounterVectorizer</span><span class=p>()</span> <span class=c1># stop_words 파라미터로 불용어 제거(&#39;english&#39; 또는 리스트 등)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;bag of words vector:&#39;</span><span class=p>,</span> <span class=n>vector</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>corpus</span><span class=p>)</span><span class=o>.</span><span class=n>toarray</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;vocabulary:&#39;</span><span class=p>,</span> <span class=n>vector</span><span class=o>.</span><span class=n>vocabulary_</span><span class=p>)</span></span></span></code></pre></div></div><h1 id=04-03-document-term-matrixdtm>04-03. Document-Term Matrix(DTM)
<a class=anchor href=#04-03-document-term-matrixdtm>#</a></h1><ul><li>다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것</li><li>One-hot vector와 마찬가지로 대부분의 값이 0인 희소 표현의 문제 발생</li><li>불용어와 중요한 단어에 대해서 가중치를 주기 위해 TF-IDF를 사용</li></ul><h1 id=04-04-tf-idf>04-04. TF-IDF
<a class=anchor href=#04-04-tf-idf>#</a></h1><ul><li>단어의 빈도와 역 문서 빈도를 사용하여 DTM 내의 각 단어들마다 중요도를 가중치로 부여하는 방법</li><li>$tf(d,t)$: 특정 문서 $d$에서의 특정 단어 $t$의 등장 횟수, DTM에서의 각 단어들의 가진 값</li><li>$df(t)$: 특정 단어 $t$가 등장한 문서의 수, 특정 단어가 각 문서에서 등장한 횟수는 무시</li><li>$idf(d,t)$: $df(t)$에 반비례하는 수,<br>총 문서의 수 n이 커질수록 기하급수적으로 증가하는 것을 방지하기 위해 $log$(일반적으로 자연 로그) 적용</li></ul><p>$$idf(d,t)=log(\frac{n}{1+df(t)})$$</p><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.feature_extraction.text</span> <span class=kn>import</span> <span class=n>TfidfVectorizer</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>vector</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>corpus</span><span class=p>)</span><span class=o>.</span><span class=n>toarray</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>ve</span><span class=p>)</span></span></span></code></pre></div></div><hr><h1 id=05-vector-similarity>05. Vector Similarity
<a class=anchor href=#05-vector-similarity>#</a></h1><h2 id=cosine-similarity>Cosine Similarity
<a class=anchor href=#cosine-similarity>#</a></h2><ul><li>두 벡터 간의 코사인 각도를 이용하여 구할 수 있는 두 벡터의 유사도</li><li>두 벡터의 방향이 동일하면 1의 값을 가지며, 값이 1에 가까울수록 유사도가 높음</li><li>문서의 길이가 다른 상황에서 비교적 공정한 비교를 할 수 있음</li></ul><h2 id=euclidean-distance>Euclidean Distance
<a class=anchor href=#euclidean-distance>#</a></h2><ul><li>다차원 공간에서 두 개의 점 $p$와 $q$가 각각 $p=(p_1,p_2,p_3,&mldr;,p_n)$과 $q=(q_1,q_2,q_3,&mldr;,q_n)$의 좌표를 가질 때<br>두 점 사이의 거리를 계산하는 유클리드 거리 공식</li></ul><p>$$\sqrt{(q_1-p_1)^2+(q_2-p_2)^2+&mldr;+(q_n-p_n)^2}=\sqrt{\Sigma^n_{i=1}(q_i-p_i)^2}$$</p><h2 id=jaccard-similarity>Jaccard Similarity
<a class=anchor href=#jaccard-similarity>#</a></h2><ul><li>합집합에서 교집합의 비율을 구한다면 두 집합 A와 B의 유사도를 구할 수 있음</li><li>자카드 유사도 J는 0과 1사이의 값을 가지며, 두 집합이 동일하면 1, 공통 원소가 없으면 0의 값을 가짐</li></ul><p>$$J(A,B)=\frac{|A \bigcap B|}{|A \bigcup B|}=\frac{|A \bigcap B|}{|A|+|B|-|A \bigcap B|}$$</p><hr><h1 id=06-machine-learning>06. Machine Learning
<a class=anchor href=#06-machine-learning>#</a></h1><h2 id=classification-and-regression>Classification and Regression
<a class=anchor href=#classification-and-regression>#</a></h2><ul><li><strong>Bianry Classification</strong>: 두 개의 선택지 중 하나의 답을 선택</li><li><strong>Multi-class Classification</strong>: 세 개 이상의 선택지 중에서 답을 선택</li><li><strong>Regression</strong>: 연속적인 값의 범위 내에서 예측값을 도출</li></ul><h2 id=learning>Learning
<a class=anchor href=#learning>#</a></h2><ul><li><strong>Supervised Learning</strong>: 정답 레이블과 함께 함습</li><li><strong>Unsupervised Learning</strong>: 데이터에 별도의 레이블이 없이 학습</li><li><strong>Self-Supervised Learning</strong>: 레이블이 없는 데이터가 주어지면, 모델이 학습을 위해 스스로 레이블을 생성</li></ul><h2 id=confusion-matrix>Confusion Matrix
<a class=anchor href=#confusion-matrix>#</a></h2><table><thead><tr><th style=text-align:center></th><th style=text-align:center>예측 참</th><th style=text-align:center>예측 거짓</th></tr></thead><tbody><tr><td style=text-align:center>실제 참</td><td style=text-align:center>TP(정답)</td><td style=text-align:center>FN(오답)</td></tr><tr><td style=text-align:center>실제 거짓</td><td style=text-align:center>FP(오답)</td><td style=text-align:center>TN(정답)</td></tr></tbody></table><ul><li><strong>Precision(정밀도)</strong>: True라고 분류한 것 중 실제 True의 비율, $Precision=\frac{TP}{TP+FP}$</li><li><strong>Recall(재현율)</strong>: 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율, $Recall=\frac{TP}{TP+FN}$</li><li><strong>Accuracy(정확도)</strong>: 전체 예측한 데이터 중 정답을 맞춘 것에 대한 비율, $Accuracy=\frac{TP+TN}{TP+FN+FP+TN}$</li></ul><h2 id=overfitting-and-underfitting>Overfitting and Underfitting
<a class=anchor href=#overfitting-and-underfitting>#</a></h2><ul><li><strong>Overfitting</strong>: 훈련 데이터를 과하게 학습, 훈련 데이터에 비해 테스트 데이터의 오차가 커짐</li><li><strong>Underfitting</strong>: 테스트 데이터의 성능이 올라갈 여지가 있음에도 훈련을 덜 한 상태, 훈련 데이터에서도 정확도가 낮음</li></ul><hr><h1 id=07-deep-learning>07. Deep Learning
<a class=anchor href=#07-deep-learning>#</a></h1><h2 id=perceptron>Perceptron
<a class=anchor href=#perceptron>#</a></h2><ul><li>입력값 $x$, 가중치 $w$, 출력값 $y$</li><li>가중치의 값이 크면 클수록 해당 입력 값이 중요하다는 것을 의미</li><li><strong>단층 퍼셉트론</strong>: 값을 보내는 input layer와 값을 받아서 출력하는 output layer로 구성</li><li><strong>다층 퍼셉트론(MLP)</strong>: 입력층과 출력층 사이에 hidden layer를 추가</li></ul><h2 id=ffnn>FFNN
<a class=anchor href=#ffnn>#</a></h2><ul><li><strong>FFNN(피드 포워드 신경망)</strong>: 오직 입력층에서 출력층 방향으로 연산이 전개되는 신경망</li><li><strong>RNN(순환 신경망)</strong>: 은닉층의 출력값이 다시 은닉층으로 입력되는 신경망</li></ul><h2 id=activision-function>Activision Function
<a class=anchor href=#activision-function>#</a></h2><ul><li>은닉층과 출력층의 뉴런에서 출력값을 결정하는 함수</li><li>Step function, Sigmoid function, ReLU 등 비선형 함수의 특성</li></ul><h2 id=loss-function>Loss Function
<a class=anchor href=#loss-function>#</a></h2><ul><li><strong>MSE</strong>: 연속형 변수 예측</li><li><strong>Binary Cross-Entropy</strong>: 시그모이드 함수 출력</li><li><strong>Categorical Cross-Entropy</strong>: 소프트맥스 함수 출력</li></ul><h2 id=optimizer>Optimizer
<a class=anchor href=#optimizer>#</a></h2><ul><li><strong>Momentum</strong>: 경사 하강법에 모멘텀을 더해 Local Minimum에 빠지더라도 빠져나갈 수 있게 함</li><li><strong>Adagrad</strong>: 각 매개변수에 서로 다른 학습률을 적용</li><li><strong>RMSprop</strong>: Adagrad가 학습을 진행할수록 학습률이 지나치게 떨어지는 단점을 개선</li><li><strong>Adam</strong>: RMSprop과 Momentum을 합친 듯한 방법, 방향과 학습률 두 가지를 모두 잡기 위한 방법</li></ul><h2 id=overfitting-방지>Overfitting 방지
<a class=anchor href=#overfitting-%eb%b0%a9%ec%a7%80>#</a></h2><ol><li><strong>데이터의 양을 늘리기</strong><br>데이터의 양이 적으면 데이터의 특정 패턴이나 노이즈까지 쉽게 암기해버림, Data Augmentation 활용</li><li><strong>모델의 복잡도 줄이기</strong><br>인공 신경망의 복잡도는 은닉층의 수나 매개변수의 수 등으로 결정</li><li><strong>가충치 규제 적용하기</strong><br>L1 규제(가중치의 절댓값 합계를 비용 함수에 추가), L2 규제(모든 가중치들의 제곱합을 비용 함수에 추가)</li><li><strong>Dropout</strong><br>학습 시에 인공 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지</li></ol><h2 id=기울기-소실>기울기 소실
<a class=anchor href=#%ea%b8%b0%ec%9a%b8%ea%b8%b0-%ec%86%8c%ec%8b%a4>#</a></h2><ul><li><strong>기울기 소실</strong>: 역전파 과정에서 입력층으로 갈 수록 기울기가 점차적으로 작아지는 현상</li><li><strong>기울기 폭주</strong>: 기울기가 점차 커지다가 가중치들이 비정상적으로 큰 값이 되면서 발산되는 경우</li><li><strong>Gradient Clipping</strong>: 기울기 폭주를 막기 위해 임계값을 넘지 않도록 값의 크기를 감소</li></ul><h2 id=weight-initialization>Weight Initialization
<a class=anchor href=#weight-initialization>#</a></h2><ul><li><strong>Xavier Initialization</strong>: 균등 분포 또는 정규 분포로 초기화 할 때 두 가지 경우로 나뉨</li><li><strong>He Initialization</strong>: Xavier 초기화와 다르게 다음 층의 뉴런의 수를 반영하지 않음</li></ul><h2 id=batch-normalization>Batch Normalization
<a class=anchor href=#batch-normalization>#</a></h2><ul><li><strong>내부 공변량 변화</strong>: 학습 과정에서 층 별로 입력 데이터 분포가 달라지는 현상</li><li><strong>배치 정규화</strong>: 한 번에 들어오는 배치 단위로 정규화하는 것</li><li>배치 정규화는 추가 계산을 발생시켜 모델을 복잡하게 하기 때문에 예측 시 실행 시간이 느려지는 단점</li><li>너무 작은 배치 크기에서는 잘 동작하지 않을 수 있기 때문에 미니 배치 크기에 의존적임</li><li>RNN은 각 시점마다 다른 통계치를 가지기 때문에 RNN에 적용하기 어려움</li></ul><h2 id=keras-api>Keras API
<a class=anchor href=#keras-api>#</a></h2><ul><li><strong>Sequential API</strong>: 단순하게 층을 쌓는 방식, 다수의 입출력 및 층 간 연산을 구현하기 어려움</li><li><strong>Functional API</strong>: 입력의 크기(shape)를 명시한 입력층을 모델의 앞단에 정의</li><li><strong>Subclassing API</strong>: Functional API로도 구현할 수 없는 모델들도 구현 가능</li></ul><h2 id=texts_to_matrix><code>texts_to_matrix()</code>
<a class=anchor href=#texts_to_matrix>#</a></h2><ul><li><code>tokenizer.texts_to_matrix(texts, mode='count')</code>: DTM 생성</li><li><code>tokenizer.texts_to_matrix(texts, mode='binary')</code>: DTM과 유사하지만 단어의 개수는 무시</li><li><code>tokenizer.texts_to_matrix(texts, mode='tfidf')</code>: TF-IDF 행렬 생성</li><li><code>tokenizer.texts_to_matrix(texts, mode='freq')</code>:<br>각 문서에서의 단어 등장 횟수를 분자로, 문서의 크기를 분모로 하는 표현하는 방법</li></ul><h2 id=nnlm>NNLM
<a class=anchor href=#nnlm>#</a></h2><ul><li>피드 포워드 신경망 언어 모델, 신경망 언어 모델의 시초로, RNNLM, BiLM 등으로 발전</li><li>기존 N-gram 언어 모델은 충분한 데이터를 관측하지 못하면 언어를 정확히 모델링하지 못하는 희소 문제를 가짐</li><li>NNLM은 N-gram 언어 모델처럼 정해진 개수(window size)의 단어만을 참고</li><li>NNLM은 N개의 input layer와 projection layer, hidden layer, output layer로 구성</li><li>Projection layer의 크기가 M일 때, 각 입력 단어들은 V x M 크기의 가중치 행렬과 곱해짐</li><li>충분한 양의 훈련 코퍼스를 학습한다면 단어 간 유사도를 구할 수 있는 임베딩 벡터값을 얻을 수 있음</li></ul><hr><h1 id=08-01-rnn>08-01. RNN
<a class=anchor href=#08-01-rnn>#</a></h1><ul><li>입력과 출력을 시퀀스 단위로 처리하는 시퀀스 모델</li><li>은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 다시 은닉층 노드의 다음 계산 입력으로 보내는 특징</li><li><strong>셀(메모리 셀, RNN 셀)</strong>: 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드, 이전의 값을 기억하는 역할</li><li><strong>Hidden state</strong>: 현재 시점을 t라 할 때, 메모리 셀이 다음 시점인 t+1의 자신에게 보내는 값</li><li>입력과 출력의 길이를 다르게 설계할 수 있어 다양한 용도로 사용 가능</li><li><strong>one-to-many</strong>: 하나의 이미지 입력에 대해서 사진의 제목인 시퀀스를 출력하는 이미지 캡셔닝 작업에 사용</li><li><strong>many-to-one</strong>: 단어 시퀀스에 대해서 하나의 출력을 하는 감성 분류, 스팸 메일 분류 등에 사용</li><li><strong>many-to-many</strong>: 사용자가 문장을 입력하면 대답 문장을 출력하는 챗봇이나 번역기에 사용</li></ul><h2 id=rnn-parameter>RNN Parameter
<a class=anchor href=#rnn-parameter>#</a></h2><ul><li>현재 시점 $t$에서의 hidden state가 $h_t$라 할 때, 두 개의 가중치 $W_x$, $W_h$가 필요</li><li>$W_x$는 입력층을 위한 가중치, $W_h$는 $t-1$의 hidden state인 $h_{t-1}$을 위한 가중치</li><li>은닉층 $h_t=tanh({W_x}{x_t}+{W_h}{h_{t-1}}+b)$, 출력층 $y_t=f({W_y}{h_t}+b)$</li><li>출력층의 활성화 함수 $f$는 이진 분류에서 시그모이드 함수, 다중 클래스 분류에서 소프트맥스 함수 등 사용</li><li>RNN의 입력 $x_t$는 단어 벤터로 간주, 단어 벡터의 차원을 $d$, hidden state의 크기를 $D_h$라 할 때,<br>메모리 셀 $h_t$ = $tanh({W_h}\times{h_{t-1}}\times{W_x}\times{x_t}+{b})$</li></ul><table><thead><tr><th style=text-align:center>$x_t$</th><th style=text-align:center>$W_x$</th><th style=text-align:center>$W_h$</th><th style=text-align:center>$h_{t-1}$</th><th style=text-align:center>$b$</th></tr></thead><tbody><tr><td style=text-align:center>$({d}\times{1})$</td><td style=text-align:center>$({D_h}\times{d})$</td><td style=text-align:center>$({D_h}\times{D_h})$</td><td style=text-align:center>$({D_h}\times{1})$</td><td style=text-align:center>$({D_h}\times{1})$</td></tr></tbody></table><h2 id=keras-rnn>Keras RNN
<a class=anchor href=#keras-rnn>#</a></h2><ul><li>hidden_units: hidden state의 크기 (output_dim)</li><li>timesteps: 입력 시퀀스(문장)의 길이 (input_length)</li><li>input_dim: 입력의 크기, 단어 벡터의 차원</li><li>RNN 층은 (batch_size, timesteps, input_dim) 크기의 3D 텐서를 입력으로 받음</li><li>메모리 셀의 최종 시점의 hidden state만 리턴할 경우 (batch_size, output_dim) 크기의 2D 텐서 반환</li><li>메모리 셀의 각 시점(time step)의 hidden state 값들을 모아 전체 시퀀스를 리턴할 경우 3D 텐서를 반환</li><li><code>return_sequences=True</code> 옵션으로 반환값 설정</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.layers</span> <span class=kn>import</span> <span class=n>SimpleRNN</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>SimpleRNN</span><span class=p>(</span><span class=n>hidden_units</span><span class=p>,</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=n>timesteps</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>)))</span></span></span></code></pre></div></div><blockquote class=book-hint><p>메모리 셀에서 hidden state 계산은 다음과 같은 코드로 동작</p></blockquote><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>hidden_state_t</span> <span class=o>=</span> <span class=mi>0</span> <span class=c1># 초기 은닉 상태를 0(벡터)로 초기화</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>input_t</span> <span class=ow>in</span> <span class=n>input_length</span><span class=p>:</span> <span class=c1># 각 시점마다 입력을 받는다.</span>
</span></span><span class=line><span class=cl>    <span class=n>output_t</span> <span class=o>=</span> <span class=n>tanh</span><span class=p>(</span><span class=n>input_t</span><span class=p>,</span> <span class=n>hidden_state_t</span><span class=p>)</span> <span class=c1># 각 시점에 대해서 입력과 은닉 상태를 가지고 연산</span>
</span></span><span class=line><span class=cl>    <span class=n>hidden_state_t</span> <span class=o>=</span> <span class=n>output_t</span> <span class=c1># 계산 결과는 현재 시점의 은닉 상태가 된다.</span></span></span></code></pre></div></div><h2 id=deep-rnn>Deep RNN
<a class=anchor href=#deep-rnn>#</a></h2><ul><li>순환 신경망에서 은닉층이 1개 더 추가되어 은닉층이 2개인 깊은 구조</li><li>첫번째 은닉층은 다음 은닉층이 존재하기 때문에 <code>return_sequences=True</code>를 설정하여 모든 시점을 전달</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.layers</span> <span class=kn>import</span> <span class=n>SimpleRNN</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>SimpleRNN</span><span class=p>(</span><span class=n>hidden_units</span><span class=p>,</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=n>timesteps</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>),</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>SimpleRNN</span><span class=p>(</span><span class=n>hidden_units</span><span class=p>,</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span></span></span></code></pre></div></div><h2 id=bidirectional-rnn>Bidirectional RNN
<a class=anchor href=#bidirectional-rnn>#</a></h2><ul><li>t에서의 출력값을 예측할 때 이전 시점의 입력 뿐 아니라, 이후 시점의 입력 또한 예측</li><li>빈칸 채우기 등의 문제에서 미래 시점의 입력에 힌트가 있기 때문에 이전과 이후의 시점을 모두 고려</li><li>하나의 출력값을 예측하기 위해 두 개의 메모리 셀을 사용</li><li>첫 번째 메모리 셀은 앞 시점의 hidden state를 전달받아 현재의 hidden state를 계산 (forward)</li><li>두 번째 메모리 셀은 뒤 시점의 hidden state를 전달받아 현재의 hidden state를 계산 (backward)</li><li>은닉층을 추가하면 학습할 수 있는 양이 많아지지만, 훈련 데이터 또한 많은 양이 필요</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.layers</span> <span class=kn>import</span> <span class=n>Bidirectional</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Bidirectional</span><span class=p>(</span><span class=n>SimpleRNN</span><span class=p>(</span><span class=n>hidden_units</span><span class=p>,</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=n>timesteps</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>)))</span></span></span></code></pre></div></div><h1 id=08-02-lstm>08-02. LSTM
<a class=anchor href=#08-02-lstm>#</a></h1><ul><li>Valina RNN(Simple RNN)은 출력 결과가 이전의 계산 결과에 의존하여 비교적 짧은 시퀀스에서면 효과를 봄</li><li><strong>장기 의존성 문제</strong>: time step이 길어질수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상</li><li>LSTM(장단기 메모리)은 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여<br>불필요한 기억을 지우고, 기억해야할 것들을 결정</li><li>전통적인 RNN에서 cell state $C_t$를 추가하여 긴 시퀀스의 입력을 처리하는데 탁월한 성능</li><li>cell state 또한 이전 시점의 cell state가 다음 시점의 cell state를 구하기 위한 입력으로서 사용</li><li><strong>입력 게이트</strong>: 현재 정보를 기억하기 위한 게이트</li><li><strong>삭제 게이트</strong>: 기억을 삭제하기 위한 게이트, 시그모이드 함수의 반환값이 0에 가까울수록 많은 정보가 삭제</li><li><strong>셀 상태</strong>: 삭제 게이트에서 일부 기억을 잃은 상태, 입력 게이트에서 선택된 기억을 삭제 게이트의 결과값과 더함</li><li>삭제 게이트는 이전 시점의 입력을 얼마나 반영할지 의미, 입력 게이트는 현재 시점의 입력을 얼마나 반영할지 결정</li><li><strong>출력 게이트</strong>: 현재 시점 $t$의 hidden state를 결정하는 일에 사용</li></ul><h1 id=08-03-gru>08-03. GRU
<a class=anchor href=#08-03-gru>#</a></h1><ul><li>LSTM의 장기 의존성 문제에 대한 해결책을 유지하면서, hidden state를 업데이트하는 계산을 감소</li><li>GRU는 업데이트 게이트와 리셋 게이트로 구성</li><li>데이터의 양이 적을 때는 매개 변수의 양이 적은 GRU가 유리, 반대의 경우엔 LSTM이 유리</li></ul><h1 id=08-04-keras-rnn-and-lstm>08-04. Keras RNN and LSTM
<a class=anchor href=#08-04-keras-rnn-and-lstm>#</a></h1><h2 id=입력-생성>입력 생성
<a class=anchor href=#%ec%9e%85%eb%a0%a5-%ec%83%9d%ec%84%b1>#</a></h2><ul><li><code>train_X</code>가 2D 텐서의 형태일 경우 batch_size 1을 추가해 3D 텐서로 변경</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>train_X</span> <span class=o>=</span> <span class=p>[[[</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>4.2</span><span class=p>,</span> <span class=mf>1.5</span><span class=p>,</span> <span class=mf>1.1</span><span class=p>,</span> <span class=mf>2.8</span><span class=p>],</span> <span class=p>[</span><span class=mf>1.0</span><span class=p>,</span> <span class=mf>3.1</span><span class=p>,</span> <span class=mf>2.5</span><span class=p>,</span> <span class=mf>0.7</span><span class=p>,</span> <span class=mf>1.1</span><span class=p>],</span> <span class=p>[</span><span class=mf>0.3</span><span class=p>,</span> <span class=mf>2.1</span><span class=p>,</span> <span class=mf>1.5</span><span class=p>,</span> <span class=mf>2.1</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>],</span> <span class=p>[</span><span class=mf>2.2</span><span class=p>,</span> <span class=mf>1.4</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.9</span><span class=p>,</span> <span class=mf>1.1</span><span class=p>]]]</span>
</span></span><span class=line><span class=cl><span class=n>train_X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>train_X</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>train_X</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span> <span class=c1># (1, 4, 5)</span></span></span></code></pre></div></div><h2 id=simpernn>SimpeRNN
<a class=anchor href=#simpernn>#</a></h2><ul><li><code>return_sequences=False</code>일 경우 2D 텐서 반환</li><li><code>return_sequences=True</code>일 경우 timesteps를 포함하는 3D 텐서 반환</li><li><code>return_state=True</code>일 경우 <code>return_sequences</code>에 관계없이 마지막 시점의 hidden state 출력</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>rnn</span> <span class=o>=</span> <span class=n>SimpleRNN</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>hidden_state</span> <span class=o>=</span> <span class=n>rnn</span><span class=p>(</span><span class=n>train_X</span><span class=p>)</span> <span class=c1># shape(1, 3)</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>rnn</span> <span class=o>=</span> <span class=n>SimpleRNN</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>hidden_states</span> <span class=o>=</span> <span class=n>rnn</span><span class=p>(</span><span class=n>train_X</span><span class=p>)</span> <span class=c1># shape(1, 4, 3)</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>rnn</span> <span class=o>=</span> <span class=n>SimpleRNN</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>return_state</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>hidden_states</span><span class=p>,</span> <span class=n>last_state</span> <span class=o>=</span> <span class=n>rnn</span><span class=p>(</span><span class=n>train_X</span><span class=p>)</span> <span class=c1># shape(1, 4, 3), shape(1, 3)</span></span></span></code></pre></div></div><h2 id=lstm>LSTM
<a class=anchor href=#lstm>#</a></h2><ul><li><code>return_state=True</code>일 경우 마지막 cell state를 포함한 세 개의 결과를 반환</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>lstm</span> <span class=o>=</span> <span class=n>LSTM</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>return_state</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>hidden_state</span><span class=p>,</span> <span class=n>last_state</span><span class=p>,</span> <span class=n>last_cell_state</span> <span class=o>=</span> <span class=n>lstm</span><span class=p>(</span><span class=n>train_X</span><span class=p>)</span> <span class=c1># each shape(1, 3)</span></span></span></code></pre></div></div><h2 id=bidirectional-lstm>Bidirectional LSTM
<a class=anchor href=#bidirectional-lstm>#</a></h2><ul><li>정방향과 역방향에 대한 hidden state와 cell state를 반환</li><li><code>return_sequences=False</code>일 경우 정방향 LSTM의 마지막 시점 hidden state와<br>역방향 LSTM의 첫번째 시점 hidden state가 연결된 채 반환</li><li><code>return_sequences=True</code>일 경우 각각의 순서 맞게 연결된 hidden state 반환</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>bilstm</span> <span class=o>=</span> <span class=n>Bidirectional</span><span class=p>(</span><span class=n>LSTM</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>return_state</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> \
</span></span><span class=line><span class=cl>                            <span class=n>kernel_initializer</span><span class=o>=</span><span class=n>k_init</span><span class=p>,</span> <span class=n>bias_initializer</span><span class=o>=</span><span class=n>b_init</span><span class=p>,</span> <span class=n>recurrent_initializer</span><span class=o>=</span><span class=n>r_init</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>hidden_states</span><span class=p>,</span> <span class=n>forward_h</span><span class=p>,</span> <span class=n>forward_c</span><span class=p>,</span> <span class=n>backward_h</span><span class=p>,</span> <span class=n>backward_c</span> <span class=o>=</span> <span class=n>bilstm</span><span class=p>(</span><span class=n>train_X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># shape(1, 6), shape(1, 3), shape(1, 3)</span></span></span></code></pre></div></div><h1 id=08-05-rnnlm>08-05. RNNLM
<a class=anchor href=#08-05-rnnlm>#</a></h1><ul><li>NNLM과 달리 time step을 도입하여 입력의 길이가 고정되지 않는 언어 모델</li><li><strong>Teaching Forcing</strong>: 테스트 과정에서 RNN 모델을 훈련시킬 때 사용하는 훈련 기법,<br>모델이 t 시점에서 예측한 값을 t+1 시점에 입력으로 사용하지 않고 실제 알고 있는 정답(t 시점의 레이블)을 사용</li><li>한 번 잘못 예측하면 뒤에서의 예측가지 영향을 미쳐 훈련 시간이 느려지기 때문에 교사 강요를 사용</li><li>훈련 과정 동안 활성화 함수로는 softmax 함수, 손실 함수로는 cross-entropy 함수를 사용</li><li>one-hot vector $x_t$를 입력받으면 NNLM에서와 동일한 embedding layer를 거쳐<br>${V}\times{M}$ 크기의 embedding vector로 변환, $e_t=lookup(x_t)$</li><li>이후 RNN과 동일한 과정을 거쳐 $\hat{y_t}$를 반환, $h_t=\tanh({W_x}{e_t}+{W_h}{h_{t-1}}+b)$</li><li>$\hat{y}_t$의 각 차원 안에서의 값은 $\hat{y}_t$의 j번째 인덱스가 가진 0과 1사이의 값이 j번째 단어가 다음 단어일 확률</li></ul><h1 id=08-06-text-generation-using-rnn>08-06. <a href=https://wikidocs.net/45101>Text Generation using RNN</a>
<a class=anchor href=#08-06-text-generation-using-rnn>#</a></h1><h2 id=데이터-전처리>데이터 전처리
<a class=anchor href=#%eb%8d%b0%ec%9d%b4%ed%84%b0-%ec%a0%84%ec%b2%98%eb%a6%ac>#</a></h2><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 원본 한국어 문장</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;경마장에 있는 말이 뛰고 있다</span><span class=se>\n</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>그의 말이 법이다</span><span class=se>\n</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>가는 말이 고와야 오는 말이 곱다</span><span class=se>\n</span><span class=s2>&#34;&#34;&#34;</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 단어 집합 생성</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>Tokenizer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>fit_on_texts</span><span class=p>([</span><span class=n>text</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>vocab_size</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>word_index</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;단어 집합의 크기 : </span><span class=si>%d</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=n>vocab_size</span><span class=p>)</span> <span class=c1># 12</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 각 라인마다 texts_to_sequences() 함수를 적용해서 훈련 데이터 생성</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>sequences</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>[[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>],</span> <span class=p>[</span><span class=mi>6</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>6</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>7</span><span class=p>],</span> <span class=p>[</span><span class=mi>8</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>8</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>9</span><span class=p>],</span> <span class=p>[</span><span class=mi>8</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>9</span><span class=p>,</span> <span class=mi>10</span><span class=p>],</span> <span class=p>[</span><span class=mi>8</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>9</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>8</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>9</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>11</span><span class=p>]]</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 패딩 후 라벨 분리 (가장 우측에 있는 단어, [경마장에, 있는]에서 &#39;있는&#39; 등을 라벨로 지정)</span>
</span></span><span class=line><span class=cl><span class=n>sequences</span> <span class=o>=</span> <span class=n>pad_sequences</span><span class=p>(</span><span class=n>sequences</span><span class=p>,</span> <span class=n>maxlen</span><span class=o>=</span><span class=n>max_len</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;pre&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sequences</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>sequences</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>sequences</span><span class=p>[:,:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>sequences</span><span class=p>[:,</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 라벨에 대해서 one-hot encoding 수행</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>to_categorical</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>num_classes</span><span class=o>=</span><span class=n>vocab_size</span><span class=p>)</span></span></span></code></pre></div></div><h2 id=rnn-모델-설계>RNN 모델 설계
<a class=anchor href=#rnn-%eb%aa%a8%eb%8d%b8-%ec%84%a4%ea%b3%84>#</a></h2><ul><li>many-to-one 구조의 RNN을 사용</li><li>모든 가능한 단어 중 마지막 시점에서 하나의 단어를 예측하는 다중 클래스 분류 문제 수행</li><li>활성화 함수로는 softmax 함수, 손실 함수로는 cross-entropy 함수 사용</li><li>첫 단어가 주어졌을 때, n번 동안 예측을 반복하면서 현재 단어와 문장에 예측 단어를 저장</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>embedding_dim</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl><span class=n>hidden_units</span> <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>SimpleRNN</span><span class=p>(</span><span class=n>hidden_units</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;softmax&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;categorical_crossentropy&#39;</span><span class=p>,</span> <span class=n>optimizer</span><span class=o>=</span><span class=s1>&#39;adam&#39;</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>200</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span></span></span></code></pre></div></div><h2 id=lstm-모델-설계>LSTM 모델 설계
<a class=anchor href=#lstm-%eb%aa%a8%eb%8d%b8-%ec%84%a4%ea%b3%84>#</a></h2><ul><li>뉴욕 타임즈 기사 제목 데이터 전처리 (단어 집합 크기 3494, 샘플 최대 길이 24)</li><li>RNN과 동일한 작업을 수행할 LSTM 모델 설계, 예측 과정 또한 동일</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>embedding_dim</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl><span class=n>hidden_units</span> <span class=o>=</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>LSTM</span><span class=p>(</span><span class=n>hidden_units</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;softmax&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;categorical_crossentropy&#39;</span><span class=p>,</span> <span class=n>optimizer</span><span class=o>=</span><span class=s1>&#39;adam&#39;</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>200</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span></span></span></code></pre></div></div><h1 id=08-07-char-rnn>08-07. <a href=https://wikidocs.net/48649>Char RNN</a>
<a class=anchor href=#08-07-char-rnn>#</a></h1><ul><li>입출력의 단위를 word-level에서 character-level로 변경한 RNN</li><li>문자 단위를 입출력으로 사용하기 때문에 embedding layer를 사용하지 않음</li><li><a href=http://www.gutenberg.org/files/11/11-0.txt>이상한 나라의 앨리스 데이터</a> 사용 (문자열 길이 159484, 문자 집합 크기 56)</li><li>훈련 데이터에 apple이라는 시퀀스가 있고 입력의 길이가 4일 때, &lsquo;appl&rsquo;을 입력하면 &lsquo;pple&rsquo;을 예측할 것으로 기대</li><li>train_X.shape(2658, 60, 56), train_y.shape(2658, 60, 56)</li></ul><h2 id=char-rnn-모델-설계>Char RNN 모델 설계
<a class=anchor href=#char-rnn-%eb%aa%a8%eb%8d%b8-%ec%84%a4%ea%b3%84>#</a></h2><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>hidden_units</span> <span class=o>=</span> <span class=mi>256</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>LSTM</span><span class=p>(</span><span class=n>hidden_units</span><span class=p>,</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=kc>None</span><span class=p>,</span> <span class=n>train_X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>2</span><span class=p>]),</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>LSTM</span><span class=p>(</span><span class=n>hidden_units</span><span class=p>,</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>TimeDistributed</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;softmax&#39;</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;categorical_crossentropy&#39;</span><span class=p>,</span> <span class=n>optimizer</span><span class=o>=</span><span class=s1>&#39;adam&#39;</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_X</span><span class=p>,</span> <span class=n>train_y</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>80</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span></span></span></code></pre></div></div></article><div class=book-mobile-nav><button class=book-nav-btn3 onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=book-nav-btn3 onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
<button class=book-nav-btn3 onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class=post-tags><a href=/tags/til/ class=tag>#TIL</a>
<a href=/tags/nlp/ class=tag>#NLP</a></div><div class=post-navigation><a href=/blog/2022-06-29/ class="post-nav-link post-nav-prev"><span class=post-nav-direction><i class="fa-solid fa-backward"></i> PREV</span>
<span class=post-nav-title>2022-06-29 Log</span>
</a><a href=/blog/2022-06-19/ class="post-nav-link post-nav-next"><span class=post-nav-direction>NEXT <i class="fa-solid fa-forward"></i></span>
<span class=post-nav-title>2022-06-19 Log</span></a></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script><div class=book-comments><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://minyeamer.github.io/blog/2022-06-28/",this.page.identifier="https://minyeamer.github.io/blog/2022-06-28/"};(function(){var e=document,t=e.createElement("script");t.src="https://minyeamer.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})();function reloadDisqus(){window.DISQUS&&DISQUS.reset({reload:!0,config:function(){this.page.url="https://minyeamer.github.io/blog/2022-06-28/",this.page.identifier="https://minyeamer.github.io/blog/2022-06-28/"}})}</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#한국어-토큰화>한국어 토큰화</a></li><li><a href=#nltk-konlpy>NLTK, KoNLPy</a></li></ul><ul><li><a href=#lemmatization>Lemmatization</a></li><li><a href=#stemming>Stemming</a></li><li><a href=#한국어에서의-어간-추출>한국어에서의 어간 추출</a></li></ul><ul><li><a href=#조건부-확률>조건부 확률</a></li><li><a href=#문장에-대한-확률>문장에 대한 확률</a></li><li><a href=#카운트-기반-접근>카운트 기반 접근</a></li></ul><ul><li><a href=#n-gram>N-gram</a></li></ul><ul><li><a href=#branching-factor>Branching Factor</a></li></ul><ul><li><a href=#cosine-similarity>Cosine Similarity</a></li><li><a href=#euclidean-distance>Euclidean Distance</a></li><li><a href=#jaccard-similarity>Jaccard Similarity</a></li></ul><ul><li><a href=#classification-and-regression>Classification and Regression</a></li><li><a href=#learning>Learning</a></li><li><a href=#confusion-matrix>Confusion Matrix</a></li><li><a href=#overfitting-and-underfitting>Overfitting and Underfitting</a></li></ul><ul><li><a href=#perceptron>Perceptron</a></li><li><a href=#ffnn>FFNN</a></li><li><a href=#activision-function>Activision Function</a></li><li><a href=#loss-function>Loss Function</a></li><li><a href=#optimizer>Optimizer</a></li><li><a href=#overfitting-방지>Overfitting 방지</a></li><li><a href=#기울기-소실>기울기 소실</a></li><li><a href=#weight-initialization>Weight Initialization</a></li><li><a href=#batch-normalization>Batch Normalization</a></li><li><a href=#keras-api>Keras API</a></li><li><a href=#texts_to_matrix><code>texts_to_matrix()</code></a></li><li><a href=#nnlm>NNLM</a></li></ul><ul><li><a href=#rnn-parameter>RNN Parameter</a></li><li><a href=#keras-rnn>Keras RNN</a></li><li><a href=#deep-rnn>Deep RNN</a></li><li><a href=#bidirectional-rnn>Bidirectional RNN</a></li></ul><ul><li><a href=#입력-생성>입력 생성</a></li><li><a href=#simpernn>SimpeRNN</a></li><li><a href=#lstm>LSTM</a></li><li><a href=#bidirectional-lstm>Bidirectional LSTM</a></li></ul><ul><li><a href=#데이터-전처리>데이터 전처리</a></li><li><a href=#rnn-모델-설계>RNN 모델 설계</a></li><li><a href=#lstm-모델-설계>LSTM 모델 설계</a></li></ul><ul><li><a href=#char-rnn-모델-설계>Char RNN 모델 설계</a></li></ul></nav><div class=book-nav><button class=book-nav-btn3 onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=book-nav-btn3 onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
<button class=book-nav-btn3 onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div></div></aside></main></body></html>