<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>2022-06-28 Log | Minystory</title><meta name=keywords content="TIL,NLP"><meta name=description content="딥 러닝을 이용한 자연어 처리 입문 1"><meta name=author content="minyeamer"><link rel=canonical href=https://minyeamer.github.io/blog/2022-06-28/><meta name=google-site-verification content="u1tWcHmHUZWfFT1cHaku6sqU-bK40N3WLR-C-4VUWN0"><meta name=naver-site-verification content="6eaf8e9da1a6104780f056f1a7797fe5a3a5a0da"><link crossorigin=anonymous href=/assets/css/stylesheet.78a14cf8249250820b49e9dc59e58b846a9beea6d16d50612c68b121ddf02146.css integrity="sha256-eKFM+CSSUIILSencWeWLhGqb7qbRbVBhLGixId3wIUY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://minyeamer.github.io/img/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://minyeamer.github.io/img/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://minyeamer.github.io/img/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://minyeamer.github.io/img/favicons/apple-touch-icon.png><link rel=mask-icon href=https://minyeamer.github.io/img/favicons/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V")</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V",{anonymize_ip:!1})}</script><meta property="og:title" content="2022-06-28 Log"><meta property="og:description" content="딥 러닝을 이용한 자연어 처리 입문 1"><meta property="og:type" content="article"><meta property="og:url" content="https://minyeamer.github.io/blog/2022-06-28/"><meta property="og:image" content="https://minyeamer.github.io/calendar.jpg"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-06-28T20:00:00+09:00"><meta property="article:modified_time" content="2022-06-28T20:00:00+09:00"><meta property="og:site_name" content="Minystory"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://minyeamer.github.io/calendar.jpg"><meta name=twitter:title content="2022-06-28 Log"><meta name=twitter:description content="딥 러닝을 이용한 자연어 처리 입문 1"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://minyeamer.github.io/post/"},{"@type":"ListItem","position":2,"name":"2022-06-28 Log","item":"https://minyeamer.github.io/blog/2022-06-28/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"2022-06-28 Log","name":"2022-06-28 Log","description":"딥 러닝을 이용한 자연어 처리 입문 1","keywords":["TIL","NLP"],"articleBody":"02-01. Tokenization Corpus에서 token이라 불리는 단위로 나누는 작업 단어 토큰화에서 단순히 구두점이나 특수문자를 제거하는 것은 의미의 손실을 발생시킬 수 있기 때문에,\n사용자의 목적과 일치하는 토큰화 도구를 사용할 필요가 있음 구두점이나 특수 문자가 필요한 경우: Ph.D, AT\u0026T, $45.55, 01/02/06 등 줄임말과 단어 내에 띄어쓰기가 있는 경우: what’re/what are, New York, rock ’n’ roll 등 문장 토큰화에서 단순히 마침표를 기준으로 문장을 잘라내는 것은 192.168.56.31, gmail.com과 같은 경우를 고려했을 때 올바르지 않음 한국어 토큰화 한국어의 경우 띄어쓰기가 가능한 단위가 어절인데,\n‘그가’, ‘그에게’, ‘그를’과 같이 어절이 독립적인 단어로 구성되는 것이 아니라\n조사 등의 무언가가 붙어있는 경우가 많기 때문에 이를 전부 형태소 단위로 분리해줘야 함 자립 형태소: 접사, 어미, 조사와 상관업싱 자립하여 사용할 수 있는 형태소, [체언, 수식언, 감탄사] 등 의존 형태소: 다른 형태소와 결합하여 사용되는 형태소, [접사, 어미, 조사, 어간] 한국어의 경우 띄어쓰기가 지켜지지 않아도 글을 쉽게 이해할 수 있어 띄어쓰기가 잘 지켜지지 않음 품사 태깅: 단어의 의미를 제대로 파악하기 위해서는 해당 단어가 어떤 품사로 쓰였는지 구분할 필요 NLTK, KoNLPy 1 2 from nltk.tokenize import word_tokenize # 단어 토큰화 from nltk.tag import pos_tag # 품사 태깅 1 2 3 4 from konlpy.tag import Okt okt = Okt() okt.porphs(sentence) # 형태소 추출 okt.pos(sentence) # 품사 태깅 02-02. Cleaning and Normalization Cleaning(정제): 갖고 있는 corpus로부터 노이즈 데이터를 제거 Normalization(정규화): 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만듦 영어권 언어에서 단어의 개수를 줄이는 정규화 방법으로 대,소문자 통합을 활용 노이즈 데이터: 아무 의미 없는 특수 문자 등, 분석하고자 하는 목적에 맞지 않는 불필요한 단어들 불필요한 단어를 제거하기 위해 불용어, 등장 빈도가 적은 단어, 길이가 짧은 단어 등을 제거 노이즈 데이터의 특징을 잡아낼 수 있다면, 정규표현식을 사용해서 제거 02-03. Stemming and Lemmatization Lemmatization Lemma(표제어): 기본 사전형 단어, [am, are, is]의 뿌리 단어 be 등 Stem(어간): 단어의 의미를 담고 있는 단어의 핵심 부분, ‘cats’에서 ‘cat’ Affix(접사): 단어에 추가적인 의미를 주는 부분, ‘cats’에서 ’s’ 1 2 3 4 from nltk.stem import WordNetLemmatizer lemmatizer = WordNetLemmatizer() lemmatizer.lemmatize(word) # am -\u003e be, having -\u003e have Stemming 1 2 3 4 from nltk.stem import PorterStemmer stemmer = PorterStemmer() stemmer.stem(word) # am -\u003e am, having -\u003e hav 한국어에서의 어간 추출 5언 9품사의 구조에서 용언에 해당되는 동사와 형용사는 어간과 어미의 결합으로 구성 활용: 용언의 어간이 어미를 가지는 일 규칙 활용: 어간이 어미를 취할 때 어간의 모습이 일정, 잡/어간 + 다/어미 불규칙 활용: 어간이 어미를 취할 때 어간의 모습이 바뀌거나 특수한 어미일 경우, ‘오르+아/어-\u003e올라’ 등 02-04. Stopword Stopword(불용어): 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 기여하지 않는 단어, [조사, 접미사] 등 1 2 from nltk.corpus import stopwords stop_words_list = stopwords.words('english') 1 2 3 okt = Okt() okt.morphs(sentence) # 조사, 접속사 등 제거 # 또는 불용어 사전을 만들어서 제거 02-05. Regular Expression 정규 표현식 참고 02-06. Integer Encoding 컴퓨터는 텍스트보다 숫자를 더 잘 처리할 수 있기 때문에 텍스트를 숫자로 변경 단어를 빈도수 순으로 정렬하고 순서대로 낮은 숫자부터 정수를 부여 dictionary, Counter, nltk.FreqDist, keras.Tokenizer 등 활용 1 2 from nltk import FreqDist FreqDist(np.hstack(preprocessed_sentences)) # np.hastack으로 문장 구분을 제거 1 2 3 4 5 6 from tensorflow.keras.preprocessing.text import Tokenizer tokenizer = Tokenizer() # num_words 파라미터로 사용할 단어 개수 지정 tokenizer.fit_on_texts(preprocessed_sentences) # 빈도수 기분으로 단어 집합 생성 print(tokenizer.word_intex) # 정수 인덱스 확인 print(tokenizer.word_counts) # 단어 빈도수 확인 print(tokenizer.texts_to_sequences(preprocessed_sentences)) # corpus를 인덱스로 변환 02-07. Padding 병렬 연산을 위해 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요 1 2 3 4 5 6 from tensorflow.keras.preprocessing.sequence import pad_sequences encoded = tokenizer.texts_to_sequences(preprocessed_sentences) padded = pad_seqences(encoded) # padding='post'를 입력해야 뒤에서 부터 0을 채움 # maxlen으로 문장 길이 조절 # truncating='post'를 통해 문장 길이 초과 시 뒤의 단어가 삭제되도록 설정 02-08. One-Hot Encoding Vocabulary(단어 집합): 서로 다른 단어들의 집합, book과 books과 같은 변형 형태도 다른 단어로 간주 One-Hot Encoding: 단어 집합의 크기를 벡터의 차원으로 하고,\n표현하고 싶은 단어에 1, 다른 인텍스에 0을 부여하는 단어의 벡터 표현 방식 단어의 개수가 늘어날 수록 벡터를 저장하기 위해 필요한 공간이 계속 늘어나는 단점 단어의 유사도를 표현하지 못하는 단점 (강아지, 개, 냉장고 등) 1 2 3 from tensorflow.keras.utils import to_categorical encoded = tokenizer.texts_to_sequences(preprocessed_sentences)[0] one_hot = to_categorical(encoded) 03-01. Language Model 단어 시퀀스(문장)에 확률을 할당하는 모델, 이전 단어들이 주어졌을 때 다음 단어를 예측 단어 시퀀스 W의 확률 $P(W)=P(w_1,w_2,w_3,w_4,w_5,…,w_n)$ 다음 단어 등장 확률 $P(w_n|w_1,…,w_{n-1})$ 03-02. Statistical Language Model 조건부 확률 남학생(A) 여학생(B) 계 중학생(C) 100 60 160 고등학생(D) 80 120 200 계 180 180 360 학생을 뽑았을 때, 고등학생이면서 남학생일 확률 $P(A \\bigcap B)=80/360$ 고등학생 중 한명을 뽑았을 때, 남학생일 확률 $P(A|D)=P(A \\bigcap D)/P(D)=(80/360)/(200/360)$ 문장에 대한 확률 ‘An adorable little boy is spreading smiles’의 확률\n$P(\\text{An adorable little boy is spreading smiles})=\\ P(\\text{An}) \\times P(\\text{adorable}|\\text{An}) \\times … \\times P(\\text{smiles}|\\text{An adorable little boy is spreading})$ 카운트 기반 접근 An adorable little boy가 100번 등장했을 때 그 다음에 is가 등장한 경우가 30번이라면,\n$P(\\text{is}|\\text{An adorable little boy})$는 30% 카운트 기반으로 훈련할 경우 단어 시퀀스가 없어 확률이 0이 되는 경우를 방지하기 위해 방대한 양의 훈련 데이터가 필요 회소 문제: 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제 $$P(\\text{is}|\\text{An adorable little boy})= \\frac{count(\\text{An adorable little boy is})}{count(\\text{An adorable little boy})}$$\n03-03. N-gram Language Model 통계적 언어 모델의 일종이지만, 모든 단어가 아닌 일부 단어만 고려하는 접근 방법 사용 An adorable little boy에서 is가 나올 확률을 boy가 나왔을 때 is가 나올 확률로 대체\n$P(\\text{is}|\\text{An adorable little boy}) \\approx P(\\text{is}|\\text{boy})$ 뒤의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 발생 전체 문장을 고려한 언어 모델보다는 정확도가 떨어짐 몇 개의 단어를 볼지 n을 정하는 것은 trade-off 문제를 발생시킴, n은 최대 5를 넘게 잡아서는 안된다고 권장 N-gram n개의 연속적인 단어 나열 An adorable little boy에 대해\nunigrams: an, adorable, little, boy\nbigrams: an adorable, adorable little, little boy 03-05. Perplexity Perplexity(PPL): 헷갈리는 정도, 낮을수록 언어 모델의 성능이 좋음 $$PPL(W)=P(w_1,w_2,w_3,…,w_N)^{-\\frac{1}{N}}=\\sqrt[N]{\\frac{1}{P(w_1,w_2,w_3,…,w_N)}}$$\nBranching Factor Branching factor(분기계수): PPL이 선택할 수 있는 가능한 경우의 수 대해 PPL이 10이 나왔을 때, 언어 모델은 테스트 데이터에 대해 다음 단어를 예측할 때 평균 10개의 단어를 고려 PPL의 값이 낮다는 것은 테스트 데이터 상에서 높은 정확도를 보이는 것일뿐, 반드시 사람이 직접 느끼기에 좋은 모델인 것은 아님 $$PPL(W)=P(w_1,w_2,w_3,…,w_N)^{-\\frac{1}{N}}=(\\frac{1}{10}^N)^{-\\frac{1}{N}}=\\frac{1}{10}^{-1}=10$$\n04-01. 단어의 표현 방법 국소 표현(이산 표현): 해당 단어 그 자체만 보고, 특정값을 맵핑하여 단어를 표현하는 방법 분산 표현(연속 표현): 그 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법 04-02. Bag of Words(BoW) 단어들의 순서는 고려하지 않고, 단어들의 출현 빈도에만 집중하는 텍스트 데이터 수치화 표현 방법 1 2 3 doc1 = '정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.' vocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9} bag of words vector : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1] 1 2 3 4 from sklearn.feature_extraction.text import CounteVectorizer vector = CounterVectorizer() # stop_words 파라미터로 불용어 제거('english' 또는 리스트 등) print('bag of words vector:', vector.fit_transform(corpus).toarray()) print('vocabulary:', vector.vocabulary_) 04-03. Document-Term Matrix(DTM) 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것 One-hot vector와 마찬가지로 대부분의 값이 0인 희소 표현의 문제 발생 불용어와 중요한 단어에 대해서 가중치를 주기 위해 TF-IDF를 사용 04-04. TF-IDF 단어의 빈도와 역 문서 빈도를 사용하여 DTM 내의 각 단어들마다 중요도를 가중치로 부여하는 방법 $tf(d,t)$: 특정 문서 $d$에서의 특정 단어 $t$의 등장 횟수, DTM에서의 각 단어들의 가진 값 $df(t)$: 특정 단어 $t$가 등장한 문서의 수, 특정 단어가 각 문서에서 등장한 횟수는 무시 $idf(d,t)$: $df(t)$에 반비례하는 수,\n총 문서의 수 n이 커질수록 기하급수적으로 증가하는 것을 방지하기 위해 $log$(일반적으로 자연 로그) 적용 $$idf(d,t)=log(\\frac{n}{1+df(t)})$$\n1 2 3 from sklearn.feature_extraction.text import TfidfVectorizer print(vector.fit_transform(corpus).toarray()) print(ve) 05. Vector Similarity Cosine Similarity 두 벡터 간의 코사인 각도를 이용하여 구할 수 있는 두 벡터의 유사도 두 벡터의 방향이 동일하면 1의 값을 가지며, 값이 1에 가까울수록 유사도가 높음 문서의 길이가 다른 상황에서 비교적 공정한 비교를 할 수 있음 Euclidean Distance 다차원 공간에서 두 개의 점 $p$와 $q$가 각각 $p=(p_1,p_2,p_3,…,p_n)$과 $q=(q_1,q_2,q_3,…,q_n)$의 좌표를 가질 때\n두 점 사이의 거리를 계산하는 유클리드 거리 공식 $$\\sqrt{(q_1-p_1)^2+(q_2-p_2)^2+…+(q_n-p_n)^2}=\\sqrt{\\Sigma^n_{i=1}(q_i-p_i)^2}$$\nJaccard Similarity 합집합에서 교집합의 비율을 구한다면 두 집합 A와 B의 유사도를 구할 수 있음 자카드 유사도 J는 0과 1사이의 값을 가지며, 두 집합이 동일하면 1, 공통 원소가 없으면 0의 값을 가짐 $$J(A,B)=\\frac{|A \\bigcap B|}{|A \\bigcup B|}=\\frac{|A \\bigcap B|}{|A|+|B|-|A \\bigcap B|}$$\n06. Machine Learning Classification and Regression Bianry Classification: 두 개의 선택지 중 하나의 답을 선택 Multi-class Classification: 세 개 이상의 선택지 중에서 답을 선택 Regression: 연속적인 값의 범위 내에서 예측값을 도출 Learning Supervised Learning: 정답 레이블과 함께 함습 Unsupervised Learning: 데이터에 별도의 레이블이 없이 학습 Self-Supervised Learning: 레이블이 없는 데이터가 주어지면, 모델이 학습을 위해 스스로 레이블을 생성 Confusion Matrix 예측 참 예측 거짓 실제 참 TP(정답) FN(오답) 실제 거짓 FP(오답) TN(정답) Precision(정밀도): True라고 분류한 것 중 실제 True의 비율, $Precision=\\frac{TP}{TP+FP}$ Recall(재현율): 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율, $Recall=\\frac{TP}{TP+FN}$ Accuracy(정확도): 전체 예측한 데이터 중 정답을 맞춘 것에 대한 비율, $Accuracy=\\frac{TP+TN}{TP+FN+FP+TN}$ Overfitting and Underfitting Overfitting: 훈련 데이터를 과하게 학습, 훈련 데이터에 비해 테스트 데이터의 오차가 커짐 Underfitting: 테스트 데이터의 성능이 올라갈 여지가 있음에도 훈련을 덜 한 상태, 훈련 데이터에서도 정확도가 낮음 07. Deep Learning Perceptron 입력값 $x$, 가중치 $w$, 출력값 $y$ 가중치의 값이 크면 클수록 해당 입력 값이 중요하다는 것을 의미 단층 퍼셉트론: 값을 보내는 input layer와 값을 받아서 출력하는 output layer로 구성 다층 퍼셉트론(MLP): 입력층과 출력층 사이에 hidden layer를 추가 FFNN FFNN(피드 포워드 신경망): 오직 입력층에서 출력층 방향으로 연산이 전개되는 신경망 RNN(순환 신경망): 은닉층의 출력값이 다시 은닉층으로 입력되는 신경망 Activision Function 은닉층과 출력층의 뉴런에서 출력값을 결정하는 함수 Step function, Sigmoid function, ReLU 등 비선형 함수의 특성 Loss Function MSE: 연속형 변수 예측 Binary Cross-Entropy: 시그모이드 함수 출력 Categorical Cross-Entropy: 소프트맥스 함수 출력 Optimizer Momentum: 경사 하강법에 모멘텀을 더해 Local Minimum에 빠지더라도 빠져나갈 수 있게 함 Adagrad: 각 매개변수에 서로 다른 학습률을 적용 RMSprop: Adagrad가 학습을 진행할수록 학습률이 지나치게 떨어지는 단점을 개선 Adam: RMSprop과 Momentum을 합친 듯한 방법, 방향과 학습률 두 가지를 모두 잡기 위한 방법 Overfitting 방지 데이터의 양을 늘리기\n데이터의 양이 적으면 데이터의 특정 패턴이나 노이즈까지 쉽게 암기해버림, Data Augmentation 활용 모델의 복잡도 줄이기\n인공 신경망의 복잡도는 은닉층의 수나 매개변수의 수 등으로 결정 가충치 규제 적용하기\nL1 규제(가중치의 절댓값 합계를 비용 함수에 추가), L2 규제(모든 가중치들의 제곱합을 비용 함수에 추가) Dropout\n학습 시에 인공 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지 기울기 소실 기울기 소실: 역전파 과정에서 입력층으로 갈 수록 기울기가 점차적으로 작아지는 현상 기울기 폭주: 기울기가 점차 커지다가 가중치들이 비정상적으로 큰 값이 되면서 발산되는 경우 Gradient Clipping: 기울기 폭주를 막기 위해 임계값을 넘지 않도록 값의 크기를 감소 Weight Initialization Xavier Initialization: 균등 분포 또는 정규 분포로 초기화 할 때 두 가지 경우로 나뉨 He Initialization: Xavier 초기화와 다르게 다음 층의 뉴런의 수를 반영하지 않음 Batch Normalization 내부 공변량 변화: 학습 과정에서 층 별로 입력 데이터 분포가 달라지는 현상 배치 정규화: 한 번에 들어오는 배치 단위로 정규화하는 것 배치 정규화는 추가 계산을 발생시켜 모델을 복잡하게 하기 때문에 예측 시 실행 시간이 느려지는 단점 너무 작은 배치 크기에서는 잘 동작하지 않을 수 있기 때문에 미니 배치 크기에 의존적임 RNN은 각 시점마다 다른 통계치를 가지기 때문에 RNN에 적용하기 어려움 Keras API Sequential API: 단순하게 층을 쌓는 방식, 다수의 입출력 및 층 간 연산을 구현하기 어려움 Functional API: 입력의 크기(shape)를 명시한 입력층을 모델의 앞단에 정의 Subclassing API: Functional API로도 구현할 수 없는 모델들도 구현 가능 texts_to_matrix() tokenizer.texts_to_matrix(texts, mode='count'): DTM 생성 tokenizer.texts_to_matrix(texts, mode='binary'): DTM과 유사하지만 단어의 개수는 무시 tokenizer.texts_to_matrix(texts, mode='tfidf'): TF-IDF 행렬 생성 tokenizer.texts_to_matrix(texts, mode='freq'):\n각 문서에서의 단어 등장 횟수를 분자로, 문서의 크기를 분모로 하는 표현하는 방법 NNLM 피드 포워드 신경망 언어 모델, 신경망 언어 모델의 시초로, RNNLM, BiLM 등으로 발전 기존 N-gram 언어 모델은 충분한 데이터를 관측하지 못하면 언어를 정확히 모델링하지 못하는 희소 문제를 가짐 NNLM은 N-gram 언어 모델처럼 정해진 개수(window size)의 단어만을 참고 NNLM은 N개의 input layer와 projection layer, hidden layer, output layer로 구성 Projection layer의 크기가 M일 때, 각 입력 단어들은 V x M 크기의 가중치 행렬과 곱해짐 충분한 양의 훈련 코퍼스를 학습한다면 단어 간 유사도를 구할 수 있는 임베딩 벡터값을 얻을 수 있음 08-01. RNN 입력과 출력을 시퀀스 단위로 처리하는 시퀀스 모델 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 다시 은닉층 노드의 다음 계산 입력으로 보내는 특징 셀(메모리 셀, RNN 셀): 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드, 이전의 값을 기억하는 역할 Hidden state: 현재 시점을 t라 할 때, 메모리 셀이 다음 시점인 t+1의 자신에게 보내는 값 입력과 출력의 길이를 다르게 설계할 수 있어 다양한 용도로 사용 가능 one-to-many: 하나의 이미지 입력에 대해서 사진의 제목인 시퀀스를 출력하는 이미지 캡셔닝 작업에 사용 many-to-one: 단어 시퀀스에 대해서 하나의 출력을 하는 감성 분류, 스팸 메일 분류 등에 사용 many-to-many: 사용자가 문장을 입력하면 대답 문장을 출력하는 챗봇이나 번역기에 사용 RNN Parameter 현재 시점 $t$에서의 hidden state가 $h_t$라 할 때, 두 개의 가중치 $W_x$, $W_h$가 필요 $W_x$는 입력층을 위한 가중치, $W_h$는 $t-1$의 hidden state인 $h_{t-1}$을 위한 가중치 은닉층 $h_t=tanh({W_x}{x_t}+{W_h}{h_{t-1}}+b)$, 출력층 $y_t=f({W_y}{h_t}+b)$ 출력층의 활성화 함수 $f$는 이진 분류에서 시그모이드 함수, 다중 클래스 분류에서 소프트맥스 함수 등 사용 RNN의 입력 $x_t$는 단어 벤터로 간주, 단어 벡터의 차원을 $d$, hidden state의 크기를 $D_h$라 할 때,\n메모리 셀 $h_t$ = $tanh({W_h}\\times{h_{t-1}}\\times{W_x}\\times{x_t}+{b})$ $x_t$ $W_x$ $W_h$ $h_{t-1}$ $b$ $({d}\\times{1})$ $({D_h}\\times{d})$ $({D_h}\\times{D_h})$ $({D_h}\\times{1})$ $({D_h}\\times{1})$ Keras RNN hidden_units: hidden state의 크기 (output_dim) timesteps: 입력 시퀀스(문장)의 길이 (input_length) input_dim: 입력의 크기, 단어 벡터의 차원 RNN 층은 (batch_size, timesteps, input_dim) 크기의 3D 텐서를 입력으로 받음 메모리 셀의 최종 시점의 hidden state만 리턴할 경우 (batch_size, output_dim) 크기의 2D 텐서 반환 메모리 셀의 각 시점(time step)의 hidden state 값들을 모아 전체 시퀀스를 리턴할 경우 3D 텐서를 반환 return_sequences=True 옵션으로 반환값 설정 1 2 3 4 from tensorflow.keras.layers import SimpleRNN model = Sequential() model.add(SimpleRNN(hidden_units, input_shape=(timesteps, input_dim))) 메모리 셀에서 hidden state 계산은 다음과 같은 코드로 동작\n1 2 3 4 hidden_state_t = 0 # 초기 은닉 상태를 0(벡터)로 초기화 for input_t in input_length: # 각 시점마다 입력을 받는다. output_t = tanh(input_t, hidden_state_t) # 각 시점에 대해서 입력과 은닉 상태를 가지고 연산 hidden_state_t = output_t # 계산 결과는 현재 시점의 은닉 상태가 된다. Deep RNN 순환 신경망에서 은닉층이 1개 더 추가되어 은닉층이 2개인 깊은 구조 첫번째 은닉층은 다음 은닉층이 존재하기 때문에 return_sequences=True를 설정하여 모든 시점을 전달 1 2 3 4 5 from tensorflow.keras.layers import SimpleRNN model = Sequential() model.add(SimpleRNN(hidden_units, input_shape=(timesteps, input_dim), return_sequences=True)) model.add(SimpleRNN(hidden_units, return_sequences=True)) Bidirectional RNN t에서의 출력값을 예측할 때 이전 시점의 입력 뿐 아니라, 이후 시점의 입력 또한 예측 빈칸 채우기 등의 문제에서 미래 시점의 입력에 힌트가 있기 때문에 이전과 이후의 시점을 모두 고려 하나의 출력값을 예측하기 위해 두 개의 메모리 셀을 사용 첫 번째 메모리 셀은 앞 시점의 hidden state를 전달받아 현재의 hidden state를 계산 (forward) 두 번째 메모리 셀은 뒤 시점의 hidden state를 전달받아 현재의 hidden state를 계산 (backward) 은닉층을 추가하면 학습할 수 있는 양이 많아지지만, 훈련 데이터 또한 많은 양이 필요 1 2 3 4 from tensorflow.keras.layers import Bidirectional model = Sequential() model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True), input_shape=(timesteps, input_dim))) 08-02. LSTM Valina RNN(Simple RNN)은 출력 결과가 이전의 계산 결과에 의존하여 비교적 짧은 시퀀스에서면 효과를 봄 장기 의존성 문제: time step이 길어질수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상 LSTM(장단기 메모리)은 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여\n불필요한 기억을 지우고, 기억해야할 것들을 결정 전통적인 RNN에서 cell state $C_t$를 추가하여 긴 시퀀스의 입력을 처리하는데 탁월한 성능 cell state 또한 이전 시점의 cell state가 다음 시점의 cell state를 구하기 위한 입력으로서 사용 입력 게이트: 현재 정보를 기억하기 위한 게이트 삭제 게이트: 기억을 삭제하기 위한 게이트, 시그모이드 함수의 반환값이 0에 가까울수록 많은 정보가 삭제 셀 상태: 삭제 게이트에서 일부 기억을 잃은 상태, 입력 게이트에서 선택된 기억을 삭제 게이트의 결과값과 더함 삭제 게이트는 이전 시점의 입력을 얼마나 반영할지 의미, 입력 게이트는 현재 시점의 입력을 얼마나 반영할지 결정 출력 게이트: 현재 시점 $t$의 hidden state를 결정하는 일에 사용 08-03. GRU LSTM의 장기 의존성 문제에 대한 해결책을 유지하면서, hidden state를 업데이트하는 계산을 감소 GRU는 업데이트 게이트와 리셋 게이트로 구성 데이터의 양이 적을 때는 매개 변수의 양이 적은 GRU가 유리, 반대의 경우엔 LSTM이 유리 08-04. Keras RNN and LSTM 입력 생성 train_X가 2D 텐서의 형태일 경우 batch_size 1을 추가해 3D 텐서로 변경 1 2 3 train_X = [[[0.1, 4.2, 1.5, 1.1, 2.8], [1.0, 3.1, 2.5, 0.7, 1.1], [0.3, 2.1, 1.5, 2.1, 0.1], [2.2, 1.4, 0.5, 0.9, 1.1]]] train_X = np.array(train_X, dtype=np.float32) print(train_X.shape) # (1, 4, 5) SimpeRNN return_sequences=False일 경우 2D 텐서 반환 return_sequences=True일 경우 timesteps를 포함하는 3D 텐서 반환 return_state=True일 경우 return_sequences에 관계없이 마지막 시점의 hidden state 출력 1 2 rnn = SimpleRNN(3) hidden_state = rnn(train_X) # shape(1, 3) 1 2 rnn = SimpleRNN(3, return_sequences=True) hidden_states = rnn(train_X) # shape(1, 4, 3) 1 2 rnn = SimpleRNN(3, return_sequences=True, return_state=True) hidden_states, last_state = rnn(train_X) # shape(1, 4, 3), shape(1, 3) LSTM return_state=True일 경우 마지막 cell state를 포함한 세 개의 결과를 반환 1 2 lstm = LSTM(3, return_sequences=False, return_state=True) hidden_state, last_state, last_cell_state = lstm(train_X) # each shape(1, 3) Bidirectional LSTM 정방향과 역방향에 대한 hidden state와 cell state를 반환 return_sequences=False일 경우 정방향 LSTM의 마지막 시점 hidden state와\n역방향 LSTM의 첫번째 시점 hidden state가 연결된 채 반환 return_sequences=True일 경우 각각의 순서 맞게 연결된 hidden state 반환 1 2 3 4 bilstm = Bidirectional(LSTM(3, return_sequences=False, return_state=True, \\ kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init)) hidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X) # shape(1, 6), shape(1, 3), shape(1, 3) 08-05. RNNLM NNLM과 달리 time step을 도입하여 입력의 길이가 고정되지 않는 언어 모델 Teaching Forcing: 테스트 과정에서 RNN 모델을 훈련시킬 때 사용하는 훈련 기법,\n모델이 t 시점에서 예측한 값을 t+1 시점에 입력으로 사용하지 않고 실제 알고 있는 정답(t 시점의 레이블)을 사용 한 번 잘못 예측하면 뒤에서의 예측가지 영향을 미쳐 훈련 시간이 느려지기 때문에 교사 강요를 사용 훈련 과정 동안 활성화 함수로는 softmax 함수, 손실 함수로는 cross-entropy 함수를 사용 one-hot vector $x_t$를 입력받으면 NNLM에서와 동일한 embedding layer를 거쳐\n${V}\\times{M}$ 크기의 embedding vector로 변환, $e_t=lookup(x_t)$ 이후 RNN과 동일한 과정을 거쳐 $\\hat{y_t}$를 반환, $h_t=\\tanh({W_x}{e_t}+{W_h}{h_{t-1}}+b)$ $\\hat{y}_t$의 각 차원 안에서의 값은 $\\hat{y}_t$의 j번째 인덱스가 가진 0과 1사이의 값이 j번째 단어가 다음 단어일 확률 08-06. Text Generation using RNN 데이터 전처리 1 2 3 4 # 원본 한국어 문장 text = \"\"\"경마장에 있는 말이 뛰고 있다\\n 그의 말이 법이다\\n 가는 말이 고와야 오는 말이 곱다\\n\"\"\" 1 2 3 4 5 # 단어 집합 생성 tokenizer = Tokenizer() tokenizer.fit_on_texts([text]) vocab_size = len(tokenizer.word_index) + 1 print('단어 집합의 크기 : %d' % vocab_size) # 12 1 2 3 # 각 라인마다 texts_to_sequences() 함수를 적용해서 훈련 데이터 생성 print(sequences) [[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]] 1 2 3 4 5 # 패딩 후 라벨 분리 (가장 우측에 있는 단어, [경마장에, 있는]에서 '있는' 등을 라벨로 지정) sequences = pad_sequences(sequences, maxlen=max_len, padding='pre') sequences = np.array(sequences) X = sequences[:,:-1] y = sequences[:,-1] 1 2 # 라벨에 대해서 one-hot encoding 수행 y = to_categorical(y, num_classes=vocab_size) RNN 모델 설계 many-to-one 구조의 RNN을 사용 모든 가능한 단어 중 마지막 시점에서 하나의 단어를 예측하는 다중 클래스 분류 문제 수행 활성화 함수로는 softmax 함수, 손실 함수로는 cross-entropy 함수 사용 첫 단어가 주어졌을 때, n번 동안 예측을 반복하면서 현재 단어와 문장에 예측 단어를 저장 1 2 3 4 5 6 7 8 9 embedding_dim = 10 hidden_units = 32 model = Sequential() model.add(Embedding(vocab_size, embedding_dim)) model.add(SimpleRNN(hidden_units)) model.add(Dense(vocab_size, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit(X, y, epochs=200, verbose=2) LSTM 모델 설계 뉴욕 타임즈 기사 제목 데이터 전처리 (단어 집합 크기 3494, 샘플 최대 길이 24) RNN과 동일한 작업을 수행할 LSTM 모델 설계, 예측 과정 또한 동일 1 2 3 4 5 6 7 8 9 embedding_dim = 10 hidden_units = 128 model = Sequential() model.add(Embedding(vocab_size, embedding_dim)) model.add(LSTM(hidden_units)) model.add(Dense(vocab_size, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit(X, y, epochs=200, verbose=2) 08-07. Char RNN 입출력의 단위를 word-level에서 character-level로 변경한 RNN 문자 단위를 입출력으로 사용하기 때문에 embedding layer를 사용하지 않음 이상한 나라의 앨리스 데이터 사용 (문자열 길이 159484, 문자 집합 크기 56) 훈련 데이터에 apple이라는 시퀀스가 있고 입력의 길이가 4일 때, ‘appl’을 입력하면 ‘pple’을 예측할 것으로 기대 train_X.shape(2658, 60, 56), train_y.shape(2658, 60, 56) Char RNN 모델 설계 1 2 3 4 5 6 7 8 9 hidden_units = 256 model = Sequential() model.add(LSTM(hidden_units, input_shape=(None, train_X.shape[2]), return_sequences=True)) model.add(LSTM(hidden_units, return_sequences=True)) model.add(TimeDistributed(Dense(vocab_size, activation='softmax'))) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) model.fit(train_X, train_y, epochs=80, verbose=2) ","wordCount":"3180","inLanguage":"en","image":"https://minyeamer.github.io/calendar.jpg","datePublished":"2022-06-28T20:00:00+09:00","dateModified":"2022-06-28T20:00:00+09:00","author":{"@type":"Person","name":"minyeamer"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://minyeamer.github.io/blog/2022-06-28/"},"publisher":{"@type":"Organization","name":"Minystory","logo":{"@type":"ImageObject","url":"https://minyeamer.github.io/img/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://minyeamer.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://minyeamer.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://minyeamer.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://minyeamer.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://minyeamer.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://minyeamer.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://minyeamer.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://minyeamer.github.io/post/>Posts</a></div><h1 class=post-title>2022-06-28 Log</h1><div class=post-meta><span title='2022-06-28 20:00:00 +0900 KST'>June 28, 2022</span>&nbsp;·&nbsp;15 min&nbsp;·&nbsp;3180 words&nbsp;·&nbsp;minyeamer&nbsp;|&nbsp;<a href=https://github.com/minyeamer/til/edit/main/2022/2022-06/2022-06-28.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><figure class=entry-cover><img loading=lazy src="https://github.com/minyeamer/til/blob/main/.media/covers/calendar.jpg?raw=true" alt></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>&nbsp;Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#02-01-tokenization>02-01. Tokenization</a><ul><li><a href=#한국어-토큰화>한국어 토큰화</a></li><li><a href=#nltk-konlpy>NLTK, KoNLPy</a></li></ul></li><li><a href=#02-02-cleaning-and-normalization>02-02. Cleaning and Normalization</a></li><li><a href=#02-03-stemming-and-lemmatization>02-03. Stemming and Lemmatization</a><ul><li><a href=#lemmatization>Lemmatization</a></li><li><a href=#stemming>Stemming</a></li><li><a href=#한국어에서의-어간-추출>한국어에서의 어간 추출</a></li></ul></li><li><a href=#02-04-stopword>02-04. Stopword</a></li><li><a href=#02-05-regular-expression>02-05. Regular Expression</a></li><li><a href=#02-06-integer-encoding>02-06. Integer Encoding</a></li><li><a href=#02-07-padding>02-07. Padding</a></li><li><a href=#02-08-one-hot-encoding>02-08. One-Hot Encoding</a></li><li><a href=#03-01-language-model>03-01. Language Model</a></li><li><a href=#03-02-statistical-language-model>03-02. Statistical Language Model</a><ul><li><a href=#조건부-확률>조건부 확률</a></li><li><a href=#문장에-대한-확률>문장에 대한 확률</a></li><li><a href=#카운트-기반-접근>카운트 기반 접근</a></li></ul></li><li><a href=#03-03-n-gram-language-model>03-03. N-gram Language Model</a><ul><li><a href=#n-gram>N-gram</a></li></ul></li><li><a href=#03-05-perplexity>03-05. Perplexity</a><ul><li><a href=#branching-factor>Branching Factor</a></li></ul></li><li><a href=#04-01-단어의-표현-방법>04-01. 단어의 표현 방법</a></li><li><a href=#04-02-bag-of-wordsbow>04-02. Bag of Words(BoW)</a></li><li><a href=#04-03-document-term-matrixdtm>04-03. Document-Term Matrix(DTM)</a></li><li><a href=#04-04-tf-idf>04-04. TF-IDF</a></li><li><a href=#05-vector-similarity>05. Vector Similarity</a><ul><li><a href=#cosine-similarity>Cosine Similarity</a></li><li><a href=#euclidean-distance>Euclidean Distance</a></li><li><a href=#jaccard-similarity>Jaccard Similarity</a></li></ul></li><li><a href=#06-machine-learning>06. Machine Learning</a><ul><li><a href=#classification-and-regression>Classification and Regression</a></li><li><a href=#learning>Learning</a></li><li><a href=#confusion-matrix>Confusion Matrix</a></li><li><a href=#overfitting-and-underfitting>Overfitting and Underfitting</a></li></ul></li><li><a href=#07-deep-learning>07. Deep Learning</a><ul><li><a href=#perceptron>Perceptron</a></li><li><a href=#ffnn>FFNN</a></li><li><a href=#activision-function>Activision Function</a></li><li><a href=#loss-function>Loss Function</a></li><li><a href=#optimizer>Optimizer</a></li><li><a href=#overfitting-방지>Overfitting 방지</a></li><li><a href=#기울기-소실>기울기 소실</a></li><li><a href=#weight-initialization>Weight Initialization</a></li><li><a href=#batch-normalization>Batch Normalization</a></li><li><a href=#keras-api>Keras API</a></li><li><a href=#texts_to_matrix><code>texts_to_matrix()</code></a></li><li><a href=#nnlm>NNLM</a></li></ul></li><li><a href=#08-01-rnn>08-01. RNN</a><ul><li><a href=#rnn-parameter>RNN Parameter</a></li><li><a href=#keras-rnn>Keras RNN</a></li><li><a href=#deep-rnn>Deep RNN</a></li><li><a href=#bidirectional-rnn>Bidirectional RNN</a></li></ul></li><li><a href=#08-02-lstm>08-02. LSTM</a></li><li><a href=#08-03-gru>08-03. GRU</a></li><li><a href=#08-04-keras-rnn-and-lstm>08-04. Keras RNN and LSTM</a><ul><li><a href=#입력-생성>입력 생성</a></li><li><a href=#simpernn>SimpeRNN</a></li><li><a href=#lstm>LSTM</a></li><li><a href=#bidirectional-lstm>Bidirectional LSTM</a></li></ul></li><li><a href=#08-05-rnnlm>08-05. RNNLM</a></li><li><a href=#08-06-text-generation-using-rnnhttpswikidocsnet45101>08-06. <a href=https://wikidocs.net/45101>Text Generation using RNN</a></a><ul><li><a href=#데이터-전처리>데이터 전처리</a></li><li><a href=#rnn-모델-설계>RNN 모델 설계</a></li><li><a href=#lstm-모델-설계>LSTM 모델 설계</a></li></ul></li><li><a href=#08-07-char-rnnhttpswikidocsnet48649>08-07. <a href=https://wikidocs.net/48649>Char RNN</a></a><ul><li><a href=#char-rnn-모델-설계>Char RNN 모델 설계</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h1 id=02-01-tokenization>02-01. Tokenization<a hidden class=anchor aria-hidden=true href=#02-01-tokenization>#</a></h1><ul><li>Corpus에서 token이라 불리는 단위로 나누는 작업</li><li>단어 토큰화에서 단순히 구두점이나 특수문자를 제거하는 것은 의미의 손실을 발생시킬 수 있기 때문에,<br>사용자의 목적과 일치하는 토큰화 도구를 사용할 필요가 있음</li><li><strong>구두점이나 특수 문자가 필요한 경우</strong>: Ph.D, AT&T, $45.55, 01/02/06 등</li><li><strong>줄임말과 단어 내에 띄어쓰기가 있는 경우</strong>: what&rsquo;re/what are, New York, rock &rsquo;n&rsquo; roll 등</li><li>문장 토큰화에서 단순히 마침표를 기준으로 문장을 잘라내는 것은
192.168.56.31, gmail.com과 같은 경우를 고려했을 때 올바르지 않음</li></ul><h2 id=한국어-토큰화>한국어 토큰화<a hidden class=anchor aria-hidden=true href=#한국어-토큰화>#</a></h2><ul><li>한국어의 경우 띄어쓰기가 가능한 단위가 어절인데,<br>&lsquo;그가&rsquo;, &lsquo;그에게&rsquo;, &lsquo;그를&rsquo;과 같이 어절이 독립적인 단어로 구성되는 것이 아니라<br>조사 등의 무언가가 붙어있는 경우가 많기 때문에 이를 전부 형태소 단위로 분리해줘야 함</li><li><strong>자립 형태소</strong>: 접사, 어미, 조사와 상관업싱 자립하여 사용할 수 있는 형태소, [체언, 수식언, 감탄사] 등</li><li><strong>의존 형태소</strong>: 다른 형태소와 결합하여 사용되는 형태소, [접사, 어미, 조사, 어간]</li><li>한국어의 경우 <strong>띄어쓰기</strong>가 지켜지지 않아도 글을 쉽게 이해할 수 있어 띄어쓰기가 잘 지켜지지 않음</li><li><strong>품사 태깅</strong>: 단어의 의미를 제대로 파악하기 위해서는 해당 단어가 어떤 품사로 쓰였는지 구분할 필요</li></ul><h2 id=nltk-konlpy>NLTK, KoNLPy<a hidden class=anchor aria-hidden=true href=#nltk-konlpy>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>nltk.tokenize</span> <span class=kn>import</span> <span class=n>word_tokenize</span> <span class=c1># 단어 토큰화</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>nltk.tag</span> <span class=kn>import</span> <span class=n>pos_tag</span> <span class=c1># 품사 태깅</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>konlpy.tag</span> <span class=kn>import</span> <span class=n>Okt</span>
</span></span><span class=line><span class=cl><span class=n>okt</span> <span class=o>=</span> <span class=n>Okt</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>okt</span><span class=o>.</span><span class=n>porphs</span><span class=p>(</span><span class=n>sentence</span><span class=p>)</span> <span class=c1># 형태소 추출</span>
</span></span><span class=line><span class=cl><span class=n>okt</span><span class=o>.</span><span class=n>pos</span><span class=p>(</span><span class=n>sentence</span><span class=p>)</span> <span class=c1># 품사 태깅</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=02-02-cleaning-and-normalization>02-02. Cleaning and Normalization<a hidden class=anchor aria-hidden=true href=#02-02-cleaning-and-normalization>#</a></h1><ul><li><strong>Cleaning(정제)</strong>: 갖고 있는 corpus로부터 노이즈 데이터를 제거</li><li><strong>Normalization(정규화)</strong>: 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만듦</li><li>영어권 언어에서 단어의 개수를 줄이는 정규화 방법으로 <strong>대,소문자 통합</strong>을 활용</li><li><strong>노이즈 데이터</strong>: 아무 의미 없는 특수 문자 등, 분석하고자 하는 목적에 맞지 않는 불필요한 단어들</li><li>불필요한 단어를 제거하기 위해 <strong>불용어</strong>, <strong>등장 빈도가 적은 단어</strong>, <strong>길이가 짧은 단어</strong> 등을 제거</li><li>노이즈 데이터의 특징을 잡아낼 수 있다면, <strong>정규표현식</strong>을 사용해서 제거</li></ul><h1 id=02-03-stemming-and-lemmatization>02-03. Stemming and Lemmatization<a hidden class=anchor aria-hidden=true href=#02-03-stemming-and-lemmatization>#</a></h1><h2 id=lemmatization>Lemmatization<a hidden class=anchor aria-hidden=true href=#lemmatization>#</a></h2><ul><li><strong>Lemma(표제어)</strong>: 기본 사전형 단어, [am, are, is]의 뿌리 단어 be 등</li><li><strong>Stem(어간)</strong>: 단어의 의미를 담고 있는 단어의 핵심 부분, &lsquo;cats&rsquo;에서 &lsquo;cat&rsquo;</li><li><strong>Affix(접사)</strong>: 단어에 추가적인 의미를 주는 부분, &lsquo;cats&rsquo;에서 &rsquo;s&rsquo;</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>nltk.stem</span> <span class=kn>import</span> <span class=n>WordNetLemmatizer</span>
</span></span><span class=line><span class=cl><span class=n>lemmatizer</span> <span class=o>=</span> <span class=n>WordNetLemmatizer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>lemmatizer</span><span class=o>.</span><span class=n>lemmatize</span><span class=p>(</span><span class=n>word</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># am -&gt; be, having -&gt; have</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=stemming>Stemming<a hidden class=anchor aria-hidden=true href=#stemming>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>nltk.stem</span> <span class=kn>import</span> <span class=n>PorterStemmer</span>
</span></span><span class=line><span class=cl><span class=n>stemmer</span> <span class=o>=</span> <span class=n>PorterStemmer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>stemmer</span><span class=o>.</span><span class=n>stem</span><span class=p>(</span><span class=n>word</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># am -&gt; am, having -&gt; hav</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=한국어에서의-어간-추출>한국어에서의 어간 추출<a hidden class=anchor aria-hidden=true href=#한국어에서의-어간-추출>#</a></h2><ul><li>5언 9품사의 구조에서 용언에 해당되는 동사와 형용사는 어간과 어미의 결합으로 구성</li><li><strong>활용</strong>: 용언의 어간이 어미를 가지는 일</li><li><strong>규칙 활용</strong>: 어간이 어미를 취할 때 어간의 모습이 일정, <code>잡/어간 + 다/어미</code></li><li><strong>불규칙 활용</strong>: 어간이 어미를 취할 때 어간의 모습이 바뀌거나 특수한 어미일 경우, &lsquo;오르+아/어->올라&rsquo; 등</li></ul><h1 id=02-04-stopword>02-04. Stopword<a hidden class=anchor aria-hidden=true href=#02-04-stopword>#</a></h1><ul><li><strong>Stopword(불용어)</strong>: 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 기여하지 않는 단어, [조사, 접미사] 등</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>nltk.corpus</span> <span class=kn>import</span> <span class=n>stopwords</span>
</span></span><span class=line><span class=cl><span class=n>stop_words_list</span> <span class=o>=</span> <span class=n>stopwords</span><span class=o>.</span><span class=n>words</span><span class=p>(</span><span class=s1>&#39;english&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>okt</span> <span class=o>=</span> <span class=n>Okt</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>okt</span><span class=o>.</span><span class=n>morphs</span><span class=p>(</span><span class=n>sentence</span><span class=p>)</span> <span class=c1># 조사, 접속사 등 제거</span>
</span></span><span class=line><span class=cl><span class=c1># 또는 불용어 사전을 만들어서 제거</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=02-05-regular-expression>02-05. Regular Expression<a hidden class=anchor aria-hidden=true href=#02-05-regular-expression>#</a></h1><ul><li>정규 표현식 <a href=https://wikidocs.net/21703 target=_blank rel=noopener>참고</a></li></ul><h1 id=02-06-integer-encoding>02-06. Integer Encoding<a hidden class=anchor aria-hidden=true href=#02-06-integer-encoding>#</a></h1><ul><li>컴퓨터는 텍스트보다 숫자를 더 잘 처리할 수 있기 때문에 텍스트를 숫자로 변경</li><li>단어를 빈도수 순으로 정렬하고 순서대로 낮은 숫자부터 정수를 부여</li><li>dictionary, Counter, nltk.FreqDist, keras.Tokenizer 등 활용</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>nltk</span> <span class=kn>import</span> <span class=n>FreqDist</span>
</span></span><span class=line><span class=cl><span class=n>FreqDist</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>hstack</span><span class=p>(</span><span class=n>preprocessed_sentences</span><span class=p>))</span> <span class=c1># np.hastack으로 문장 구분을 제거</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.preprocessing.text</span> <span class=kn>import</span> <span class=n>Tokenizer</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>Tokenizer</span><span class=p>()</span> <span class=c1># num_words 파라미터로 사용할 단어 개수 지정</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>fit_on_texts</span><span class=p>(</span><span class=n>preprocessed_sentences</span><span class=p>)</span> <span class=c1># 빈도수 기분으로 단어 집합 생성</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>word_intex</span><span class=p>)</span> <span class=c1># 정수 인덱스 확인</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>word_counts</span><span class=p>)</span> <span class=c1># 단어 빈도수 확인</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>texts_to_sequences</span><span class=p>(</span><span class=n>preprocessed_sentences</span><span class=p>))</span> <span class=c1># corpus를 인덱스로 변환</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=02-07-padding>02-07. Padding<a hidden class=anchor aria-hidden=true href=#02-07-padding>#</a></h1><ul><li>병렬 연산을 위해 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.preprocessing.sequence</span> <span class=kn>import</span> <span class=n>pad_sequences</span>
</span></span><span class=line><span class=cl><span class=n>encoded</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>texts_to_sequences</span><span class=p>(</span><span class=n>preprocessed_sentences</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>padded</span> <span class=o>=</span> <span class=n>pad_seqences</span><span class=p>(</span><span class=n>encoded</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># padding=&#39;post&#39;를 입력해야 뒤에서 부터 0을 채움</span>
</span></span><span class=line><span class=cl><span class=c1># maxlen으로 문장 길이 조절</span>
</span></span><span class=line><span class=cl><span class=c1># truncating=&#39;post&#39;를 통해 문장 길이 초과 시 뒤의 단어가 삭제되도록 설정</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=02-08-one-hot-encoding>02-08. One-Hot Encoding<a hidden class=anchor aria-hidden=true href=#02-08-one-hot-encoding>#</a></h1><ul><li><strong>Vocabulary(단어 집합)</strong>: 서로 다른 단어들의 집합, book과 books과 같은 변형 형태도 다른 단어로 간주</li><li><strong>One-Hot Encoding</strong>: 단어 집합의 크기를 벡터의 차원으로 하고,<br>표현하고 싶은 단어에 1, 다른 인텍스에 0을 부여하는 단어의 벡터 표현 방식</li><li>단어의 개수가 늘어날 수록 벡터를 저장하기 위해 필요한 공간이 계속 늘어나는 단점</li><li>단어의 유사도를 표현하지 못하는 단점 (강아지, 개, 냉장고 등)</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.utils</span> <span class=kn>import</span> <span class=n>to_categorical</span>
</span></span><span class=line><span class=cl><span class=n>encoded</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>texts_to_sequences</span><span class=p>(</span><span class=n>preprocessed_sentences</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>one_hot</span> <span class=o>=</span> <span class=n>to_categorical</span><span class=p>(</span><span class=n>encoded</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><hr><h1 id=03-01-language-model>03-01. Language Model<a hidden class=anchor aria-hidden=true href=#03-01-language-model>#</a></h1><ul><li>단어 시퀀스(문장)에 확률을 할당하는 모델, 이전 단어들이 주어졌을 때 다음 단어를 예측</li><li>단어 시퀀스 W의 확률 $P(W)=P(w_1,w_2,w_3,w_4,w_5,&mldr;,w_n)$</li><li>다음 단어 등장 확률 $P(w_n|w_1,&mldr;,w_{n-1})$</li></ul><h1 id=03-02-statistical-language-model>03-02. Statistical Language Model<a hidden class=anchor aria-hidden=true href=#03-02-statistical-language-model>#</a></h1><h2 id=조건부-확률>조건부 확률<a hidden class=anchor aria-hidden=true href=#조건부-확률>#</a></h2><table><thead><tr><th style=text-align:center></th><th style=text-align:center>남학생(A)</th><th style=text-align:center>여학생(B)</th><th style=text-align:center>계</th></tr></thead><tbody><tr><td style=text-align:center>중학생(C)</td><td style=text-align:center>100</td><td style=text-align:center>60</td><td style=text-align:center>160</td></tr><tr><td style=text-align:center>고등학생(D)</td><td style=text-align:center>80</td><td style=text-align:center>120</td><td style=text-align:center>200</td></tr><tr><td style=text-align:center>계</td><td style=text-align:center>180</td><td style=text-align:center>180</td><td style=text-align:center>360</td></tr></tbody></table><ul><li>학생을 뽑았을 때, 고등학생이면서 남학생일 확률 $P(A \bigcap B)=80/360$</li><li>고등학생 중 한명을 뽑았을 때, 남학생일 확률 $P(A|D)=P(A \bigcap D)/P(D)=(80/360)/(200/360)$</li></ul><h2 id=문장에-대한-확률>문장에 대한 확률<a hidden class=anchor aria-hidden=true href=#문장에-대한-확률>#</a></h2><ul><li>&lsquo;An adorable little boy is spreading smiles&rsquo;의 확률<br>$P(\text{An adorable little boy is spreading smiles})=\
P(\text{An}) \times P(\text{adorable}|\text{An}) \times &mldr; \times P(\text{smiles}|\text{An adorable little boy is spreading})$</li></ul><h2 id=카운트-기반-접근>카운트 기반 접근<a hidden class=anchor aria-hidden=true href=#카운트-기반-접근>#</a></h2><ul><li>An adorable little boy가 100번 등장했을 때 그 다음에 is가 등장한 경우가 30번이라면,<br>$P(\text{is}|\text{An adorable little boy})$는 30%</li><li>카운트 기반으로 훈련할 경우 단어 시퀀스가 없어 확률이 0이 되는 경우를 방지하기 위해 방대한 양의 훈련 데이터가 필요</li><li><strong>회소 문제</strong>: 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제</li></ul><p>$$P(\text{is}|\text{An adorable little boy})= \frac{count(\text{An adorable little boy is})}{count(\text{An adorable little boy})}$$</p><h1 id=03-03-n-gram-language-model>03-03. N-gram Language Model<a hidden class=anchor aria-hidden=true href=#03-03-n-gram-language-model>#</a></h1><ul><li>통계적 언어 모델의 일종이지만, 모든 단어가 아닌 일부 단어만 고려하는 접근 방법 사용</li><li>An adorable little boy에서 is가 나올 확률을 boy가 나왔을 때 is가 나올 확률로 대체<br>$P(\text{is}|\text{An adorable little boy}) \approx P(\text{is}|\text{boy})$</li><li>뒤의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 발생</li><li>전체 문장을 고려한 언어 모델보다는 정확도가 떨어짐</li><li>몇 개의 단어를 볼지 n을 정하는 것은 trade-off 문제를 발생시킴, n은 최대 5를 넘게 잡아서는 안된다고 권장</li></ul><h2 id=n-gram>N-gram<a hidden class=anchor aria-hidden=true href=#n-gram>#</a></h2><ul><li>n개의 연속적인 단어 나열</li><li>An adorable little boy에 대해<br>unigrams: an, adorable, little, boy<br>bigrams: an adorable, adorable little, little boy</li></ul><h1 id=03-05-perplexity>03-05. Perplexity<a hidden class=anchor aria-hidden=true href=#03-05-perplexity>#</a></h1><ul><li><strong>Perplexity(PPL)</strong>: 헷갈리는 정도, 낮을수록 언어 모델의 성능이 좋음</li></ul><p>$$PPL(W)=P(w_1,w_2,w_3,&mldr;,w_N)^{-\frac{1}{N}}=\sqrt[N]{\frac{1}{P(w_1,w_2,w_3,&mldr;,w_N)}}$$</p><h2 id=branching-factor>Branching Factor<a hidden class=anchor aria-hidden=true href=#branching-factor>#</a></h2><ul><li><strong>Branching factor(분기계수)</strong>: PPL이 선택할 수 있는 가능한 경우의 수</li><li>대해 PPL이 10이 나왔을 때, 언어 모델은 테스트 데이터에 대해 다음 단어를 예측할 때 평균 10개의 단어를 고려</li><li>PPL의 값이 낮다는 것은 테스트 데이터 상에서 높은 정확도를 보이는 것일뿐, 반드시 사람이 직접 느끼기에 좋은 모델인 것은 아님</li></ul><p>$$PPL(W)=P(w_1,w_2,w_3,&mldr;,w_N)^{-\frac{1}{N}}=(\frac{1}{10}^N)^{-\frac{1}{N}}=\frac{1}{10}^{-1}=10$$</p><hr><h1 id=04-01-단어의-표현-방법>04-01. 단어의 표현 방법<a hidden class=anchor aria-hidden=true href=#04-01-단어의-표현-방법>#</a></h1><ul><li><strong>국소 표현(이산 표현)</strong>: 해당 단어 그 자체만 보고, 특정값을 맵핑하여 단어를 표현하는 방법</li><li><strong>분산 표현(연속 표현)</strong>: 그 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법</li></ul><h1 id=04-02-bag-of-wordsbow>04-02. Bag of Words(BoW)<a hidden class=anchor aria-hidden=true href=#04-02-bag-of-wordsbow>#</a></h1><ul><li>단어들의 순서는 고려하지 않고, 단어들의 출현 빈도에만 집중하는 텍스트 데이터 수치화 표현 방법</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>doc1</span> <span class=o>=</span> <span class=s1>&#39;정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.&#39;</span>
</span></span><span class=line><span class=cl><span class=n>vocabulary</span> <span class=p>:</span> <span class=p>{</span><span class=s1>&#39;정부&#39;</span><span class=p>:</span> <span class=mi>0</span><span class=p>,</span> <span class=s1>&#39;가&#39;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s1>&#39;발표&#39;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span> <span class=s1>&#39;하는&#39;</span><span class=p>:</span> <span class=mi>3</span><span class=p>,</span> <span class=s1>&#39;물가상승률&#39;</span><span class=p>:</span> <span class=mi>4</span><span class=p>,</span> <span class=s1>&#39;과&#39;</span><span class=p>:</span> <span class=mi>5</span><span class=p>,</span> <span class=s1>&#39;소비자&#39;</span><span class=p>:</span> <span class=mi>6</span><span class=p>,</span> <span class=s1>&#39;느끼는&#39;</span><span class=p>:</span> <span class=mi>7</span><span class=p>,</span> <span class=s1>&#39;은&#39;</span><span class=p>:</span> <span class=mi>8</span><span class=p>,</span> <span class=s1>&#39;다르다&#39;</span><span class=p>:</span> <span class=mi>9</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>bag</span> <span class=n>of</span> <span class=n>words</span> <span class=n>vector</span> <span class=p>:</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.feature_extraction.text</span> <span class=kn>import</span> <span class=n>CounteVectorizer</span>
</span></span><span class=line><span class=cl><span class=n>vector</span> <span class=o>=</span> <span class=n>CounterVectorizer</span><span class=p>()</span> <span class=c1># stop_words 파라미터로 불용어 제거(&#39;english&#39; 또는 리스트 등)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;bag of words vector:&#39;</span><span class=p>,</span> <span class=n>vector</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>corpus</span><span class=p>)</span><span class=o>.</span><span class=n>toarray</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;vocabulary:&#39;</span><span class=p>,</span> <span class=n>vector</span><span class=o>.</span><span class=n>vocabulary_</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=04-03-document-term-matrixdtm>04-03. Document-Term Matrix(DTM)<a hidden class=anchor aria-hidden=true href=#04-03-document-term-matrixdtm>#</a></h1><ul><li>다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것</li><li>One-hot vector와 마찬가지로 대부분의 값이 0인 희소 표현의 문제 발생</li><li>불용어와 중요한 단어에 대해서 가중치를 주기 위해 TF-IDF를 사용</li></ul><h1 id=04-04-tf-idf>04-04. TF-IDF<a hidden class=anchor aria-hidden=true href=#04-04-tf-idf>#</a></h1><ul><li>단어의 빈도와 역 문서 빈도를 사용하여 DTM 내의 각 단어들마다 중요도를 가중치로 부여하는 방법</li><li>$tf(d,t)$: 특정 문서 $d$에서의 특정 단어 $t$의 등장 횟수, DTM에서의 각 단어들의 가진 값</li><li>$df(t)$: 특정 단어 $t$가 등장한 문서의 수, 특정 단어가 각 문서에서 등장한 횟수는 무시</li><li>$idf(d,t)$: $df(t)$에 반비례하는 수,<br>총 문서의 수 n이 커질수록 기하급수적으로 증가하는 것을 방지하기 위해 $log$(일반적으로 자연 로그) 적용</li></ul><p>$$idf(d,t)=log(\frac{n}{1+df(t)})$$</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.feature_extraction.text</span> <span class=kn>import</span> <span class=n>TfidfVectorizer</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>vector</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>corpus</span><span class=p>)</span><span class=o>.</span><span class=n>toarray</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>ve</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><hr><h1 id=05-vector-similarity>05. Vector Similarity<a hidden class=anchor aria-hidden=true href=#05-vector-similarity>#</a></h1><h2 id=cosine-similarity>Cosine Similarity<a hidden class=anchor aria-hidden=true href=#cosine-similarity>#</a></h2><ul><li>두 벡터 간의 코사인 각도를 이용하여 구할 수 있는 두 벡터의 유사도</li><li>두 벡터의 방향이 동일하면 1의 값을 가지며, 값이 1에 가까울수록 유사도가 높음</li><li>문서의 길이가 다른 상황에서 비교적 공정한 비교를 할 수 있음</li></ul><h2 id=euclidean-distance>Euclidean Distance<a hidden class=anchor aria-hidden=true href=#euclidean-distance>#</a></h2><ul><li>다차원 공간에서 두 개의 점 $p$와 $q$가 각각 $p=(p_1,p_2,p_3,&mldr;,p_n)$과 $q=(q_1,q_2,q_3,&mldr;,q_n)$의 좌표를 가질 때<br>두 점 사이의 거리를 계산하는 유클리드 거리 공식</li></ul><p>$$\sqrt{(q_1-p_1)^2+(q_2-p_2)^2+&mldr;+(q_n-p_n)^2}=\sqrt{\Sigma^n_{i=1}(q_i-p_i)^2}$$</p><h2 id=jaccard-similarity>Jaccard Similarity<a hidden class=anchor aria-hidden=true href=#jaccard-similarity>#</a></h2><ul><li>합집합에서 교집합의 비율을 구한다면 두 집합 A와 B의 유사도를 구할 수 있음</li><li>자카드 유사도 J는 0과 1사이의 값을 가지며, 두 집합이 동일하면 1, 공통 원소가 없으면 0의 값을 가짐</li></ul><p>$$J(A,B)=\frac{|A \bigcap B|}{|A \bigcup B|}=\frac{|A \bigcap B|}{|A|+|B|-|A \bigcap B|}$$</p><hr><h1 id=06-machine-learning>06. Machine Learning<a hidden class=anchor aria-hidden=true href=#06-machine-learning>#</a></h1><h2 id=classification-and-regression>Classification and Regression<a hidden class=anchor aria-hidden=true href=#classification-and-regression>#</a></h2><ul><li><strong>Bianry Classification</strong>: 두 개의 선택지 중 하나의 답을 선택</li><li><strong>Multi-class Classification</strong>: 세 개 이상의 선택지 중에서 답을 선택</li><li><strong>Regression</strong>: 연속적인 값의 범위 내에서 예측값을 도출</li></ul><h2 id=learning>Learning<a hidden class=anchor aria-hidden=true href=#learning>#</a></h2><ul><li><strong>Supervised Learning</strong>: 정답 레이블과 함께 함습</li><li><strong>Unsupervised Learning</strong>: 데이터에 별도의 레이블이 없이 학습</li><li><strong>Self-Supervised Learning</strong>: 레이블이 없는 데이터가 주어지면, 모델이 학습을 위해 스스로 레이블을 생성</li></ul><h2 id=confusion-matrix>Confusion Matrix<a hidden class=anchor aria-hidden=true href=#confusion-matrix>#</a></h2><table><thead><tr><th style=text-align:center></th><th style=text-align:center>예측 참</th><th style=text-align:center>예측 거짓</th></tr></thead><tbody><tr><td style=text-align:center>실제 참</td><td style=text-align:center>TP(정답)</td><td style=text-align:center>FN(오답)</td></tr><tr><td style=text-align:center>실제 거짓</td><td style=text-align:center>FP(오답)</td><td style=text-align:center>TN(정답)</td></tr></tbody></table><ul><li><strong>Precision(정밀도)</strong>: True라고 분류한 것 중 실제 True의 비율, $Precision=\frac{TP}{TP+FP}$</li><li><strong>Recall(재현율)</strong>: 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율, $Recall=\frac{TP}{TP+FN}$</li><li><strong>Accuracy(정확도)</strong>: 전체 예측한 데이터 중 정답을 맞춘 것에 대한 비율, $Accuracy=\frac{TP+TN}{TP+FN+FP+TN}$</li></ul><h2 id=overfitting-and-underfitting>Overfitting and Underfitting<a hidden class=anchor aria-hidden=true href=#overfitting-and-underfitting>#</a></h2><ul><li><strong>Overfitting</strong>: 훈련 데이터를 과하게 학습, 훈련 데이터에 비해 테스트 데이터의 오차가 커짐</li><li><strong>Underfitting</strong>: 테스트 데이터의 성능이 올라갈 여지가 있음에도 훈련을 덜 한 상태, 훈련 데이터에서도 정확도가 낮음</li></ul><hr><h1 id=07-deep-learning>07. Deep Learning<a hidden class=anchor aria-hidden=true href=#07-deep-learning>#</a></h1><h2 id=perceptron>Perceptron<a hidden class=anchor aria-hidden=true href=#perceptron>#</a></h2><ul><li>입력값 $x$, 가중치 $w$, 출력값 $y$</li><li>가중치의 값이 크면 클수록 해당 입력 값이 중요하다는 것을 의미</li><li><strong>단층 퍼셉트론</strong>: 값을 보내는 input layer와 값을 받아서 출력하는 output layer로 구성</li><li><strong>다층 퍼셉트론(MLP)</strong>: 입력층과 출력층 사이에 hidden layer를 추가</li></ul><h2 id=ffnn>FFNN<a hidden class=anchor aria-hidden=true href=#ffnn>#</a></h2><ul><li><strong>FFNN(피드 포워드 신경망)</strong>: 오직 입력층에서 출력층 방향으로 연산이 전개되는 신경망</li><li><strong>RNN(순환 신경망)</strong>: 은닉층의 출력값이 다시 은닉층으로 입력되는 신경망</li></ul><h2 id=activision-function>Activision Function<a hidden class=anchor aria-hidden=true href=#activision-function>#</a></h2><ul><li>은닉층과 출력층의 뉴런에서 출력값을 결정하는 함수</li><li>Step function, Sigmoid function, ReLU 등 비선형 함수의 특성</li></ul><h2 id=loss-function>Loss Function<a hidden class=anchor aria-hidden=true href=#loss-function>#</a></h2><ul><li><strong>MSE</strong>: 연속형 변수 예측</li><li><strong>Binary Cross-Entropy</strong>: 시그모이드 함수 출력</li><li><strong>Categorical Cross-Entropy</strong>: 소프트맥스 함수 출력</li></ul><h2 id=optimizer>Optimizer<a hidden class=anchor aria-hidden=true href=#optimizer>#</a></h2><ul><li><strong>Momentum</strong>: 경사 하강법에 모멘텀을 더해 Local Minimum에 빠지더라도 빠져나갈 수 있게 함</li><li><strong>Adagrad</strong>: 각 매개변수에 서로 다른 학습률을 적용</li><li><strong>RMSprop</strong>: Adagrad가 학습을 진행할수록 학습률이 지나치게 떨어지는 단점을 개선</li><li><strong>Adam</strong>: RMSprop과 Momentum을 합친 듯한 방법, 방향과 학습률 두 가지를 모두 잡기 위한 방법</li></ul><h2 id=overfitting-방지>Overfitting 방지<a hidden class=anchor aria-hidden=true href=#overfitting-방지>#</a></h2><ol><li><strong>데이터의 양을 늘리기</strong><br>데이터의 양이 적으면 데이터의 특정 패턴이나 노이즈까지 쉽게 암기해버림, Data Augmentation 활용</li><li><strong>모델의 복잡도 줄이기</strong><br>인공 신경망의 복잡도는 은닉층의 수나 매개변수의 수 등으로 결정</li><li><strong>가충치 규제 적용하기</strong><br>L1 규제(가중치의 절댓값 합계를 비용 함수에 추가), L2 규제(모든 가중치들의 제곱합을 비용 함수에 추가)</li><li><strong>Dropout</strong><br>학습 시에 인공 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지</li></ol><h2 id=기울기-소실>기울기 소실<a hidden class=anchor aria-hidden=true href=#기울기-소실>#</a></h2><ul><li><strong>기울기 소실</strong>: 역전파 과정에서 입력층으로 갈 수록 기울기가 점차적으로 작아지는 현상</li><li><strong>기울기 폭주</strong>: 기울기가 점차 커지다가 가중치들이 비정상적으로 큰 값이 되면서 발산되는 경우</li><li><strong>Gradient Clipping</strong>: 기울기 폭주를 막기 위해 임계값을 넘지 않도록 값의 크기를 감소</li></ul><h2 id=weight-initialization>Weight Initialization<a hidden class=anchor aria-hidden=true href=#weight-initialization>#</a></h2><ul><li><strong>Xavier Initialization</strong>: 균등 분포 또는 정규 분포로 초기화 할 때 두 가지 경우로 나뉨</li><li><strong>He Initialization</strong>: Xavier 초기화와 다르게 다음 층의 뉴런의 수를 반영하지 않음</li></ul><h2 id=batch-normalization>Batch Normalization<a hidden class=anchor aria-hidden=true href=#batch-normalization>#</a></h2><ul><li><strong>내부 공변량 변화</strong>: 학습 과정에서 층 별로 입력 데이터 분포가 달라지는 현상</li><li><strong>배치 정규화</strong>: 한 번에 들어오는 배치 단위로 정규화하는 것</li><li>배치 정규화는 추가 계산을 발생시켜 모델을 복잡하게 하기 때문에 예측 시 실행 시간이 느려지는 단점</li><li>너무 작은 배치 크기에서는 잘 동작하지 않을 수 있기 때문에 미니 배치 크기에 의존적임</li><li>RNN은 각 시점마다 다른 통계치를 가지기 때문에 RNN에 적용하기 어려움</li></ul><h2 id=keras-api>Keras API<a hidden class=anchor aria-hidden=true href=#keras-api>#</a></h2><ul><li><strong>Sequential API</strong>: 단순하게 층을 쌓는 방식, 다수의 입출력 및 층 간 연산을 구현하기 어려움</li><li><strong>Functional API</strong>: 입력의 크기(shape)를 명시한 입력층을 모델의 앞단에 정의</li><li><strong>Subclassing API</strong>: Functional API로도 구현할 수 없는 모델들도 구현 가능</li></ul><h2 id=texts_to_matrix><code>texts_to_matrix()</code><a hidden class=anchor aria-hidden=true href=#texts_to_matrix>#</a></h2><ul><li><code>tokenizer.texts_to_matrix(texts, mode='count')</code>: DTM 생성</li><li><code>tokenizer.texts_to_matrix(texts, mode='binary')</code>: DTM과 유사하지만 단어의 개수는 무시</li><li><code>tokenizer.texts_to_matrix(texts, mode='tfidf')</code>: TF-IDF 행렬 생성</li><li><code>tokenizer.texts_to_matrix(texts, mode='freq')</code>:<br>각 문서에서의 단어 등장 횟수를 분자로, 문서의 크기를 분모로 하는 표현하는 방법</li></ul><h2 id=nnlm>NNLM<a hidden class=anchor aria-hidden=true href=#nnlm>#</a></h2><ul><li>피드 포워드 신경망 언어 모델, 신경망 언어 모델의 시초로, RNNLM, BiLM 등으로 발전</li><li>기존 N-gram 언어 모델은 충분한 데이터를 관측하지 못하면 언어를 정확히 모델링하지 못하는 희소 문제를 가짐</li><li>NNLM은 N-gram 언어 모델처럼 정해진 개수(window size)의 단어만을 참고</li><li>NNLM은 N개의 input layer와 projection layer, hidden layer, output layer로 구성</li><li>Projection layer의 크기가 M일 때, 각 입력 단어들은 V x M 크기의 가중치 행렬과 곱해짐</li><li>충분한 양의 훈련 코퍼스를 학습한다면 단어 간 유사도를 구할 수 있는 임베딩 벡터값을 얻을 수 있음</li></ul><hr><h1 id=08-01-rnn>08-01. RNN<a hidden class=anchor aria-hidden=true href=#08-01-rnn>#</a></h1><ul><li>입력과 출력을 시퀀스 단위로 처리하는 시퀀스 모델</li><li>은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 다시 은닉층 노드의 다음 계산 입력으로 보내는 특징</li><li><strong>셀(메모리 셀, RNN 셀)</strong>: 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드, 이전의 값을 기억하는 역할</li><li><strong>Hidden state</strong>: 현재 시점을 t라 할 때, 메모리 셀이 다음 시점인 t+1의 자신에게 보내는 값</li><li>입력과 출력의 길이를 다르게 설계할 수 있어 다양한 용도로 사용 가능</li><li><strong>one-to-many</strong>: 하나의 이미지 입력에 대해서 사진의 제목인 시퀀스를 출력하는 이미지 캡셔닝 작업에 사용</li><li><strong>many-to-one</strong>: 단어 시퀀스에 대해서 하나의 출력을 하는 감성 분류, 스팸 메일 분류 등에 사용</li><li><strong>many-to-many</strong>: 사용자가 문장을 입력하면 대답 문장을 출력하는 챗봇이나 번역기에 사용</li></ul><h2 id=rnn-parameter>RNN Parameter<a hidden class=anchor aria-hidden=true href=#rnn-parameter>#</a></h2><ul><li>현재 시점 $t$에서의 hidden state가 $h_t$라 할 때, 두 개의 가중치 $W_x$, $W_h$가 필요</li><li>$W_x$는 입력층을 위한 가중치, $W_h$는 $t-1$의 hidden state인 $h_{t-1}$을 위한 가중치</li><li>은닉층 $h_t=tanh({W_x}{x_t}+{W_h}{h_{t-1}}+b)$, 출력층 $y_t=f({W_y}{h_t}+b)$</li><li>출력층의 활성화 함수 $f$는 이진 분류에서 시그모이드 함수, 다중 클래스 분류에서 소프트맥스 함수 등 사용</li><li>RNN의 입력 $x_t$는 단어 벤터로 간주, 단어 벡터의 차원을 $d$, hidden state의 크기를 $D_h$라 할 때,<br>메모리 셀 $h_t$ = $tanh({W_h}\times{h_{t-1}}\times{W_x}\times{x_t}+{b})$</li></ul><table><thead><tr><th style=text-align:center>$x_t$</th><th style=text-align:center>$W_x$</th><th style=text-align:center>$W_h$</th><th style=text-align:center>$h_{t-1}$</th><th style=text-align:center>$b$</th></tr></thead><tbody><tr><td style=text-align:center>$({d}\times{1})$</td><td style=text-align:center>$({D_h}\times{d})$</td><td style=text-align:center>$({D_h}\times{D_h})$</td><td style=text-align:center>$({D_h}\times{1})$</td><td style=text-align:center>$({D_h}\times{1})$</td></tr></tbody></table><h2 id=keras-rnn>Keras RNN<a hidden class=anchor aria-hidden=true href=#keras-rnn>#</a></h2><ul><li>hidden_units: hidden state의 크기 (output_dim)</li><li>timesteps: 입력 시퀀스(문장)의 길이 (input_length)</li><li>input_dim: 입력의 크기, 단어 벡터의 차원</li><li>RNN 층은 (batch_size, timesteps, input_dim) 크기의 3D 텐서를 입력으로 받음</li><li>메모리 셀의 최종 시점의 hidden state만 리턴할 경우 (batch_size, output_dim) 크기의 2D 텐서 반환</li><li>메모리 셀의 각 시점(time step)의 hidden state 값들을 모아 전체 시퀀스를 리턴할 경우 3D 텐서를 반환</li><li><code>return_sequences=True</code> 옵션으로 반환값 설정</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.layers</span> <span class=kn>import</span> <span class=n>SimpleRNN</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>SimpleRNN</span><span class=p>(</span><span class=n>hidden_units</span><span class=p>,</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=n>timesteps</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>)))</span>
</span></span></code></pre></td></tr></table></div></div><blockquote><p>메모리 셀에서 hidden state 계산은 다음과 같은 코드로 동작</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>hidden_state_t</span> <span class=o>=</span> <span class=mi>0</span> <span class=c1># 초기 은닉 상태를 0(벡터)로 초기화</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>input_t</span> <span class=ow>in</span> <span class=n>input_length</span><span class=p>:</span> <span class=c1># 각 시점마다 입력을 받는다.</span>
</span></span><span class=line><span class=cl>    <span class=n>output_t</span> <span class=o>=</span> <span class=n>tanh</span><span class=p>(</span><span class=n>input_t</span><span class=p>,</span> <span class=n>hidden_state_t</span><span class=p>)</span> <span class=c1># 각 시점에 대해서 입력과 은닉 상태를 가지고 연산</span>
</span></span><span class=line><span class=cl>    <span class=n>hidden_state_t</span> <span class=o>=</span> <span class=n>output_t</span> <span class=c1># 계산 결과는 현재 시점의 은닉 상태가 된다.</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=deep-rnn>Deep RNN<a hidden class=anchor aria-hidden=true href=#deep-rnn>#</a></h2><ul><li>순환 신경망에서 은닉층이 1개 더 추가되어 은닉층이 2개인 깊은 구조</li><li>첫번째 은닉층은 다음 은닉층이 존재하기 때문에 <code>return_sequences=True</code>를 설정하여 모든 시점을 전달</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.layers</span> <span class=kn>import</span> <span class=n>SimpleRNN</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>SimpleRNN</span><span class=p>(</span><span class=n>hidden_units</span><span class=p>,</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=n>timesteps</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>),</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>SimpleRNN</span><span class=p>(</span><span class=n>hidden_units</span><span class=p>,</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=bidirectional-rnn>Bidirectional RNN<a hidden class=anchor aria-hidden=true href=#bidirectional-rnn>#</a></h2><ul><li>t에서의 출력값을 예측할 때 이전 시점의 입력 뿐 아니라, 이후 시점의 입력 또한 예측</li><li>빈칸 채우기 등의 문제에서 미래 시점의 입력에 힌트가 있기 때문에 이전과 이후의 시점을 모두 고려</li><li>하나의 출력값을 예측하기 위해 두 개의 메모리 셀을 사용</li><li>첫 번째 메모리 셀은 앞 시점의 hidden state를 전달받아 현재의 hidden state를 계산 (forward)</li><li>두 번째 메모리 셀은 뒤 시점의 hidden state를 전달받아 현재의 hidden state를 계산 (backward)</li><li>은닉층을 추가하면 학습할 수 있는 양이 많아지지만, 훈련 데이터 또한 많은 양이 필요</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.layers</span> <span class=kn>import</span> <span class=n>Bidirectional</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Bidirectional</span><span class=p>(</span><span class=n>SimpleRNN</span><span class=p>(</span><span class=n>hidden_units</span><span class=p>,</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=n>timesteps</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>)))</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=08-02-lstm>08-02. LSTM<a hidden class=anchor aria-hidden=true href=#08-02-lstm>#</a></h1><ul><li>Valina RNN(Simple RNN)은 출력 결과가 이전의 계산 결과에 의존하여 비교적 짧은 시퀀스에서면 효과를 봄</li><li><strong>장기 의존성 문제</strong>: time step이 길어질수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상</li><li>LSTM(장단기 메모리)은 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여<br>불필요한 기억을 지우고, 기억해야할 것들을 결정</li><li>전통적인 RNN에서 cell state $C_t$를 추가하여 긴 시퀀스의 입력을 처리하는데 탁월한 성능</li><li>cell state 또한 이전 시점의 cell state가 다음 시점의 cell state를 구하기 위한 입력으로서 사용</li><li><strong>입력 게이트</strong>: 현재 정보를 기억하기 위한 게이트</li><li><strong>삭제 게이트</strong>: 기억을 삭제하기 위한 게이트, 시그모이드 함수의 반환값이 0에 가까울수록 많은 정보가 삭제</li><li><strong>셀 상태</strong>: 삭제 게이트에서 일부 기억을 잃은 상태, 입력 게이트에서 선택된 기억을 삭제 게이트의 결과값과 더함</li><li>삭제 게이트는 이전 시점의 입력을 얼마나 반영할지 의미, 입력 게이트는 현재 시점의 입력을 얼마나 반영할지 결정</li><li><strong>출력 게이트</strong>: 현재 시점 $t$의 hidden state를 결정하는 일에 사용</li></ul><h1 id=08-03-gru>08-03. GRU<a hidden class=anchor aria-hidden=true href=#08-03-gru>#</a></h1><ul><li>LSTM의 장기 의존성 문제에 대한 해결책을 유지하면서, hidden state를 업데이트하는 계산을 감소</li><li>GRU는 업데이트 게이트와 리셋 게이트로 구성</li><li>데이터의 양이 적을 때는 매개 변수의 양이 적은 GRU가 유리, 반대의 경우엔 LSTM이 유리</li></ul><h1 id=08-04-keras-rnn-and-lstm>08-04. Keras RNN and LSTM<a hidden class=anchor aria-hidden=true href=#08-04-keras-rnn-and-lstm>#</a></h1><h2 id=입력-생성>입력 생성<a hidden class=anchor aria-hidden=true href=#입력-생성>#</a></h2><ul><li><code>train_X</code>가 2D 텐서의 형태일 경우 batch_size 1을 추가해 3D 텐서로 변경</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>train_X</span> <span class=o>=</span> <span class=p>[[[</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>4.2</span><span class=p>,</span> <span class=mf>1.5</span><span class=p>,</span> <span class=mf>1.1</span><span class=p>,</span> <span class=mf>2.8</span><span class=p>],</span> <span class=p>[</span><span class=mf>1.0</span><span class=p>,</span> <span class=mf>3.1</span><span class=p>,</span> <span class=mf>2.5</span><span class=p>,</span> <span class=mf>0.7</span><span class=p>,</span> <span class=mf>1.1</span><span class=p>],</span> <span class=p>[</span><span class=mf>0.3</span><span class=p>,</span> <span class=mf>2.1</span><span class=p>,</span> <span class=mf>1.5</span><span class=p>,</span> <span class=mf>2.1</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>],</span> <span class=p>[</span><span class=mf>2.2</span><span class=p>,</span> <span class=mf>1.4</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.9</span><span class=p>,</span> <span class=mf>1.1</span><span class=p>]]]</span>
</span></span><span class=line><span class=cl><span class=n>train_X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>train_X</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>train_X</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span> <span class=c1># (1, 4, 5)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=simpernn>SimpeRNN<a hidden class=anchor aria-hidden=true href=#simpernn>#</a></h2><ul><li><code>return_sequences=False</code>일 경우 2D 텐서 반환</li><li><code>return_sequences=True</code>일 경우 timesteps를 포함하는 3D 텐서 반환</li><li><code>return_state=True</code>일 경우 <code>return_sequences</code>에 관계없이 마지막 시점의 hidden state 출력</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>rnn</span> <span class=o>=</span> <span class=n>SimpleRNN</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>hidden_state</span> <span class=o>=</span> <span class=n>rnn</span><span class=p>(</span><span class=n>train_X</span><span class=p>)</span> <span class=c1># shape(1, 3)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>rnn</span> <span class=o>=</span> <span class=n>SimpleRNN</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>hidden_states</span> <span class=o>=</span> <span class=n>rnn</span><span class=p>(</span><span class=n>train_X</span><span class=p>)</span> <span class=c1># shape(1, 4, 3)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>rnn</span> <span class=o>=</span> <span class=n>SimpleRNN</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>return_state</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>hidden_states</span><span class=p>,</span> <span class=n>last_state</span> <span class=o>=</span> <span class=n>rnn</span><span class=p>(</span><span class=n>train_X</span><span class=p>)</span> <span class=c1># shape(1, 4, 3), shape(1, 3)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=lstm>LSTM<a hidden class=anchor aria-hidden=true href=#lstm>#</a></h2><ul><li><code>return_state=True</code>일 경우 마지막 cell state를 포함한 세 개의 결과를 반환</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>lstm</span> <span class=o>=</span> <span class=n>LSTM</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>return_state</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>hidden_state</span><span class=p>,</span> <span class=n>last_state</span><span class=p>,</span> <span class=n>last_cell_state</span> <span class=o>=</span> <span class=n>lstm</span><span class=p>(</span><span class=n>train_X</span><span class=p>)</span> <span class=c1># each shape(1, 3)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=bidirectional-lstm>Bidirectional LSTM<a hidden class=anchor aria-hidden=true href=#bidirectional-lstm>#</a></h2><ul><li>정방향과 역방향에 대한 hidden state와 cell state를 반환</li><li><code>return_sequences=False</code>일 경우 정방향 LSTM의 마지막 시점 hidden state와<br>역방향 LSTM의 첫번째 시점 hidden state가 연결된 채 반환</li><li><code>return_sequences=True</code>일 경우 각각의 순서 맞게 연결된 hidden state 반환</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>bilstm</span> <span class=o>=</span> <span class=n>Bidirectional</span><span class=p>(</span><span class=n>LSTM</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>return_state</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> \
</span></span><span class=line><span class=cl>                            <span class=n>kernel_initializer</span><span class=o>=</span><span class=n>k_init</span><span class=p>,</span> <span class=n>bias_initializer</span><span class=o>=</span><span class=n>b_init</span><span class=p>,</span> <span class=n>recurrent_initializer</span><span class=o>=</span><span class=n>r_init</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>hidden_states</span><span class=p>,</span> <span class=n>forward_h</span><span class=p>,</span> <span class=n>forward_c</span><span class=p>,</span> <span class=n>backward_h</span><span class=p>,</span> <span class=n>backward_c</span> <span class=o>=</span> <span class=n>bilstm</span><span class=p>(</span><span class=n>train_X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># shape(1, 6), shape(1, 3), shape(1, 3)</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=08-05-rnnlm>08-05. RNNLM<a hidden class=anchor aria-hidden=true href=#08-05-rnnlm>#</a></h1><ul><li>NNLM과 달리 time step을 도입하여 입력의 길이가 고정되지 않는 언어 모델</li><li><strong>Teaching Forcing</strong>: 테스트 과정에서 RNN 모델을 훈련시킬 때 사용하는 훈련 기법,<br>모델이 t 시점에서 예측한 값을 t+1 시점에 입력으로 사용하지 않고 실제 알고 있는 정답(t 시점의 레이블)을 사용</li><li>한 번 잘못 예측하면 뒤에서의 예측가지 영향을 미쳐 훈련 시간이 느려지기 때문에 교사 강요를 사용</li><li>훈련 과정 동안 활성화 함수로는 softmax 함수, 손실 함수로는 cross-entropy 함수를 사용</li><li>one-hot vector $x_t$를 입력받으면 NNLM에서와 동일한 embedding layer를 거쳐<br>${V}\times{M}$ 크기의 embedding vector로 변환, $e_t=lookup(x_t)$</li><li>이후 RNN과 동일한 과정을 거쳐 $\hat{y_t}$를 반환, $h_t=\tanh({W_x}{e_t}+{W_h}{h_{t-1}}+b)$</li><li>$\hat{y}_t$의 각 차원 안에서의 값은 $\hat{y}_t$의 j번째 인덱스가 가진 0과 1사이의 값이 j번째 단어가 다음 단어일 확률</li></ul><h1 id=08-06-text-generation-using-rnnhttpswikidocsnet45101>08-06. <a href=https://wikidocs.net/45101 target=_blank rel=noopener>Text Generation using RNN</a><a hidden class=anchor aria-hidden=true href=#08-06-text-generation-using-rnnhttpswikidocsnet45101>#</a></h1><h2 id=데이터-전처리>데이터 전처리<a hidden class=anchor aria-hidden=true href=#데이터-전처리>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 원본 한국어 문장</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;경마장에 있는 말이 뛰고 있다</span><span class=se>\n</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>그의 말이 법이다</span><span class=se>\n</span><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>가는 말이 고와야 오는 말이 곱다</span><span class=se>\n</span><span class=s2>&#34;&#34;&#34;</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 단어 집합 생성</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>Tokenizer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>fit_on_texts</span><span class=p>([</span><span class=n>text</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>vocab_size</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>word_index</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;단어 집합의 크기 : </span><span class=si>%d</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=n>vocab_size</span><span class=p>)</span> <span class=c1># 12</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 각 라인마다 texts_to_sequences() 함수를 적용해서 훈련 데이터 생성</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>sequences</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>[[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>],</span> <span class=p>[</span><span class=mi>6</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>6</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>7</span><span class=p>],</span> <span class=p>[</span><span class=mi>8</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>8</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>9</span><span class=p>],</span> <span class=p>[</span><span class=mi>8</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>9</span><span class=p>,</span> <span class=mi>10</span><span class=p>],</span> <span class=p>[</span><span class=mi>8</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>9</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>8</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>9</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>11</span><span class=p>]]</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 패딩 후 라벨 분리 (가장 우측에 있는 단어, [경마장에, 있는]에서 &#39;있는&#39; 등을 라벨로 지정)</span>
</span></span><span class=line><span class=cl><span class=n>sequences</span> <span class=o>=</span> <span class=n>pad_sequences</span><span class=p>(</span><span class=n>sequences</span><span class=p>,</span> <span class=n>maxlen</span><span class=o>=</span><span class=n>max_len</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;pre&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sequences</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>sequences</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>sequences</span><span class=p>[:,:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>sequences</span><span class=p>[:,</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 라벨에 대해서 one-hot encoding 수행</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>to_categorical</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>num_classes</span><span class=o>=</span><span class=n>vocab_size</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=rnn-모델-설계>RNN 모델 설계<a hidden class=anchor aria-hidden=true href=#rnn-모델-설계>#</a></h2><ul><li>many-to-one 구조의 RNN을 사용</li><li>모든 가능한 단어 중 마지막 시점에서 하나의 단어를 예측하는 다중 클래스 분류 문제 수행</li><li>활성화 함수로는 softmax 함수, 손실 함수로는 cross-entropy 함수 사용</li><li>첫 단어가 주어졌을 때, n번 동안 예측을 반복하면서 현재 단어와 문장에 예측 단어를 저장</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>embedding_dim</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl><span class=n>hidden_units</span> <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>SimpleRNN</span><span class=p>(</span><span class=n>hidden_units</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;softmax&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;categorical_crossentropy&#39;</span><span class=p>,</span> <span class=n>optimizer</span><span class=o>=</span><span class=s1>&#39;adam&#39;</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>200</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=lstm-모델-설계>LSTM 모델 설계<a hidden class=anchor aria-hidden=true href=#lstm-모델-설계>#</a></h2><ul><li>뉴욕 타임즈 기사 제목 데이터 전처리 (단어 집합 크기 3494, 샘플 최대 길이 24)</li><li>RNN과 동일한 작업을 수행할 LSTM 모델 설계, 예측 과정 또한 동일</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>embedding_dim</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl><span class=n>hidden_units</span> <span class=o>=</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>LSTM</span><span class=p>(</span><span class=n>hidden_units</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;softmax&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;categorical_crossentropy&#39;</span><span class=p>,</span> <span class=n>optimizer</span><span class=o>=</span><span class=s1>&#39;adam&#39;</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>200</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=08-07-char-rnnhttpswikidocsnet48649>08-07. <a href=https://wikidocs.net/48649 target=_blank rel=noopener>Char RNN</a><a hidden class=anchor aria-hidden=true href=#08-07-char-rnnhttpswikidocsnet48649>#</a></h1><ul><li>입출력의 단위를 word-level에서 character-level로 변경한 RNN</li><li>문자 단위를 입출력으로 사용하기 때문에 embedding layer를 사용하지 않음</li><li><a href=http://www.gutenberg.org/files/11/11-0.txt target=_blank rel=noopener>이상한 나라의 앨리스 데이터</a> 사용 (문자열 길이 159484, 문자 집합 크기 56)</li><li>훈련 데이터에 apple이라는 시퀀스가 있고 입력의 길이가 4일 때, &lsquo;appl&rsquo;을 입력하면 &lsquo;pple&rsquo;을 예측할 것으로 기대</li><li>train_X.shape(2658, 60, 56), train_y.shape(2658, 60, 56)</li></ul><h2 id=char-rnn-모델-설계>Char RNN 모델 설계<a hidden class=anchor aria-hidden=true href=#char-rnn-모델-설계>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>hidden_units</span> <span class=o>=</span> <span class=mi>256</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>LSTM</span><span class=p>(</span><span class=n>hidden_units</span><span class=p>,</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=kc>None</span><span class=p>,</span> <span class=n>train_X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>2</span><span class=p>]),</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>LSTM</span><span class=p>(</span><span class=n>hidden_units</span><span class=p>,</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>TimeDistributed</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;softmax&#39;</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;categorical_crossentropy&#39;</span><span class=p>,</span> <span class=n>optimizer</span><span class=o>=</span><span class=s1>&#39;adam&#39;</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>train_X</span><span class=p>,</span> <span class=n>train_y</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>80</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://minyeamer.github.io/tags/til/>TIL</a></li><li><a href=https://minyeamer.github.io/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=prev href=https://minyeamer.github.io/blog/2022-06-29/><span class=title>« Prev</span><br><span>2022-06-29 Log</span></a>
<a class=next href=https://minyeamer.github.io/blog/2022-06-19/><span class=title>Next »</span><br><span>2022-06-19 Log</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 2022-06-28 Log on twitter" href="https://twitter.com/intent/tweet/?text=2022-06-28%20Log&url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-28%2f&hashtags=TIL%2cNLP"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-06-28 Log on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-28%2f&title=2022-06-28%20Log&summary=2022-06-28%20Log&source=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-28%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-06-28 Log on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-28%2f&title=2022-06-28%20Log"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-06-28 Log on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-28%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-06-28 Log on whatsapp" href="https://api.whatsapp.com/send?text=2022-06-28%20Log%20-%20https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-28%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-06-28 Log on telegram" href="https://telegram.me/share/url?text=2022-06-28%20Log&url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-28%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=minyeamer/til issue-term=pathname label=comments theme=preferred-color-scheme crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://minyeamer.github.io/>Minystory</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>