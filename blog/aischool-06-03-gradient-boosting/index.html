<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[AI SCHOOL 5기] 머신 러닝 실습 - Gradient Boosting | Minystory</title><meta name=keywords content="AI SCHOOL,멋쟁이사자처럼,코드라이언,Machine Learning,Gradient Boosting"><meta name=description content="XG Boost Extreme Gradient Boosting 대용량 분산 처리를 위한 Gradient Boosting 라이브러리 Decision Tree(의사결정나무) 에 Boosting 기법을 적용한 알고리즘 AdaBoost는 학습 성능은 좋으나, 모델의 학습 시간이 오래 걸리는 단점 병렬 처리 기법을 적용하여 Gradient Boost보다 학습 속도를 끌어올림 Hyper-Parameter가 너무 많기 때문에 권장 세팅 사용 @ http://j.mp/2PukeTS Decision Tree 이해하기 쉽고 해석도 용이함 입력 데이터의 작은 변동에도 Tree의 구성이 크게 달라짐 과적합이 쉽게 발생 (중간에 멈추지 않으면 Leaf 노드에 하나의 데이터만 남게 됨) 의사결정나무의 문제를 해결하기 위해 Boosting 기법 활용 ex) 테니스를 쳤던 과거 데이터를 보고 날씨 정보를 이용해 의사결정 AdaBoost Adaptive Boosting 데이터를 바탕으로 여러 weak learner 들을 반복적으로 생성 앞선 learner가 잘못 예측한 데이터에 가중치를 부여하고 학습 최종적으로 만들어진 strong learner를 이용하여 실제 예측 진행 에러를 최소화하는 weight를 매기기 위해 경사 하강법 사용 ex) Regression: 평균/가중평균, Classification: 투표 XG Boost References NGBoost Explained (Comparison to LightGBM and XGBoost) Gradient Boosting Interactive Playground Gradient Boosting explained Comparison for hyperparams of XGBoost & LightGBM XGBoost Parameters XG Boost 하이퍼 파라미터 상세 설명 Complete Guide to Parameter Tuning in XGBoost (with python codes) Microsoft EBM (Explainable Boosting Machine) 정형데이터를 위한 인공신경망 모델, TabNet Ensemble 주어진 데이터를 이용하여 여러 개의 서로 다른 예측 모형을 생성한 후,"><meta name=author content="minyeamer"><link rel=canonical href=https://minyeamer.github.io/blog/aischool-06-03-gradient-boosting/><meta name=google-site-verification content="u1tWcHmHUZWfFT1cHaku6sqU-bK40N3WLR-C-4VUWN0"><meta name=naver-site-verification content="6eaf8e9da1a6104780f056f1a7797fe5a3a5a0da"><link crossorigin=anonymous href=/assets/css/stylesheet.78a14cf8249250820b49e9dc59e58b846a9beea6d16d50612c68b121ddf02146.css integrity="sha256-eKFM+CSSUIILSencWeWLhGqb7qbRbVBhLGixId3wIUY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://minyeamer.github.io/img/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://minyeamer.github.io/img/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://minyeamer.github.io/img/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://minyeamer.github.io/img/favicons/apple-touch-icon.png><link rel=mask-icon href=https://minyeamer.github.io/img/favicons/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V")</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V",{anonymize_ip:!1})}</script><meta property="og:title" content="[AI SCHOOL 5기] 머신 러닝 실습 - Gradient Boosting"><meta property="og:description" content="XG Boost Extreme Gradient Boosting 대용량 분산 처리를 위한 Gradient Boosting 라이브러리 Decision Tree(의사결정나무) 에 Boosting 기법을 적용한 알고리즘 AdaBoost는 학습 성능은 좋으나, 모델의 학습 시간이 오래 걸리는 단점 병렬 처리 기법을 적용하여 Gradient Boost보다 학습 속도를 끌어올림 Hyper-Parameter가 너무 많기 때문에 권장 세팅 사용 @ http://j.mp/2PukeTS Decision Tree 이해하기 쉽고 해석도 용이함 입력 데이터의 작은 변동에도 Tree의 구성이 크게 달라짐 과적합이 쉽게 발생 (중간에 멈추지 않으면 Leaf 노드에 하나의 데이터만 남게 됨) 의사결정나무의 문제를 해결하기 위해 Boosting 기법 활용 ex) 테니스를 쳤던 과거 데이터를 보고 날씨 정보를 이용해 의사결정 AdaBoost Adaptive Boosting 데이터를 바탕으로 여러 weak learner 들을 반복적으로 생성 앞선 learner가 잘못 예측한 데이터에 가중치를 부여하고 학습 최종적으로 만들어진 strong learner를 이용하여 실제 예측 진행 에러를 최소화하는 weight를 매기기 위해 경사 하강법 사용 ex) Regression: 평균/가중평균, Classification: 투표 XG Boost References NGBoost Explained (Comparison to LightGBM and XGBoost) Gradient Boosting Interactive Playground Gradient Boosting explained Comparison for hyperparams of XGBoost & LightGBM XGBoost Parameters XG Boost 하이퍼 파라미터 상세 설명 Complete Guide to Parameter Tuning in XGBoost (with python codes) Microsoft EBM (Explainable Boosting Machine) 정형데이터를 위한 인공신경망 모델, TabNet Ensemble 주어진 데이터를 이용하여 여러 개의 서로 다른 예측 모형을 생성한 후,"><meta property="og:type" content="article"><meta property="og:url" content="https://minyeamer.github.io/blog/aischool-06-03-gradient-boosting/"><meta property="og:image" content="https://minyeamer.github.io/ai-school.png"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-04-13T20:51:00+09:00"><meta property="article:modified_time" content="2022-04-13T20:51:00+09:00"><meta property="og:site_name" content="Minystory"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://minyeamer.github.io/ai-school.png"><meta name=twitter:title content="[AI SCHOOL 5기] 머신 러닝 실습 - Gradient Boosting"><meta name=twitter:description content="XG Boost Extreme Gradient Boosting 대용량 분산 처리를 위한 Gradient Boosting 라이브러리 Decision Tree(의사결정나무) 에 Boosting 기법을 적용한 알고리즘 AdaBoost는 학습 성능은 좋으나, 모델의 학습 시간이 오래 걸리는 단점 병렬 처리 기법을 적용하여 Gradient Boost보다 학습 속도를 끌어올림 Hyper-Parameter가 너무 많기 때문에 권장 세팅 사용 @ http://j.mp/2PukeTS Decision Tree 이해하기 쉽고 해석도 용이함 입력 데이터의 작은 변동에도 Tree의 구성이 크게 달라짐 과적합이 쉽게 발생 (중간에 멈추지 않으면 Leaf 노드에 하나의 데이터만 남게 됨) 의사결정나무의 문제를 해결하기 위해 Boosting 기법 활용 ex) 테니스를 쳤던 과거 데이터를 보고 날씨 정보를 이용해 의사결정 AdaBoost Adaptive Boosting 데이터를 바탕으로 여러 weak learner 들을 반복적으로 생성 앞선 learner가 잘못 예측한 데이터에 가중치를 부여하고 학습 최종적으로 만들어진 strong learner를 이용하여 실제 예측 진행 에러를 최소화하는 weight를 매기기 위해 경사 하강법 사용 ex) Regression: 평균/가중평균, Classification: 투표 XG Boost References NGBoost Explained (Comparison to LightGBM and XGBoost) Gradient Boosting Interactive Playground Gradient Boosting explained Comparison for hyperparams of XGBoost & LightGBM XGBoost Parameters XG Boost 하이퍼 파라미터 상세 설명 Complete Guide to Parameter Tuning in XGBoost (with python codes) Microsoft EBM (Explainable Boosting Machine) 정형데이터를 위한 인공신경망 모델, TabNet Ensemble 주어진 데이터를 이용하여 여러 개의 서로 다른 예측 모형을 생성한 후,"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://minyeamer.github.io/post/"},{"@type":"ListItem","position":2,"name":"[AI SCHOOL 5기] 머신 러닝 실습 - Gradient Boosting","item":"https://minyeamer.github.io/blog/aischool-06-03-gradient-boosting/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[AI SCHOOL 5기] 머신 러닝 실습 - Gradient Boosting","name":"[AI SCHOOL 5기] 머신 러닝 실습 - Gradient Boosting","description":"XG Boost Extreme Gradient Boosting 대용량 분산 처리를 위한 Gradient Boosting 라이브러리 Decision Tree(의사결정나무) 에 Boosting 기법을 적용한 알고리즘 AdaBoost는 학습 성능은 좋으나, 모델의 학습 시간이 오래 걸리는 단점 병렬 처리 기법을 적용하여 Gradient Boost보다 학습 속도를 끌어올림 Hyper-Parameter가 너무 많기 때문에 권장 세팅 사용 @ http://j.mp/2PukeTS Decision Tree 이해하기 쉽고 해석도 용이함 입력 데이터의 작은 변동에도 Tree의 구성이 크게 달라짐 과적합이 쉽게 발생 (중간에 멈추지 않으면 Leaf 노드에 하나의 데이터만 남게 됨) 의사결정나무의 문제를 해결하기 위해 Boosting 기법 활용 ex) 테니스를 쳤던 과거 데이터를 보고 날씨 정보를 이용해 의사결정 AdaBoost Adaptive Boosting 데이터를 바탕으로 여러 weak learner 들을 반복적으로 생성 앞선 learner가 잘못 예측한 데이터에 가중치를 부여하고 학습 최종적으로 만들어진 strong learner를 이용하여 실제 예측 진행 에러를 최소화하는 weight를 매기기 위해 경사 하강법 사용 ex) Regression: 평균/가중평균, Classification: 투표 XG Boost References NGBoost Explained (Comparison to LightGBM and XGBoost) Gradient Boosting Interactive Playground Gradient Boosting explained Comparison for hyperparams of XGBoost \u0026amp; LightGBM XGBoost Parameters XG Boost 하이퍼 파라미터 상세 설명 Complete Guide to Parameter Tuning in XGBoost (with python codes) Microsoft EBM (Explainable Boosting Machine) 정형데이터를 위한 인공신경망 모델, TabNet Ensemble 주어진 데이터를 이용하여 여러 개의 서로 다른 예측 모형을 생성한 후,","keywords":["AI SCHOOL","멋쟁이사자처럼","코드라이언","Machine Learning","Gradient Boosting"],"articleBody":"XG Boost Extreme Gradient Boosting 대용량 분산 처리를 위한 Gradient Boosting 라이브러리 Decision Tree(의사결정나무) 에 Boosting 기법을 적용한 알고리즘 AdaBoost는 학습 성능은 좋으나, 모델의 학습 시간이 오래 걸리는 단점 병렬 처리 기법을 적용하여 Gradient Boost보다 학습 속도를 끌어올림 Hyper-Parameter가 너무 많기 때문에 권장 세팅 사용 @ http://j.mp/2PukeTS Decision Tree 이해하기 쉽고 해석도 용이함 입력 데이터의 작은 변동에도 Tree의 구성이 크게 달라짐 과적합이 쉽게 발생 (중간에 멈추지 않으면 Leaf 노드에 하나의 데이터만 남게 됨) 의사결정나무의 문제를 해결하기 위해 Boosting 기법 활용 ex) 테니스를 쳤던 과거 데이터를 보고 날씨 정보를 이용해 의사결정 AdaBoost Adaptive Boosting 데이터를 바탕으로 여러 weak learner 들을 반복적으로 생성 앞선 learner가 잘못 예측한 데이터에 가중치를 부여하고 학습 최종적으로 만들어진 strong learner를 이용하여 실제 예측 진행 에러를 최소화하는 weight를 매기기 위해 경사 하강법 사용 ex) Regression: 평균/가중평균, Classification: 투표 XG Boost References NGBoost Explained (Comparison to LightGBM and XGBoost) Gradient Boosting Interactive Playground Gradient Boosting explained Comparison for hyperparams of XGBoost \u0026 LightGBM XGBoost Parameters XG Boost 하이퍼 파라미터 상세 설명 Complete Guide to Parameter Tuning in XGBoost (with python codes) Microsoft EBM (Explainable Boosting Machine) 정형데이터를 위한 인공신경망 모델, TabNet Ensemble 주어진 데이터를 이용하여 여러 개의 서로 다른 예측 모형을 생성한 후,\n예측 모형의 예측 결과를 종합하여 하나의 최종 예측결과를 도출해내는 방법 다양한 모델이 문제 공간의 다른 측면을 보면서 각기 다른 방식으로 오점이 있다고 가정\n(모델 별로 약점을 보완) Boosting weak learner들을 strong learner로 변환시키는 알고리즘\n(약한 학습기를 여러개 사용해서 하나의 강건한 학습기를 만들어내는 것) 의사결정나무 모델을 합리적인 수준(60~70% 성능)에서 여러 종류 생성 ex) AdaBoost Gradient Boosting 경사 하강법을 사용해서 AdaBoost보다 성능을 개선한 Boosting 기법 AdaBoost는 높은 가중치를 가진 지점이 존재하게 되면 성능이 크게 떨어지는 단점\n(높은 가중치를 가진 지점과 가까운 다른 데이터들이 잘못 분류될 가능성이 높음) Gradient Boosting 기법은 이전 모델에 종속적이기 때문에 병렬 처리가 불가능 Bagging Bootstrap Aggregating 가중치를 매기지 않고 각각의 모델이 서로 독립적 x 데이터 열들의 서로 다른 조합으로 독립적인 모델을 여러 종류 생성 ex) Random Forest Random Forest 각 모델은 서로 다른 샘플 데이터셋을 활용 (Bootstrap Sampling \u0026 Bagging) 각 데이터셋은 복원추출을 통해 원래 데이터셋 만큼의 크기로 샘플링 (누락 \u0026 중복 발생) 위 서로 다른 샘플로 각 모델 생성 시, 각 노드 지점마다 x열 n개 중 랜덤하게 m개 중 분기 선택 Classification에서는 root n을 m으로 사용 Regression에서는 n/3을 m으로 사용 @ https://j.mp/3rZ05bN \u0026 https://j.mp/3GJ7QqH Learning Process @ https://j.mp/3jRJH6n Import Libraries 1 2 3 4 5 6 7 import numpy as np import matplotlib.pyplot as plt from sklearn import ensemble from sklearn import datasets from sklearn.utils import shuffle from sklearn.metrics import mean_squared_error Load Data 1 2 3 4 5 6 7 8 boston = datasets.load_boston() X, y = shuffle(boston.data, boston.target, random_state=13) X = X.astype(np.float32) offset = int(X.shape[0] * 0.9) X_train, y_train = X[:offset], y[:offset] X_test, y_test = X[offset:], y[offset:] load 부분에서 다른 데이터도 사용 가능 boston.data: x data boston.target: y data int(X.shape[0] * 0.9): 전체 행 중 90 퍼센트 의미 boston.feature_names: 데이터셋에서 열 이름 boston.DESCR: 데이터에 대한 상세 설명 Model Fitting 1 2 3 4 5 6 7 params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2, 'learning_rate': 0.01, 'loss': 'ls'} clf = ensemble.GradientBoostingRegressor(**params) clf.fit(X_train, y_train) mse = mean_squared_error(y_test, clf.predict(X_test)) print(\"MSE: %.4f\" % mse) **params: 딕셔너리를 파라미터로 변환 @ https://j.mp/2IPuJzY clf: Classifier Plot Deviance 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 test_score = np.zeros((params['n_estimators'],), dtype=np.float64) for i, y_pred in enumerate(clf.staged_predict(X_test)): test_score[i] = clf.loss_(y_test, y_pred) plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title('Deviance') plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-', label='Training Set Deviance') plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-', label='Test Set Deviance') plt.legend(loc='upper right') plt.xlabel('Boosting Iterations') plt.ylabel('Deviance') Deviance: 편차값, 에러 n_estimators: 의사결정나무 모델을 몇 개 만들었는지 Test Data에 대한 에러 라인이 튕겨올라가는 지점이 Overfitting Plot Feature Importance 1 2 3 4 5 6 7 8 9 10 11 feature_importance = clf.feature_importances_ feature_importance = 100.0 * (feature_importance / feature_importance.max()) sorted_idx = np.argsort(feature_importance) pos = np.arange(sorted_idx.shape[0]) + .5 plt.subplot(1, 2, 2) plt.barh(pos, feature_importance[sorted_idx], align='center') plt.yticks(pos, boston.feature_names[sorted_idx]) plt.xlabel('Relative Importance') plt.title('Variable Importance') plt.show() feature_importances_: 트리 기반 모델이 가지고 있는 변수, 각각의 열마다의 중요도 Feature Importance는 상대적인 중요도이기 때문에 합계가 1 LSTAT(인구 중 하위 계층 비율)이 집값을 예측할 때 가장 중요함 Feature Importance References Feature Importance Analysis (LIME) Permutation importance 한글 설명 Permutation importance with Pipeline ","wordCount":"678","inLanguage":"en","image":"https://minyeamer.github.io/ai-school.png","datePublished":"2022-04-13T20:51:00+09:00","dateModified":"2022-04-13T20:51:00+09:00","author":{"@type":"Person","name":"minyeamer"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://minyeamer.github.io/blog/aischool-06-03-gradient-boosting/"},"publisher":{"@type":"Organization","name":"Minystory","logo":{"@type":"ImageObject","url":"https://minyeamer.github.io/img/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://minyeamer.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://minyeamer.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://minyeamer.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://minyeamer.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://minyeamer.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://minyeamer.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://minyeamer.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://minyeamer.github.io/post/>Posts</a></div><h1 class=post-title>[AI SCHOOL 5기] 머신 러닝 실습 - Gradient Boosting</h1><div class=post-meta><span title='2022-04-13 20:51:00 +0900 KST'>April 13, 2022</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;678 words&nbsp;·&nbsp;minyeamer&nbsp;|&nbsp;<a href=https://github.com/minyeamer/til/edit/main/activities/ai-school/practices/06-machine-learning/03-gradient-boosting.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><figure class=entry-cover><img loading=lazy src="https://github.com/minyeamer/til/blob/main/.media/covers/ai-school.png?raw=true" alt></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>&nbsp;Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#xg-boost>XG Boost</a><ul><li><a href=#decision-tree>Decision Tree</a></li><li><a href=#adaboost>AdaBoost</a></li><li><a href=#xg-boost-references>XG Boost References</a></li></ul></li><li><a href=#ensemble>Ensemble</a><ul><li><a href=#boosting>Boosting</a></li><li><a href=#gradient-boosting>Gradient Boosting</a></li><li><a href=#bagging>Bagging</a></li><li><a href=#random-forest>Random Forest</a></li></ul></li><li><a href=#learning-process>Learning Process</a><ul><li><a href=#import-libraries>Import Libraries</a></li><li><a href=#load-data>Load Data</a></li><li><a href=#model-fitting>Model Fitting</a></li></ul></li><li><a href=#plot-deviance>Plot Deviance</a></li><li><a href=#plot-feature-importance>Plot Feature Importance</a><ul><li><a href=#feature-importance-references>Feature Importance References</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h1 id=xg-boost>XG Boost<a hidden class=anchor aria-hidden=true href=#xg-boost>#</a></h1><ul><li>Extreme Gradient Boosting</li><li>대용량 분산 처리를 위한 Gradient Boosting 라이브러리</li><li><strong>Decision Tree(의사결정나무)</strong> 에 <strong>Boosting</strong> 기법을 적용한 알고리즘</li><li><strong>AdaBoost</strong>는 학습 성능은 좋으나, 모델의 학습 시간이 오래 걸리는 단점</li><li>병렬 처리 기법을 적용하여 <strong>Gradient Boost</strong>보다 학습 속도를 끌어올림</li><li>Hyper-Parameter가 너무 많기 때문에 권장 세팅 사용 @ <a href=http://j.mp/2PukeTS target=_blank rel=noopener>http://j.mp/2PukeTS</a></li></ul><h2 id=decision-tree>Decision Tree<a hidden class=anchor aria-hidden=true href=#decision-tree>#</a></h2><ul><li>이해하기 쉽고 해석도 용이함</li><li>입력 데이터의 작은 변동에도 Tree의 구성이 크게 달라짐</li><li>과적합이 쉽게 발생 (중간에 멈추지 않으면 Leaf 노드에 하나의 데이터만 남게 됨)</li><li>의사결정나무의 문제를 해결하기 위해 <strong>Boosting</strong> 기법 활용</li><li>ex) 테니스를 쳤던 과거 데이터를 보고 날씨 정보를 이용해 의사결정</li></ul><h2 id=adaboost>AdaBoost<a hidden class=anchor aria-hidden=true href=#adaboost>#</a></h2><ul><li>Adaptive Boosting</li><li>데이터를 바탕으로 여러 weak learner 들을 반복적으로 생성</li><li>앞선 learner가 잘못 예측한 데이터에 가중치를 부여하고 학습</li><li>최종적으로 만들어진 strong learner를 이용하여 실제 예측 진행</li><li>에러를 최소화하는 weight를 매기기 위해 <strong>경사 하강법</strong> 사용</li><li>ex) Regression: 평균/가중평균, Classification: 투표</li></ul><h2 id=xg-boost-references>XG Boost References<a hidden class=anchor aria-hidden=true href=#xg-boost-references>#</a></h2><ul><li><a href=http://j.mp/2WzoyTl target=_blank rel=noopener>NGBoost Explained (Comparison to LightGBM and XGBoost)</a></li><li><a href=http://j.mp/34xfO2Y target=_blank rel=noopener>Gradient Boosting Interactive Playground</a></li><li><a href=http://j.mp/34z8BiK target=_blank rel=noopener>Gradient Boosting explained</a></li><li><a href=http://j.mp/2PukeTS target=_blank rel=noopener>Comparison for hyperparams of XGBoost & LightGBM</a></li><li><a href=https://goo.gl/9fD4G9 target=_blank rel=noopener>XGBoost Parameters</a></li><li><a href=https://goo.gl/1hWjNT target=_blank rel=noopener>XG Boost 하이퍼 파라미터 상세 설명</a></li><li><a href=https://goo.gl/rvWfXY target=_blank rel=noopener>Complete Guide to Parameter Tuning in XGBoost (with python codes)</a></li><li><a href=https://j.mp/3wBIyWx target=_blank rel=noopener>Microsoft EBM (Explainable Boosting Machine)</a></li><li><a href=https://j.mp/38ABU8e target=_blank rel=noopener>정형데이터를 위한 인공신경망 모델, TabNet</a></li></ul><hr><h1 id=ensemble>Ensemble<a hidden class=anchor aria-hidden=true href=#ensemble>#</a></h1><ul><li>주어진 데이터를 이용하여 여러 개의 서로 다른 예측 모형을 생성한 후,<br>예측 모형의 예측 결과를 종합하여 하나의 최종 예측결과를 도출해내는 방법</li><li>다양한 모델이 문제 공간의 다른 측면을 보면서 각기 다른 방식으로 오점이 있다고 가정<br>(모델 별로 약점을 보완)</li></ul><h2 id=boosting>Boosting<a hidden class=anchor aria-hidden=true href=#boosting>#</a></h2><ul><li>weak learner들을 strong learner로 변환시키는 알고리즘<br>(약한 학습기를 여러개 사용해서 하나의 강건한 학습기를 만들어내는 것)</li><li>의사결정나무 모델을 합리적인 수준(60~70% 성능)에서 여러 종류 생성</li><li>ex) AdaBoost</li></ul><h2 id=gradient-boosting>Gradient Boosting<a hidden class=anchor aria-hidden=true href=#gradient-boosting>#</a></h2><ul><li>경사 하강법을 사용해서 AdaBoost보다 성능을 개선한 Boosting 기법</li><li>AdaBoost는 높은 가중치를 가진 지점이 존재하게 되면 성능이 크게 떨어지는 단점<br>(높은 가중치를 가진 지점과 가까운 다른 데이터들이 잘못 분류될 가능성이 높음)</li><li>Gradient Boosting 기법은 이전 모델에 종속적이기 때문에 병렬 처리가 불가능</li></ul><h2 id=bagging>Bagging<a hidden class=anchor aria-hidden=true href=#bagging>#</a></h2><ul><li>Bootstrap Aggregating</li><li>가중치를 매기지 않고 각각의 모델이 서로 독립적</li><li>x 데이터 열들의 서로 다른 조합으로 독립적인 모델을 여러 종류 생성</li><li>ex) Random Forest</li></ul><h2 id=random-forest>Random Forest<a hidden class=anchor aria-hidden=true href=#random-forest>#</a></h2><ul><li>각 모델은 서로 다른 샘플 데이터셋을 활용 (Bootstrap Sampling & Bagging)</li><li>각 데이터셋은 복원추출을 통해 원래 데이터셋 만큼의 크기로 샘플링 (누락 & 중복 발생)</li><li>위 서로 다른 샘플로 각 모델 생성 시, 각 노드 지점마다 x열 n개 중 랜덤하게 m개 중 분기 선택</li><li>Classification에서는 root n을 m으로 사용</li><li>Regression에서는 n/3을 m으로 사용</li><li>@ <a href=https://j.mp/3rZ05bN target=_blank rel=noopener>https://j.mp/3rZ05bN</a> & <a href=https://j.mp/3GJ7QqH target=_blank rel=noopener>https://j.mp/3GJ7QqH</a></li></ul><hr><h1 id=learning-process>Learning Process<a hidden class=anchor aria-hidden=true href=#learning-process>#</a></h1><ul><li>@ <a href=https://j.mp/3jRJH6n target=_blank rel=noopener>https://j.mp/3jRJH6n</a></li></ul><h2 id=import-libraries>Import Libraries<a hidden class=anchor aria-hidden=true href=#import-libraries>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>ensemble</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn</span> <span class=kn>import</span> <span class=n>datasets</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.utils</span> <span class=kn>import</span> <span class=n>shuffle</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.metrics</span> <span class=kn>import</span> <span class=n>mean_squared_error</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=load-data>Load Data<a hidden class=anchor aria-hidden=true href=#load-data>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>boston</span> <span class=o>=</span> <span class=n>datasets</span><span class=o>.</span><span class=n>load_boston</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>shuffle</span><span class=p>(</span><span class=n>boston</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>boston</span><span class=o>.</span><span class=n>target</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>13</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>offset</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>*</span> <span class=mf>0.9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:</span><span class=n>offset</span><span class=p>],</span> <span class=n>y</span><span class=p>[:</span><span class=n>offset</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=n>offset</span><span class=p>:],</span> <span class=n>y</span><span class=p>[</span><span class=n>offset</span><span class=p>:]</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><code>load</code> 부분에서 다른 데이터도 사용 가능</li><li><code>boston.data</code>: x data</li><li><code>boston.target</code>: y data</li><li><code>int(X.shape[0] * 0.9)</code>: 전체 행 중 90 퍼센트 의미</li><li><code>boston.feature_names</code>: 데이터셋에서 열 이름</li><li><code>boston.DESCR</code>: 데이터에 대한 상세 설명</li></ul><h2 id=model-fitting>Model Fitting<a hidden class=anchor aria-hidden=true href=#model-fitting>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>params</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;n_estimators&#39;</span><span class=p>:</span> <span class=mi>500</span><span class=p>,</span> <span class=s1>&#39;max_depth&#39;</span><span class=p>:</span> <span class=mi>4</span><span class=p>,</span> <span class=s1>&#39;min_samples_split&#39;</span><span class=p>:</span> <span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;learning_rate&#39;</span><span class=p>:</span> <span class=mf>0.01</span><span class=p>,</span> <span class=s1>&#39;loss&#39;</span><span class=p>:</span> <span class=s1>&#39;ls&#39;</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>clf</span> <span class=o>=</span> <span class=n>ensemble</span><span class=o>.</span><span class=n>GradientBoostingRegressor</span><span class=p>(</span><span class=o>**</span><span class=n>params</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>clf</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>mse</span> <span class=o>=</span> <span class=n>mean_squared_error</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>clf</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;MSE: </span><span class=si>%.4f</span><span class=s2>&#34;</span> <span class=o>%</span> <span class=n>mse</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><code>**params</code>: 딕셔너리를 파라미터로 변환 @ <a href=https://j.mp/2IPuJzY target=_blank rel=noopener>https://j.mp/2IPuJzY</a></li><li><code>clf</code>: Classifier</li></ul><hr><h1 id=plot-deviance>Plot Deviance<a hidden class=anchor aria-hidden=true href=#plot-deviance>#</a></h1><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>test_score</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>params</span><span class=p>[</span><span class=s1>&#39;n_estimators&#39;</span><span class=p>],),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>float64</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>y_pred</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>clf</span><span class=o>.</span><span class=n>staged_predict</span><span class=p>(</span><span class=n>X_test</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>    <span class=n>test_score</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>clf</span><span class=o>.</span><span class=n>loss_</span><span class=p>(</span><span class=n>y_test</span><span class=p>,</span> <span class=n>y_pred</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Deviance&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>params</span><span class=p>[</span><span class=s1>&#39;n_estimators&#39;</span><span class=p>])</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>clf</span><span class=o>.</span><span class=n>train_score_</span><span class=p>,</span> <span class=s1>&#39;b-&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Training Set Deviance&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>params</span><span class=p>[</span><span class=s1>&#39;n_estimators&#39;</span><span class=p>])</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>test_score</span><span class=p>,</span> <span class=s1>&#39;r-&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>         <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Test Set Deviance&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>(</span><span class=n>loc</span><span class=o>=</span><span class=s1>&#39;upper right&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Boosting Iterations&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Deviance&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>Deviance: 편차값, 에러</li><li><code>n_estimators</code>: 의사결정나무 모델을 몇 개 만들었는지</li><li>Test Data에 대한 에러 라인이 튕겨올라가는 지점이 Overfitting</li></ul><p><img loading=lazy src="https://github.com/minyeamer/til/blob/main/.media/activities/ai-school/06-machine-learning/03-gradient-boosting/deviance.png?raw=true" alt=deviance></p><h1 id=plot-feature-importance>Plot Feature Importance<a hidden class=anchor aria-hidden=true href=#plot-feature-importance>#</a></h1><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>feature_importance</span> <span class=o>=</span> <span class=n>clf</span><span class=o>.</span><span class=n>feature_importances_</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>feature_importance</span> <span class=o>=</span> <span class=mf>100.0</span> <span class=o>*</span> <span class=p>(</span><span class=n>feature_importance</span> <span class=o>/</span> <span class=n>feature_importance</span><span class=o>.</span><span class=n>max</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>sorted_idx</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argsort</span><span class=p>(</span><span class=n>feature_importance</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>pos</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>sorted_idx</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span> <span class=o>+</span> <span class=mf>.5</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>barh</span><span class=p>(</span><span class=n>pos</span><span class=p>,</span> <span class=n>feature_importance</span><span class=p>[</span><span class=n>sorted_idx</span><span class=p>],</span> <span class=n>align</span><span class=o>=</span><span class=s1>&#39;center&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>yticks</span><span class=p>(</span><span class=n>pos</span><span class=p>,</span> <span class=n>boston</span><span class=o>.</span><span class=n>feature_names</span><span class=p>[</span><span class=n>sorted_idx</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Relative Importance&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Variable Importance&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><code>feature_importances_</code>: 트리 기반 모델이 가지고 있는 변수, 각각의 열마다의 중요도</li><li>Feature Importance는 상대적인 중요도이기 때문에 합계가 1</li><li>LSTAT(인구 중 하위 계층 비율)이 집값을 예측할 때 가장 중요함</li></ul><p><img loading=lazy src="https://github.com/minyeamer/til/blob/main/.media/activities/ai-school/06-machine-learning/03-gradient-boosting/variable-importance.png?raw=true" alt=variable-importance></p><h2 id=feature-importance-references>Feature Importance References<a hidden class=anchor aria-hidden=true href=#feature-importance-references>#</a></h2><ul><li><a href=https://goo.gl/dvoMao target=_blank rel=noopener>Feature Importance Analysis (LIME)</a></li><li><a href=https://j.mp/3AVL6B9 target=_blank rel=noopener>Permutation importance 한글 설명</a></li><li><a href=https://j.mp/3um5M2F target=_blank rel=noopener>Permutation importance with Pipeline</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://minyeamer.github.io/tags/ai-school/>AI SCHOOL</a></li><li><a href=https://minyeamer.github.io/tags/%EB%A9%8B%EC%9F%81%EC%9D%B4%EC%82%AC%EC%9E%90%EC%B2%98%EB%9F%BC/>멋쟁이사자처럼</a></li><li><a href=https://minyeamer.github.io/tags/%EC%BD%94%EB%93%9C%EB%9D%BC%EC%9D%B4%EC%96%B8/>코드라이언</a></li><li><a href=https://minyeamer.github.io/tags/machine-learning/>Machine Learning</a></li><li><a href=https://minyeamer.github.io/tags/gradient-boosting/>Gradient Boosting</a></li></ul><nav class=paginav><a class=prev href=https://minyeamer.github.io/blog/aischool-06-04-knn/><span class=title>« Prev</span><br><span>[AI SCHOOL 5기] 머신 러닝 실습 - KNN</span></a>
<a class=next href=https://minyeamer.github.io/blog/aischool-06-02-logistic-regression/><span class=title>Next »</span><br><span>[AI SCHOOL 5기] 머신 러닝 실습 - 로지스틱 회귀</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share [AI SCHOOL 5기] 머신 러닝 실습 - Gradient Boosting on twitter" href="https://twitter.com/intent/tweet/?text=%5bAI%20SCHOOL%205%ea%b8%b0%5d%20%eb%a8%b8%ec%8b%a0%20%eb%9f%ac%eb%8b%9d%20%ec%8b%a4%ec%8a%b5%20-%20Gradient%20Boosting&url=https%3a%2f%2fminyeamer.github.io%2fblog%2faischool-06-03-gradient-boosting%2f&hashtags=AISCHOOL%2c%eb%a9%8b%ec%9f%81%ec%9d%b4%ec%82%ac%ec%9e%90%ec%b2%98%eb%9f%bc%2c%ec%bd%94%eb%93%9c%eb%9d%bc%ec%9d%b4%ec%96%b8%2cMachineLearning%2cGradientBoosting"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [AI SCHOOL 5기] 머신 러닝 실습 - Gradient Boosting on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fminyeamer.github.io%2fblog%2faischool-06-03-gradient-boosting%2f&title=%5bAI%20SCHOOL%205%ea%b8%b0%5d%20%eb%a8%b8%ec%8b%a0%20%eb%9f%ac%eb%8b%9d%20%ec%8b%a4%ec%8a%b5%20-%20Gradient%20Boosting&summary=%5bAI%20SCHOOL%205%ea%b8%b0%5d%20%eb%a8%b8%ec%8b%a0%20%eb%9f%ac%eb%8b%9d%20%ec%8b%a4%ec%8a%b5%20-%20Gradient%20Boosting&source=https%3a%2f%2fminyeamer.github.io%2fblog%2faischool-06-03-gradient-boosting%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [AI SCHOOL 5기] 머신 러닝 실습 - Gradient Boosting on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fminyeamer.github.io%2fblog%2faischool-06-03-gradient-boosting%2f&title=%5bAI%20SCHOOL%205%ea%b8%b0%5d%20%eb%a8%b8%ec%8b%a0%20%eb%9f%ac%eb%8b%9d%20%ec%8b%a4%ec%8a%b5%20-%20Gradient%20Boosting"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [AI SCHOOL 5기] 머신 러닝 실습 - Gradient Boosting on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fminyeamer.github.io%2fblog%2faischool-06-03-gradient-boosting%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [AI SCHOOL 5기] 머신 러닝 실습 - Gradient Boosting on whatsapp" href="https://api.whatsapp.com/send?text=%5bAI%20SCHOOL%205%ea%b8%b0%5d%20%eb%a8%b8%ec%8b%a0%20%eb%9f%ac%eb%8b%9d%20%ec%8b%a4%ec%8a%b5%20-%20Gradient%20Boosting%20-%20https%3a%2f%2fminyeamer.github.io%2fblog%2faischool-06-03-gradient-boosting%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [AI SCHOOL 5기] 머신 러닝 실습 - Gradient Boosting on telegram" href="https://telegram.me/share/url?text=%5bAI%20SCHOOL%205%ea%b8%b0%5d%20%eb%a8%b8%ec%8b%a0%20%eb%9f%ac%eb%8b%9d%20%ec%8b%a4%ec%8a%b5%20-%20Gradient%20Boosting&url=https%3a%2f%2fminyeamer.github.io%2fblog%2faischool-06-03-gradient-boosting%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=minyeamer/til issue-term=pathname label=comments theme=preferred-color-scheme crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://minyeamer.github.io/>Minystory</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>