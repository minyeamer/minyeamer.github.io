<!doctype html><html lang=ko-kr dir=ltr><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=keywords content="Apache Spark,Structured API,DataFrame,Dataset,PySpark,데이터 엔지니어링,스파크,Study"><meta name=description content="Apache Spark의 Structured API를 다루며, DataFrame과 Dataset의 기본 개념부터 Schema 정의, Column 연산, Row 처리까지 단계별로 안 …"><meta name=author content="minyeamer"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><link rel=canonical href=https://minyeamer.github.io/blog/spark-study-4/><link rel=icon href=https://minyeamer.github.io/images/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://minyeamer.github.io/images/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://minyeamer.github.io/images/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://minyeamer.github.io/images/favicons/apple-touch-icon.png><link rel=mask-icon href=https://minyeamer.github.io/images/favicons/apple-touch-icon.png><meta name=google-site-verification content="u1tWcHmHUZWfFT1cHaku6sqU-bK40N3WLR-C-4VUWN0"><meta name=naver-site-verification content="6eaf8e9da1a6104780f056f1a7797fe5a3a5a0da"><meta property="og:title" content="Apache Spark - DataFrame과 Dataset API 활용하기"><meta property="og:description" content="Apache Spark의 Structured API를 다루며, DataFrame과 Dataset의 기본 개념부터 Schema 정의, Column 연산, Row 처리까지 단계별로 안 …"><meta property="og:type" content="article"><meta property="og:url" content="https://minyeamer.github.io/blog/spark-study-4/"><meta property="og:image" content="https://dl.dropboxusercontent.com/scl/fi/iafnblb6k95kbw7bwn2xj/spark-00-cover.webp?rlkey=6995tacnu3mvr7s31akl5sca6&amp;dl=0"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-29T20:01:48+09:00"><meta property="article:modified_time" content="2025-06-29T20:01:48+09:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dl.dropboxusercontent.com/scl/fi/iafnblb6k95kbw7bwn2xj/spark-00-cover.webp?rlkey=6995tacnu3mvr7s31akl5sca6&amp;dl=0"><meta name=twitter:title content="Apache Spark - DataFrame과 Dataset API 활용하기"><meta name=twitter:description content="Apache Spark의 Structured API를 다루며, DataFrame과 Dataset의 기본 개념부터 Schema 정의, Column 연산, Row 처리까지 단계별로 안 …"><meta name=twitter:site content="@https://x.com/minyeamer"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://minyeamer.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Apache Spark - DataFrame과 Dataset API 활용하기","item":"https://minyeamer.github.io/blog/spark-study-4/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Apache Spark - DataFrame과 Dataset API 활용하기","name":"Apache Spark - DataFrame과 Dataset API 활용하기","description":"Apache Spark의 Structured API를 다루며, DataFrame과 Dataset의 기본 개념부터 Schema 정의, Column 연산, Row 처리까지 단계별로 안내합니다. 데이터 엔지니어링을 위한 효율적인 데이터 조작 기법을 배우고 실무에 적용하세요.\n","keywords":["Apache Spark","Structured API","DataFrame","Dataset","PySpark","데이터 엔지니어링","스파크","Study"],"articleBody":"Spark Structure # 정형화 API에 대해 알아보기에 앞서, 정형적 모델 이전의 RDD 프로그래밍 API 모델을 확인해본다.\nRDD # RDD는 Spark 1.x 버전에 있던 저수준의 DSL을 의미하고, 스파크에서 가장 기본적인 추상적인 부분이다. RDD에는 세 가지의 핵심으로 특성이 있다.\n의존성\n어떤 입력을 필요로 하고 RDD가 어떻게 만들어지는지 Spark에게 가르쳐 주는 의존성이 필요하다.\n파티션\nExecutor들에 작업을 분산해 파티션별로 병렬 연산할 수 있는 능력을 부여한다. 지역성 정보를 사용하여 각 Executor가 가까이 있는 Executor에게 우선적으로 작업을 보낸다.\n연산 함수\n파티션에 저장되는 데이터를 Iterator[T] 형태로 만들어준다. 하지만, Iterator[T] 데이터 타입이 파이썬 RDD에서 기본 객체로만 인식이 가능해 불투명했다. Spark가 함수에서 연산이나 표현식을 검사하지 못해 객체를 바이트 뭉치로 직렬화해 쓰는 것밖에 못했다. 이로 인해 연산 순서를 재정렬해 효과적인 질의 계획으로 바꾸기가 어려웠다.\nSpark DSL # Spark 2.x는 RDD의 한계를 극복하기 위해 고수준의 DSL을 도입했다. Spark DSL은 다음과 같은 네 가지 특징이 있다.\n도메인 특화 언어\nSpark DSL은 분산 데이터 처리와 분석에 최적화된 명령어와 함수를 제공하여, 대규모 데이터셋에 대한 복잡한 연산을 간결하게 표현할 수 있다.\n다중 언어 지원\nScala 언어 뿐 아니라, Java, Python, R 등 다양한 언어에서 Spark DSL의 기능을 사용할 수 있게 지원한다.\n함수형 프로그래밍 지원\n람다 함수, 고차 함수 등 함수형 프로그래밍 기법을 활용하여 Transformation 및 Action을 간결하게 구현할 수 있다.\nSQL 통합\nSpark SQ DSL을 통해 SQL 쿼리와 유사한 구문으로 DataFrame 및 Dataset을 조작할 수 있다.\n고수준 DSL을 통한 Spark 구조를 갖추면서 더 나은 성능과 공간 효율성 등 많은 이득을 얻을 수 있었다. DataFrame API나 Dataset API를 다루면서 표현성, 단순성, 구성 용이성, 통일성 등의 장점도 가지게 되었다.\n이름별로 모든 나이들을 모아서 그룹화하고, 나이의 평균을 구하는 예제를 저수준의 RDD API로 구현한다고 하면 다음과 같을 수 있다.\nCopy python dataRDD = sc.parallelize([ (\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), (\"Brooke\", 25)]) agesRDD = (dataRDD .map(lambda x: (x[0], (x[1], 1))) .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) .map(lambda x: x[0], x[1][0]/x[1][1])) 해당 코드를 Spark에게 쿼리를 계산하는 과정을 직접적으로 지시하여 의도가 전달되지 않는다. 동일한 질의를 Python의 고수준 DSL 연산자들과 DataFrame API를 사용하면 다음과 같다.\nCopy python from pyspark.sql import SparkSession from pyspark.sql.functions import avg # SparkSession 객체 생성 data_df = spark.createDataFrame( [(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), (\"Brooke\", 25)]) avg_df = data_df.groupBy(\"name\").agg(avg(\"age\")) avg_df.show() 고수준 DSL은 표현력이 높고 저수준 DSL보다 간단하다. Spark는 groupBy, avg 등의 연산자들을 통해 사용자의 의도를 이해하고 효과적인 실행을 위해 연산자들을 최적화하거나 적절하게 재배열할 수 있다.\n단순히 간단하기만 할 뿐 아니라 고수준 DSL은 언어 간에 일관성을 갖고 있다. 예를 들어 이름별로 나이의 평균을 집계하는 코드는 아래와 같다. 겉보기에도 똑같고 실제로 하는 일도 동일하다.\nCopy python # 파이썬 예제 avg_df = data_df.groupBy(\"name\").agg(avg(\"age\")) Copy kotlin // 스칼라 예제 val avgDf = dataDf.groupBy(\"name\").agg(avg(\"age\")) DataFrame API # pandas의 DataFrame에 영향을 받은 Spark DataFrame은 칼럼과 스키마를 가진 분산된 테이블처럼 동작하며, 각 칼럼은 정수, 문자열, 배열, 날짜 등 특정한 데이터 타입을 가질 수 있다.\n기본 데이터 타입 # 데이터 타입은 Spark Application에서 선언하거나, 스키마에서 정의할 수 있다. 먼저, Scala와 Python의 기본적인 데이터 타입은 아래와 같다.\n데이터 타입 스칼라에서 할당되는 값 파이썬에서 할당되는 값 ByteType Byte int ShortType Short int IntegerType Integer int LongType Long int FloatType Float float DoubleType Double float StringType String str BooleanType Boolean bool DecimalType java.math.BigDecimal decimal.Decimal 정형화 타입과 복합 타입 # 복합 데이터 분석을 위해서는 기본적인 데이터 타입을 사용하지 않는다. 대상 데이터는 맵, 배열, 구조체 등 자체적 구조를 갖고 있기 때문에, 이를 다루기 위한 타입을 지원한다.\n데이터 타입 스칼라에서 할당되는 값 파이썬에서 할당되는 값 BinaryType Array[Byte] bytearray TimestampType java.sqlTimestamp datetime.datetime DateType java.sql.Date datetime.date ArrayType scala.collection.Seq list, tuple, array 등 MapType scala.collection.Map dict StructType org.apache.spark.sql.Row list 또는 tuple StructField 해당 필드와 맞는 값의 타입 해당 필드와 맞는 값의 타입 Schema # 스키마는 DataFrame의 칼럼명과 데이터 타입을 정의한 것이다. 보통 외부 데이터 소스에서 구조화된 데이터를 읽어 들일 때 사용한다. 미리 스키마를 정의할 경우 두 가지 장점이 있다.\nSpark가 스키마를 추측하기 위해 파일을 읽어들이는 과정을 방지한다. 파일이 큰 경우, 비용과 시간을 절약할 수 있다. 데이터가 스키마와 맞지 않는 경우, 조기에 발견할 수 있다. 스키마 정의 # 스키마를 정의하는 방법은 두 가지가 있다.\n프로그래밍 스타일로 정의하는 것 Copy kotlin // 스칼라 예제 import org.apache.spark.sql.types._ val schema = StructType(Array( StructField(\"author\", StringType, false), StructField(\"title\", StringType, false), StructField(\"pages\", IntegerType, false))) Copy python # 파이썬 예제 from pyspark.sql.types import * schema = StructType([ StructField(\"author\", StringType(), False), StructField(\"title\", StringType(), False), StructField(\"pages\", IntegerType(), False)]) DDL(Data Definition Language)을 사용하는 것 Copy python schema = \"author STRING, title, STRING, pages INT\" 스키마 활용 (Python) # databricks/LearningSparkV2/chapter3 에서 스키마 활용 예제를 가져온다.\nCopy python # src/example_schema.py from pyspark.sql.types import * from pyspark.sql import SparkSession from pyspark.sql.functions import * # 프로그래밍 스타일로 스키마를 정의한다. schema = StructType([ StructField(\"Id\", IntegerType(), False), StructField(\"First\", StringType(), False), StructField(\"Last\", StringType(), False), StructField(\"Url\", StringType(), False), StructField(\"Published\", StringType(), False), StructField(\"Hits\", IntegerType(), False), StructField(\"Campaigns\", ArrayType(StringType()), False)]) # DDL을 사용해서 스키마를 정의할 수도 있다. # schema = \"'Id' INT, 'First', STRING, 'Last' STRING, 'Url' STRING, \" \\ # \"'Published' STRING, 'Hits' INT, 'Campaigns' ARRAY\" # 예제 데이터를 생성한다. data = [ [1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"]], [2, \"Brooke\",\"Wenig\",\"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]], [3, \"Denny\", \"Lee\", \"https://tinyurl.3\",\"6/7/2019\",7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]], [4, \"Tathagata\", \"Das\",\"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]], [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]], [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]] if __name__ == \"__main__\": spark = (SparkSession .builder .appName(\"Example-3_6\") .getOrCreate()) # 위에서 정의한 스키마로 DataFrame을 생성하고 상위 행을 출력한다. blogs_df = spark.createDataFrame(data, schema) blogs_df.show() # DataFrame 처리에 사용된 스키마를 출력한다. print(blogs_df.printSchema()) spark.stop() 예제 데이터에 대해 프로그래밍 스타일과 DDL을 사용하는, 두 가지 스타일로 스키마를 정의할 수 있다. DataFrame 생성 시 스키마를 전달하고, printSchema() 를 실행하여 어떤 스키마가 적용되었는지 출력해 볼 수 있다.\nspark-submit 에 예제 파일을 전달하면 아래와 같은 결과를 확인할 수 있다.\nCopy bash (spark) % spark-submit src/example_schema.py +---+---------+-------+-----------------+---------+-----+--------------------+ | Id| First| Last| Url|Published| Hits| Campaigns| +---+---------+-------+-----------------+---------+-----+--------------------+ | 1| Jules| Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]| | 2| Brooke| Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]| | 3| Denny| Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...| | 4|Tathagata| Das|https://tinyurl.4|5/12/2018|10568| [twitter, FB]| | 5| Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...| | 6| Reynold| Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]| +---+---------+-------+-----------------+---------+-----+--------------------+ root |-- Id: integer (nullable = false) |-- First: string (nullable = false) |-- Last: string (nullable = false) |-- Url: string (nullable = false) |-- Published: string (nullable = false) |-- Hits: integer (nullable = false) |-- Campaigns: array (nullable = false) | |-- element: string (containsNull = true) DataFrame에 할당된 스키마를 다른 곳에서 사용하고 싶다면, blogs_df.schema 와 같이 호출하여 스키마 객체를 반환할 수 있다. 스키마 객체는 스키마를 정의할 때 사용했던 것과 동일한 pyspark.sql.types.StructType 타입이다.\nScala를 사용하는 경우에도 Python과 동일하게 정의한 스키마를 JSON 파일을 읽는데 적용한다면 아래와 같이 표현할 수 있다.\nCopy kotlin // 스칼라 예제 val blogsDF = spark.read.schema(schema).json(jsonFile) Column # 칼럼은 pandas의 DataFrame과 유사하게 어떤 특정한 타입의 필드를 나타내는 개념이다. RDBMS를 다루는 것처럼 관계형 표현이나 계산식 형태의 표현식으로 칼럼 단위의 값들에 연산을 수행할 수 있다.\n칼럼명에 대해 expr(\"columnName * 5\") 같은 단순한 표현식으로 연산을 수행할 수 있다. 파이썬에서 expr() 은 pyspark.sql.functions 패키지에서 가져올 수 있다.\n표현식 활용 (Python) # 스키마 활용 예제에서 만든 blogs_df 객체를 사용한다.\nCopy python # src/example_schema.py from pyspark.sql.types import * from pyspark.sql import SparkSession from pyspark.sql.functions import * if __name__ == \"__main__\": # SparkSession 및 blogs_df 객체 생성 # 표현식을 사용해 값을 계산하고 결과를 출력한다. 모두 동일한 결과를 보여준다. blogs_df.select(expr(\"Hits\") * 2).show(2) blogs_df.select(col(\"Hits\") * 2).show(2) blogs_df.select(expr(\"Hits * 2\")).show(2) # 블로그 우수 방문자를 계산하고 결과를 출력한다. blogs_df.withColumn(\"Big Hitters\", (expr(\"Hits \u003e 10000\"))).show() spark-submit 에 예제 파일을 전달하면 아래와 같은 결과를 확인할 수 있다.\nCopy bash (spark) % spark-submit src/example_schema.py +----------+ |(Hits * 2)| +----------+ | 9070| | 17816| +----------+ only showing top 2 rows +---+---------+-------+-----------------+---------+-----+--------------------+-----------+ | Id| First| Last| Url|Published| Hits| Campaigns|Big Hitters| +---+---------+-------+-----------------+---------+-----+--------------------+-----------+ | 1| Jules| Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]| false| | 2| Brooke| Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]| false| | 3| Denny| Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...| false| | 4|Tathagata| Das|https://tinyurl.4|5/12/2018|10568| [twitter, FB]| true| | 5| Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...| true| | 6| Reynold| Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]| true| +---+---------+-------+-----------------+---------+-----+--------------------+-----------+ 첫 번째 표현식으로 계산한 결과는 모두 동일하여 하나만 출력했다. expr() 또는 col() 표현식으로 칼럼 연산을 수행할 수 있다.\nwithColumn() 을 호출하면 새로운 칼럼을 추가할 수 있다. 기존의 \"Hits\" 칼럼에 표현식을 사용해 블로그 우수 방문자를 분류하고, \"Big Hitters\" 라는 새로운 칼럼을 붙여서 출력했다.\nScala에서는 col() 대신에 칼럼명 앞에 $ 를 붙여서 Column 타입으로 변환할 수도 있다.\nCopy kotlin // \"Id\" 칼럼값에 따라 역순으로 정렬한다. blogsDF.sort(col(.desc).show() blogsDF.sort($\"Id\".desc).show() Row # Spark에서 하나의 행은 하나 이상의 칼럼을 갖고 있는 Row 객체로 표현된다. Row 객체에 속하는 칼럼들은 동일한 타입일 수도 있고 다른 타입일 수도 있다. Row는 순서가 있는 필드 집합 객체이므로 0부터 시작하는 인덱스로 접근한다.\nCopy python # 파이썬 예제 from pyspark.sql import Row blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\", [\"twitter\", \"LinedIn\"]) # 인덱스로 개별 값에 접근한다. blog_row[1] 'Reynold' Row 객체들을 DataFrame으로 만들 수 있다.\nCopy python # 파이썬 예제 rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")] authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"]) authors_df.show() DataFrame 작업 # 읽기/쓰기 # 데이터 소스에서 DataFrame으로 로드하기 위해 DataFrameReader 를 사용할 수 있다. JSON, CSV, Parquet, 텍스트, Avro, ORC 같은 다양한 포맷의 데이터 소스를 지원한다. 반대로 특정 포맷으로 DataFrame을 내보낼 때는 DataFrameWriter 를 사용할 수 있다.\nPython과 Scala에서 spark.read.csv() 함수로 CSV 파일을 읽을 수 있다.\nCopy python # 파이썬 예제 sf_fire_file = \"data/sf-fire-calls.csv\" fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema) Copy kotlin // 스칼라 예제 val sfFireFile = \"data/sf-fire-calls.csv\" val fireDF = spark.read.schema(fireSchema).option(\"header\", \"true\").csv(sfFireFile) DataFrame을 외부 데이터 소스에 내보내려면 DataFrame 객체가 가진 write() 메서드를 사용할 수 있다. 기본 포맷으로 인기있는 포맷은 칼럼 지향적인 Parquet 포맷이다. Parquet에는 스키마가 메타데이터에 들어있어 수동으로 스키마를 적용할 필요가 없다.\nCopy python # 파이썬 예제 fire_df.write.format(\"parquet\").save(parquet_path) Copy kotlin // 스칼라 예제 fireDF.write.format(\"parquet\").save(parquetPath) 프로젝션/필터 # 프로젝션은 필터를 이용해 특정 관계 상태와 매치되는 행들만 반환하는 방법이다. 프로젝션은 select(), 필터는 filter() 또는 where() 메서드로 표현된다.\nCopy python # 파이썬 예제 few_fire_df = (fire_df .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\") .where(col(\"CallType\") != \"Medical Incident\")) few_fire_df.show(5, truncate=False) 칼럼 변경 # 칼럼의 이름을 변경하거나 추가 또는 삭제하는 경우가 있다. 컬럼명을 변경할 때는 withColumnRenamed() 함수를 사용할 수 있다. 아래 예제는 \"Delay\" 칼럼의 명칭을 \"ResponseDelayedinMins\" 라고 변경한다.\nCopy python # 파이썬 예제 new_fire_df = fire_df.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\") (new_fire_df .select(\"ResponseDelayedinMins\") .where(col(\"ResponseDelayedinMins\") \u003e 5) .show(5, False)) 기존 칼럼을 가공해 새로운 칼럼을 만들 때는 withColumn() 메서드를 사용할 수 있다. 이때, spark.sql.functions 패키지에 있는 to_timestamp() 또는 to_date() 같은 함수들을 같이 사용할 수 있다. 가공된 칼럼을 추가한 후 필요하지 않은 칼럼을 제거하려면 drop() 메서드를 사용할 수 있다.\nCopy python # 파이썬 예제 fire_ts_df = (new_fire_df .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\")).drop(\"CallDate\") .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\")).drop(\"WatchDate\") .withColumn(\"AvailableDtTs\", to_timestamp(col(\"AvailableDtTm\"), \"MM/dd/yyyy hh:mm:ss\")).drop(\"AvailableDtTm\")) (fire_ts_df .select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTs\") .show(5, False)) 집계 연산 # groupBy(), orderBy(), count() 와 같은 Transformation 또는 Action을 사용하여 칼럼명을 가지고 집계할 수 있다. 아래 예제는 \"CallType\" 칼럼을 기준으로 행 개수를 세는 연산을 표현한다. 내림차순으로 정렬하여 가장 일반적인 신고 타입(CallType)을 확인할 수 있다.\nCopy python # 파이썬 예제 (fire_ts_df .select(\"CallType\") .where(col(\"CallType\").isNotNull()) .groupBy(\"CallType\") .count() .orderBy(\"count\", ascending=False) .show(n=10, truncate=False)) 집계 함수로는 min(), max(), sum(), avg() 등의 통계 함수들을 지원한다.\nCopy python # 파이썬 예제 import pyspark.sql.functions as F (fire_ts_df .select(F.sum(\"NumAlarms\"), F.avg(\"ResponseDelayedinMins\"), F.min(\"ResponseDelayedinMins\"), F.max(\"ResponseDelayedinMins\")) .show()) Dataset API # Dataset는 정적 타입 API와 동적 타입 API의 두 가지 특성을 모두 가진다.\nDataset # DataFrame은 Dataset[Row] 로 표현할 수 있다. Row는 서로 다른 타입의 값을 저장할 수 있는 JVM 객체다. 반면에 Dataset는 엄격하게 타입이 정해진 JVM 객체의 집합으로, Java의 클래스와 유사하다.\nDataset는 Java와 Scala에서만 사용할 수 있고, Python과 R에서는 DataFrame만 사용할 수 있다. 이것은 Python과 R이 컴파일 시 타입의 안전을 보장하는 언어가 아니기 때문이다. 반대로 Java는 컴파일 시점에 타입 안정성을 제공하기 때문에 Dataset만 사용할 수 있다. Scala는 DataFrame을 Dataset[Row] 로 표현하며, Dataset[T] 도 같이 사용할 수 있다.\nCase Class # DataFrame에서 스키마로 데이터 타입을 정의한느 것처럼, Scala에서 Dataset를 만들 때 스키마를 지정하기 위해 케이스 클래스를 사용할 수 있다. Java에서는 JavaBean 클래스를 쓸 수 있다.\n예제로, IoT 디바이스에서 JSON 파일을 읽어 들일 때 케이스 클래스를 아래와 같이 정의한다.\nCopy kotlin // 스칼라 예제 case class DeviceIoTData ( battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long, device_name: String, humidity: Long, ip: String, latitude: Double, longitude: Double, scale: String, temp: Long, timestamp: Long ) val ds = spark.read.json(\"iot_devices.json\").as[DeviceIoTData] Dataset는 DataFrame과 같은 연산이 가능하다. 예제로, filter() 에 함수를 인자로 전달하는 질의는 아래와 같다.\nCopy kotlin // 스칼라 예제 val filterTempDS = ds.filter(d =\u003e d.temp \u003e 30 \u0026\u0026 d.humidity \u003e 70) DataFrame vs Dataset # DataFrame과 Dataset을 사용 중 오류가 발생하는 시점을 정리하면 아래 표와 같다. Dataset가 DataFrame과 다른점은 컴파일 시점에 엄격한 타입 체크를 한다는 것이다. 반대로, SQL과 유사한 질의를 쓰는 관계형 변환을 필요로 한다면 DataFrame을 사용한다.\nSQL DataFrame Dataset 문법 오류 실행 시점 컴파일 시점 컴파일 시점 분석 오류 실행 시점 실행 시점 컴파일 시점 Spark SQL # Spark SQL은 고수준 정형화 기능들이 구축되도록 하는 방대한 엔진으로 진화해 왔다. Spark SQL 엔진은 다음과 같은 일을 한다.\n스파크 컴포넌트들을 통합하고 DataFrame/Dataset 관련 작업을 단순화할 수 있도록 추상화를 한다. 정형화된 파일 포맷(JSON, CSV 등)을 읽고 쓰며 데이터를 임시 테이블로 변환한다. 빠른 데이터 탐색을 위한 대화형 Spark SQL 쉘을 제공한다. JDBC/ODBC 커넥터를 통해 외부의 도구들과 연결할 수 있는 중간 역할을 한다. JVM을 위한 최적화된 코드를 생성한다. Catalyst Optimizer # Spark SQL 엔진의 핵심은 Catalyst Optimizer다. Catalyst Optimizer는 두 가지 목적으로 설계되었다.\nSpark SQL에 최적화 기법을 쉽게 추가한다. 개발자가 최적화 프로그램을 확장할 수 있도록 한다. 예시로, 데이터 소스별 규칙을 추가하거나 새로운 데이터 유형을 지원하는 것 등이 있다. Catalyst Optimizer는 연산 쿼리를 받아 실행 계획으로 변환한다. 그 과정은 아래 그림과 같이 4단계의 과정을 거친다.\n분석\n제공된 코드가 유효하고 오류가 없는지 확인한다. 칼럼, 데이터 타입, 함수, 테이블, 데이터베이스 이름 목록을 갖고 있는 Catalog 객체를 참조한다. 분석 단계를 성공적으로 통과하면 Spark에서 이해하고 해결할 수 있는 요소만이 포함되어 있다는 의미를 가진다.\n논리적 최적화\n표준적인 규칙을 기반으로 최적화 접근 방식을 적용하여 효율성을 향상시킨다. 최적화를 위한 여러 계획들을 수립하는데, 예를 들면 조건절 하부 배치, 칼럼 걸러내기, 부울 표현식 단순화 등이 포함된다. 논리 계획은 물리 계획 수립의 입력 데이터가 된다.\n물리 계획 수립\n논리 계획을 바탕으로 대응되는 물리적 연산자를 사용해 최적화된 물리 계획을 생성한다. CPU, 메모리, I/O 활용을 포함한 컴퓨팅 리소스 비용을 기반으로 실행 전략을 평가한다. 리소스 가용성을 기반으로 가장 비용이 적게 드는 계획을 선택한다.\n코드 생성\n물리 계획을 Java 바이트 코드로 변환한다. 최신 컴파일러 기술을 활용해 최적화된 바이트 코드를 생성한다. Spark가 JIT(Just-In-Time) 컴파일러처럼 작동하여 런타임 성능을 최적화하고 실행 속도를 크게 향상시킨다.\nReferences # https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ https://itwiki.kr/w/아파치_스파크_DSL https://github.com/databricks/LearningSparkV2 https://www.databricks.com/spark/getting-started-with-apache-spark/datasets https://www.databricks.com/glossary/catalyst-optimizer https://blog.det.life/apache-spark-sql-engine-and-query-planning-37cafb2b98f6 ","wordCount":"2272","inLanguage":"en","image":"https:\/\/dl.dropboxusercontent.com\/scl\/fi\/iafnblb6k95kbw7bwn2xj\/spark-00-cover.webp?rlkey=6995tacnu3mvr7s31akl5sca6\u0026dl=0","datePublished":"2025-06-29T20:01:48+09:00","dateModified":"2025-06-29T20:01:48+09:00","author":{"@type":"Person","name":"minyeamer"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://minyeamer.github.io/blog/spark-study-4/"},"publisher":{"@type":"Person","name":"Minystory","logo":{"@type":"ImageObject","url":"https://minyeamer.github.io/images/favicons/favicon.ico"}}}</script><title>Apache Spark - DataFrame과 Dataset API 활용하기 | Minystory</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://minyeamer.github.io/blog/spark-study-4/><link rel=stylesheet href=/book.min.da9f864e1bccfac13510edef0c8dbe217c58d1ba58855d698051f162d9101fc5.css integrity="sha256-2p+GThvM+sE1EO3vDI2+IXxY0bpYhV1pgFHxYtkQH8U=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/search-input.min.14cad5494ccc0535fda20cdf35667c523278c700c0725af04708c4c53af8f2e6.js integrity="sha256-FMrVSUzMBTX9ogzfNWZ8UjJ4xwDAclrwRwjExTr48uY=" crossorigin=anonymous></script><link rel=preload href=/search-data.min.125289a0655fe4ee106f0668f2f05784c72a361ded0fdb56cf6c84729eed4bb3.json as=fetch crossorigin><script>window.SEARCH_DATA_URL="/search-data.min.125289a0655fe4ee106f0668f2f05784c72a361ded0fdb56cf6c84729eed4bb3.json"</script><script defer src=/search.min.f30f9834d4764fd9751da64098c954d01085f648ba9ca421a3c97582f8c47253.js integrity="sha256-8w+YNNR2T9l1HaZAmMlU0BCF9ki6nKQho8l1gvjEclM=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-BJ8Z9RMBPJ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BJ8Z9RMBPJ")</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.1/css/all.min.css crossorigin=anonymous><script defer src=/scroll-progress.min.841ade7e507a5f6d59c4e7bf2fe2b2ca034070677ff7957eec55610a024dd776.js integrity="sha256-hBreflB6X21ZxOe/L+KyygNAcGd/95V+7FVhCgJN13Y=" crossorigin=anonymous></script><script defer src=/dark-mode.min.e41c6440ffd9967d6f6a419ff3ce09b862009fe1646ab265f5cb2817d2a508e3.js integrity="sha256-5BxkQP/Zln1vakGf884JuGIAn+FkarJl9csoF9KlCOM=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlightjs-line-numbers.js/2.9.0/highlightjs-line-numbers.min.js></script><script defer src=/copy-code.min.aaeef965f0b4992e55f976edaecb34a89d414e1791caa18c3f4f4376c6d8b5a8.js integrity="sha256-qu75ZfC0mS5V+Xbtrss0qJ1BTheRyqGMP09DdsbYtag=" crossorigin=anonymous></script><script defer src=/toc-highlightjs.093016f0ef312174ad862fdcf5792e88ab5442bd39beecc38d15643f71ab5c31.min integrity="sha256-CTAW8O8xIXSthi/c9XkuiKtUQr05vuzDjRVkP3GrXDE=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-posts book-layout-post"><div class=scroll-progress><div class=scroll-progress-bar></div></div><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><div class=sidebar-profile><div class=profile-img-wrap><a href=https://minyeamer.github.io/><img src=/images/profile/menu.jpg alt=Profile class=profile-img></a></div><div class=sidebar-social><a href=https://github.com/minyeamer target=_blank title=GitHub><i class="fa-brands fa-github"></i>
</a><a href=/categories/ title=Categories><i class="fa-solid fa-folder"></i>
</a><a href=/tags/ title=Tags><i class="fa-solid fa-tags"></i>
</a><button id=dark-mode-toggle class=dark-mode-toggle aria-label="Toggle dark mode">
<i class="fa-solid fa-circle-half-stroke"></i></button></div></div><h2 class=book-brand><a class="flex align-center" href=/><span>Minystory</span></a></h2><div class="book-search hidden"><div class=search-input-container><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/ onkeydown='event.key==="Enter"&&goToSearchPage()'>
<button type=button id=book-search-button class=book-search-btn onclick=goToSearchPage()>
<i class="fa-solid fa-magnifying-glass"></i></button></div><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden");function goToSearchPage(){const t=document.getElementById("book-search-input"),e=t.value.trim();e&&(window.location.href="/search/?q="+encodeURIComponent(e))}</script><div class=book-categories><input type=checkbox class="hidden toggle" id=categories-control checked>
<label for=categories-control class="categories-toggle categories-link"><a href=/categories/><i class="fa-solid fa-folder"></i>
<span>전체</span>
<span class=category-count>(36)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul class=categories-menu id=categories-menu><li><input type=checkbox class="hidden toggle" id=cat-algorithm>
<label for=cat-algorithm class="categories-toggle categories-link"><a href=/categories/algorithm/><i class="fa-solid fa-folder"></i>
Algorithm
<span class=category-count>(3)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/algorithm/graph/><i class="fa-solid fa-file"></i>
Graph
<span class=category-count>(1)</span></a></li><li class=categories-link><a href=/categories/algorithm/python/><i class="fa-solid fa-file"></i>
Python
<span class=category-count>(1)</span></a></li><li class=categories-link><a href=/categories/algorithm/sql/><i class="fa-solid fa-file"></i>
SQL
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-cloud>
<label for=cat-cloud class="categories-toggle categories-link"><a href=/categories/cloud/><i class="fa-solid fa-folder"></i>
Cloud
<span class=category-count>(2)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/cloud/kubernetes/><i class="fa-solid fa-file"></i>
Kubernetes
<span class=category-count>(2)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-data-analysis>
<label for=cat-data-analysis class="categories-toggle categories-link"><a href=/categories/data-analysis/><i class="fa-solid fa-folder"></i>
Data Analysis
<span class=category-count>(2)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/data-analysis/dacon/><i class="fa-solid fa-file"></i>
Dacon
<span class=category-count>(2)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-data-engineering>
<label for=cat-data-engineering class="categories-toggle categories-link"><a href=/categories/data-engineering/><i class="fa-solid fa-folder"></i>
Data Engineering
<span class=category-count>(19)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/data-engineering/apache-airflow/><i class="fa-solid fa-file"></i>
Apache Airflow
<span class=category-count>(7)</span></a></li><li class=categories-link><a href=/categories/data-engineering/apache-spark/><i class="fa-solid fa-file"></i>
Apache Spark
<span class=category-count>(8)</span></a></li><li class=categories-link><a href=/categories/data-engineering/crawling/><i class="fa-solid fa-file"></i>
Crawling
<span class=category-count>(4)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-frontend>
<label for=cat-frontend class="categories-toggle categories-link"><a href=/categories/frontend/><i class="fa-solid fa-folder"></i>
Frontend
<span class=category-count>(7)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/frontend/blog/><i class="fa-solid fa-file"></i>
Blog
<span class=category-count>(7)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-linux>
<label for=cat-linux class="categories-toggle categories-link"><a href=/categories/linux/><i class="fa-solid fa-folder"></i>
Linux
<span class=category-count>(1)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/linux/ubuntu/><i class="fa-solid fa-file"></i>
Ubuntu
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-project>
<label for=cat-project class="categories-toggle categories-link"><a href=/categories/project/><i class="fa-solid fa-folder"></i>
Project
<span class=category-count>(2)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/project/open-source/><i class="fa-solid fa-file"></i>
Open Source
<span class=category-count>(1)</span></a></li><li class=categories-link><a href=/categories/project/tools/><i class="fa-solid fa-file"></i>
Tools
<span class=category-count>(1)</span></a></li></ul></li></ul></div><div class=recent-posts><div class=recent-posts-header><i class="fa-solid fa-clock"></i>
<span>최신글</span></div><ul class=recent-posts-list><li class=recent-post-item><a href=/blog/hugo-blog-3/ title="Hugo 블로그 만들기 (3) - Taxonomies로 태그/카테고리 페이지 커스터마이징"><div class=recent-post-title>Hugo 블로그 만들기 (3) - Taxonomies로 태그/카테고리 페이지 커스터마이징</div><div class=recent-post-date><time datetime=2025-11-22>2025.11.22</time></div></a></li><li class=recent-post-item><a href=/blog/hugo-blog-2/ title="Hugo 블로그 만들기 (2) - 메인 레이아웃 커스터마이징 (메뉴, 목차, 헤더)"><div class=recent-post-title>Hugo 블로그 만들기 (2) - 메인 레이아웃 커스터마이징 (메뉴, 목차, 헤더)</div><div class=recent-post-date><time datetime=2025-11-04>2025.11.04</time></div></a></li><li class=recent-post-item><a href=/blog/hugo-blog-1/ title="Hugo 블로그 만들기 (1) - 프로젝트 구성과 GitHub Pages 배포 (Submodule 활용)"><div class=recent-post-title>Hugo 블로그 만들기 (1) - 프로젝트 구성과 GitHub Pages 배포 (Submodule 활용)</div><div class=recent-post-date><time datetime=2025-11-01>2025.11.01</time></div></a></li><li class=recent-post-item><a href=/blog/openup-handson/ title="[OSSCA] 2025 오픈소스 컨트리뷰션 아카데미 - PyTorch 문서 한글화 참여 후기"><div class=recent-post-title>[OSSCA] 2025 오픈소스 컨트리뷰션 아카데미 - PyTorch 문서 한글화 참여 후기</div><div class=recent-post-date><time datetime=2025-10-28>2025.10.28</time></div></a></li><li class=recent-post-item><a href=/blog/uv-project/ title="[Python] uv로 프로젝트 구성하고 PyPI 배포하기 - Rust 기반 고속 패키지 관리"><div class=recent-post-title>[Python] uv로 프로젝트 구성하고 PyPI 배포하기 - Rust 기반 고속 패키지 관리</div><div class=recent-post-date><time datetime=2025-07-23>2025.07.23</time></div></a></li></ul></div></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><i class="fa-solid fa-bars book-icon" id=menu-icon></i></label><h3><a href=https://minyeamer.github.io/ class=site-title>Minystory</a></h3><label for=toc-control><i class="fa-solid fa-list book-icon" id=toc-icon></i></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#spark-structure>Spark Structure</a><ul><li><a href=#rdd>RDD</a></li><li><a href=#spark-dsl>Spark DSL</a></li></ul></li><li><a href=#dataframe-api>DataFrame API</a><ul><li><a href=#기본-데이터-타입>기본 데이터 타입</a></li><li><a href=#정형화-타입과-복합-타입>정형화 타입과 복합 타입</a></li></ul></li><li><a href=#schema>Schema</a><ul><li><a href=#스키마-정의>스키마 정의</a></li><li><a href=#스키마-활용-python>스키마 활용 (Python)</a></li></ul></li><li><a href=#column>Column</a><ul><li><a href=#표현식-활용-python>표현식 활용 (Python)</a></li></ul></li><li><a href=#row>Row</a></li><li><a href=#dataframe-작업>DataFrame 작업</a><ul><li><a href=#읽기쓰기>읽기/쓰기</a></li><li><a href=#프로젝션필터>프로젝션/필터</a></li><li><a href=#칼럼-변경>칼럼 변경</a></li><li><a href=#집계-연산>집계 연산</a></li></ul></li><li><a href=#dataset-api>Dataset API</a><ul><li><a href=#dataset>Dataset</a></li><li><a href=#case-class>Case Class</a></li><li><a href=#dataframe-vs-dataset>DataFrame vs Dataset</a></li></ul></li><li><a href=#spark-sql>Spark SQL</a><ul><li><a href=#catalyst-optimizer>Catalyst Optimizer</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></aside></header><article class="markdown book-article"><header class=post-header><div class=post-header-category><a href=/categories/data-engineering/apache-spark/ class=post-header-category-link>Data Engineering/Apache Spark</a></div><h1 class=post-header-title>Apache Spark - DataFrame과 Dataset API 활용하기</h1><div class=post-header-date><time datetime=2025-06-29T20:01:48+09:00>2025. 6. 29. 20:01</time></div></header><div class=book-cover><img src="https://dl.dropboxusercontent.com/scl/fi/iafnblb6k95kbw7bwn2xj/spark-00-cover.webp?rlkey=6995tacnu3mvr7s31akl5sca6&amp;dl=0" alt="Cover Image" class=book-cover-img></div><h2 id=spark-structure>Spark Structure
<a class=anchor href=#spark-structure>#</a></h2><p>정형화 API에 대해 알아보기에 앞서, 정형적 모델 이전의 RDD 프로그래밍 API 모델을 확인해본다.</p><h3 id=rdd>RDD
<a class=anchor href=#rdd>#</a></h3><p>RDD는 Spark 1.x 버전에 있던 저수준의 DSL을 의미하고, 스파크에서 가장 기본적인 추상적인 부분이다.
RDD에는 세 가지의 핵심으로 특성이 있다.</p><ol><li><p>의존성<br>어떤 입력을 필요로 하고 RDD가 어떻게 만들어지는지 Spark에게 가르쳐 주는 의존성이 필요하다.</p></li><li><p>파티션<br>Executor들에 작업을 분산해 파티션별로 병렬 연산할 수 있는 능력을 부여한다. 지역성 정보를 사용하여
각 Executor가 가까이 있는 Executor에게 우선적으로 작업을 보낸다.</p></li><li><p>연산 함수<br>파티션에 저장되는 데이터를 <code>Iterator[T]</code> 형태로 만들어준다. 하지만, <code>Iterator[T]</code> 데이터 타입이
파이썬 RDD에서 기본 객체로만 인식이 가능해 불투명했다. Spark가 함수에서 연산이나 표현식을 검사하지 못해
객체를 바이트 뭉치로 직렬화해 쓰는 것밖에 못했다. 이로 인해 연산 순서를 재정렬해 효과적인 질의 계획으로 바꾸기가 어려웠다.</p></li></ol><h3 id=spark-dsl>Spark DSL
<a class=anchor href=#spark-dsl>#</a></h3><p>Spark 2.x는 RDD의 한계를 극복하기 위해 고수준의 DSL을 도입했다. Spark DSL은 다음과 같은 네 가지 특징이 있다.</p><ol><li><p>도메인 특화 언어<br>Spark DSL은 분산 데이터 처리와 분석에 최적화된 명령어와 함수를 제공하여,
대규모 데이터셋에 대한 복잡한 연산을 간결하게 표현할 수 있다.</p></li><li><p>다중 언어 지원<br>Scala 언어 뿐 아니라, Java, Python, R 등 다양한 언어에서 Spark DSL의 기능을 사용할 수 있게 지원한다.</p></li><li><p>함수형 프로그래밍 지원<br>람다 함수, 고차 함수 등 함수형 프로그래밍 기법을 활용하여 Transformation 및 Action을 간결하게 구현할 수 있다.</p></li><li><p>SQL 통합<br>Spark SQ DSL을 통해 SQL 쿼리와 유사한 구문으로 DataFrame 및 Dataset을 조작할 수 있다.</p></li></ol><p>고수준 DSL을 통한 Spark 구조를 갖추면서 더 나은 성능과 공간 효율성 등 많은 이득을 얻을 수 있었다.
DataFrame API나 Dataset API를 다루면서 표현성, 단순성, 구성 용이성, 통일성 등의 장점도 가지게 되었다.</p><p>이름별로 모든 나이들을 모아서 그룹화하고, 나이의 평균을 구하는 예제를 저수준의 RDD API로 구현한다고 하면 다음과 같을 수 있다.</p><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>dataRDD</span> <span class=o>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>parallelize</span><span class=p>([</span>
</span></span><span class=line><span class=cl>	<span class=p>(</span><span class=s2>&#34;Brooke&#34;</span><span class=p>,</span> <span class=mi>20</span><span class=p>),</span> <span class=p>(</span><span class=s2>&#34;Denny&#34;</span><span class=p>,</span> <span class=mi>31</span><span class=p>),</span> <span class=p>(</span><span class=s2>&#34;Jules&#34;</span><span class=p>,</span> <span class=mi>30</span><span class=p>),</span> <span class=p>(</span><span class=s2>&#34;TD&#34;</span><span class=p>,</span> <span class=mi>35</span><span class=p>),</span> <span class=p>(</span><span class=s2>&#34;Brooke&#34;</span><span class=p>,</span> <span class=mi>25</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>agesRDD</span> <span class=o>=</span> <span class=p>(</span><span class=n>dataRDD</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=p>(</span><span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=p>(</span><span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=mi>1</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>reduceByKey</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>:</span> <span class=p>(</span><span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>+</span> <span class=n>y</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>y</span><span class=p>[</span><span class=mi>1</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span><span class=o>/</span><span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=mi>1</span><span class=p>]))</span></span></span></code></pre></div></div><p>해당 코드를 Spark에게 쿼리를 계산하는 과정을 직접적으로 지시하여 의도가 전달되지 않는다.
동일한 질의를 Python의 고수준 DSL 연산자들과 DataFrame API를 사용하면 다음과 같다.</p><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>SparkSession</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql.functions</span> <span class=kn>import</span> <span class=n>avg</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># SparkSession 객체 생성</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>data_df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=p>[(</span><span class=s2>&#34;Brooke&#34;</span><span class=p>,</span> <span class=mi>20</span><span class=p>),</span> <span class=p>(</span><span class=s2>&#34;Denny&#34;</span><span class=p>,</span> <span class=mi>31</span><span class=p>),</span> <span class=p>(</span><span class=s2>&#34;Jules&#34;</span><span class=p>,</span> <span class=mi>30</span><span class=p>),</span> <span class=p>(</span><span class=s2>&#34;TD&#34;</span><span class=p>,</span> <span class=mi>35</span><span class=p>),</span> <span class=p>(</span><span class=s2>&#34;Brooke&#34;</span><span class=p>,</span> <span class=mi>25</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>avg_df</span> <span class=o>=</span> <span class=n>data_df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>&#34;name&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>agg</span><span class=p>(</span><span class=n>avg</span><span class=p>(</span><span class=s2>&#34;age&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>avg_df</span><span class=o>.</span><span class=n>show</span><span class=p>()</span></span></span></code></pre></div></div><p>고수준 DSL은 표현력이 높고 저수준 DSL보다 간단하다. Spark는 <code>groupBy</code>, <code>avg</code> 등의
연산자들을 통해 사용자의 의도를 이해하고 효과적인 실행을 위해 연산자들을 최적화하거나 적절하게 재배열할 수 있다.</p><p>단순히 간단하기만 할 뿐 아니라 고수준 DSL은 언어 간에 일관성을 갖고 있다.
예를 들어 이름별로 나이의 평균을 집계하는 코드는 아래와 같다. 겉보기에도 똑같고 실제로 하는 일도 동일하다.</p><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 파이썬 예제</span>
</span></span><span class=line><span class=cl><span class=n>avg_df</span> <span class=o>=</span> <span class=n>data_df</span><span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>&#34;name&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>agg</span><span class=p>(</span><span class=n>avg</span><span class=p>(</span><span class=s2>&#34;age&#34;</span><span class=p>))</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=kotlin><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">kotlin</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-kotlin data-lang=kotlin><span class=line><span class=cl><span class=c1>// 스칼라 예제
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=py>avgDf</span> <span class=p>=</span> <span class=n>dataDf</span><span class=p>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>&#34;name&#34;</span><span class=p>).</span><span class=n>agg</span><span class=p>(</span><span class=n>avg</span><span class=p>(</span><span class=s2>&#34;age&#34;</span><span class=p>))</span></span></span></code></pre></div></div><h2 id=dataframe-api>DataFrame API
<a class=anchor href=#dataframe-api>#</a></h2><p>pandas의 DataFrame에 영향을 받은 Spark DataFrame은 칼럼과 스키마를 가진 분산된 테이블처럼 동작하며,
각 칼럼은 정수, 문자열, 배열, 날짜 등 특정한 데이터 타입을 가질 수 있다.</p><h3 id=기본-데이터-타입>기본 데이터 타입
<a class=anchor href=#%ea%b8%b0%eb%b3%b8-%eb%8d%b0%ec%9d%b4%ed%84%b0-%ed%83%80%ec%9e%85>#</a></h3><p>데이터 타입은 Spark Application에서 선언하거나, 스키마에서 정의할 수 있다.
먼저, Scala와 Python의 기본적인 데이터 타입은 아래와 같다.</p><table><thead><tr><th>데이터 타입</th><th>스칼라에서 할당되는 값</th><th>파이썬에서 할당되는 값</th></tr></thead><tbody><tr><td>ByteType</td><td>Byte</td><td>int</td></tr><tr><td>ShortType</td><td>Short</td><td>int</td></tr><tr><td>IntegerType</td><td>Integer</td><td>int</td></tr><tr><td>LongType</td><td>Long</td><td>int</td></tr><tr><td>FloatType</td><td>Float</td><td>float</td></tr><tr><td>DoubleType</td><td>Double</td><td>float</td></tr><tr><td>StringType</td><td>String</td><td>str</td></tr><tr><td>BooleanType</td><td>Boolean</td><td>bool</td></tr><tr><td>DecimalType</td><td>java.math.BigDecimal</td><td>decimal.Decimal</td></tr></tbody></table><h3 id=정형화-타입과-복합-타입>정형화 타입과 복합 타입
<a class=anchor href=#%ec%a0%95%ed%98%95%ed%99%94-%ed%83%80%ec%9e%85%ea%b3%bc-%eb%b3%b5%ed%95%a9-%ed%83%80%ec%9e%85>#</a></h3><p>복합 데이터 분석을 위해서는 기본적인 데이터 타입을 사용하지 않는다. 대상 데이터는
맵, 배열, 구조체 등 자체적 구조를 갖고 있기 때문에, 이를 다루기 위한 타입을 지원한다.</p><table><thead><tr><th>데이터 타입</th><th>스칼라에서 할당되는 값</th><th>파이썬에서 할당되는 값</th></tr></thead><tbody><tr><td>BinaryType</td><td>Array[Byte]</td><td>bytearray</td></tr><tr><td>TimestampType</td><td>java.sqlTimestamp</td><td>datetime.datetime</td></tr><tr><td>DateType</td><td>java.sql.Date</td><td>datetime.date</td></tr><tr><td>ArrayType</td><td>scala.collection.Seq</td><td>list, tuple, array 등</td></tr><tr><td>MapType</td><td>scala.collection.Map</td><td>dict</td></tr><tr><td>StructType</td><td>org.apache.spark.sql.Row</td><td>list 또는 tuple</td></tr><tr><td>StructField</td><td>해당 필드와 맞는 값의 타입</td><td>해당 필드와 맞는 값의 타입</td></tr></tbody></table><h2 id=schema>Schema
<a class=anchor href=#schema>#</a></h2><p>스키마는 DataFrame의 칼럼명과 데이터 타입을 정의한 것이다. 보통 외부 데이터 소스에서
구조화된 데이터를 읽어 들일 때 사용한다. 미리 스키마를 정의할 경우 두 가지 장점이 있다.</p><ol><li>Spark가 스키마를 추측하기 위해 파일을 읽어들이는 과정을 방지한다. 파일이 큰 경우, 비용과 시간을 절약할 수 있다.</li><li>데이터가 스키마와 맞지 않는 경우, 조기에 발견할 수 있다.</li></ol><h3 id=스키마-정의>스키마 정의
<a class=anchor href=#%ec%8a%a4%ed%82%a4%eb%a7%88-%ec%a0%95%ec%9d%98>#</a></h3><p>스키마를 정의하는 방법은 두 가지가 있다.</p><ol><li>프로그래밍 스타일로 정의하는 것</li></ol><div class=book-codeblock data-lang=kotlin><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">kotlin</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-kotlin data-lang=kotlin><span class=line><span class=cl><span class=c1>// 스칼라 예제
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>import</span> <span class=nn>org.apache.spark.sql.types._</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=py>schema</span> <span class=p>=</span> <span class=n>StructType</span><span class=p>(</span><span class=n>Array</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>StructField</span><span class=p>(</span><span class=s2>&#34;author&#34;</span><span class=p>,</span> <span class=n>StringType</span><span class=p>,</span> <span class=k>false</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>StructField</span><span class=p>(</span><span class=s2>&#34;title&#34;</span><span class=p>,</span> <span class=n>StringType</span><span class=p>,</span> <span class=k>false</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>StructField</span><span class=p>(</span><span class=s2>&#34;pages&#34;</span><span class=p>,</span> <span class=n>IntegerType</span><span class=p>,</span> <span class=k>false</span><span class=p>)))</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 파이썬 예제</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql.types</span> <span class=kn>import</span> <span class=o>*</span>
</span></span><span class=line><span class=cl><span class=n>schema</span> <span class=o>=</span> <span class=n>StructType</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=n>StructField</span><span class=p>(</span><span class=s2>&#34;author&#34;</span><span class=p>,</span> <span class=n>StringType</span><span class=p>(),</span> <span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>StructField</span><span class=p>(</span><span class=s2>&#34;title&#34;</span><span class=p>,</span> <span class=n>StringType</span><span class=p>(),</span> <span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>StructField</span><span class=p>(</span><span class=s2>&#34;pages&#34;</span><span class=p>,</span> <span class=n>IntegerType</span><span class=p>(),</span> <span class=kc>False</span><span class=p>)])</span></span></span></code></pre></div></div><ol start=2><li>DDL(Data Definition Language)을 사용하는 것</li></ol><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>schema</span> <span class=o>=</span> <span class=s2>&#34;author STRING, title, STRING, pages INT&#34;</span></span></span></code></pre></div></div><h3 id=스키마-활용-python>스키마 활용 (Python)
<a class=anchor href=#%ec%8a%a4%ed%82%a4%eb%a7%88-%ed%99%9c%ec%9a%a9-python>#</a></h3><p><a href=https://github.com/databricks/LearningSparkV2/tree/master/chapter3/py/src target=_blank rel="noopener noreferrer">databricks/LearningSparkV2/chapter3</a>
에서 스키마 활용 예제를 가져온다.</p><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># src/example_schema.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql.types</span> <span class=kn>import</span> <span class=o>*</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>SparkSession</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql.functions</span> <span class=kn>import</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 프로그래밍 스타일로 스키마를 정의한다.</span>
</span></span><span class=line><span class=cl><span class=n>schema</span> <span class=o>=</span> <span class=n>StructType</span><span class=p>([</span>
</span></span><span class=line><span class=cl>    <span class=n>StructField</span><span class=p>(</span><span class=s2>&#34;Id&#34;</span><span class=p>,</span> <span class=n>IntegerType</span><span class=p>(),</span> <span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>StructField</span><span class=p>(</span><span class=s2>&#34;First&#34;</span><span class=p>,</span> <span class=n>StringType</span><span class=p>(),</span> <span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>StructField</span><span class=p>(</span><span class=s2>&#34;Last&#34;</span><span class=p>,</span> <span class=n>StringType</span><span class=p>(),</span> <span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>StructField</span><span class=p>(</span><span class=s2>&#34;Url&#34;</span><span class=p>,</span> <span class=n>StringType</span><span class=p>(),</span> <span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>StructField</span><span class=p>(</span><span class=s2>&#34;Published&#34;</span><span class=p>,</span> <span class=n>StringType</span><span class=p>(),</span> <span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>StructField</span><span class=p>(</span><span class=s2>&#34;Hits&#34;</span><span class=p>,</span> <span class=n>IntegerType</span><span class=p>(),</span> <span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>StructField</span><span class=p>(</span><span class=s2>&#34;Campaigns&#34;</span><span class=p>,</span> <span class=n>ArrayType</span><span class=p>(</span><span class=n>StringType</span><span class=p>()),</span> <span class=kc>False</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># DDL을 사용해서 스키마를 정의할 수도 있다.</span>
</span></span><span class=line><span class=cl><span class=c1># schema = &#34;&#39;Id&#39; INT, &#39;First&#39;, STRING, &#39;Last&#39; STRING, &#39;Url&#39; STRING, &#34; \</span>
</span></span><span class=line><span class=cl><span class=c1>#         &#34;&#39;Published&#39; STRING, &#39;Hits&#39; INT, &#39;Campaigns&#39; ARRAY&lt;STRING&gt;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 예제 데이터를 생성한다.</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=s2>&#34;Jules&#34;</span><span class=p>,</span> <span class=s2>&#34;Damji&#34;</span><span class=p>,</span> <span class=s2>&#34;https://tinyurl.1&#34;</span><span class=p>,</span> <span class=s2>&#34;1/4/2016&#34;</span><span class=p>,</span> <span class=mi>4535</span><span class=p>,</span> <span class=p>[</span><span class=s2>&#34;twitter&#34;</span><span class=p>,</span> <span class=s2>&#34;LinkedIn&#34;</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=s2>&#34;Brooke&#34;</span><span class=p>,</span><span class=s2>&#34;Wenig&#34;</span><span class=p>,</span><span class=s2>&#34;https://tinyurl.2&#34;</span><span class=p>,</span> <span class=s2>&#34;5/5/2018&#34;</span><span class=p>,</span> <span class=mi>8908</span><span class=p>,</span> <span class=p>[</span><span class=s2>&#34;twitter&#34;</span><span class=p>,</span> <span class=s2>&#34;LinkedIn&#34;</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=s2>&#34;Denny&#34;</span><span class=p>,</span> <span class=s2>&#34;Lee&#34;</span><span class=p>,</span> <span class=s2>&#34;https://tinyurl.3&#34;</span><span class=p>,</span><span class=s2>&#34;6/7/2019&#34;</span><span class=p>,</span><span class=mi>7659</span><span class=p>,</span> <span class=p>[</span><span class=s2>&#34;web&#34;</span><span class=p>,</span> <span class=s2>&#34;twitter&#34;</span><span class=p>,</span> <span class=s2>&#34;FB&#34;</span><span class=p>,</span> <span class=s2>&#34;LinkedIn&#34;</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=s2>&#34;Tathagata&#34;</span><span class=p>,</span> <span class=s2>&#34;Das&#34;</span><span class=p>,</span><span class=s2>&#34;https://tinyurl.4&#34;</span><span class=p>,</span> <span class=s2>&#34;5/12/2018&#34;</span><span class=p>,</span> <span class=mi>10568</span><span class=p>,</span> <span class=p>[</span><span class=s2>&#34;twitter&#34;</span><span class=p>,</span> <span class=s2>&#34;FB&#34;</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>5</span><span class=p>,</span> <span class=s2>&#34;Matei&#34;</span><span class=p>,</span><span class=s2>&#34;Zaharia&#34;</span><span class=p>,</span> <span class=s2>&#34;https://tinyurl.5&#34;</span><span class=p>,</span> <span class=s2>&#34;5/14/2014&#34;</span><span class=p>,</span> <span class=mi>40578</span><span class=p>,</span> <span class=p>[</span><span class=s2>&#34;web&#34;</span><span class=p>,</span> <span class=s2>&#34;twitter&#34;</span><span class=p>,</span> <span class=s2>&#34;FB&#34;</span><span class=p>,</span> <span class=s2>&#34;LinkedIn&#34;</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=mi>6</span><span class=p>,</span> <span class=s2>&#34;Reynold&#34;</span><span class=p>,</span> <span class=s2>&#34;Xin&#34;</span><span class=p>,</span> <span class=s2>&#34;https://tinyurl.6&#34;</span><span class=p>,</span> <span class=s2>&#34;3/2/2015&#34;</span><span class=p>,</span> <span class=mi>25568</span><span class=p>,</span> <span class=p>[</span><span class=s2>&#34;twitter&#34;</span><span class=p>,</span> <span class=s2>&#34;LinkedIn&#34;</span><span class=p>]]]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>spark</span> <span class=o>=</span> <span class=p>(</span><span class=n>SparkSession</span>
</span></span><span class=line><span class=cl>        <span class=o>.</span><span class=n>builder</span>
</span></span><span class=line><span class=cl>        <span class=o>.</span><span class=n>appName</span><span class=p>(</span><span class=s2>&#34;Example-3_6&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=o>.</span><span class=n>getOrCreate</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 위에서 정의한 스키마로 DataFrame을 생성하고 상위 행을 출력한다.</span>
</span></span><span class=line><span class=cl>    <span class=n>blogs_df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>schema</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>blogs_df</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># DataFrame 처리에 사용된 스키마를 출력한다.</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>blogs_df</span><span class=o>.</span><span class=n>printSchema</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>spark</span><span class=o>.</span><span class=n>stop</span><span class=p>()</span></span></span></code></pre></div></div><p>예제 데이터에 대해 프로그래밍 스타일과 DDL을 사용하는, 두 가지 스타일로 스키마를 정의할 수 있다.
DataFrame 생성 시 스키마를 전달하고, <code>printSchema()</code> 를 실행하여 어떤 스키마가 적용되었는지 출력해 볼 수 있다.</p><p><code>spark-submit</code> 에 예제 파일을 전달하면 아래와 같은 결과를 확인할 수 있다.</p><div class=book-codeblock data-lang=bash><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>(</span>spark<span class=o>)</span> % spark-submit src/example_schema.py
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>+---+---------+-------+-----------------+---------+-----+--------------------+
</span></span><span class=line><span class=cl><span class=p>|</span> Id<span class=p>|</span>    First<span class=p>|</span>   Last<span class=p>|</span>              Url<span class=p>|</span>Published<span class=p>|</span> Hits<span class=p>|</span>           Campaigns<span class=p>|</span>
</span></span><span class=line><span class=cl>+---+---------+-------+-----------------+---------+-----+--------------------+
</span></span><span class=line><span class=cl><span class=p>|</span>  1<span class=p>|</span>    Jules<span class=p>|</span>  Damji<span class=p>|</span>https://tinyurl.1<span class=p>|</span> 1/4/2016<span class=p>|</span> 4535<span class=p>|</span> <span class=o>[</span>twitter, LinkedIn<span class=o>]</span><span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>  2<span class=p>|</span>   Brooke<span class=p>|</span>  Wenig<span class=p>|</span>https://tinyurl.2<span class=p>|</span> 5/5/2018<span class=p>|</span> 8908<span class=p>|</span> <span class=o>[</span>twitter, LinkedIn<span class=o>]</span><span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>  3<span class=p>|</span>    Denny<span class=p>|</span>    Lee<span class=p>|</span>https://tinyurl.3<span class=p>|</span> 6/7/2019<span class=p>|</span> 7659<span class=p>|</span><span class=o>[</span>web, twitter, FB...<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>  4<span class=p>|</span>Tathagata<span class=p>|</span>    Das<span class=p>|</span>https://tinyurl.4<span class=p>|</span>5/12/2018<span class=p>|</span>10568<span class=p>|</span>       <span class=o>[</span>twitter, FB<span class=o>]</span><span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>  5<span class=p>|</span>    Matei<span class=p>|</span>Zaharia<span class=p>|</span>https://tinyurl.5<span class=p>|</span>5/14/2014<span class=p>|</span>40578<span class=p>|</span><span class=o>[</span>web, twitter, FB...<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>  6<span class=p>|</span>  Reynold<span class=p>|</span>    Xin<span class=p>|</span>https://tinyurl.6<span class=p>|</span> 3/2/2015<span class=p>|</span>25568<span class=p>|</span> <span class=o>[</span>twitter, LinkedIn<span class=o>]</span><span class=p>|</span>
</span></span><span class=line><span class=cl>+---+---------+-------+-----------------+---------+-----+--------------------+
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>root
</span></span><span class=line><span class=cl> <span class=p>|</span>-- Id: integer <span class=o>(</span><span class=nv>nullable</span> <span class=o>=</span> <span class=nb>false</span><span class=o>)</span>
</span></span><span class=line><span class=cl> <span class=p>|</span>-- First: string <span class=o>(</span><span class=nv>nullable</span> <span class=o>=</span> <span class=nb>false</span><span class=o>)</span>
</span></span><span class=line><span class=cl> <span class=p>|</span>-- Last: string <span class=o>(</span><span class=nv>nullable</span> <span class=o>=</span> <span class=nb>false</span><span class=o>)</span>
</span></span><span class=line><span class=cl> <span class=p>|</span>-- Url: string <span class=o>(</span><span class=nv>nullable</span> <span class=o>=</span> <span class=nb>false</span><span class=o>)</span>
</span></span><span class=line><span class=cl> <span class=p>|</span>-- Published: string <span class=o>(</span><span class=nv>nullable</span> <span class=o>=</span> <span class=nb>false</span><span class=o>)</span>
</span></span><span class=line><span class=cl> <span class=p>|</span>-- Hits: integer <span class=o>(</span><span class=nv>nullable</span> <span class=o>=</span> <span class=nb>false</span><span class=o>)</span>
</span></span><span class=line><span class=cl> <span class=p>|</span>-- Campaigns: array <span class=o>(</span><span class=nv>nullable</span> <span class=o>=</span> <span class=nb>false</span><span class=o>)</span>
</span></span><span class=line><span class=cl> <span class=p>|</span>    <span class=p>|</span>-- element: string <span class=o>(</span><span class=nv>containsNull</span> <span class=o>=</span> <span class=nb>true</span><span class=o>)</span></span></span></code></pre></div></div><p>DataFrame에 할당된 스키마를 다른 곳에서 사용하고 싶다면, <code>blogs_df.schema</code> 와 같이 호출하여
스키마 객체를 반환할 수 있다. 스키마 객체는 스키마를 정의할 때 사용했던 것과
동일한 <code>pyspark.sql.types.StructType</code> 타입이다.</p><p>Scala를 사용하는 경우에도 Python과 동일하게 정의한 스키마를 JSON 파일을 읽는데 적용한다면 아래와 같이 표현할 수 있다.</p><div class=book-codeblock data-lang=kotlin><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">kotlin</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-kotlin data-lang=kotlin><span class=line><span class=cl><span class=c1>// 스칼라 예제
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=py>blogsDF</span> <span class=p>=</span> <span class=n>spark</span><span class=p>.</span><span class=n>read</span><span class=p>.</span><span class=n>schema</span><span class=p>(</span><span class=n>schema</span><span class=p>).</span><span class=n>json</span><span class=p>(</span><span class=n>jsonFile</span><span class=p>)</span></span></span></code></pre></div></div><h2 id=column>Column
<a class=anchor href=#column>#</a></h2><p>칼럼은 pandas의 DataFrame과 유사하게 어떤 특정한 타입의 필드를 나타내는 개념이다.
RDBMS를 다루는 것처럼 관계형 표현이나 계산식 형태의 표현식으로 칼럼 단위의 값들에 연산을 수행할 수 있다.</p><p>칼럼명에 대해 <code>expr("columnName * 5")</code> 같은 단순한 표현식으로 연산을 수행할 수 있다.
파이썬에서 <code>expr()</code> 은 <code>pyspark.sql.functions</code> 패키지에서 가져올 수 있다.</p><h3 id=표현식-활용-python>표현식 활용 (Python)
<a class=anchor href=#%ed%91%9c%ed%98%84%ec%8b%9d-%ed%99%9c%ec%9a%a9-python>#</a></h3><p>스키마 활용 예제에서 만든 <code>blogs_df</code> 객체를 사용한다.</p><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># src/example_schema.py</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql.types</span> <span class=kn>import</span> <span class=o>*</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>SparkSession</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql.functions</span> <span class=kn>import</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>	<span class=c1># SparkSession 및 blogs_df 객체 생성</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 표현식을 사용해 값을 계산하고 결과를 출력한다. 모두 동일한 결과를 보여준다.</span>
</span></span><span class=line><span class=cl>    <span class=n>blogs_df</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=n>expr</span><span class=p>(</span><span class=s2>&#34;Hits&#34;</span><span class=p>)</span> <span class=o>*</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>blogs_df</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>&#34;Hits&#34;</span><span class=p>)</span> <span class=o>*</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>blogs_df</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=n>expr</span><span class=p>(</span><span class=s2>&#34;Hits * 2&#34;</span><span class=p>))</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 블로그 우수 방문자를 계산하고 결과를 출력한다.</span>
</span></span><span class=line><span class=cl>    <span class=n>blogs_df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;Big Hitters&#34;</span><span class=p>,</span> <span class=p>(</span><span class=n>expr</span><span class=p>(</span><span class=s2>&#34;Hits &gt; 10000&#34;</span><span class=p>)))</span><span class=o>.</span><span class=n>show</span><span class=p>()</span></span></span></code></pre></div></div><p><code>spark-submit</code> 에 예제 파일을 전달하면 아래와 같은 결과를 확인할 수 있다.</p><div class=book-codeblock data-lang=bash><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>(</span>spark<span class=o>)</span> % spark-submit src/example_schema.py
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>+----------+
</span></span><span class=line><span class=cl><span class=p>|</span><span class=o>(</span>Hits * 2<span class=o>)</span><span class=p>|</span>
</span></span><span class=line><span class=cl>+----------+
</span></span><span class=line><span class=cl><span class=p>|</span>      9070<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>     17816<span class=p>|</span>
</span></span><span class=line><span class=cl>+----------+
</span></span><span class=line><span class=cl>only showing top <span class=m>2</span> rows
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>+---+---------+-------+-----------------+---------+-----+--------------------+-----------+
</span></span><span class=line><span class=cl><span class=p>|</span> Id<span class=p>|</span>    First<span class=p>|</span>   Last<span class=p>|</span>              Url<span class=p>|</span>Published<span class=p>|</span> Hits<span class=p>|</span>           Campaigns<span class=p>|</span>Big Hitters<span class=p>|</span>
</span></span><span class=line><span class=cl>+---+---------+-------+-----------------+---------+-----+--------------------+-----------+
</span></span><span class=line><span class=cl><span class=p>|</span>  1<span class=p>|</span>    Jules<span class=p>|</span>  Damji<span class=p>|</span>https://tinyurl.1<span class=p>|</span> 1/4/2016<span class=p>|</span> 4535<span class=p>|</span> <span class=o>[</span>twitter, LinkedIn<span class=o>]</span><span class=p>|</span>      false<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>  2<span class=p>|</span>   Brooke<span class=p>|</span>  Wenig<span class=p>|</span>https://tinyurl.2<span class=p>|</span> 5/5/2018<span class=p>|</span> 8908<span class=p>|</span> <span class=o>[</span>twitter, LinkedIn<span class=o>]</span><span class=p>|</span>      false<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>  3<span class=p>|</span>    Denny<span class=p>|</span>    Lee<span class=p>|</span>https://tinyurl.3<span class=p>|</span> 6/7/2019<span class=p>|</span> 7659<span class=p>|</span><span class=o>[</span>web, twitter, FB...<span class=p>|</span>      false<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>  4<span class=p>|</span>Tathagata<span class=p>|</span>    Das<span class=p>|</span>https://tinyurl.4<span class=p>|</span>5/12/2018<span class=p>|</span>10568<span class=p>|</span>       <span class=o>[</span>twitter, FB<span class=o>]</span><span class=p>|</span>       true<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>  5<span class=p>|</span>    Matei<span class=p>|</span>Zaharia<span class=p>|</span>https://tinyurl.5<span class=p>|</span>5/14/2014<span class=p>|</span>40578<span class=p>|</span><span class=o>[</span>web, twitter, FB...<span class=p>|</span>       true<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=p>|</span>  6<span class=p>|</span>  Reynold<span class=p>|</span>    Xin<span class=p>|</span>https://tinyurl.6<span class=p>|</span> 3/2/2015<span class=p>|</span>25568<span class=p>|</span> <span class=o>[</span>twitter, LinkedIn<span class=o>]</span><span class=p>|</span>       true<span class=p>|</span>
</span></span><span class=line><span class=cl>+---+---------+-------+-----------------+---------+-----+--------------------+-----------+</span></span></code></pre></div></div><p>첫 번째 표현식으로 계산한 결과는 모두 동일하여 하나만 출력했다.
<code>expr()</code> 또는 <code>col()</code> 표현식으로 칼럼 연산을 수행할 수 있다.</p><p><code>withColumn()</code> 을 호출하면 새로운 칼럼을 추가할 수 있다. 기존의 "Hits" 칼럼에 표현식을 사용해
블로그 우수 방문자를 분류하고, "Big Hitters" 라는 새로운 칼럼을 붙여서 출력했다.</p><p>Scala에서는 <code>col()</code> 대신에 칼럼명 앞에 <code>$</code> 를 붙여서 <code>Column</code> 타입으로 변환할 수도 있다.</p><div class=book-codeblock data-lang=kotlin><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">kotlin</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-kotlin data-lang=kotlin><span class=line><span class=cl><span class=c1>// &#34;Id&#34; 칼럼값에 따라 역순으로 정렬한다.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>blogsDF</span><span class=p>.</span><span class=n>sort</span><span class=p>(</span><span class=n>col</span><span class=p>(.</span><span class=n>desc</span><span class=p>).</span><span class=n>show</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>blogsDF</span><span class=p>.</span><span class=n>sort</span><span class=p>(</span><span class=err>$</span><span class=s2>&#34;Id&#34;</span><span class=p>.</span><span class=n>desc</span><span class=p>).</span><span class=n>show</span><span class=p>()</span></span></span></code></pre></div></div><h2 id=row>Row
<a class=anchor href=#row>#</a></h2><p>Spark에서 하나의 행은 하나 이상의 칼럼을 갖고 있는 Row 객체로 표현된다.
Row 객체에 속하는 칼럼들은 동일한 타입일 수도 있고 다른 타입일 수도 있다.
Row는 순서가 있는 필드 집합 객체이므로 0부터 시작하는 인덱스로 접근한다.</p><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 파이썬 예제</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql</span> <span class=kn>import</span> <span class=n>Row</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>blog_row</span> <span class=o>=</span> <span class=n>Row</span><span class=p>(</span><span class=mi>6</span><span class=p>,</span> <span class=s2>&#34;Reynold&#34;</span><span class=p>,</span> <span class=s2>&#34;Xin&#34;</span><span class=p>,</span> <span class=s2>&#34;https://tinyurl.6&#34;</span><span class=p>,</span> <span class=mi>255568</span><span class=p>,</span> <span class=s2>&#34;3/2/2015&#34;</span><span class=p>,</span> <span class=p>[</span><span class=s2>&#34;twitter&#34;</span><span class=p>,</span> <span class=s2>&#34;LinedIn&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 인덱스로 개별 값에 접근한다.</span>
</span></span><span class=line><span class=cl><span class=n>blog_row</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;Reynold&#39;</span></span></span></code></pre></div></div><p>Row 객체들을 DataFrame으로 만들 수 있다.</p><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 파이썬 예제</span>
</span></span><span class=line><span class=cl><span class=n>rows</span> <span class=o>=</span> <span class=p>[</span><span class=n>Row</span><span class=p>(</span><span class=s2>&#34;Matei Zaharia&#34;</span><span class=p>,</span> <span class=s2>&#34;CA&#34;</span><span class=p>),</span> <span class=n>Row</span><span class=p>(</span><span class=s2>&#34;Reynold Xin&#34;</span><span class=p>,</span> <span class=s2>&#34;CA&#34;</span><span class=p>)]</span>
</span></span><span class=line><span class=cl><span class=n>authors_df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span><span class=n>rows</span><span class=p>,</span> <span class=p>[</span><span class=s2>&#34;Authors&#34;</span><span class=p>,</span> <span class=s2>&#34;State&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>authors_df</span><span class=o>.</span><span class=n>show</span><span class=p>()</span></span></span></code></pre></div></div><h2 id=dataframe-작업>DataFrame 작업
<a class=anchor href=#dataframe-%ec%9e%91%ec%97%85>#</a></h2><h3 id=읽기쓰기>읽기/쓰기
<a class=anchor href=#%ec%9d%bd%ea%b8%b0%ec%93%b0%ea%b8%b0>#</a></h3><p>데이터 소스에서 DataFrame으로 로드하기 위해 <code>DataFrameReader</code> 를 사용할 수 있다.
JSON, CSV, Parquet, 텍스트, Avro, ORC 같은 다양한 포맷의 데이터 소스를 지원한다.
반대로 특정 포맷으로 DataFrame을 내보낼 때는 <code>DataFrameWriter</code> 를 사용할 수 있다.</p><p>Python과 Scala에서 <code>spark.read.csv()</code> 함수로 CSV 파일을 읽을 수 있다.</p><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 파이썬 예제</span>
</span></span><span class=line><span class=cl><span class=n>sf_fire_file</span> <span class=o>=</span> <span class=s2>&#34;data/sf-fire-calls.csv&#34;</span>
</span></span><span class=line><span class=cl><span class=n>fire_df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>csv</span><span class=p>(</span><span class=n>sf_fire_file</span><span class=p>,</span> <span class=n>header</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>schema</span><span class=o>=</span><span class=n>fire_schema</span><span class=p>)</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=kotlin><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">kotlin</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-kotlin data-lang=kotlin><span class=line><span class=cl><span class=c1>// 스칼라 예제
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=py>sfFireFile</span> <span class=p>=</span> <span class=s2>&#34;data/sf-fire-calls.csv&#34;</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=py>fireDF</span> <span class=p>=</span> <span class=n>spark</span><span class=p>.</span><span class=n>read</span><span class=p>.</span><span class=n>schema</span><span class=p>(</span><span class=n>fireSchema</span><span class=p>).</span><span class=n>option</span><span class=p>(</span><span class=s2>&#34;header&#34;</span><span class=p>,</span> <span class=s2>&#34;true&#34;</span><span class=p>).</span><span class=n>csv</span><span class=p>(</span><span class=n>sfFireFile</span><span class=p>)</span></span></span></code></pre></div></div><p>DataFrame을 외부 데이터 소스에 내보내려면 DataFrame 객체가 가진 <code>write()</code> 메서드를 사용할 수 있다.
기본 포맷으로 인기있는 포맷은 칼럼 지향적인 Parquet 포맷이다.
Parquet에는 스키마가 메타데이터에 들어있어 수동으로 스키마를 적용할 필요가 없다.</p><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 파이썬 예제</span>
</span></span><span class=line><span class=cl><span class=n>fire_df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=s2>&#34;parquet&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>parquet_path</span><span class=p>)</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=kotlin><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">kotlin</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-kotlin data-lang=kotlin><span class=line><span class=cl><span class=c1>// 스칼라 예제
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>fireDF</span><span class=p>.</span><span class=n>write</span><span class=p>.</span><span class=n>format</span><span class=p>(</span><span class=s2>&#34;parquet&#34;</span><span class=p>).</span><span class=n>save</span><span class=p>(</span><span class=n>parquetPath</span><span class=p>)</span></span></span></code></pre></div></div><h3 id=프로젝션필터>프로젝션/필터
<a class=anchor href=#%ed%94%84%eb%a1%9c%ec%a0%9d%ec%85%98%ed%95%84%ed%84%b0>#</a></h3><p>프로젝션은 필터를 이용해 특정 관계 상태와 매치되는 행들만 반환하는 방법이다.
프로젝션은 <code>select()</code>, 필터는 <code>filter()</code> 또는 <code>where()</code> 메서드로 표현된다.</p><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 파이썬 예제</span>
</span></span><span class=line><span class=cl><span class=n>few_fire_df</span> <span class=o>=</span> <span class=p>(</span><span class=n>fire_df</span>
</span></span><span class=line><span class=cl>	<span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=s2>&#34;IncidentNumber&#34;</span><span class=p>,</span> <span class=s2>&#34;AvailableDtTm&#34;</span><span class=p>,</span> <span class=s2>&#34;CallType&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>&#34;CallType&#34;</span><span class=p>)</span> <span class=o>!=</span> <span class=s2>&#34;Medical Incident&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>few_fire_df</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=n>truncate</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span></span></span></code></pre></div></div><h3 id=칼럼-변경>칼럼 변경
<a class=anchor href=#%ec%b9%bc%eb%9f%bc-%eb%b3%80%ea%b2%bd>#</a></h3><p>칼럼의 이름을 변경하거나 추가 또는 삭제하는 경우가 있다.
컬럼명을 변경할 때는 <code>withColumnRenamed()</code> 함수를 사용할 수 있다.
아래 예제는 "Delay" 칼럼의 명칭을 "ResponseDelayedinMins" 라고 변경한다.</p><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 파이썬 예제</span>
</span></span><span class=line><span class=cl><span class=n>new_fire_df</span> <span class=o>=</span> <span class=n>fire_df</span><span class=o>.</span><span class=n>withColumnRenamed</span><span class=p>(</span><span class=s2>&#34;Delay&#34;</span><span class=p>,</span> <span class=s2>&#34;ResponseDelayedinMins&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=n>new_fire_df</span>
</span></span><span class=line><span class=cl>	<span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=s2>&#34;ResponseDelayedinMins&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>&#34;ResponseDelayedinMins&#34;</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=kc>False</span><span class=p>))</span></span></span></code></pre></div></div><p>기존 칼럼을 가공해 새로운 칼럼을 만들 때는 <code>withColumn()</code> 메서드를 사용할 수 있다.
이때, <code>spark.sql.functions</code> 패키지에 있는 <code>to_timestamp()</code> 또는 <code>to_date()</code> 같은 함수들을
같이 사용할 수 있다. 가공된 칼럼을 추가한 후 필요하지 않은 칼럼을 제거하려면 <code>drop()</code> 메서드를 사용할 수 있다.</p><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 파이썬 예제</span>
</span></span><span class=line><span class=cl><span class=n>fire_ts_df</span> <span class=o>=</span> <span class=p>(</span><span class=n>new_fire_df</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;IncidentDate&#34;</span><span class=p>,</span> <span class=n>to_timestamp</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>&#34;CallDate&#34;</span><span class=p>),</span> <span class=s2>&#34;MM/dd/yyyy&#34;</span><span class=p>))</span><span class=o>.</span><span class=n>drop</span><span class=p>(</span><span class=s2>&#34;CallDate&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;OnWatchDate&#34;</span><span class=p>,</span> <span class=n>to_timestamp</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>&#34;WatchDate&#34;</span><span class=p>),</span> <span class=s2>&#34;MM/dd/yyyy&#34;</span><span class=p>))</span><span class=o>.</span><span class=n>drop</span><span class=p>(</span><span class=s2>&#34;WatchDate&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;AvailableDtTs&#34;</span><span class=p>,</span> <span class=n>to_timestamp</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>&#34;AvailableDtTm&#34;</span><span class=p>),</span> <span class=s2>&#34;MM/dd/yyyy hh:mm:ss&#34;</span><span class=p>))</span><span class=o>.</span><span class=n>drop</span><span class=p>(</span><span class=s2>&#34;AvailableDtTm&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=n>fire_ts_df</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=s2>&#34;IncidentDate&#34;</span><span class=p>,</span> <span class=s2>&#34;OnWatchDate&#34;</span><span class=p>,</span> <span class=s2>&#34;AvailableDtTs&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=kc>False</span><span class=p>))</span></span></span></code></pre></div></div><h3 id=집계-연산>집계 연산
<a class=anchor href=#%ec%a7%91%ea%b3%84-%ec%97%b0%ec%82%b0>#</a></h3><p><code>groupBy()</code>, <code>orderBy()</code>, <code>count()</code> 와 같은 Transformation 또는 Action을 사용하여
칼럼명을 가지고 집계할 수 있다. 아래 예제는 "CallType" 칼럼을 기준으로 행 개수를 세는 연산을 표현한다.
내림차순으로 정렬하여 가장 일반적인 신고 타입(CallType)을 확인할 수 있다.</p><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 파이썬 예제</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=n>fire_ts_df</span>
</span></span><span class=line><span class=cl>	<span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=s2>&#34;CallType&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>col</span><span class=p>(</span><span class=s2>&#34;CallType&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>isNotNull</span><span class=p>())</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>groupBy</span><span class=p>(</span><span class=s2>&#34;CallType&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>count</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>orderBy</span><span class=p>(</span><span class=s2>&#34;count&#34;</span><span class=p>,</span> <span class=n>ascending</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=n>n</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>truncate</span><span class=o>=</span><span class=kc>False</span><span class=p>))</span></span></span></code></pre></div></div><p>집계 함수로는 <code>min()</code>, <code>max()</code>, <code>sum()</code>, <code>avg()</code> 등의 통계 함수들을 지원한다.</p><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 파이썬 예제</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pyspark.sql.functions</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=n>fire_ts_df</span>
</span></span><span class=line><span class=cl>	<span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=n>F</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=s2>&#34;NumAlarms&#34;</span><span class=p>),</span> <span class=n>F</span><span class=o>.</span><span class=n>avg</span><span class=p>(</span><span class=s2>&#34;ResponseDelayedinMins&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    		<span class=n>F</span><span class=o>.</span><span class=n>min</span><span class=p>(</span><span class=s2>&#34;ResponseDelayedinMins&#34;</span><span class=p>),</span> <span class=n>F</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=s2>&#34;ResponseDelayedinMins&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>	<span class=o>.</span><span class=n>show</span><span class=p>())</span></span></span></code></pre></div></div><h2 id=dataset-api>Dataset API
<a class=anchor href=#dataset-api>#</a></h2><p>Dataset는 정적 타입 API와 동적 타입 API의 두 가지 특성을 모두 가진다.</p><h3 id=dataset>Dataset
<a class=anchor href=#dataset>#</a></h3><div style=text-align:center><img src="https://dl.dropboxusercontent.com/scl/fi/u1a2mkzn6mvvldypq7zi5/spark-13-unified-api.webp?rlkey=nkt5uf9qce0h6ezrurbjmjqzw&amp;dl=0" alt="Unified Apache Spark 2.0 API - databricks" style=width:100%;max-width:691px></div><p>DataFrame은 <code>Dataset[Row]</code> 로 표현할 수 있다. Row는 서로 다른 타입의 값을 저장할 수 있는 JVM 객체다.
반면에 Dataset는 엄격하게 타입이 정해진 JVM 객체의 집합으로, Java의 클래스와 유사하다.</p><p>Dataset는 Java와 Scala에서만 사용할 수 있고, Python과 R에서는 DataFrame만 사용할 수 있다.
이것은 Python과 R이 컴파일 시 타입의 안전을 보장하는 언어가 아니기 때문이다. 반대로 Java는 컴파일 시점에
타입 안정성을 제공하기 때문에 Dataset만 사용할 수 있다.
Scala는 DataFrame을 <code>Dataset[Row]</code> 로 표현하며, <code>Dataset[T]</code> 도 같이 사용할 수 있다.</p><h3 id=case-class>Case Class
<a class=anchor href=#case-class>#</a></h3><p>DataFrame에서 스키마로 데이터 타입을 정의한느 것처럼, Scala에서 Dataset를 만들 때
스키마를 지정하기 위해 케이스 클래스를 사용할 수 있다. Java에서는 JavaBean 클래스를 쓸 수 있다.</p><p>예제로, IoT 디바이스에서 JSON 파일을 읽어 들일 때 케이스 클래스를 아래와 같이 정의한다.</p><div class=book-codeblock data-lang=kotlin><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">kotlin</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-kotlin data-lang=kotlin><span class=line><span class=cl><span class=c1>// 스칼라 예제
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>case</span> <span class=k>class</span> <span class=nc>DeviceIoTData</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>battery_level</span><span class=p>:</span> <span class=n>Long</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>c02_level</span><span class=p>:</span> <span class=n>Long</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>cca2</span><span class=p>:</span> <span class=n>String</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>cca3</span><span class=p>:</span> <span class=n>String</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>cn</span><span class=p>:</span> <span class=n>String</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>device_id</span><span class=p>:</span> <span class=n>Long</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>device_name</span><span class=p>:</span> <span class=n>String</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>humidity</span><span class=p>:</span> <span class=n>Long</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>ip</span><span class=p>:</span> <span class=n>String</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>latitude</span><span class=p>:</span> <span class=n>Double</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>longitude</span><span class=p>:</span> <span class=n>Double</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>scale</span><span class=p>:</span> <span class=n>String</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>temp</span><span class=p>:</span> <span class=n>Long</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>timestamp</span><span class=p>:</span> <span class=n>Long</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=py>ds</span> <span class=p>=</span> <span class=n>spark</span><span class=p>.</span><span class=n>read</span><span class=p>.</span><span class=n>json</span><span class=p>(</span><span class=s2>&#34;iot_devices.json&#34;</span><span class=p>).</span><span class=k>as</span><span class=p>[</span><span class=n>DeviceIoTData</span><span class=p>]</span></span></span></code></pre></div></div><p>Dataset는 DataFrame과 같은 연산이 가능하다. 예제로, <code>filter()</code> 에 함수를 인자로 전달하는 질의는 아래와 같다.</p><div class=book-codeblock data-lang=kotlin><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">kotlin</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-kotlin data-lang=kotlin><span class=line><span class=cl><span class=c1>// 스칼라 예제
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=py>filterTempDS</span> <span class=p>=</span> <span class=n>ds</span><span class=p>.</span><span class=n>filter</span><span class=p>(</span><span class=n>d</span> <span class=p>=&gt;</span> <span class=n>d</span><span class=p>.</span><span class=n>temp</span> <span class=p>&gt;</span> <span class=m>30</span> <span class=o>&amp;&amp;</span> <span class=n>d</span><span class=p>.</span><span class=n>humidity</span> <span class=p>&gt;</span> <span class=m>70</span><span class=p>)</span></span></span></code></pre></div></div><h3 id=dataframe-vs-dataset>DataFrame vs Dataset
<a class=anchor href=#dataframe-vs-dataset>#</a></h3><p>DataFrame과 Dataset을 사용 중 오류가 발생하는 시점을 정리하면 아래 표와 같다.
Dataset가 DataFrame과 다른점은 컴파일 시점에 엄격한 타입 체크를 한다는 것이다.
반대로, SQL과 유사한 질의를 쓰는 관계형 변환을 필요로 한다면 DataFrame을 사용한다.</p><table><thead><tr><th></th><th>SQL</th><th>DataFrame</th><th>Dataset</th></tr></thead><tbody><tr><td>문법 오류</td><td>실행 시점</td><td>컴파일 시점</td><td>컴파일 시점</td></tr><tr><td>분석 오류</td><td>실행 시점</td><td>실행 시점</td><td>컴파일 시점</td></tr></tbody></table><h2 id=spark-sql>Spark SQL
<a class=anchor href=#spark-sql>#</a></h2><p>Spark SQL은 고수준 정형화 기능들이 구축되도록 하는 방대한 엔진으로 진화해 왔다. Spark SQL 엔진은 다음과 같은 일을 한다.</p><ul><li>스파크 컴포넌트들을 통합하고 DataFrame/Dataset 관련 작업을 단순화할 수 있도록 추상화를 한다.</li><li>정형화된 파일 포맷(JSON, CSV 등)을 읽고 쓰며 데이터를 임시 테이블로 변환한다.</li><li>빠른 데이터 탐색을 위한 대화형 Spark SQL 쉘을 제공한다.</li><li>JDBC/ODBC 커넥터를 통해 외부의 도구들과 연결할 수 있는 중간 역할을 한다.</li><li>JVM을 위한 최적화된 코드를 생성한다.</li></ul><div style=text-align:center><img src="https://dl.dropboxusercontent.com/scl/fi/zf3tn8nbir6j0rw492p96/spark-14-sql-connectors.webp?rlkey=m81odrfx2wd463g9xqsuqcdy5&amp;dl=0" alt="Figure 4-1. Spark SQL connectors and data sources" style=width:100%;max-width:691px></div><h3 id=catalyst-optimizer>Catalyst Optimizer
<a class=anchor href=#catalyst-optimizer>#</a></h3><p>Spark SQL 엔진의 핵심은 Catalyst Optimizer다. Catalyst Optimizer는 두 가지 목적으로 설계되었다.</p><ol><li>Spark SQL에 최적화 기법을 쉽게 추가한다.</li><li>개발자가 최적화 프로그램을 확장할 수 있도록 한다.
예시로, 데이터 소스별 규칙을 추가하거나 새로운 데이터 유형을 지원하는 것 등이 있다.</li></ol><p>Catalyst Optimizer는 연산 쿼리를 받아 실행 계획으로 변환한다. 그 과정은 아래 그림과 같이 4단계의 과정을 거친다.</p><p><img src="https://dl.dropboxusercontent.com/scl/fi/ji5e1hhmiewssku8c3zdg/spark-15-catalyst-optimizer.webp?rlkey=ujqf2c8wiqinzmfqf9ohui0zv&amp;dl=0" alt="Catalyst Optimizer - databricks"></p><ol><li><p>분석<br>제공된 코드가 유효하고 오류가 없는지 확인한다. 칼럼, 데이터 타입, 함수, 테이블, 데이터베이스 이름 목록을 갖고 있는 Catalog 객체를 참조한다. 분석 단계를 성공적으로 통과하면 Spark에서 이해하고 해결할 수 있는 요소만이 포함되어 있다는 의미를 가진다.</p></li><li><p>논리적 최적화<br>표준적인 규칙을 기반으로 최적화 접근 방식을 적용하여 효율성을 향상시킨다. 최적화를 위한 여러 계획들을 수립하는데, 예를 들면 조건절 하부 배치, 칼럼 걸러내기, 부울 표현식 단순화 등이 포함된다. 논리 계획은 물리 계획 수립의 입력 데이터가 된다.</p></li><li><p>물리 계획 수립<br>논리 계획을 바탕으로 대응되는 물리적 연산자를 사용해 최적화된 물리 계획을 생성한다. CPU, 메모리, I/O 활용을 포함한 컴퓨팅 리소스 비용을 기반으로 실행 전략을 평가한다. 리소스 가용성을 기반으로 가장 비용이 적게 드는 계획을 선택한다.</p></li><li><p>코드 생성<br>물리 계획을 Java 바이트 코드로 변환한다. 최신 컴파일러 기술을 활용해 최적화된 바이트 코드를 생성한다. Spark가 JIT(Just-In-Time) 컴파일러처럼 작동하여 런타임 성능을 최적화하고 실행 속도를 크게 향상시킨다.</p></li></ol><h2 id=references>References
<a class=anchor href=#references>#</a></h2><ul><li><a href=https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ target=_blank rel="noopener noreferrer">https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/</a></li><li><a href=https://itwiki.kr/w/%ec%95%84%ed%8c%8c%ec%b9%98_%ec%8a%a4%ed%8c%8c%ed%81%ac_DSL target=_blank rel="noopener noreferrer">https://itwiki.kr/w/아파치_스파크_DSL</a></li><li><a href=https://github.com/databricks/LearningSparkV2 target=_blank rel="noopener noreferrer">https://github.com/databricks/LearningSparkV2</a></li><li><a href=https://www.databricks.com/spark/getting-started-with-apache-spark/datasets target=_blank rel="noopener noreferrer">https://www.databricks.com/spark/getting-started-with-apache-spark/datasets</a></li><li><a href=https://www.databricks.com/glossary/catalyst-optimizer target=_blank rel="noopener noreferrer">https://www.databricks.com/glossary/catalyst-optimizer</a></li><li><a href=https://blog.det.life/apache-spark-sql-engine-and-query-planning-37cafb2b98f6 target=_blank rel="noopener noreferrer">https://blog.det.life/apache-spark-sql-engine-and-query-planning-37cafb2b98f6</a></li></ul></article><div class=book-nav><button class=book-nav-btn3 onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=book-nav-btn3 onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
<button class=book-nav-btn3 onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class=post-tags><a href=/tags/apache-spark/ class=tag>#Apache Spark</a>
<a href=/tags/structured-api/ class=tag>#Structured API</a>
<a href=/tags/dataframe/ class=tag>#DataFrame</a>
<a href=/tags/dataset/ class=tag>#Dataset</a>
<a href=/tags/pyspark/ class=tag>#PySpark</a>
<a href=/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81/ class=tag>#데이터 엔지니어링</a>
<a href=/tags/%EC%8A%A4%ED%8C%8C%ED%81%AC/ class=tag>#스파크</a>
<a href=/tags/study/ class=tag>#Study</a></div><div class=post-navigation><a href=/blog/spark-study-3/ class="post-nav-link post-nav-prev"><span class=post-nav-direction><i class="fa-solid fa-backward"></i> PREV</span>
<span class=post-nav-title>Apache Spark - 스파크 애플리케이션 구조와 RDD 이해하기</span>
</a><a href=/blog/spark-study-5/ class="post-nav-link post-nav-next"><span class=post-nav-direction>NEXT <i class="fa-solid fa-forward"></i></span>
<span class=post-nav-title>Apache Spark - 스파크 SQL과 테이블/뷰 관리</span></a></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script><div class=book-comments><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://minyeamer.github.io/blog/spark-study-4/",this.page.identifier="https://minyeamer.github.io/blog/spark-study-4/"};(function(){var e=document,t=e.createElement("script");t.src="https://minyeamer.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})();function reloadDisqus(){window.DISQUS&&DISQUS.reset({reload:!0,config:function(){this.page.url="https://minyeamer.github.io/blog/spark-study-4/",this.page.identifier="https://minyeamer.github.io/blog/spark-study-4/"}})}</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#spark-structure>Spark Structure</a><ul><li><a href=#rdd>RDD</a></li><li><a href=#spark-dsl>Spark DSL</a></li></ul></li><li><a href=#dataframe-api>DataFrame API</a><ul><li><a href=#기본-데이터-타입>기본 데이터 타입</a></li><li><a href=#정형화-타입과-복합-타입>정형화 타입과 복합 타입</a></li></ul></li><li><a href=#schema>Schema</a><ul><li><a href=#스키마-정의>스키마 정의</a></li><li><a href=#스키마-활용-python>스키마 활용 (Python)</a></li></ul></li><li><a href=#column>Column</a><ul><li><a href=#표현식-활용-python>표현식 활용 (Python)</a></li></ul></li><li><a href=#row>Row</a></li><li><a href=#dataframe-작업>DataFrame 작업</a><ul><li><a href=#읽기쓰기>읽기/쓰기</a></li><li><a href=#프로젝션필터>프로젝션/필터</a></li><li><a href=#칼럼-변경>칼럼 변경</a></li><li><a href=#집계-연산>집계 연산</a></li></ul></li><li><a href=#dataset-api>Dataset API</a><ul><li><a href=#dataset>Dataset</a></li><li><a href=#case-class>Case Class</a></li><li><a href=#dataframe-vs-dataset>DataFrame vs Dataset</a></li></ul></li><li><a href=#spark-sql>Spark SQL</a><ul><li><a href=#catalyst-optimizer>Catalyst Optimizer</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></aside></main></body></html>