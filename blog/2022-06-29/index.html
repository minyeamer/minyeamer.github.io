<!doctype html><html lang=ko-kr dir=ltr><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=keywords content="TIL,NLP"><meta name=description content="딥 러닝을 이용한 자연어 처리 입문 2"><meta name=author content="minyeamer"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><link rel=canonical href=https://minyeamer.github.io/blog/2022-06-29/><link rel=icon href=https://minyeamer.github.io/images/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://minyeamer.github.io/images/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://minyeamer.github.io/images/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://minyeamer.github.io/images/favicons/apple-touch-icon.png><link rel=mask-icon href=https://minyeamer.github.io/images/favicons/apple-touch-icon.png><meta name=google-site-verification content="u1tWcHmHUZWfFT1cHaku6sqU-bK40N3WLR-C-4VUWN0"><meta name=naver-site-verification content="6eaf8e9da1a6104780f056f1a7797fe5a3a5a0da"><meta property="og:url" content="https://minyeamer.github.io/blog/2022-06-29/"><meta property="og:site_name" content="Minystory"><meta property="og:title" content="2022-06-29 Log"><meta property="og:description" content="딥 러닝을 이용한 자연어 처리 입문 2"><meta property="og:locale" content="ko_kr"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-06-29T20:00:00+09:00"><meta property="article:modified_time" content="2022-06-29T20:00:00+09:00"><meta property="article:tag" content="TIL"><meta property="article:tag" content="NLP"><title>2022-06-29 Log | Minystory</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://minyeamer.github.io/blog/2022-06-29/><link rel=stylesheet href=/book.min.2f0b8e358d607b091af6602f2ba7e898282882ad0bf2ef1e908b00058dda4781.css integrity="sha256-LwuONY1gewka9mAvK6fomCgogq0L8u8ekIsABY3aR4E=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/search-input.min.97bd2c69bb66aba393499c7ad9ff319905e14e001ef050bf45d8b47a9c6d9278.js integrity="sha256-l70sabtmq6OTSZx62f8xmQXhTgAe8FC/Rdi0epxtkng=" crossorigin=anonymous></script><link rel=preload href=/search-data.min.7cb06c5a504171147e5c23f8fa923c5413dc6769b4d635e327bb3c3acb570140.json as=fetch crossorigin><script>window.SEARCH_DATA_URL="/search-data.min.7cb06c5a504171147e5c23f8fa923c5413dc6769b4d635e327bb3c3acb570140.json"</script><script defer src=/search.min.f30f9834d4764fd9751da64098c954d01085f648ba9ca421a3c97582f8c47253.js integrity="sha256-8w+YNNR2T9l1HaZAmMlU0BCF9ki6nKQho8l1gvjEclM=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V")</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.1/css/all.min.css crossorigin=anonymous><script defer src=/scroll-progress.min.841ade7e507a5f6d59c4e7bf2fe2b2ca034070677ff7957eec55610a024dd776.js integrity="sha256-hBreflB6X21ZxOe/L+KyygNAcGd/95V+7FVhCgJN13Y=" crossorigin=anonymous></script><script defer src=/dark-mode.min.e41c6440ffd9967d6f6a419ff3ce09b862009fe1646ab265f5cb2817d2a508e3.js integrity="sha256-5BxkQP/Zln1vakGf884JuGIAn+FkarJl9csoF9KlCOM=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlightjs-line-numbers.js/2.9.0/highlightjs-line-numbers.min.js></script><script defer src=/copy-code.min.aaeef965f0b4992e55f976edaecb34a89d414e1791caa18c3f4f4376c6d8b5a8.js integrity="sha256-qu75ZfC0mS5V+Xbtrss0qJ1BTheRyqGMP09DdsbYtag=" crossorigin=anonymous></script><script defer src=/toc-highlightjs.093016f0ef312174ad862fdcf5792e88ab5442bd39beecc38d15643f71ab5c31.min integrity="sha256-CTAW8O8xIXSthi/c9XkuiKtUQr05vuzDjRVkP3GrXDE=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-post book-layout-post"><div class=scroll-progress><div class=scroll-progress-bar></div></div><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><div class=sidebar-profile><div class=profile-img-wrap><a href=https://minyeamer.github.io/><img src="https://avatars.githubusercontent.com/u/17109173?v=4" alt=Profile class=profile-img></a></div><div class=sidebar-social><a href=https://github.com/minyeamer target=_blank title=GitHub><i class="fa-brands fa-github"></i>
</a><a href=/categories/ title=Categories><i class="fa-solid fa-folder"></i>
</a><a href=/tags/ title=Tags><i class="fa-solid fa-tags"></i>
</a><button id=dark-mode-toggle class=dark-mode-toggle aria-label="Toggle dark mode">
<i class="fa-solid fa-circle-half-stroke"></i></button></div></div><h2 class=book-brand><a class="flex align-center" href=/><span>Minystory</span></a></h2><div class="book-search hidden"><div class=search-input-container><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/ onkeydown='event.key==="Enter"&&goToSearchPage()'>
<button type=button id=book-search-button class=book-search-btn onclick=goToSearchPage()>
<i class="fa-solid fa-magnifying-glass"></i></button></div><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden");function goToSearchPage(){const t=document.getElementById("book-search-input"),e=t.value.trim();e&&(window.location.href="/search/?q="+encodeURIComponent(e))}</script><div class=book-categories><input type=checkbox class="hidden toggle" id=categories-control checked>
<label for=categories-control class="categories-toggle categories-link"><a href=/categories/><i class="fa-solid fa-folder"></i>
<span>전체</span>
<span class=category-count>(127)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul class=categories-menu id=categories-menu><li><input type=checkbox class="hidden toggle" id=cat-algorithm>
<label for=cat-algorithm class="categories-toggle categories-link"><a href=/categories/algorithm/><i class="fa-solid fa-folder"></i>
Algorithm
<span class=category-count>(51)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/algorithm/baekjoon/><i class="fa-solid fa-file"></i>
Baekjoon
<span class=category-count>(31)</span></a></li><li class=categories-link><a href=/categories/algorithm/leetcode/><i class="fa-solid fa-file"></i>
LeetCode
<span class=category-count>(2)</span></a></li><li class=categories-link><a href=/categories/algorithm/programmers/><i class="fa-solid fa-file"></i>
Programmers
<span class=category-count>(17)</span></a></li><li class=categories-link><a href=/categories/algorithm/references/><i class="fa-solid fa-file"></i>
References
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-blog>
<label for=cat-blog class="categories-toggle categories-link"><a href=/categories/blog/><i class="fa-solid fa-folder"></i>
Blog
<span class=category-count>(5)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/blog/review/><i class="fa-solid fa-file"></i>
Review
<span class=category-count>(1)</span></a></li><li class=categories-link><a href=/categories/blog/tech/><i class="fa-solid fa-file"></i>
Tech
<span class=category-count>(4)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-book>
<label for=cat-book class="categories-toggle categories-link"><a href=/categories/book/><i class="fa-solid fa-folder"></i>
Book
<span class=category-count>(1)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/book/finance/><i class="fa-solid fa-file"></i>
Finance
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-data>
<label for=cat-data class="categories-toggle categories-link"><a href=/categories/data/><i class="fa-solid fa-folder"></i>
Data
<span class=category-count>(4)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/data/crawling/><i class="fa-solid fa-file"></i>
Crawling
<span class=category-count>(4)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-diary>
<label for=cat-diary class="categories-toggle categories-link"><a href=/categories/diary/><i class="fa-solid fa-folder"></i>
Diary
<span class=category-count>(3)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/diary/2022/><i class="fa-solid fa-file"></i>
2022
<span class=category-count>(2)</span></a></li><li class=categories-link><a href=/categories/diary/2023/><i class="fa-solid fa-file"></i>
2023
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-study>
<label for=cat-study class="categories-toggle categories-link"><a href=/categories/study/><i class="fa-solid fa-folder"></i>
Study
<span class=category-count>(61)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/study/2022/><i class="fa-solid fa-file"></i>
2022
<span class=category-count>(16)</span></a></li><li class=categories-link><a href=/categories/study/2023/><i class="fa-solid fa-file"></i>
2023
<span class=category-count>(10)</span></a></li><li class=categories-link><a href=/categories/study/ai-school/><i class="fa-solid fa-file"></i>
AI SCHOOL
<span class=category-count>(34)</span></a></li><li class=categories-link><a href=/categories/study/datacamp/><i class="fa-solid fa-file"></i>
DataCamp
<span class=category-count>(1)</span></a></li></ul></li></ul></div><div class=recent-posts><div class=recent-posts-header><i class="fa-solid fa-clock"></i>
<span>최신글</span></div><ul class=recent-posts-list><li class=recent-post-item><a href=/blog/2023-04-02/ title="2023-04-02 Log"><div class=recent-post-title>2023-04-02 Log</div><div class=recent-post-date><time datetime=2023-04-02>2023.04.02</time></div></a></li><li class=recent-post-item><a href=/blog/10000-recipe/ title="[Python] 만개의 레시피 데이터 수집"><div class=recent-post-title>[Python] 만개의 레시피 데이터 수집</div><div class=recent-post-date><time datetime=2023-03-26>2023.03.26</time></div></a></li><li class=recent-post-item><a href=/blog/2023-03-25/ title="2023-03-25 Log"><div class=recent-post-title>2023-03-25 Log</div><div class=recent-post-date><time datetime=2023-03-25>2023.03.25</time></div></a></li><li class=recent-post-item><a href=/blog/2023-03-21/ title="2023-03-21 Log"><div class=recent-post-title>2023-03-21 Log</div><div class=recent-post-date><time datetime=2023-03-21>2023.03.21</time></div></a></li><li class=recent-post-item><a href=/blog/2023-02-19/ title="2023-02-19 Log"><div class=recent-post-title>2023-02-19 Log</div><div class=recent-post-date><time datetime=2023-02-19>2023.02.19</time></div></a></li></ul></div></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><i class="fa-solid fa-bars book-icon" id=menu-icon></i></label><h3><a href=https://minyeamer.github.io/ class=site-title>Minystory</a></h3><label for=toc-control><i class="fa-solid fa-list book-icon" id=toc-icon></i></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#sparse-representation>Sparse Representation</a></li><li><a href=#dense-representation>Dense Representation</a></li><li><a href=#word-embedding>Word Embedding</a></li></ul><ul><li><a href=#distributed-representation>Distributed Representation</a></li><li><a href=#cbow>CBOW</a></li><li><a href=#skip-gram>Skip-gram</a></li><li><a href=#nnlm-vs-word2vec>NNLM vs Word2Vec</a></li></ul><ul><li><a href=#sgns>SGNS</a></li><li><a href=#sgns-구현>SGNS 구현</a></li></ul><ul><li><a href=#window-based-co-occurrence-matrix>Window based Co-occurrence Matrix</a></li><li><a href=#co-occurence-probability>Co-occurence Probability</a></li></ul><ul><li><a href=#out-of-vocabulary>Out Of Vocabulary</a></li><li><a href=#rare-word>Rare Word</a></li></ul><ul><li><a href=#bilm>biLM</a></li><li><a href=#elmo-representation>ELMo Representation</a></li><li><a href=#elmo-활용>ELMo 활용</a></li></ul><ul><li><a href=#bayes-theorem>Bayes&rsquo; Theorem</a></li><li><a href=#naive-bayes-classifier>Naive Bayes Classifier</a></li></ul><ul><li><a href=#convolution-operation>Convolution Operation</a></li></ul><ul><li><a href=#1d-convolutions>1D Convolutions</a></li><li><a href=#max-pooling>Max-pooling</a></li><li><a href=#1d-cnn-구현>1D CNN 구현</a></li></ul><ul><li><a href=#bio-표현>BIO 표현</a></li><li><a href=#crfconditional-random-field>CRF(Conditional Random Field)</a></li></ul><ul><li><a href=#wordpiece-tokenizer>WordPiece Tokenizer</a></li><li><a href=#unigram-language-model-tokenizer>Unigram Language Model Tokenizer</a></li></ul><ul><li><a href=#bertwordpiecetokenizer>BertWordPieceTokenizer</a></li></ul><ul><li><a href=#encoder>Encoder</a></li><li><a href=#decoder>Decoder</a></li><li><a href=#seq2seq-구현>seq2seq 구현</a></li><li><a href=#seq2seq-동작>seq2seq 동작</a></li></ul><ul><li><a href=#bilingual-evaluation-understudybleu>Bilingual Evaluation Understudy(BLEU)</a></li><li><a href=#unigram-precision>Unigram Precision</a></li><li><a href=#modified-unigram-precision>Modified Unigram Precision</a></li><li><a href=#bleu-score>BLEU Score</a></li></ul></nav><div class=book-nav><button class=book-nav-btn3 onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=book-nav-btn3 onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
<button class=book-nav-btn3 onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div></aside></header><article class="markdown book-article"><header class=post-header><div class=post-header-category><a href=/categories/study/2022/ class=post-header-category-link>Study/2022</a></div><h1 class=post-header-title>2022-06-29 Log</h1><div class=post-header-date><time datetime=2022-06-29T20:00:00+09:00>2022. 6. 29. 20:00</time></div></header><h1 id=09-01-word-embedding>09-01. Word Embedding
<a class=anchor href=#09-01-word-embedding>#</a></h1><h2 id=sparse-representation>Sparse Representation
<a class=anchor href=#sparse-representation>#</a></h2><ul><li>벡터 또는 행렬의 값이 대부분 0으로 표현되는 방법, one-hot vector 등</li><li>단어의 개수가 늘어나면 벡터의 차원이 한없이 커지는 문제, 공간적 낭비 발생</li></ul><h2 id=dense-representation>Dense Representation
<a class=anchor href=#dense-representation>#</a></h2><ul><li>사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤 (0과 1뿐 아니라 실수 포함)</li></ul><h2 id=word-embedding>Word Embedding
<a class=anchor href=#word-embedding>#</a></h2><ul><li>단어를 밀집 벡터의 형태로 표현하는 방법</li><li><strong>Embedding Vector</strong>: 워드 임베딩 과정을 통해 나온 결과</li><li>LSA, Word2Vec, FastText, Glove 등</li></ul><h1 id=09-02-word2vec>09-02. Word2Vec
<a class=anchor href=#09-02-word2vec>#</a></h1><h2 id=distributed-representation>Distributed Representation
<a class=anchor href=#distributed-representation>#</a></h2><ul><li>희소 표현을 다차원 공간에 벡터화하는 방법</li><li>분산 표현 방법은 분포 가설이라는 가정 하에 만들어진 표현 방법</li><li><strong>분포 가설</strong>: 비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다
(귀엽다, 예쁘다, 애교 등의 단어가 주로 함께 등장할 경우 벡터화 시 유사한 벡터값을 가짐)</li><li>저차원에 단어의 의미를 여러 차원에다가 분산하여 표현하기 대문에 단어 벡터 간 유의미한 유사도 계산 가능</li><li>Word2Vec의 학습 방식으로 CBOW(Continuous Bag of Words)와 Skip-Gram이 존재</li></ul><h2 id=cbow>CBOW
<a class=anchor href=#cbow>#</a></h2><ul><li>주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법</li><li>중심 단어를 기준으로 window size만큼 앞뒤로 단어를 확인 (2n개의 단어 확인)</li><li><strong>Sliding Window</strong>: window를 옆으로 움직여서 주변 단어와 중심 단어의 선택을 변경해가며 학습 데이터셋 생성</li><li>CBOW의 인공 신경망은 주변 단어들의 one-hot vector를 입력으로 중간 단어의 one-hot vector를 예측</li><li>Word2Vec은 은닉층이 1개이며, 활성화 함수 없이 룩업 테이블이라는 연산을 담당하는 projection layer로 불림</li><li>입력층과 투사층 사이의 가중치 W는 ${V}\times{M}$ 행렬, 투사층에서 출력층 사이의 가중치 W&rsquo;는 ${M}\times{V}$ 행렬</li><li>W와 W&rsquo;는 동일한 행렬의 전치가 아니라 서로 다른 행렬이기 대문에 CBOW는 W와 W&rsquo;를 학습해가는 구조를 가짐</li><li>입력 벡터와 가중치 W 행렬의 곱은 W 행렬의 i번째 행을 그대로 읽어오는 것(lookup)과 동일</li><li>주변 단어의 one-hot vector와 가중치 W를 곱한 결과 벡터들은 투사층에서 만나 평균인 벡터를 구함</li><li>평균 벡터는 두 번째 가중치 행렬 W&rsquo;와 곱해져서 one-hot vector들과 차원이 V로 동일한 벡터가 나옴</li><li>해당 벡터에 softmax 함수를 거쳐 다중 클래스 분류 문제를 위한 score vector를 생성</li><li>score vector의 j번째 인덱스가 가진 값은 j번째 단어가 중심 단어일 확률로,<br>score vector $\hat{y}$와 중심 단어의 one-hot vector $y$의 오차를 줄이기 위해 cross-entropy 함수 사용</li></ul><h2 id=skip-gram>Skip-gram
<a class=anchor href=#skip-gram>#</a></h2><ul><li>CBOW와 반대로 중심 단어에서 주변 단어를 예측</li><li>중심 단어만을 입력으로 받기 때문에 투사층에서 벡터들의 평균을 구하는 과정은 없음</li><li>전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려짐</li></ul><h2 id=nnlm-vs-word2vec>NNLM vs Word2Vec
<a class=anchor href=#nnlm-vs-word2vec>#</a></h2><ul><li>NNLM은 다음 단어를 예측하는 목적이지만, Word2Vec(CBOW)은 중심 단어를 예측하는 목적,
때문에 NNLM이 이전 단어들만 참고한다면, Word2Vec은 예측 단어의 전후 단어들을 모두 참고</li><li>Word2Vec은 NNLM에 존재하던 활성화 함수가 있는 은닉층을 제거하여 학습 속도에서 강점을 가짐</li></ul><h1 id=09-03-word2vec-실습>09-03. <a href=https://wikidocs.net/50739>Word2Vec 실습</a>
<a class=anchor href=#09-03-word2vec-%ec%8b%a4%ec%8a%b5>#</a></h1><ul><li><a href=https://wikidocs.net/152606>위키피디아 실습</a></li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>gensim.models</span> <span class=kn>import</span> <span class=n>Word2Vec</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>gensim.models</span> <span class=kn>import</span> <span class=n>KeyedVectors</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Word2Vec</span><span class=p>(</span><span class=n>sentences</span><span class=o>=</span><span class=n>result</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>window</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>min_count</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>workers</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>sg</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span></span></span></code></pre></div></div><ul><li><code>size</code>: 워드 벡터의 특징 값, 임베딩된 벡터의 차원</li><li><code>window</code>: context window size</li><li><code>min_count</code>: 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 무시)</li><li><code>workers</code>: 학습을 위한 프로세스 수</li><li><code>sg</code>: 0은 CBOW, 1은 Skip-gram</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 입력한 단어에 대해서 가장 유사한 단어들을 출력</span>
</span></span><span class=line><span class=cl><span class=n>model_result</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>wv</span><span class=o>.</span><span class=n>most_similar</span><span class=p>(</span><span class=s2>&#34;man&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>model_result</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>[(</span><span class=s1>&#39;woman&#39;</span><span class=p>,</span> <span class=mf>0.842622697353363</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;guy&#39;</span><span class=p>,</span> <span class=mf>0.8178728818893433</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;boy&#39;</span><span class=p>,</span> <span class=mf>0.7774451375007629</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;lady&#39;</span><span class=p>,</span> <span class=mf>0.7767927646636963</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;girl&#39;</span><span class=p>,</span> <span class=mf>0.7583760023117065</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;gentleman&#39;</span><span class=p>,</span> <span class=mf>0.7437191009521484</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;soldier&#39;</span><span class=p>,</span> <span class=mf>0.7413754463195801</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;poet&#39;</span><span class=p>,</span> <span class=mf>0.7060446739196777</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;kid&#39;</span><span class=p>,</span> <span class=mf>0.6925194263458252</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;friend&#39;</span><span class=p>,</span> <span class=mf>0.6572611331939697</span><span class=p>)]</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>wv</span><span class=o>.</span><span class=n>save_word2vec_format</span><span class=p>(</span><span class=s1>&#39;eng_w2v&#39;</span><span class=p>)</span> <span class=c1># 모델 저장</span>
</span></span><span class=line><span class=cl><span class=n>loaded_model</span> <span class=o>=</span> <span class=n>KeyedVectors</span><span class=o>.</span><span class=n>load_word2vec_format</span><span class=p>(</span><span class=s2>&#34;eng_w2v&#34;</span><span class=p>)</span> <span class=c1># 모델 로드</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 사전 훈련된 Word2Vec 임베딩</span>
</span></span><span class=line><span class=cl><span class=n>urllib</span><span class=o>.</span><span class=n>request</span><span class=o>.</span><span class=n>urlretrieve</span><span class=p>(</span><span class=s2>&#34;https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>							<span class=n>filename</span><span class=o>=</span><span class=s2>&#34;GoogleNews-vectors-negative300.bin.gz&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>word2vec_model</span> <span class=o>=</span> <span class=n>KeyedVectors</span><span class=o>.</span><span class=n>load_word2vec_format</span><span class=p>(</span><span class=s1>&#39;GoogleNews-vectors-negative300.bin.gz&#39;</span><span class=p>,</span> <span class=n>binary</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># shape(3000000, 300)</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 두 단어의 유사도 계산</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>word2vec_model</span><span class=o>.</span><span class=n>similarity</span><span class=p>(</span><span class=s1>&#39;this&#39;</span><span class=p>,</span> <span class=s1>&#39;is&#39;</span><span class=p>))</span> <span class=c1># 0.407970363878</span></span></span></code></pre></div></div><h1 id=09-04-negative-sampling>09-04. Negative Sampling
<a class=anchor href=#09-04-negative-sampling>#</a></h1><ul><li>Word2Vec이 학습 과정에서 전체 단어 집합이 아니라 일부 단어 집합에만 집중할 수 있도록 하는 방법</li><li>중심 단어에 대해 무작위로 선택된 일부 단어 집합에 대해 긍정 또는 부정을 예측하는 이진 분류 수행</li><li>전체 단어 집합의 크기만큼 선택지를 두고 다중 클래스 분류 문제를 푸는 Word2Vec보다 효율적인 연산</li></ul><h2 id=sgns>SGNS
<a class=anchor href=#sgns>#</a></h2><ul><li>네거티브 샘플링을 사용하는 Skip-gram은 중심 단어와 주변 단어가 모두 입력이 되고,<br>두 단어가 실제로 윈도우 크개 내에 존재하는 이웃 관계인지 확률을 예측</li><li>중심 단어에 대한 라벨로 주변 단어를 사용하지 않고,<br>중심 단어와 주변 단어에 대한 이웃 관계를 표시하기 위한 라벨로 1 또는 0을 사용</li></ul><h2 id=sgns-구현><a href=https://wikidocs.net/69141>SGNS 구현</a>
<a class=anchor href=#sgns-%ea%b5%ac%ed%98%84>#</a></h2><ul><li>20newsgroups 데이터 사용</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 네거티브 샘플링 데이터셋 생성</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.preprocessing.sequence</span> <span class=kn>import</span> <span class=n>skipgrams</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>skip_grams</span> <span class=o>=</span> <span class=p>[</span><span class=n>skipgrams</span><span class=p>(</span><span class=n>sample</span><span class=p>,</span> <span class=n>vocabulary_size</span><span class=o>=</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>window_size</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span> <span class=k>for</span> <span class=n>sample</span> <span class=ow>in</span> <span class=n>encoded</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># (commited (7837), badar (34572)) -&gt; 0</span>
</span></span><span class=line><span class=cl><span class=c1># (whole (217), realize (1036)) -&gt; 1</span>
</span></span><span class=line><span class=cl><span class=c1># (reason (149), commited (7837)) -&gt; 1</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.models</span> <span class=kn>import</span> <span class=n>Sequential</span><span class=p>,</span> <span class=n>Model</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.layers</span> <span class=kn>import</span> <span class=n>Embedding</span><span class=p>,</span> <span class=n>Reshape</span><span class=p>,</span> <span class=n>Activation</span><span class=p>,</span> <span class=n>Input</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.layers</span> <span class=kn>import</span> <span class=n>Dot</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>embedding_dim</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 중심 단어를 위한 임베딩 테이블</span>
</span></span><span class=line><span class=cl><span class=n>w_inputs</span> <span class=o>=</span> <span class=n>Input</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=s1>&#39;int32&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>word_embedding</span> <span class=o>=</span> <span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>)(</span><span class=n>w_inputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 주변 단어를 위한 임베딩 테이블</span>
</span></span><span class=line><span class=cl><span class=n>c_inputs</span> <span class=o>=</span> <span class=n>Input</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=s1>&#39;int32&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>context_embedding</span>  <span class=o>=</span> <span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>)(</span><span class=n>c_inputs</span><span class=p>)</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 두 임베딩 테이블에 대한 내적의 결과로 1 또는 0을 예측하기 위해 시그모이드 함수 사용</span>
</span></span><span class=line><span class=cl><span class=n>dot_product</span> <span class=o>=</span> <span class=n>Dot</span><span class=p>(</span><span class=n>axes</span><span class=o>=</span><span class=mi>2</span><span class=p>)([</span><span class=n>word_embedding</span><span class=p>,</span> <span class=n>context_embedding</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>dot_product</span> <span class=o>=</span> <span class=n>Reshape</span><span class=p>((</span><span class=mi>1</span><span class=p>,),</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>))(</span><span class=n>dot_product</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>Activation</span><span class=p>(</span><span class=s1>&#39;sigmoid&#39;</span><span class=p>)(</span><span class=n>dot_product</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Model</span><span class=p>(</span><span class=n>inputs</span><span class=o>=</span><span class=p>[</span><span class=n>w_inputs</span><span class=p>,</span> <span class=n>c_inputs</span><span class=p>],</span> <span class=n>outputs</span><span class=o>=</span><span class=n>output</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>summary</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;binary_crossentropy&#39;</span><span class=p>,</span> <span class=n>optimizer</span><span class=o>=</span><span class=s1>&#39;adam&#39;</span><span class=p>)</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 5epochs 학습</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>6</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>_</span><span class=p>,</span> <span class=n>elem</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>skip_grams</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>first_elem</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=o>*</span><span class=n>elem</span><span class=p>[</span><span class=mi>0</span><span class=p>]))[</span><span class=mi>0</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=s1>&#39;int32&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>second_elem</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=o>*</span><span class=n>elem</span><span class=p>[</span><span class=mi>0</span><span class=p>]))[</span><span class=mi>1</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=s1>&#39;int32&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>labels</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>elem</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=s1>&#39;int32&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>X</span> <span class=o>=</span> <span class=p>[</span><span class=n>first_elem</span><span class=p>,</span> <span class=n>second_elem</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>Y</span> <span class=o>=</span> <span class=n>labels</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>+=</span> <span class=n>model</span><span class=o>.</span><span class=n>train_on_batch</span><span class=p>(</span><span class=n>X</span><span class=p>,</span><span class=n>Y</span><span class=p>)</span>  
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Epoch :&#39;</span><span class=p>,</span><span class=n>epoch</span><span class=p>,</span> <span class=s1>&#39;Loss :&#39;</span><span class=p>,</span><span class=n>loss</span><span class=p>)</span></span></span></code></pre></div></div><h1 id=09-05-glove>09-05. GloVe
<a class=anchor href=#09-05-glove>#</a></h1><ul><li>카운트 기반과 예측 기반을 모두 사용하는 방법론으로, LSA와 Word2Vec의 단점 보완</li><li>LSA는 단어의 빈도수를 차원 축소하여 잠재된 의미를 끌어내지만, 같은 단어 의미의 유추 작업 성능은 떨어짐</li><li>Word2Vec는 예측 기반으로 단어 간 유추 작업에는 LSA보다 뛰어나지만,<br>window size 내 주변 단어만을 고려하여 전체적인 통계 정보를 반영하지 못함</li><li>임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것이 목적</li></ul><h2 id=window-based-co-occurrence-matrix><a href=http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture02-wordvecs2.pdf>Window based Co-occurrence Matrix</a>
<a class=anchor href=#window-based-co-occurrence-matrix>#</a></h2><ul><li>행과 열을 전체 단어 집합의 단어들로 구성하고,<br>i 단어의 window size 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬</li></ul><h2 id=co-occurence-probability>Co-occurence Probability
<a class=anchor href=#co-occurence-probability>#</a></h2><ul><li>동시 등장 확률 $P(k|i)$는 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고,<br>특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># pip install glove_python_binary</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>glove</span> <span class=kn>import</span> <span class=n>Corpus</span><span class=p>,</span> <span class=n>Glove</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>corpus</span> <span class=o>=</span> <span class=n>Corpus</span><span class=p>()</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성</span>
</span></span><span class=line><span class=cl><span class=n>corpus</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>result</span><span class=p>,</span> <span class=n>window</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>glove</span> <span class=o>=</span> <span class=n>Glove</span><span class=p>(</span><span class=n>no_components</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.05</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>glove</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>corpus</span><span class=o>.</span><span class=n>matrix</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=n>no_threads</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>glove</span><span class=o>.</span><span class=n>add_dictionary</span><span class=p>(</span><span class=n>corpus</span><span class=o>.</span><span class=n>dictionary</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>glove</span><span class=o>.</span><span class=n>most_similar</span><span class=p>(</span><span class=s2>&#34;man&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>[(</span><span class=s1>&#39;woman&#39;</span><span class=p>,</span> <span class=mf>0.9621753707315267</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;guy&#39;</span><span class=p>,</span> <span class=mf>0.8860281455579162</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;girl&#39;</span><span class=p>,</span> <span class=mf>0.8609057388487154</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;kid&#39;</span><span class=p>,</span> <span class=mf>0.8383640509911114</span><span class=p>)]</span></span></span></code></pre></div></div><h1 id=09-06-fasttext>09-06. FastText
<a class=anchor href=#09-06-fasttext>#</a></h1><ul><li>Word2Vec가 단어를 쪼개질 수 없는 단위로 생각한다면,<br>FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주</li><li>각 단어를 글자 단위 n-gram의 구성으로 취급하여 n에 따라 단어들이 얼마나 분리되는지 결정</li><li>tri-gram의 경우 apple에 대해서 [&lt;ap, app, ppl, ple, le>]로 분리된 벡터 생성<br>(&lt;, >는 시작과 끝을 의미)</li><li>내부 단어들을 Word2Vec로 벡터화하고 apple의 벡터값은 내부 단어의 벡터값들의 총 합으로 구성</li></ul><h2 id=out-of-vocabulary>Out Of Vocabulary
<a class=anchor href=#out-of-vocabulary>#</a></h2><ul><li>FastText는 데이터셋만 충분하다면 내부 단어를 통해 모르는 단어에 대해서도 유사도 계산 가능</li><li>birthplace를 학습하지 않은 상태라도, birth와 place라는 내부 단어가 있다면 벡터를 얻을 수 있음</li></ul><h2 id=rare-word>Rare Word
<a class=anchor href=#rare-word>#</a></h2><ul><li>Word2Vec는 등장 빈도 수가 적은 단어에 대해서 임베딩의 정확도가 높지 않은 단점</li><li>FastText는 희귀 단어라도 n-gram이 다른 단어의 n-gram과 겹치는 경우라면,<br>Word2Vec보다 비교적 높은 임베딩 벡터값을 얻음</li><li>오타와 같은 노이즈가 많은 코퍼스에서도 일정 수준의 성능을 보임 (apple, appple)</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>gensim.models</span> <span class=kn>import</span> <span class=n>FastText</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>FastText</span><span class=p>(</span><span class=n>result</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>window</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>min_count</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>workers</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>sg</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span></span></span></code></pre></div></div><h1 id=09-08-pre-trained-word-embedding>09-08. Pre-trained Word Embedding
<a class=anchor href=#09-08-pre-trained-word-embedding>#</a></h1><ul><li>사전 훈련된 워드 임베딩 <a href=https://wikidocs.net/33793>참고</a></li></ul><h1 id=09-09-elmo>09-09. ELMo
<a class=anchor href=#09-09-elmo>#</a></h1><ul><li>언어 모델로 하는 임베딩이라는 뜻으로, 사전 훈련된 언어 모델을 사용</li><li>Word2Vec는 Bank Account와 River Bank에서 Bank의 차이를 구분하지 못하지만,<br>ELMo는 문맥을 반영한 워드 임베딩을 수행</li><li>ELMo 표현을 기존 임베딩 벡터와 연결(concatenate)해서 입력으로 사용 가능</li></ul><h2 id=bilm>biLM
<a class=anchor href=#bilm>#</a></h2><ul><li>RNN 언어 모델에서 $h_t$는 시점이 지날수록 업데이트되기 때문에,<br>문장의 문맥 정보를 점차적으로 반영함</li><li>ELMo는 양쪽 방향의 언어 모델(biLM)을 학습하여 활용</li><li>biLM은 은닉층이 최소 2개 이상인 다층 구조를 전제로 함</li><li>양방향 RNN은 순방향 RNN의 hidden state와 역방향 RNN의 hidden state를 연결하는 것이지만,<br>biLM은 순방향 언어 모델과 역방향 언어 모델이라는 두 개의 언어 모델을 별개의 모델로 보고 학습</li><li>각 층(embedding, hidden state)의 출력값이 가진 정보가 서로 다른 것이므로,<br>이를 모두 활용하여 순방향 언어 모델과 역방향 언어 모델의 각 층의 출력값을 연결</li></ul><h2 id=elmo-representation>ELMo Representation
<a class=anchor href=#elmo-representation>#</a></h2><ol><li>각 층의 출력값을 연결(concatenate)</li><li>각 층의 출력값 별로 가중치($s_1, s_2, s_3$) 부여</li><li>각 층의 출력값을 모두 더함 (2번과 3번을 요약하여 가중합이라 표현)</li><li>벡터의 크기를 결정하는 스칼라 매개변수($\gamma$)를 곱함</li></ol><h2 id=elmo-활용><a href=https://wikidocs.net/33930>ELMo 활용</a>
<a class=anchor href=#elmo-%ed%99%9c%ec%9a%a9>#</a></h2><ul><li><a href=https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv>스팸 메일 분류하기 데이터</a> 사용</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 텐서플로우 1버전에서 사용 가능</span>
</span></span><span class=line><span class=cl><span class=o>%</span><span class=n>tensorflow_version</span> <span class=mf>1.</span><span class=n>x</span>
</span></span><span class=line><span class=cl><span class=n>pip</span> <span class=n>install</span> <span class=n>tensorflow</span><span class=o>-</span><span class=n>hub</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorflow_hub</span> <span class=k>as</span> <span class=nn>hub</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 텐서플로우 허브로부터 ELMo를 다운로드</span>
</span></span><span class=line><span class=cl><span class=n>elmo</span> <span class=o>=</span> <span class=n>hub</span><span class=o>.</span><span class=n>Module</span><span class=p>(</span><span class=s2>&#34;https://tfhub.dev/google/elmo/1&#34;</span><span class=p>,</span> <span class=n>trainable</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>sess</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Session</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>K</span><span class=o>.</span><span class=n>set_session</span><span class=p>(</span><span class=n>sess</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>global_variables_initializer</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>tables_initializer</span><span class=p>())</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 데이터의 이동이 케라스 → 텐서플로우 → 케라스가 되도록 하는 함수</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>ELMoEmbedding</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>elmo</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>string</span><span class=p>)),</span> <span class=n>as_dict</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>signature</span><span class=o>=</span><span class=s2>&#34;default&#34;</span><span class=p>)</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>keras.models</span> <span class=kn>import</span> <span class=n>Model</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>keras.layers</span> <span class=kn>import</span> <span class=n>Dense</span><span class=p>,</span> <span class=n>Lambda</span><span class=p>,</span> <span class=n>Input</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>input_text</span> <span class=o>=</span> <span class=n>Input</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>1</span><span class=p>,),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>string</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>embedding_layer</span> <span class=o>=</span> <span class=n>Lambda</span><span class=p>(</span><span class=n>ELMoEmbedding</span><span class=p>,</span> <span class=n>output_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>1024</span><span class=p>,</span> <span class=p>))(</span><span class=n>input_text</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>hidden_layer</span> <span class=o>=</span> <span class=n>Dense</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>)(</span><span class=n>embedding_layer</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output_layer</span> <span class=o>=</span> <span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;sigmoid&#39;</span><span class=p>)(</span><span class=n>hidden_layer</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Model</span><span class=p>(</span><span class=n>inputs</span><span class=o>=</span><span class=p>[</span><span class=n>input_text</span><span class=p>],</span> <span class=n>outputs</span><span class=o>=</span><span class=n>output_layer</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;binary_crossentropy&#39;</span><span class=p>,</span> <span class=n>optimizer</span><span class=o>=</span><span class=s1>&#39;adam&#39;</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span></span></span></code></pre></div></div><h1 id=09-10-embedding-visualization>09-10. Embedding Visualization
<a class=anchor href=#09-10-embedding-visualization>#</a></h1><ul><li>구글 <a href=https://projector.tensorflow.org/>embedding projector</a> 시각화 도구 (<a href=https://arxiv.org/pdf/1611.05469v1.pdf>논문 참고</a>)</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># !python -m gensim.scripts.word2vec2tensor --input 모델이름 --output 모델이름</span>
</span></span><span class=line><span class=cl><span class=err>!</span><span class=n>python</span> <span class=o>-</span><span class=n>m</span> <span class=n>gensim</span><span class=o>.</span><span class=n>scripts</span><span class=o>.</span><span class=n>word2vec2tensor</span> <span class=o>--</span><span class=nb>input</span> <span class=n>eng_w2v</span> <span class=o>--</span><span class=n>output</span> <span class=n>eng_w2v</span>
</span></span><span class=line><span class=cl><span class=c1># 임베딩 프로젝트에 사용할 metadata.tsv와 tensor.tsv 파일 생성</span></span></span></code></pre></div></div><h1 id=09-11-document-embedding>09-11. Document Embedding
<a class=anchor href=#09-11-document-embedding>#</a></h1><ul><li>문서 벡터를 이용한 추천 시스템 <a href=https://wikidocs.net/102705>참고</a></li><li>문서 임베딩 : 워드 임베딩의 평균 <a href=https://wikidocs.net/103496>참고</a></li><li>Doc2Vec으로 공시 사업보고서 유사도 계산하기 <a href=https://wikidocs.net/155356>참고</a></li></ul><hr><h1 id=10-rnn-text-classification>10. RNN Text Classification
<a class=anchor href=#10-rnn-text-classification>#</a></h1><ul><li><a href=https://wikidocs.net/24873>케라스를 이용한 텍스트 분류 개요</a></li><li><a href=https://wikidocs.net/22894>스팸 메일 분류하기 (RNN)</a></li><li><a href=https://wikidocs.net/22933>로이터 뉴스 분류하기 (LSTM)</a></li><li><a href=https://wikidocs.net/24586>IMDB 리뷰 감성 분류하기 (GRU)</a></li><li><a href=https://wikidocs.net/22892>나이브 베이즈 분류기</a></li><li><a href=https://wikidocs.net/44249>네이버 영화 리뷰 감성 분류하기(LSTM)</a></li><li><a href=https://wikidocs.net/94600>네이버 쇼핑 리뷰 감성 분류하기(GRU)</a></li><li><a href=https://wikidocs.net/94748>BiLSTM으로 한국어 스팀 리뷰 감성 분류하기</a></li></ul><h2 id=bayes-theorem>Bayes&rsquo; Theorem
<a class=anchor href=#bayes-theorem>#</a></h2><p>$$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$</p><h2 id=naive-bayes-classifier>Naive Bayes Classifier
<a class=anchor href=#naive-bayes-classifier>#</a></h2><ul><li>베이즈 정리를 이용한 스팸 메일 확률 표현<br>P(정상 메일 | 입력 텍스트) = (P(입력 텍스트 | 정상 메일) x P(정상 메일)) / P(입력 텍스트)<br>P(스팸 메일 | 입력 텍스트) = (P(입력 텍스트 | 스팸 메일) x P(스팸 메일)) / P(입력 텍스트)</li><li>나이브 베이즈 분류기에서 토큰화 이전의 단어의 순서는 중요하지 않음<br>(BoW와 같이 단어의 순서를 무시하고 빈도수만 고려)</li><li>정상 메일에 입력 텍스트가 없어 확률이 0%가 되는 것을 방지하기 위해<br>각 단어에 대한 확률의 분모, 분자에 전부 숫자를 더해서 분자가 0이 되는 것을 방지하는 라플라스 스무딩 사용</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.feature_extraction.text</span> <span class=kn>import</span> <span class=n>TfidfTransformer</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.naive_bayes</span> <span class=kn>import</span> <span class=n>MultinomialNB</span> <span class=c1># 다항분포 나이브 베이즈 모델</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># alpha=1.0: 라플라스 스무딩 적용</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>MultinomialNB</span><span class=p>()</span> <span class=c1># MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>tfidfv</span><span class=p>,</span> <span class=n>newsdata</span><span class=o>.</span><span class=n>target</span><span class=p>)</span></span></span></code></pre></div></div><hr><h1 id=11-01-convolutional-neural-network>11-01. Convolutional Neural Network
<a class=anchor href=#11-01-convolutional-neural-network>#</a></h1><ul><li>이미지 처리에 탁월한 성능을 보이는 신경망</li><li>합성곱 신경망은 convolutional layer와 pooling layer로 구성</li><li>합성곱 연산(CONV)의 결과가 ReLU를 거쳐서 POOL 구간을 지나는 과정</li><li><strong>Channel</strong>: 이미지는 (높이, 너비, 채널)이라는 3차원 텐서로 구성, 채널은 색 성분을 의미</li><li>합성곱 신경망은 이미지의 모든 픽셀이 아닌, 커널과 맵핑되는 픽셀만을 입력으로 사용하여<br>다층 퍼셉트론보다 훨씬 적은 수의 가중치를 사용하여 공간적 구조 정보를 보존</li><li>편향을 추가할 경우 커널을 적용한 뒤에 더해지며, 단 하나의 편향이 커널이 적용된 결과의 모든 원소에 더해짐</li><li>다수의 채널을 가진 입력 데이터일 경우 커널의 채널 수도 입력의 채널 수만큼 존재,<br>각 채널 간 합성곱 연산을 마치고 그 결과를 모두 더해서 하나의 채널을 가지는 특성 맵 생성</li></ul><h2 id=convolution-operation>Convolution Operation
<a class=anchor href=#convolution-operation>#</a></h2><ul><li>이미지의 특징을 추출</li><li>Kernel(filter)라는 ${n}\times{m}$ 크기의 행렬로 각 이미지를 순차적으로 훑음</li><li><strong>Feature map</strong>: 합성곱 연산을 통해 나온 결과</li><li><strong>Stride</strong>: 커널의 이동 범위, 특성 맵의 크기</li><li><strong>Padding</strong>: 합성곱 연산 이후에도 특성 맵의 크기가 입력과 동일하도록 행과 열 추가</li></ul><h1 id=11-02-1d-cnn>11-02. 1D CNN
<a class=anchor href=#11-02-1d-cnn>#</a></h1><h2 id=1d-convolutions>1D Convolutions
<a class=anchor href=#1d-convolutions>#</a></h2><ul><li>LSTM과 동일하게 각 단어가 벡터로 변환된 문장 행렬을 입력으로 받음</li><li>커널의 너비는 임베딩 벡터의 차원과 동일, 커널의 높이만으로 해당 커널의 크기라 간주</li><li>커널의 너비가 임베딩 벡터의 차원이기 때문에 너비 방향으로 움직이지 못하고 높이 방향으로만 움직임</li></ul><h2 id=max-pooling>Max-pooling
<a class=anchor href=#max-pooling>#</a></h2><ul><li>1D CNN에서의 폴링 층</li><li>각 합성곱 연산으로부터 얻은 결과 벡터에서 가장 큰 값을 가진 스칼라 값을 빼내는 연산을 수행</li></ul><h2 id=1d-cnn-구현><a href=https://wikidocs.net/80783>1D CNN 구현</a>
<a class=anchor href=#1d-cnn-%ea%b5%ac%ed%98%84>#</a></h2><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.models</span> <span class=kn>import</span> <span class=n>Sequential</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.layers</span> <span class=kn>import</span> <span class=n>Embedding</span><span class=p>,</span> <span class=n>Dropout</span><span class=p>,</span> <span class=n>Conv1D</span><span class=p>,</span> <span class=n>GlobalMaxPooling1D</span><span class=p>,</span> <span class=n>Dense</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.callbacks</span> <span class=kn>import</span> <span class=n>EarlyStopping</span><span class=p>,</span> <span class=n>ModelCheckpoint</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>embedding_dim</span> <span class=o>=</span> <span class=mi>256</span> <span class=c1># 임베딩 벡터의 차원</span>
</span></span><span class=line><span class=cl><span class=n>dropout_ratio</span> <span class=o>=</span> <span class=mf>0.3</span> <span class=c1># 드롭아웃 비율</span>
</span></span><span class=line><span class=cl><span class=n>num_filters</span> <span class=o>=</span> <span class=mi>256</span> <span class=c1># 커널의 수</span>
</span></span><span class=line><span class=cl><span class=n>kernel_size</span> <span class=o>=</span> <span class=mi>3</span> <span class=c1># 커널의 크기</span>
</span></span><span class=line><span class=cl><span class=n>hidden_units</span> <span class=o>=</span> <span class=mi>128</span> <span class=c1># 뉴런의 수</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout_ratio</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Conv1D</span><span class=p>(</span><span class=n>num_filters</span><span class=p>,</span> <span class=n>kernel_size</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;valid&#39;</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>GlobalMaxPooling1D</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=n>hidden_units</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout_ratio</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;sigmoid&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>es</span> <span class=o>=</span> <span class=n>EarlyStopping</span><span class=p>(</span><span class=n>monitor</span><span class=o>=</span><span class=s1>&#39;val_loss&#39;</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s1>&#39;min&#39;</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>patience</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>mc</span> <span class=o>=</span> <span class=n>ModelCheckpoint</span><span class=p>(</span><span class=s1>&#39;best_model.h5&#39;</span><span class=p>,</span> <span class=n>monitor</span><span class=o>=</span><span class=s1>&#39;val_acc&#39;</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s1>&#39;max&#39;</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>save_best_only</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=s1>&#39;adam&#39;</span><span class=p>,</span> <span class=n>loss</span><span class=o>=</span><span class=s1>&#39;binary_crossentropy&#39;</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;acc&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>history</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=n>validation_data</span><span class=o>=</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>),</span> <span class=n>callbacks</span><span class=o>=</span><span class=p>[</span><span class=n>es</span><span class=p>,</span> <span class=n>mc</span><span class=p>])</span></span></span></code></pre></div></div><h1 id=11-06-intent-classification>11-06. Intent Classification
<a class=anchor href=#11-06-intent-classification>#</a></h1><ul><li>사전 훈련된 워드 임베딩을 이용한 의도 분류 <a href=https://wikidocs.net/86083>참고</a></li></ul><h1 id=11-07-character-embedding>11-07. Character Embedding
<a class=anchor href=#11-07-character-embedding>#</a></h1><ul><li>&lsquo;misunderstand&rsquo;의 의미를 &lsquo;mis-&lsquo;라는 접두사와 &lsquo;understand&rsquo;를 통해 추측하는 것과 같이,<br>사람의 이해 능력을 흉내내는 알고리즘</li><li>1D CNN에서는 단어를 문자 단위로 쪼개기만하면 되기 때문에 OOV라도 벡터를 얻을 수 있음</li><li>BiLSTM에서도 문자에 대한 임베딩을 통해 얻은 벡터를 단어에 대한 벡터로 사용</li></ul><hr><h1 id=12-tagging-task>12. Tagging Task
<a class=anchor href=#12-tagging-task>#</a></h1><ul><li><a href=https://wikidocs.net/33532>양방향 LSTM를 이용한 품사 태깅</a></li><li><a href=https://wikidocs.net/30682>개체명 인식</a></li><li><a href=https://wikidocs.net/24682>개체명 인식의 BIO 표현 이해하기</a></li><li><a href=https://wikidocs.net/147219>BiLSTM을 이용한 개체명 인식</a></li><li><a href=https://wikidocs.net/147234>BiLSTM-CRF를 이용한 개체명 인식</a></li><li><a href=https://wikidocs.net/147299>문자 임베딩 활용하기</a></li></ul><h2 id=bio-표현>BIO 표현
<a class=anchor href=#bio-%ed%91%9c%ed%98%84>#</a></h2><ul><li>개체명이 시작되는 부분에 B(Begin), 개체명의 내부에 I(Inside), 나머지로 O(Outside) 태깅</li><li>개체명 태깅엔 LOC(location), ORG(organization), PER(person), MISC(miscellaneous)<br>태그가 추가로 붙음 (B-ORG 등)</li></ul><h2 id=crfconditional-random-field>CRF(Conditional Random Field)
<a class=anchor href=#crfconditional-random-field>#</a></h2><ul><li>LSTM 위에 CRF 층을 추가하면 모델은 예측 개체명(레이블 간 의존성)을 고려</li><li>기존 양방향 LSTM 모델은 활성화 함수를 지난 시점에서 개체명을 결정했지만,<br>CRF 층을 추가한 모델에서는 활성화 함수의 결과들이 CRF 층의 입력으로 전달</li><li>CRF 층은 [문장의 첫번쨰 단어에서는 I가 나오지 않는다, O-I 패턴은 나오지 않는다] 등의 제약사항을 학습</li><li>양방향 LSTM은 입력 단어에 대한 양방향 문맥을 반영하며, CRF는 출력 레이블에 대한 양방향 문맥을 반영</li><li>CRF 층은 one-hot encoding된 라벨을 지원하지 않음</li></ul><hr><h1 id=13-01-byte-pair-encoding>13-01. Byte Pair Encoding
<a class=anchor href=#13-01-byte-pair-encoding>#</a></h1><ul><li>UNK(Unknown Token) 등의 OOV 문제를 해결하기 위해 서브워드 분리 작업을 수행</li><li>BPE 알고리즘은 연속적으로 가장 많이 등장한 글자의 쌍을 찾아서 하나의 글자로 병합</li><li>BPE는 글자 단위에서 점차적으로 단어 집합을 만들어내는 Bottom up 방식의 접근 사용</li></ul><h2 id=wordpiece-tokenizer>WordPiece Tokenizer
<a class=anchor href=#wordpiece-tokenizer>#</a></h2><ul><li>BPE의 변형 알고리즘으로, 코퍼스의 likelihood를 가장 높이는 쌍을 병합</li><li>모든 단어의 맨 앞에 _를 붙이고, 단어는 subword로 통계에 기반하여 띄어쓰기로 분리</li><li>WordPiece TOkenizer 겨로가를 되돌리기 위해서는 모든 띄어쓰기를 제거하고 언더바를 띄어쓰기로 바꿈</li></ul><h2 id=unigram-language-model-tokenizer>Unigram Language Model Tokenizer
<a class=anchor href=#unigram-language-model-tokenizer>#</a></h2><ul><li>각각의 서브워드들에 대해서 손실(loss)을 계산</li><li>서브 단어의 손실은 해당 서브워드가 단어 집합에서 제거되었을 경우, 코퍼스의 likelihood가 감소하는 정도</li><li>서브워드들의 손실의 정도를 정렬하여, 최악의 영향을 주는 10~20%의 토큰을 제거</li></ul><h1 id=13-02-sentencepiece>13-02. SentencePiece
<a class=anchor href=#13-02-sentencepiece>#</a></h1><ul><li>내부 단어 분리를 위한 <a href=https://github.com/google/sentencepiece>구글의 패키지</a></li><li>사전 토큰화 작업없이 단어 분리 토큰화를 수행하여 언어에 종속적이지 않음</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>sentencepiece</span> <span class=k>as</span> <span class=nn>spm</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># IMDB 리뷰 데이터 사용</span>
</span></span><span class=line><span class=cl><span class=n>spm</span><span class=o>.</span><span class=n>SentencePieceTrainer</span><span class=o>.</span><span class=n>Train</span><span class=p>(</span><span class=s1>&#39;--input=imdb_review.txt --model_prefix=imdb --vocab_size=5000 --model_type=bpe --max_sentence_length=9999&#39;</span><span class=p>)</span></span></span></code></pre></div></div><ul><li>input: 학습시킬 파일</li><li>model_prefix: 만들어질 모델 이름</li><li>vocab_size: 단어 집합의 크기</li><li>model_type: 사용할 모델 (unigram(default), bpe, char, word)</li><li>max_sentence_length: 문장의 최대 길이</li></ul><h1 id=13-03-subwordtextencoder>13-03. SubwordTextEncoder
<a class=anchor href=#13-03-subwordtextencoder>#</a></h1><ul><li>Wordpiece 모델을 채택한 텐서플로우의 서브워드 토크나이저</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorflow_datasets</span> <span class=k>as</span> <span class=nn>tfds</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>tfds</span><span class=o>.</span><span class=n>features</span><span class=o>.</span><span class=n>text</span><span class=o>.</span><span class=n>SubwordTextEncoder</span><span class=o>.</span><span class=n>build_from_corpus</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>train_df</span><span class=p>[</span><span class=s1>&#39;review&#39;</span><span class=p>],</span> <span class=n>target_vocab_size</span><span class=o>=</span><span class=mi>2</span><span class=o>**</span><span class=mi>13</span><span class=p>)</span></span></span></code></pre></div></div><h1 id=13-04-huggingface-tokenizer>13-04. Huggingface Tokenizer
<a class=anchor href=#13-04-huggingface-tokenizer>#</a></h1><ul><li><strong>BertWordPieceTokenizer</strong>: BERT에서 사용된 워드피스 토크나이저(WordPiece Tokenizer)</li><li><strong>CharBPETokenizer</strong>: 오리지널 BPE</li><li><strong>ByteLevelBPETokenizer</strong>: BPE의 바이트 레벨 버전</li><li><strong>SentencePieceBPETokenizer</strong>: 앞서 본 패키지 센텐스피스(SentencePiece)와 호환되는 BPE 구현체</li></ul><h2 id=bertwordpiecetokenizer>BertWordPieceTokenizer
<a class=anchor href=#bertwordpiecetokenizer>#</a></h2><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tokenizers</span> <span class=kn>import</span> <span class=n>BertWordPieceTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>BertWordPieceTokenizer</span><span class=p>(</span><span class=n>lowercase</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>trip_accents</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>data_file</span> <span class=o>=</span> <span class=s1>&#39;naver_review.txt&#39;</span>
</span></span><span class=line><span class=cl><span class=n>vocab_size</span> <span class=o>=</span> <span class=mi>30000</span>
</span></span><span class=line><span class=cl><span class=n>limit_alphabet</span> <span class=o>=</span> <span class=mi>6000</span>
</span></span><span class=line><span class=cl><span class=n>min_frequency</span> <span class=o>=</span> <span class=mi>5</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>files</span><span class=o>=</span><span class=n>data_file</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>vocab_size</span><span class=o>=</span><span class=n>vocab_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>limit_alphabet</span><span class=o>=</span><span class=n>limit_alphabet</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>min_frequency</span><span class=o>=</span><span class=n>min_frequency</span><span class=p>)</span></span></span></code></pre></div></div><hr><h1 id=14-01-sequence-to-sequenceseq2seq>14-01. Sequence-to-Sequence(seq2seq)
<a class=anchor href=#14-01-sequence-to-sequenceseq2seq>#</a></h1><ul><li>seq2seq는 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 다양한 분야에서 사용되는 모델</li><li>챗봇, 기계 번역, 내용 요약, STT(Speech to Text) 등에서 주로 사용</li><li>seq2seq는 인코더와 디코더로 나눠지며, 둘 다 LSTM 셀 또는 GRU 셀을 사용하는 RNN 아키텍처로 구성</li><li>softmax 함수를 통해 출력 시퀀스의 각 단어별 확률값을 반환하고, 디코더는 출력 단어를 결정</li></ul><h2 id=encoder>Encoder
<a class=anchor href=#encoder>#</a></h2><ul><li>입력 문장의 모든 단어들을 순차적으로 입력받고 모든 단어 정보들을 압축해서 하나의 벡터 생성</li><li>인코더 RNN 셀의 마지막 hidden state를 context vector로 디코더에 넘겨줌</li></ul><h2 id=decoder>Decoder
<a class=anchor href=#decoder>#</a></h2><ul><li>압축된 컨텍스트 벡터를 받아서 번역된 단어를 한 개씩 순차적으로 출력</li><li>기본적으로 RNNLM으로, 초기 입력으로 문장의 시작을 의미하는 심볼 <sos>가 들어감</li><li>첫번째 시점의 디코더 RNN 셀은 예측된 단어를 다음 시점의 RNN 셀 입력으로 넣으며,<br>문장의 끝을 의미하는 심볼인 <eos>가 다음 단어로 예측될 때까지 반복해서 예측</li><li>훈련 과정에서는 실제 정답 상황에서 <eos>가 나와야 된다고 정답을 알려줌<br>(교사 강요: 이전 시점의 디코더 셀의 예측이 틀릴 경우 연쇄 작용을 방지)</li></ul><h2 id=seq2seq-구현><a href=https://wikidocs.net/24996>seq2seq 구현</a>
<a class=anchor href=#seq2seq-%ea%b5%ac%ed%98%84>#</a></h2><ul><li><a href=http://www.manythings.org/anki>프랑스-영어 병렬 코퍼스 데이터</a> 사용</li><li>병렬 코퍼스 데이터에서 쌍이 되는 데이터의 길이가 같지 않음에 주의</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Encoder</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>encoder_inputs</span> <span class=o>=</span> <span class=n>Input</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=kc>None</span><span class=p>,</span> <span class=n>src_vocab_size</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>encoder_lstm</span> <span class=o>=</span> <span class=n>LSTM</span><span class=p>(</span><span class=n>units</span><span class=o>=</span><span class=mi>256</span><span class=p>,</span> <span class=n>return_state</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># encoder_outputs은 여기서는 불필요</span>
</span></span><span class=line><span class=cl><span class=n>encoder_outputs</span><span class=p>,</span> <span class=n>state_h</span><span class=p>,</span> <span class=n>state_c</span> <span class=o>=</span> <span class=n>encoder_lstm</span><span class=p>(</span><span class=n>encoder_inputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># LSTM은 바닐라 RNN과는 달리 상태가 두 개, 은닉 상태와 셀 상태</span>
</span></span><span class=line><span class=cl><span class=n>encoder_states</span> <span class=o>=</span> <span class=p>[</span><span class=n>state_h</span><span class=p>,</span> <span class=n>state_c</span><span class=p>]</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Decoder</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>decoder_inputs</span> <span class=o>=</span> <span class=n>Input</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=kc>None</span><span class=p>,</span> <span class=n>tar_vocab_size</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>decoder_lstm</span> <span class=o>=</span> <span class=n>LSTM</span><span class=p>(</span><span class=n>units</span><span class=o>=</span><span class=mi>256</span><span class=p>,</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>return_state</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 디코더에게 인코더의 은닉 상태, 셀 상태를 전달</span>
</span></span><span class=line><span class=cl><span class=n>decoder_outputs</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>_</span><span class=o>=</span> <span class=n>decoder_lstm</span><span class=p>(</span><span class=n>decoder_inputs</span><span class=p>,</span> <span class=n>initial_state</span><span class=o>=</span><span class=n>encoder_states</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>decoder_softmax_layer</span> <span class=o>=</span> <span class=n>Dense</span><span class=p>(</span><span class=n>tar_vocab_size</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;softmax&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>decoder_outputs</span> <span class=o>=</span> <span class=n>decoder_softmax_layer</span><span class=p>(</span><span class=n>decoder_outputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Model</span><span class=p>([</span><span class=n>encoder_inputs</span><span class=p>,</span> <span class=n>decoder_inputs</span><span class=p>],</span> <span class=n>decoder_outputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=s2>&#34;rmsprop&#34;</span><span class=p>,</span> <span class=n>loss</span><span class=o>=</span><span class=s2>&#34;categorical_crossentropy&#34;</span><span class=p>)</span></span></span></code></pre></div></div><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>x</span><span class=o>=</span><span class=p>[</span><span class=n>encoder_input</span><span class=p>,</span> <span class=n>decoder_input</span><span class=p>],</span> <span class=n>y</span><span class=o>=</span><span class=n>decoder_target</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>40</span><span class=p>,</span> <span class=n>validation_split</span><span class=o>=</span><span class=mf>0.2</span><span class=p>)</span></span></span></code></pre></div></div><h2 id=seq2seq-동작>seq2seq 동작
<a class=anchor href=#seq2seq-%eb%8f%99%ec%9e%91>#</a></h2><ol><li>번역하고자 하는 입력 문장이 인코더에 들어가서 은닉 상태와 셀 상태를 얻음</li><li>상태와 <sos>에 해당하는 &lsquo;\t&rsquo;를 디코더로 보냄</li><li>디코더가 <eos>에 해당하는 &lsquo;\n&rsquo;이 나올 때까지 다음 문자를 예측하는 행동을 반복</li></ol><h1 id=14-02-bleu-score>14-02. BLEU Score
<a class=anchor href=#14-02-bleu-score>#</a></h1><h2 id=bilingual-evaluation-understudybleu>Bilingual Evaluation Understudy(BLEU)
<a class=anchor href=#bilingual-evaluation-understudybleu>#</a></h2><ul><li>기계 번역 결과와 사람이 직접 번역한 결과가 얼마나 유사한지 비교하여 번역에 대한 성능을 측정하는 방법</li><li>측정 기준은 n-gram에 기반</li><li>언어에 구애받지 않고 사용할 수 있으며, 계산 속도가 빠른 이점</li><li>PPL과 달리 높을 수록 성능이 더 좋음을 의미</li></ul><h2 id=unigram-precision>Unigram Precision
<a class=anchor href=#unigram-precision>#</a></h2><ul><li>사람이 번역한 문장 중 어느 한 문장이라도 등장한 단어의 개수를 카운트하는 측정 방법</li><li>기계 번역기가 번역한 문장을 Ca, 사람이 번역한 문장을 Ref라 표현</li></ul><p>$$\text{Unigram Precision}=\frac{\text{Ref들 중에서 존재하는 Ca의 단어의 수}}{\text{Ca의 총 단어 수}}$$</p><h2 id=modified-unigram-precision>Modified Unigram Precision
<a class=anchor href=#modified-unigram-precision>#</a></h2><ul><li>하나의 단어가 여러번 반복되는 경우에서 정밀도가 1이 나오는 문제를 개선하기 위해<br>유니그램이 이미 매칭된 적이 있는지를 고려</li><li>$Max_Ref_Count$: 유니그램이 하나의 Ref에서 최대 몇 번 등장했는지 카운트</li><li>$Count_{dip}=min(Count,Max_Ref_Count)$</li></ul><p>$$\text{Modified Unigram Precision}=\frac{\text{Ca의 각 유니그램에 대해 }Count_{dip}\text{을 수행한 값의 총 합}}{\text{Ca의 총 유니그램 수}}$$</p><h2 id=bleu-score>BLEU Score
<a class=anchor href=#bleu-score>#</a></h2><ul><li>유니그램 정밀도는 단어의 빈도수로 접근하기 때문에 단어의 순서를 고려하기 위해 n-gram 이용</li><li>BLEU 최종 식은 보정된 정밀도 $p_1,p_2,&mldr;,p_n$을 모두 조합</li><li>해당 BLEU 식의 경우 문장의 길이가 짧을 때 높은 점수를 받는 문제가 있기 때문에,<br>길이가 짧은 문장에게 Brevity Penalty를 줄 필요가 있음</li><li>$BP$는 Ca와 가장 길이 차이가 작은 Ref의 길이 $r$을 기준으로 $e^{(1-r/c)}$ 값을 곱하며,<br>문장이 $r$보다 길어 패널티를 줄 필요가 없는 경우 1이어야 함</li></ul><p>$$\text{보정된 정밀도 } p_1=\frac{\Sigma_{{unigram}\in{Candidate}}Count_{dip}(unigram)}{\Sigma_{{unigram}\in{Candidate}}Count(unigram)}$$
$$\text{n-gram 일반화 } p_n=\frac{\Sigma_{{n\text{-}gram}\in{Candidate}}Count_{dip}(n\text{-}gram)}{\Sigma_{{n\text{-}gram}\in{Candidate}}Count(n\text{-}gram)}$$
$$BLEU={BP}\times{exp(\Sigma^N_{n=1}{w_n}{\log{p_n}})}$$</p><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>nltk.translate.bleu_score</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>bleu_score</span><span class=p>(</span><span class=n>candidate</span><span class=o>.</span><span class=n>split</span><span class=p>(),</span><span class=nb>list</span><span class=p>(</span><span class=nb>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>ref</span><span class=p>:</span> <span class=n>ref</span><span class=o>.</span><span class=n>split</span><span class=p>(),</span> <span class=n>references</span><span class=p>)))</span></span></span></code></pre></div></div></article><div class=book-mobile-nav><button class=book-nav-btn3 onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=book-nav-btn3 onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
<button class=book-nav-btn3 onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class=post-tags><a href=/tags/til/ class=tag>#TIL</a>
<a href=/tags/nlp/ class=tag>#NLP</a></div><div class=post-navigation><a href=/blog/2022-06-30/ class="post-nav-link post-nav-prev"><span class=post-nav-direction><i class="fa-solid fa-backward"></i> PREV</span>
<span class=post-nav-title>2022-06-30 Log</span>
</a><a href=/blog/2022-06-28/ class="post-nav-link post-nav-next"><span class=post-nav-direction>NEXT <i class="fa-solid fa-forward"></i></span>
<span class=post-nav-title>2022-06-28 Log</span></a></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script><div class=book-comments><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://minyeamer.github.io/blog/2022-06-29/",this.page.identifier="https://minyeamer.github.io/blog/2022-06-29/"};(function(){var e=document,t=e.createElement("script");t.src="https://minyeamer.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})();function reloadDisqus(){window.DISQUS&&DISQUS.reset({reload:!0,config:function(){this.page.url="https://minyeamer.github.io/blog/2022-06-29/",this.page.identifier="https://minyeamer.github.io/blog/2022-06-29/"}})}</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#sparse-representation>Sparse Representation</a></li><li><a href=#dense-representation>Dense Representation</a></li><li><a href=#word-embedding>Word Embedding</a></li></ul><ul><li><a href=#distributed-representation>Distributed Representation</a></li><li><a href=#cbow>CBOW</a></li><li><a href=#skip-gram>Skip-gram</a></li><li><a href=#nnlm-vs-word2vec>NNLM vs Word2Vec</a></li></ul><ul><li><a href=#sgns>SGNS</a></li><li><a href=#sgns-구현>SGNS 구현</a></li></ul><ul><li><a href=#window-based-co-occurrence-matrix>Window based Co-occurrence Matrix</a></li><li><a href=#co-occurence-probability>Co-occurence Probability</a></li></ul><ul><li><a href=#out-of-vocabulary>Out Of Vocabulary</a></li><li><a href=#rare-word>Rare Word</a></li></ul><ul><li><a href=#bilm>biLM</a></li><li><a href=#elmo-representation>ELMo Representation</a></li><li><a href=#elmo-활용>ELMo 활용</a></li></ul><ul><li><a href=#bayes-theorem>Bayes&rsquo; Theorem</a></li><li><a href=#naive-bayes-classifier>Naive Bayes Classifier</a></li></ul><ul><li><a href=#convolution-operation>Convolution Operation</a></li></ul><ul><li><a href=#1d-convolutions>1D Convolutions</a></li><li><a href=#max-pooling>Max-pooling</a></li><li><a href=#1d-cnn-구현>1D CNN 구현</a></li></ul><ul><li><a href=#bio-표현>BIO 표현</a></li><li><a href=#crfconditional-random-field>CRF(Conditional Random Field)</a></li></ul><ul><li><a href=#wordpiece-tokenizer>WordPiece Tokenizer</a></li><li><a href=#unigram-language-model-tokenizer>Unigram Language Model Tokenizer</a></li></ul><ul><li><a href=#bertwordpiecetokenizer>BertWordPieceTokenizer</a></li></ul><ul><li><a href=#encoder>Encoder</a></li><li><a href=#decoder>Decoder</a></li><li><a href=#seq2seq-구현>seq2seq 구현</a></li><li><a href=#seq2seq-동작>seq2seq 동작</a></li></ul><ul><li><a href=#bilingual-evaluation-understudybleu>Bilingual Evaluation Understudy(BLEU)</a></li><li><a href=#unigram-precision>Unigram Precision</a></li><li><a href=#modified-unigram-precision>Modified Unigram Precision</a></li><li><a href=#bleu-score>BLEU Score</a></li></ul></nav><div class=book-nav><button class=book-nav-btn3 onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=book-nav-btn3 onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
<button class=book-nav-btn3 onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div></div></aside></main></body></html>