<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>2022-06-29 Log | Minystory</title><meta name=keywords content="TIL,NLP"><meta name=description content="딥 러닝을 이용한 자연어 처리 입문 2"><meta name=author content="minyeamer"><link rel=canonical href=https://minyeamer.github.io/blog/2022-06-29/><meta name=google-site-verification content="u1tWcHmHUZWfFT1cHaku6sqU-bK40N3WLR-C-4VUWN0"><meta name=naver-site-verification content="6eaf8e9da1a6104780f056f1a7797fe5a3a5a0da"><link crossorigin=anonymous href=/assets/css/stylesheet.78a14cf8249250820b49e9dc59e58b846a9beea6d16d50612c68b121ddf02146.css integrity="sha256-eKFM+CSSUIILSencWeWLhGqb7qbRbVBhLGixId3wIUY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://minyeamer.github.io/img/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://minyeamer.github.io/img/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://minyeamer.github.io/img/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://minyeamer.github.io/img/favicons/apple-touch-icon.png><link rel=mask-icon href=https://minyeamer.github.io/img/favicons/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V")</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V",{anonymize_ip:!1})}</script><meta property="og:title" content="2022-06-29 Log"><meta property="og:description" content="딥 러닝을 이용한 자연어 처리 입문 2"><meta property="og:type" content="article"><meta property="og:url" content="https://minyeamer.github.io/blog/2022-06-29/"><meta property="og:image" content="https://github.com/minyeamer/til/blob/main/.media/main/thumbnail.png?raw=true"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-06-29T20:00:00+09:00"><meta property="article:modified_time" content="2022-06-29T20:00:00+09:00"><meta property="og:site_name" content="Minystory"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://github.com/minyeamer/til/blob/main/.media/main/thumbnail.png?raw=true"><meta name=twitter:title content="2022-06-29 Log"><meta name=twitter:description content="딥 러닝을 이용한 자연어 처리 입문 2"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://minyeamer.github.io/post/"},{"@type":"ListItem","position":2,"name":"2022-06-29 Log","item":"https://minyeamer.github.io/blog/2022-06-29/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"2022-06-29 Log","name":"2022-06-29 Log","description":"딥 러닝을 이용한 자연어 처리 입문 2","keywords":["TIL","NLP"],"articleBody":"09-01. Word Embedding Sparse Representation 벡터 또는 행렬의 값이 대부분 0으로 표현되는 방법, one-hot vector 등 단어의 개수가 늘어나면 벡터의 차원이 한없이 커지는 문제, 공간적 낭비 발생 Dense Representation 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤 (0과 1뿐 아니라 실수 포함) Word Embedding 단어를 밀집 벡터의 형태로 표현하는 방법 Embedding Vector: 워드 임베딩 과정을 통해 나온 결과 LSA, Word2Vec, FastText, Glove 등 09-02. Word2Vec Distributed Representation 희소 표현을 다차원 공간에 벡터화하는 방법 분산 표현 방법은 분포 가설이라는 가정 하에 만들어진 표현 방법 분포 가설: 비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다 (귀엽다, 예쁘다, 애교 등의 단어가 주로 함께 등장할 경우 벡터화 시 유사한 벡터값을 가짐) 저차원에 단어의 의미를 여러 차원에다가 분산하여 표현하기 대문에 단어 벡터 간 유의미한 유사도 계산 가능 Word2Vec의 학습 방식으로 CBOW(Continuous Bag of Words)와 Skip-Gram이 존재 CBOW 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법 중심 단어를 기준으로 window size만큼 앞뒤로 단어를 확인 (2n개의 단어 확인) Sliding Window: window를 옆으로 움직여서 주변 단어와 중심 단어의 선택을 변경해가며 학습 데이터셋 생성 CBOW의 인공 신경망은 주변 단어들의 one-hot vector를 입력으로 중간 단어의 one-hot vector를 예측 Word2Vec은 은닉층이 1개이며, 활성화 함수 없이 룩업 테이블이라는 연산을 담당하는 projection layer로 불림 입력층과 투사층 사이의 가중치 W는 ${V}\\times{M}$ 행렬, 투사층에서 출력층 사이의 가중치 W’는 ${M}\\times{V}$ 행렬 W와 W’는 동일한 행렬의 전치가 아니라 서로 다른 행렬이기 대문에 CBOW는 W와 W’를 학습해가는 구조를 가짐 입력 벡터와 가중치 W 행렬의 곱은 W 행렬의 i번째 행을 그대로 읽어오는 것(lookup)과 동일 주변 단어의 one-hot vector와 가중치 W를 곱한 결과 벡터들은 투사층에서 만나 평균인 벡터를 구함 평균 벡터는 두 번째 가중치 행렬 W’와 곱해져서 one-hot vector들과 차원이 V로 동일한 벡터가 나옴 해당 벡터에 softmax 함수를 거쳐 다중 클래스 분류 문제를 위한 score vector를 생성 score vector의 j번째 인덱스가 가진 값은 j번째 단어가 중심 단어일 확률로,\nscore vector $\\hat{y}$와 중심 단어의 one-hot vector $y$의 오차를 줄이기 위해 cross-entropy 함수 사용 Skip-gram CBOW와 반대로 중심 단어에서 주변 단어를 예측 중심 단어만을 입력으로 받기 때문에 투사층에서 벡터들의 평균을 구하는 과정은 없음 전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려짐 NNLM vs Word2Vec NNLM은 다음 단어를 예측하는 목적이지만, Word2Vec(CBOW)은 중심 단어를 예측하는 목적, 때문에 NNLM이 이전 단어들만 참고한다면, Word2Vec은 예측 단어의 전후 단어들을 모두 참고 Word2Vec은 NNLM에 존재하던 활성화 함수가 있는 은닉층을 제거하여 학습 속도에서 강점을 가짐 09-03. Word2Vec 실습 위키피디아 실습 1 2 3 4 from gensim.models import Word2Vec from gensim.models import KeyedVectors model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0) size: 워드 벡터의 특징 값, 임베딩된 벡터의 차원 window: context window size min_count: 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 무시) workers: 학습을 위한 프로세스 수 sg: 0은 CBOW, 1은 Skip-gram 1 2 3 4 5 # 입력한 단어에 대해서 가장 유사한 단어들을 출력 model_result = model.wv.most_similar(\"man\") print(model_result) [('woman', 0.842622697353363), ('guy', 0.8178728818893433), ('boy', 0.7774451375007629), ('lady', 0.7767927646636963), ('girl', 0.7583760023117065), ('gentleman', 0.7437191009521484), ('soldier', 0.7413754463195801), ('poet', 0.7060446739196777), ('kid', 0.6925194263458252), ('friend', 0.6572611331939697)] 1 2 model.wv.save_word2vec_format('eng_w2v') # 모델 저장 loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\") # 모델 로드 1 2 3 4 5 # 사전 훈련된 Word2Vec 임베딩 urllib.request.urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", filename=\"GoogleNews-vectors-negative300.bin.gz\") word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True) # shape(3000000, 300) 1 2 # 두 단어의 유사도 계산 print(word2vec_model.similarity('this', 'is')) # 0.407970363878 09-04. Negative Sampling Word2Vec이 학습 과정에서 전체 단어 집합이 아니라 일부 단어 집합에만 집중할 수 있도록 하는 방법 중심 단어에 대해 무작위로 선택된 일부 단어 집합에 대해 긍정 또는 부정을 예측하는 이진 분류 수행 전체 단어 집합의 크기만큼 선택지를 두고 다중 클래스 분류 문제를 푸는 Word2Vec보다 효율적인 연산 SGNS 네거티브 샘플링을 사용하는 Skip-gram은 중심 단어와 주변 단어가 모두 입력이 되고,\n두 단어가 실제로 윈도우 크개 내에 존재하는 이웃 관계인지 확률을 예측 중심 단어에 대한 라벨로 주변 단어를 사용하지 않고,\n중심 단어와 주변 단어에 대한 이웃 관계를 표시하기 위한 라벨로 1 또는 0을 사용 SGNS 구현 20newsgroups 데이터 사용 1 2 3 4 5 6 7 8 # 네거티브 샘플링 데이터셋 생성 from tensorflow.keras.preprocessing.sequence import skipgrams skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded] # (commited (7837), badar (34572)) -\u003e 0 # (whole (217), realize (1036)) -\u003e 1 # (reason (149), commited (7837)) -\u003e 1 1 2 3 from tensorflow.keras.models import Sequential, Model from tensorflow.keras.layers import Embedding, Reshape, Activation, Input from tensorflow.keras.layers import Dot 1 2 3 4 5 6 7 8 9 embedding_dim = 100 # 중심 단어를 위한 임베딩 테이블 w_inputs = Input(shape=(1, ), dtype='int32') word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs) # 주변 단어를 위한 임베딩 테이블 c_inputs = Input(shape=(1, ), dtype='int32') context_embedding = Embedding(vocab_size, embedding_dim)(c_inputs) 1 2 3 4 5 6 7 8 # 두 임베딩 테이블에 대한 내적의 결과로 1 또는 0을 예측하기 위해 시그모이드 함수 사용 dot_product = Dot(axes=2)([word_embedding, context_embedding]) dot_product = Reshape((1,), input_shape=(1, 1))(dot_product) output = Activation('sigmoid')(dot_product) model = Model(inputs=[w_inputs, c_inputs], outputs=output) model.summary() model.compile(loss='binary_crossentropy', optimizer='adam') 1 2 3 4 5 6 7 8 9 10 11 # 5epochs 학습 for epoch in range(1, 6): loss = 0 for _, elem in enumerate(skip_grams): first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32') second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32') labels = np.array(elem[1], dtype='int32') X = [first_elem, second_elem] Y = labels loss += model.train_on_batch(X,Y) print('Epoch :',epoch, 'Loss :',loss) 09-05. GloVe 카운트 기반과 예측 기반을 모두 사용하는 방법론으로, LSA와 Word2Vec의 단점 보완 LSA는 단어의 빈도수를 차원 축소하여 잠재된 의미를 끌어내지만, 같은 단어 의미의 유추 작업 성능은 떨어짐 Word2Vec는 예측 기반으로 단어 간 유추 작업에는 LSA보다 뛰어나지만,\nwindow size 내 주변 단어만을 고려하여 전체적인 통계 정보를 반영하지 못함 임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것이 목적 Window based Co-occurrence Matrix 행과 열을 전체 단어 집합의 단어들로 구성하고,\ni 단어의 window size 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬 Co-occurence Probability 동시 등장 확률 $P(k|i)$는 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고,\n특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # pip install glove_python_binary from glove import Corpus, Glove corpus = Corpus() # 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성 corpus.fit(result, window=5) glove = Glove(no_components=100, learning_rate=0.05) glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True) glove.add_dictionary(corpus.dictionary) print(glove.most_similar(\"man\")) [('woman', 0.9621753707315267), ('guy', 0.8860281455579162), ('girl', 0.8609057388487154), ('kid', 0.8383640509911114)] 09-06. FastText Word2Vec가 단어를 쪼개질 수 없는 단위로 생각한다면,\nFastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주 각 단어를 글자 단위 n-gram의 구성으로 취급하여 n에 따라 단어들이 얼마나 분리되는지 결정 tri-gram의 경우 apple에 대해서 []로 분리된 벡터 생성\n(\u003c, \u003e는 시작과 끝을 의미) 내부 단어들을 Word2Vec로 벡터화하고 apple의 벡터값은 내부 단어의 벡터값들의 총 합으로 구성 Out Of Vocabulary FastText는 데이터셋만 충분하다면 내부 단어를 통해 모르는 단어에 대해서도 유사도 계산 가능 birthplace를 학습하지 않은 상태라도, birth와 place라는 내부 단어가 있다면 벡터를 얻을 수 있음 Rare Word Word2Vec는 등장 빈도 수가 적은 단어에 대해서 임베딩의 정확도가 높지 않은 단점 FastText는 희귀 단어라도 n-gram이 다른 단어의 n-gram과 겹치는 경우라면,\nWord2Vec보다 비교적 높은 임베딩 벡터값을 얻음 오타와 같은 노이즈가 많은 코퍼스에서도 일정 수준의 성능을 보임 (apple, appple) 1 2 3 from gensim.models import FastText model = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1) 09-08. Pre-trained Word Embedding 사전 훈련된 워드 임베딩 참고 09-09. ELMo 언어 모델로 하는 임베딩이라는 뜻으로, 사전 훈련된 언어 모델을 사용 Word2Vec는 Bank Account와 River Bank에서 Bank의 차이를 구분하지 못하지만,\nELMo는 문맥을 반영한 워드 임베딩을 수행 ELMo 표현을 기존 임베딩 벡터와 연결(concatenate)해서 입력으로 사용 가능 biLM RNN 언어 모델에서 $h_t$는 시점이 지날수록 업데이트되기 때문에,\n문장의 문맥 정보를 점차적으로 반영함 ELMo는 양쪽 방향의 언어 모델(biLM)을 학습하여 활용 biLM은 은닉층이 최소 2개 이상인 다층 구조를 전제로 함 양방향 RNN은 순방향 RNN의 hidden state와 역방향 RNN의 hidden state를 연결하는 것이지만,\nbiLM은 순방향 언어 모델과 역방향 언어 모델이라는 두 개의 언어 모델을 별개의 모델로 보고 학습 각 층(embedding, hidden state)의 출력값이 가진 정보가 서로 다른 것이므로,\n이를 모두 활용하여 순방향 언어 모델과 역방향 언어 모델의 각 층의 출력값을 연결 ELMo Representation 각 층의 출력값을 연결(concatenate) 각 층의 출력값 별로 가중치($s_1, s_2, s_3$) 부여 각 층의 출력값을 모두 더함 (2번과 3번을 요약하여 가중합이라 표현) 벡터의 크기를 결정하는 스칼라 매개변수($\\gamma$)를 곱함 ELMo 활용 스팸 메일 분류하기 데이터 사용 1 2 3 4 # 텐서플로우 1버전에서 사용 가능 %tensorflow_version 1.x pip install tensorflow-hub import tensorflow_hub as hub 1 2 3 4 5 6 7 # 텐서플로우 허브로부터 ELMo를 다운로드 elmo = hub.Module(\"https://tfhub.dev/google/elmo/1\", trainable=True) sess = tf.Session() K.set_session(sess) sess.run(tf.global_variables_initializer()) sess.run(tf.tables_initializer()) 1 2 3 # 데이터의 이동이 케라스 → 텐서플로우 → 케라스가 되도록 하는 함수 def ELMoEmbedding(x): return elmo(tf.squeeze(tf.cast(x, tf.string)), as_dict=True, signature=\"default\") 1 2 3 4 5 6 7 8 9 from keras.models import Model from keras.layers import Dense, Lambda, Input input_text = Input(shape=(1,), dtype=tf.string) embedding_layer = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text) hidden_layer = Dense(256, activation='relu')(embedding_layer) output_layer = Dense(1, activation='sigmoid')(hidden_layer) model = Model(inputs=[input_text], outputs=output_layer) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) 09-10. Embedding Visualization 구글 embedding projector 시각화 도구 (논문 참고) 1 2 3 # !python -m gensim.scripts.word2vec2tensor --input 모델이름 --output 모델이름 !python -m gensim.scripts.word2vec2tensor --input eng_w2v --output eng_w2v # 임베딩 프로젝트에 사용할 metadata.tsv와 tensor.tsv 파일 생성 09-11. Document Embedding 문서 벡터를 이용한 추천 시스템 참고 문서 임베딩 : 워드 임베딩의 평균 참고 Doc2Vec으로 공시 사업보고서 유사도 계산하기 참고 10. RNN Text Classification 케라스를 이용한 텍스트 분류 개요 스팸 메일 분류하기 (RNN) 로이터 뉴스 분류하기 (LSTM) IMDB 리뷰 감성 분류하기 (GRU) 나이브 베이즈 분류기 네이버 영화 리뷰 감성 분류하기(LSTM) 네이버 쇼핑 리뷰 감성 분류하기(GRU) BiLSTM으로 한국어 스팀 리뷰 감성 분류하기 Bayes’ Theorem $$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$$\nNaive Bayes Classifier 베이즈 정리를 이용한 스팸 메일 확률 표현\nP(정상 메일 | 입력 텍스트) = (P(입력 텍스트 | 정상 메일) x P(정상 메일)) / P(입력 텍스트)\nP(스팸 메일 | 입력 텍스트) = (P(입력 텍스트 | 스팸 메일) x P(스팸 메일)) / P(입력 텍스트) 나이브 베이즈 분류기에서 토큰화 이전의 단어의 순서는 중요하지 않음\n(BoW와 같이 단어의 순서를 무시하고 빈도수만 고려) 정상 메일에 입력 텍스트가 없어 확률이 0%가 되는 것을 방지하기 위해\n각 단어에 대한 확률의 분모, 분자에 전부 숫자를 더해서 분자가 0이 되는 것을 방지하는 라플라스 스무딩 사용 1 2 3 4 5 6 from sklearn.feature_extraction.text import TfidfTransformer from sklearn.naive_bayes import MultinomialNB # 다항분포 나이브 베이즈 모델 # alpha=1.0: 라플라스 스무딩 적용 model = MultinomialNB() # MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) model.fit(tfidfv, newsdata.target) 11-01. Convolutional Neural Network 이미지 처리에 탁월한 성능을 보이는 신경망 합성곱 신경망은 convolutional layer와 pooling layer로 구성 합성곱 연산(CONV)의 결과가 ReLU를 거쳐서 POOL 구간을 지나는 과정 Channel: 이미지는 (높이, 너비, 채널)이라는 3차원 텐서로 구성, 채널은 색 성분을 의미 합성곱 신경망은 이미지의 모든 픽셀이 아닌, 커널과 맵핑되는 픽셀만을 입력으로 사용하여\n다층 퍼셉트론보다 훨씬 적은 수의 가중치를 사용하여 공간적 구조 정보를 보존 편향을 추가할 경우 커널을 적용한 뒤에 더해지며, 단 하나의 편향이 커널이 적용된 결과의 모든 원소에 더해짐 다수의 채널을 가진 입력 데이터일 경우 커널의 채널 수도 입력의 채널 수만큼 존재,\n각 채널 간 합성곱 연산을 마치고 그 결과를 모두 더해서 하나의 채널을 가지는 특성 맵 생성 Convolution Operation 이미지의 특징을 추출 Kernel(filter)라는 ${n}\\times{m}$ 크기의 행렬로 각 이미지를 순차적으로 훑음 Feature map: 합성곱 연산을 통해 나온 결과 Stride: 커널의 이동 범위, 특성 맵의 크기 Padding: 합성곱 연산 이후에도 특성 맵의 크기가 입력과 동일하도록 행과 열 추가 11-02. 1D CNN 1D Convolutions LSTM과 동일하게 각 단어가 벡터로 변환된 문장 행렬을 입력으로 받음 커널의 너비는 임베딩 벡터의 차원과 동일, 커널의 높이만으로 해당 커널의 크기라 간주 커널의 너비가 임베딩 벡터의 차원이기 때문에 너비 방향으로 움직이지 못하고 높이 방향으로만 움직임 Max-pooling 1D CNN에서의 폴링 층 각 합성곱 연산으로부터 얻은 결과 벡터에서 가장 큰 값을 가진 스칼라 값을 빼내는 연산을 수행 1D CNN 구현 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint embedding_dim = 256 # 임베딩 벡터의 차원 dropout_ratio = 0.3 # 드롭아웃 비율 num_filters = 256 # 커널의 수 kernel_size = 3 # 커널의 크기 hidden_units = 128 # 뉴런의 수 model = Sequential() model.add(Embedding(vocab_size, embedding_dim)) model.add(Dropout(dropout_ratio)) model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu')) model.add(GlobalMaxPooling1D()) model.add(Dense(hidden_units, activation='relu')) model.add(Dropout(dropout_ratio)) model.add(Dense(1, activation='sigmoid')) es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3) mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc']) history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[es, mc]) 11-06. Intent Classification 사전 훈련된 워드 임베딩을 이용한 의도 분류 참고 11-07. Character Embedding ‘misunderstand’의 의미를 ‘mis-‘라는 접두사와 ‘understand’를 통해 추측하는 것과 같이,\n사람의 이해 능력을 흉내내는 알고리즘 1D CNN에서는 단어를 문자 단위로 쪼개기만하면 되기 때문에 OOV라도 벡터를 얻을 수 있음 BiLSTM에서도 문자에 대한 임베딩을 통해 얻은 벡터를 단어에 대한 벡터로 사용 12. Tagging Task 양방향 LSTM를 이용한 품사 태깅 개체명 인식 개체명 인식의 BIO 표현 이해하기 BiLSTM을 이용한 개체명 인식 BiLSTM-CRF를 이용한 개체명 인식 문자 임베딩 활용하기 BIO 표현 개체명이 시작되는 부분에 B(Begin), 개체명의 내부에 I(Inside), 나머지로 O(Outside) 태깅 개체명 태깅엔 LOC(location), ORG(organization), PER(person), MISC(miscellaneous)\n태그가 추가로 붙음 (B-ORG 등) CRF(Conditional Random Field) LSTM 위에 CRF 층을 추가하면 모델은 예측 개체명(레이블 간 의존성)을 고려 기존 양방향 LSTM 모델은 활성화 함수를 지난 시점에서 개체명을 결정했지만,\nCRF 층을 추가한 모델에서는 활성화 함수의 결과들이 CRF 층의 입력으로 전달 CRF 층은 [문장의 첫번쨰 단어에서는 I가 나오지 않는다, O-I 패턴은 나오지 않는다] 등의 제약사항을 학습 양방향 LSTM은 입력 단어에 대한 양방향 문맥을 반영하며, CRF는 출력 레이블에 대한 양방향 문맥을 반영 CRF 층은 one-hot encoding된 라벨을 지원하지 않음 13-01. Byte Pair Encoding UNK(Unknown Token) 등의 OOV 문제를 해결하기 위해 서브워드 분리 작업을 수행 BPE 알고리즘은 연속적으로 가장 많이 등장한 글자의 쌍을 찾아서 하나의 글자로 병합 BPE는 글자 단위에서 점차적으로 단어 집합을 만들어내는 Bottom up 방식의 접근 사용 WordPiece Tokenizer BPE의 변형 알고리즘으로, 코퍼스의 likelihood를 가장 높이는 쌍을 병합 모든 단어의 맨 앞에 _를 붙이고, 단어는 subword로 통계에 기반하여 띄어쓰기로 분리 WordPiece TOkenizer 겨로가를 되돌리기 위해서는 모든 띄어쓰기를 제거하고 언더바를 띄어쓰기로 바꿈 Unigram Language Model Tokenizer 각각의 서브워드들에 대해서 손실(loss)을 계산 서브 단어의 손실은 해당 서브워드가 단어 집합에서 제거되었을 경우, 코퍼스의 likelihood가 감소하는 정도 서브워드들의 손실의 정도를 정렬하여, 최악의 영향을 주는 10~20%의 토큰을 제거 13-02. SentencePiece 내부 단어 분리를 위한 구글의 패키지 사전 토큰화 작업없이 단어 분리 토큰화를 수행하여 언어에 종속적이지 않음 1 2 3 4 import sentencepiece as spm # IMDB 리뷰 데이터 사용 spm.SentencePieceTrainer.Train('--input=imdb_review.txt --model_prefix=imdb --vocab_size=5000 --model_type=bpe --max_sentence_length=9999') input: 학습시킬 파일 model_prefix: 만들어질 모델 이름 vocab_size: 단어 집합의 크기 model_type: 사용할 모델 (unigram(default), bpe, char, word) max_sentence_length: 문장의 최대 길이 13-03. SubwordTextEncoder Wordpiece 모델을 채택한 텐서플로우의 서브워드 토크나이저 1 2 3 4 import tensorflow_datasets as tfds tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus( train_df['review'], target_vocab_size=2**13) 13-04. Huggingface Tokenizer BertWordPieceTokenizer: BERT에서 사용된 워드피스 토크나이저(WordPiece Tokenizer) CharBPETokenizer: 오리지널 BPE ByteLevelBPETokenizer: BPE의 바이트 레벨 버전 SentencePieceBPETokenizer: 앞서 본 패키지 센텐스피스(SentencePiece)와 호환되는 BPE 구현체 BertWordPieceTokenizer 1 2 3 from tokenizers import BertWordPieceTokenizer tokenizer = BertWordPieceTokenizer(lowercase=False, trip_accents=False) 1 2 3 4 5 6 7 8 9 data_file = 'naver_review.txt' vocab_size = 30000 limit_alphabet = 6000 min_frequency = 5 tokenizer.train(files=data_file, vocab_size=vocab_size, limit_alphabet=limit_alphabet, min_frequency=min_frequency) 14-01. Sequence-to-Sequence(seq2seq) seq2seq는 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 다양한 분야에서 사용되는 모델 챗봇, 기계 번역, 내용 요약, STT(Speech to Text) 등에서 주로 사용 seq2seq는 인코더와 디코더로 나눠지며, 둘 다 LSTM 셀 또는 GRU 셀을 사용하는 RNN 아키텍처로 구성 softmax 함수를 통해 출력 시퀀스의 각 단어별 확률값을 반환하고, 디코더는 출력 단어를 결정 Encoder 입력 문장의 모든 단어들을 순차적으로 입력받고 모든 단어 정보들을 압축해서 하나의 벡터 생성 인코더 RNN 셀의 마지막 hidden state를 context vector로 디코더에 넘겨줌 Decoder 압축된 컨텍스트 벡터를 받아서 번역된 단어를 한 개씩 순차적으로 출력 기본적으로 RNNLM으로, 초기 입력으로 문장의 시작을 의미하는 심볼 가 들어감 첫번째 시점의 디코더 RNN 셀은 예측된 단어를 다음 시점의 RNN 셀 입력으로 넣으며,\n문장의 끝을 의미하는 심볼인 가 다음 단어로 예측될 때까지 반복해서 예측 훈련 과정에서는 실제 정답 상황에서 가 나와야 된다고 정답을 알려줌\n(교사 강요: 이전 시점의 디코더 셀의 예측이 틀릴 경우 연쇄 작용을 방지) seq2seq 구현 프랑스-영어 병렬 코퍼스 데이터 사용 병렬 코퍼스 데이터에서 쌍이 되는 데이터의 길이가 같지 않음에 주의 1 2 3 4 5 6 7 8 9 10 # Encoder encoder_inputs = Input(shape=(None, src_vocab_size)) encoder_lstm = LSTM(units=256, return_state=True) # encoder_outputs은 여기서는 불필요 encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs) # LSTM은 바닐라 RNN과는 달리 상태가 두 개, 은닉 상태와 셀 상태 encoder_states = [state_h, state_c] 1 2 3 4 5 6 7 8 9 10 11 12 13 # Decoder decoder_inputs = Input(shape=(None, tar_vocab_size)) decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True) # 디코더에게 인코더의 은닉 상태, 셀 상태를 전달 decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states) decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax') decoder_outputs = decoder_softmax_layer(decoder_outputs) model = Model([encoder_inputs, decoder_inputs], decoder_outputs) model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\") 1 model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=40, validation_split=0.2) seq2seq 동작 번역하고자 하는 입력 문장이 인코더에 들어가서 은닉 상태와 셀 상태를 얻음 상태와 에 해당하는 ‘\\t’를 디코더로 보냄 디코더가 에 해당하는 ‘\\n’이 나올 때까지 다음 문자를 예측하는 행동을 반복 14-02. BLEU Score Bilingual Evaluation Understudy(BLEU) 기계 번역 결과와 사람이 직접 번역한 결과가 얼마나 유사한지 비교하여 번역에 대한 성능을 측정하는 방법 측정 기준은 n-gram에 기반 언어에 구애받지 않고 사용할 수 있으며, 계산 속도가 빠른 이점 PPL과 달리 높을 수록 성능이 더 좋음을 의미 Unigram Precision 사람이 번역한 문장 중 어느 한 문장이라도 등장한 단어의 개수를 카운트하는 측정 방법 기계 번역기가 번역한 문장을 Ca, 사람이 번역한 문장을 Ref라 표현 $$\\text{Unigram Precision}=\\frac{\\text{Ref들 중에서 존재하는 Ca의 단어의 수}}{\\text{Ca의 총 단어 수}}$$\nModified Unigram Precision 하나의 단어가 여러번 반복되는 경우에서 정밀도가 1이 나오는 문제를 개선하기 위해\n유니그램이 이미 매칭된 적이 있는지를 고려 $Max_Ref_Count$: 유니그램이 하나의 Ref에서 최대 몇 번 등장했는지 카운트 $Count_{dip}=min(Count,Max_Ref_Count)$ $$\\text{Modified Unigram Precision}=\\frac{\\text{Ca의 각 유니그램에 대해 }Count_{dip}\\text{을 수행한 값의 총 합}}{\\text{Ca의 총 유니그램 수}}$$\nBLEU Score 유니그램 정밀도는 단어의 빈도수로 접근하기 때문에 단어의 순서를 고려하기 위해 n-gram 이용 BLEU 최종 식은 보정된 정밀도 $p_1,p_2,…,p_n$을 모두 조합 해당 BLEU 식의 경우 문장의 길이가 짧을 때 높은 점수를 받는 문제가 있기 때문에,\n길이가 짧은 문장에게 Brevity Penalty를 줄 필요가 있음 $BP$는 Ca와 가장 길이 차이가 작은 Ref의 길이 $r$을 기준으로 $e^{(1-r/c)}$ 값을 곱하며,\n문장이 $r$보다 길어 패널티를 줄 필요가 없는 경우 1이어야 함 $$\\text{보정된 정밀도 } p_1=\\frac{\\Sigma_{{unigram}\\in{Candidate}}Count_{dip}(unigram)}{\\Sigma_{{unigram}\\in{Candidate}}Count(unigram)}$$ $$\\text{n-gram 일반화 } p_n=\\frac{\\Sigma_{{n\\text{-}gram}\\in{Candidate}}Count_{dip}(n\\text{-}gram)}{\\Sigma_{{n\\text{-}gram}\\in{Candidate}}Count(n\\text{-}gram)}$$ $$BLEU={BP}\\times{exp(\\Sigma^N_{n=1}{w_n}{\\log{p_n}})}$$\n1 2 3 import nltk.translate.bleu_score bleu_score(candidate.split(),list(map(lambda ref: ref.split(), references))) ","wordCount":"2850","inLanguage":"en","datePublished":"2022-06-29T20:00:00+09:00","dateModified":"2022-06-29T20:00:00+09:00","author":{"@type":"Person","name":"minyeamer"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://minyeamer.github.io/blog/2022-06-29/"},"publisher":{"@type":"Organization","name":"Minystory","logo":{"@type":"ImageObject","url":"https://minyeamer.github.io/img/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://minyeamer.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://minyeamer.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://minyeamer.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://minyeamer.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://minyeamer.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://minyeamer.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://minyeamer.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://minyeamer.github.io/post/>Posts</a></div><h1 class=post-title>2022-06-29 Log</h1><div class=post-meta><span title='2022-06-29 20:00:00 +0900 KST'>June 29, 2022</span>&nbsp;·&nbsp;14 min&nbsp;·&nbsp;2850 words&nbsp;·&nbsp;minyeamer&nbsp;|&nbsp;<a href=https://github.com/minyeamer/til/edit/main/study/2022/2022-06/2022-06-29.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>&nbsp;Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#09-01-word-embedding>09-01. Word Embedding</a><ul><li><a href=#sparse-representation>Sparse Representation</a></li><li><a href=#dense-representation>Dense Representation</a></li><li><a href=#word-embedding>Word Embedding</a></li></ul></li><li><a href=#09-02-word2vec>09-02. Word2Vec</a><ul><li><a href=#distributed-representation>Distributed Representation</a></li><li><a href=#cbow>CBOW</a></li><li><a href=#skip-gram>Skip-gram</a></li><li><a href=#nnlm-vs-word2vec>NNLM vs Word2Vec</a></li></ul></li><li><a href=#09-03-word2vec-실습httpswikidocsnet50739>09-03. <a href=https://wikidocs.net/50739>Word2Vec 실습</a></a></li><li><a href=#09-04-negative-sampling>09-04. Negative Sampling</a><ul><li><a href=#sgns>SGNS</a></li><li><a href=#sgns-구현httpswikidocsnet69141><a href=https://wikidocs.net/69141>SGNS 구현</a></a></li></ul></li><li><a href=#09-05-glove>09-05. GloVe</a><ul><li><a href=#window-based-co-occurrence-matrixhttpwebstanfordeduclasscs224nslidescs224n-2019-lecture02-wordvecs2pdf><a href=http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture02-wordvecs2.pdf>Window based Co-occurrence Matrix</a></a></li><li><a href=#co-occurence-probability>Co-occurence Probability</a></li></ul></li><li><a href=#09-06-fasttext>09-06. FastText</a><ul><li><a href=#out-of-vocabulary>Out Of Vocabulary</a></li><li><a href=#rare-word>Rare Word</a></li></ul></li><li><a href=#09-08-pre-trained-word-embedding>09-08. Pre-trained Word Embedding</a></li><li><a href=#09-09-elmo>09-09. ELMo</a><ul><li><a href=#bilm>biLM</a></li><li><a href=#elmo-representation>ELMo Representation</a></li><li><a href=#elmo-활용httpswikidocsnet33930><a href=https://wikidocs.net/33930>ELMo 활용</a></a></li></ul></li><li><a href=#09-10-embedding-visualization>09-10. Embedding Visualization</a></li><li><a href=#09-11-document-embedding>09-11. Document Embedding</a></li><li><a href=#10-rnn-text-classification>10. RNN Text Classification</a><ul><li><a href=#bayes-theorem>Bayes&rsquo; Theorem</a></li><li><a href=#naive-bayes-classifier>Naive Bayes Classifier</a></li></ul></li><li><a href=#11-01-convolutional-neural-network>11-01. Convolutional Neural Network</a><ul><li><a href=#convolution-operation>Convolution Operation</a></li></ul></li><li><a href=#11-02-1d-cnn>11-02. 1D CNN</a><ul><li><a href=#1d-convolutions>1D Convolutions</a></li><li><a href=#max-pooling>Max-pooling</a></li><li><a href=#1d-cnn-구현httpswikidocsnet80783><a href=https://wikidocs.net/80783>1D CNN 구현</a></a></li></ul></li><li><a href=#11-06-intent-classification>11-06. Intent Classification</a></li><li><a href=#11-07-character-embedding>11-07. Character Embedding</a></li><li><a href=#12-tagging-task>12. Tagging Task</a><ul><li><a href=#bio-표현>BIO 표현</a></li><li><a href=#crfconditional-random-field>CRF(Conditional Random Field)</a></li></ul></li><li><a href=#13-01-byte-pair-encoding>13-01. Byte Pair Encoding</a><ul><li><a href=#wordpiece-tokenizer>WordPiece Tokenizer</a></li><li><a href=#unigram-language-model-tokenizer>Unigram Language Model Tokenizer</a></li></ul></li><li><a href=#13-02-sentencepiece>13-02. SentencePiece</a></li><li><a href=#13-03-subwordtextencoder>13-03. SubwordTextEncoder</a></li><li><a href=#13-04-huggingface-tokenizer>13-04. Huggingface Tokenizer</a><ul><li><a href=#bertwordpiecetokenizer>BertWordPieceTokenizer</a></li></ul></li><li><a href=#14-01-sequence-to-sequenceseq2seq>14-01. Sequence-to-Sequence(seq2seq)</a><ul><li><a href=#encoder>Encoder</a></li><li><a href=#decoder>Decoder</a></li><li><a href=#seq2seq-구현httpswikidocsnet24996><a href=https://wikidocs.net/24996>seq2seq 구현</a></a></li><li><a href=#seq2seq-동작>seq2seq 동작</a></li></ul></li><li><a href=#14-02-bleu-score>14-02. BLEU Score</a><ul><li><a href=#bilingual-evaluation-understudybleu>Bilingual Evaluation Understudy(BLEU)</a></li><li><a href=#unigram-precision>Unigram Precision</a></li><li><a href=#modified-unigram-precision>Modified Unigram Precision</a></li><li><a href=#bleu-score>BLEU Score</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h1 id=09-01-word-embedding>09-01. Word Embedding<a hidden class=anchor aria-hidden=true href=#09-01-word-embedding>#</a></h1><h2 id=sparse-representation>Sparse Representation<a hidden class=anchor aria-hidden=true href=#sparse-representation>#</a></h2><ul><li>벡터 또는 행렬의 값이 대부분 0으로 표현되는 방법, one-hot vector 등</li><li>단어의 개수가 늘어나면 벡터의 차원이 한없이 커지는 문제, 공간적 낭비 발생</li></ul><h2 id=dense-representation>Dense Representation<a hidden class=anchor aria-hidden=true href=#dense-representation>#</a></h2><ul><li>사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤 (0과 1뿐 아니라 실수 포함)</li></ul><h2 id=word-embedding>Word Embedding<a hidden class=anchor aria-hidden=true href=#word-embedding>#</a></h2><ul><li>단어를 밀집 벡터의 형태로 표현하는 방법</li><li><strong>Embedding Vector</strong>: 워드 임베딩 과정을 통해 나온 결과</li><li>LSA, Word2Vec, FastText, Glove 등</li></ul><h1 id=09-02-word2vec>09-02. Word2Vec<a hidden class=anchor aria-hidden=true href=#09-02-word2vec>#</a></h1><h2 id=distributed-representation>Distributed Representation<a hidden class=anchor aria-hidden=true href=#distributed-representation>#</a></h2><ul><li>희소 표현을 다차원 공간에 벡터화하는 방법</li><li>분산 표현 방법은 분포 가설이라는 가정 하에 만들어진 표현 방법</li><li><strong>분포 가설</strong>: 비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다
(귀엽다, 예쁘다, 애교 등의 단어가 주로 함께 등장할 경우 벡터화 시 유사한 벡터값을 가짐)</li><li>저차원에 단어의 의미를 여러 차원에다가 분산하여 표현하기 대문에 단어 벡터 간 유의미한 유사도 계산 가능</li><li>Word2Vec의 학습 방식으로 CBOW(Continuous Bag of Words)와 Skip-Gram이 존재</li></ul><h2 id=cbow>CBOW<a hidden class=anchor aria-hidden=true href=#cbow>#</a></h2><ul><li>주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법</li><li>중심 단어를 기준으로 window size만큼 앞뒤로 단어를 확인 (2n개의 단어 확인)</li><li><strong>Sliding Window</strong>: window를 옆으로 움직여서 주변 단어와 중심 단어의 선택을 변경해가며 학습 데이터셋 생성</li><li>CBOW의 인공 신경망은 주변 단어들의 one-hot vector를 입력으로 중간 단어의 one-hot vector를 예측</li><li>Word2Vec은 은닉층이 1개이며, 활성화 함수 없이 룩업 테이블이라는 연산을 담당하는 projection layer로 불림</li><li>입력층과 투사층 사이의 가중치 W는 ${V}\times{M}$ 행렬, 투사층에서 출력층 사이의 가중치 W&rsquo;는 ${M}\times{V}$ 행렬</li><li>W와 W&rsquo;는 동일한 행렬의 전치가 아니라 서로 다른 행렬이기 대문에 CBOW는 W와 W&rsquo;를 학습해가는 구조를 가짐</li><li>입력 벡터와 가중치 W 행렬의 곱은 W 행렬의 i번째 행을 그대로 읽어오는 것(lookup)과 동일</li><li>주변 단어의 one-hot vector와 가중치 W를 곱한 결과 벡터들은 투사층에서 만나 평균인 벡터를 구함</li><li>평균 벡터는 두 번째 가중치 행렬 W&rsquo;와 곱해져서 one-hot vector들과 차원이 V로 동일한 벡터가 나옴</li><li>해당 벡터에 softmax 함수를 거쳐 다중 클래스 분류 문제를 위한 score vector를 생성</li><li>score vector의 j번째 인덱스가 가진 값은 j번째 단어가 중심 단어일 확률로,<br>score vector $\hat{y}$와 중심 단어의 one-hot vector $y$의 오차를 줄이기 위해 cross-entropy 함수 사용</li></ul><h2 id=skip-gram>Skip-gram<a hidden class=anchor aria-hidden=true href=#skip-gram>#</a></h2><ul><li>CBOW와 반대로 중심 단어에서 주변 단어를 예측</li><li>중심 단어만을 입력으로 받기 때문에 투사층에서 벡터들의 평균을 구하는 과정은 없음</li><li>전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려짐</li></ul><h2 id=nnlm-vs-word2vec>NNLM vs Word2Vec<a hidden class=anchor aria-hidden=true href=#nnlm-vs-word2vec>#</a></h2><ul><li>NNLM은 다음 단어를 예측하는 목적이지만, Word2Vec(CBOW)은 중심 단어를 예측하는 목적,
때문에 NNLM이 이전 단어들만 참고한다면, Word2Vec은 예측 단어의 전후 단어들을 모두 참고</li><li>Word2Vec은 NNLM에 존재하던 활성화 함수가 있는 은닉층을 제거하여 학습 속도에서 강점을 가짐</li></ul><h1 id=09-03-word2vec-실습httpswikidocsnet50739>09-03. <a href=https://wikidocs.net/50739 target=_blank rel=noopener>Word2Vec 실습</a><a hidden class=anchor aria-hidden=true href=#09-03-word2vec-실습httpswikidocsnet50739>#</a></h1><ul><li><a href=https://wikidocs.net/152606 target=_blank rel=noopener>위키피디아 실습</a></li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>gensim.models</span> <span class=kn>import</span> <span class=n>Word2Vec</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>gensim.models</span> <span class=kn>import</span> <span class=n>KeyedVectors</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Word2Vec</span><span class=p>(</span><span class=n>sentences</span><span class=o>=</span><span class=n>result</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>window</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>min_count</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>workers</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>sg</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><code>size</code>: 워드 벡터의 특징 값, 임베딩된 벡터의 차원</li><li><code>window</code>: context window size</li><li><code>min_count</code>: 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 무시)</li><li><code>workers</code>: 학습을 위한 프로세스 수</li><li><code>sg</code>: 0은 CBOW, 1은 Skip-gram</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 입력한 단어에 대해서 가장 유사한 단어들을 출력</span>
</span></span><span class=line><span class=cl><span class=n>model_result</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>wv</span><span class=o>.</span><span class=n>most_similar</span><span class=p>(</span><span class=s2>&#34;man&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>model_result</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>[(</span><span class=s1>&#39;woman&#39;</span><span class=p>,</span> <span class=mf>0.842622697353363</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;guy&#39;</span><span class=p>,</span> <span class=mf>0.8178728818893433</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;boy&#39;</span><span class=p>,</span> <span class=mf>0.7774451375007629</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;lady&#39;</span><span class=p>,</span> <span class=mf>0.7767927646636963</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;girl&#39;</span><span class=p>,</span> <span class=mf>0.7583760023117065</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;gentleman&#39;</span><span class=p>,</span> <span class=mf>0.7437191009521484</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;soldier&#39;</span><span class=p>,</span> <span class=mf>0.7413754463195801</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;poet&#39;</span><span class=p>,</span> <span class=mf>0.7060446739196777</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;kid&#39;</span><span class=p>,</span> <span class=mf>0.6925194263458252</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;friend&#39;</span><span class=p>,</span> <span class=mf>0.6572611331939697</span><span class=p>)]</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>wv</span><span class=o>.</span><span class=n>save_word2vec_format</span><span class=p>(</span><span class=s1>&#39;eng_w2v&#39;</span><span class=p>)</span> <span class=c1># 모델 저장</span>
</span></span><span class=line><span class=cl><span class=n>loaded_model</span> <span class=o>=</span> <span class=n>KeyedVectors</span><span class=o>.</span><span class=n>load_word2vec_format</span><span class=p>(</span><span class=s2>&#34;eng_w2v&#34;</span><span class=p>)</span> <span class=c1># 모델 로드</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 사전 훈련된 Word2Vec 임베딩</span>
</span></span><span class=line><span class=cl><span class=n>urllib</span><span class=o>.</span><span class=n>request</span><span class=o>.</span><span class=n>urlretrieve</span><span class=p>(</span><span class=s2>&#34;https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>							<span class=n>filename</span><span class=o>=</span><span class=s2>&#34;GoogleNews-vectors-negative300.bin.gz&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>word2vec_model</span> <span class=o>=</span> <span class=n>KeyedVectors</span><span class=o>.</span><span class=n>load_word2vec_format</span><span class=p>(</span><span class=s1>&#39;GoogleNews-vectors-negative300.bin.gz&#39;</span><span class=p>,</span> <span class=n>binary</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># shape(3000000, 300)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 두 단어의 유사도 계산</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>word2vec_model</span><span class=o>.</span><span class=n>similarity</span><span class=p>(</span><span class=s1>&#39;this&#39;</span><span class=p>,</span> <span class=s1>&#39;is&#39;</span><span class=p>))</span> <span class=c1># 0.407970363878</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=09-04-negative-sampling>09-04. Negative Sampling<a hidden class=anchor aria-hidden=true href=#09-04-negative-sampling>#</a></h1><ul><li>Word2Vec이 학습 과정에서 전체 단어 집합이 아니라 일부 단어 집합에만 집중할 수 있도록 하는 방법</li><li>중심 단어에 대해 무작위로 선택된 일부 단어 집합에 대해 긍정 또는 부정을 예측하는 이진 분류 수행</li><li>전체 단어 집합의 크기만큼 선택지를 두고 다중 클래스 분류 문제를 푸는 Word2Vec보다 효율적인 연산</li></ul><h2 id=sgns>SGNS<a hidden class=anchor aria-hidden=true href=#sgns>#</a></h2><ul><li>네거티브 샘플링을 사용하는 Skip-gram은 중심 단어와 주변 단어가 모두 입력이 되고,<br>두 단어가 실제로 윈도우 크개 내에 존재하는 이웃 관계인지 확률을 예측</li><li>중심 단어에 대한 라벨로 주변 단어를 사용하지 않고,<br>중심 단어와 주변 단어에 대한 이웃 관계를 표시하기 위한 라벨로 1 또는 0을 사용</li></ul><h2 id=sgns-구현httpswikidocsnet69141><a href=https://wikidocs.net/69141 target=_blank rel=noopener>SGNS 구현</a><a hidden class=anchor aria-hidden=true href=#sgns-구현httpswikidocsnet69141>#</a></h2><ul><li>20newsgroups 데이터 사용</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 네거티브 샘플링 데이터셋 생성</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.preprocessing.sequence</span> <span class=kn>import</span> <span class=n>skipgrams</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>skip_grams</span> <span class=o>=</span> <span class=p>[</span><span class=n>skipgrams</span><span class=p>(</span><span class=n>sample</span><span class=p>,</span> <span class=n>vocabulary_size</span><span class=o>=</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>window_size</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span> <span class=k>for</span> <span class=n>sample</span> <span class=ow>in</span> <span class=n>encoded</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># (commited (7837), badar (34572)) -&gt; 0</span>
</span></span><span class=line><span class=cl><span class=c1># (whole (217), realize (1036)) -&gt; 1</span>
</span></span><span class=line><span class=cl><span class=c1># (reason (149), commited (7837)) -&gt; 1</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.models</span> <span class=kn>import</span> <span class=n>Sequential</span><span class=p>,</span> <span class=n>Model</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.layers</span> <span class=kn>import</span> <span class=n>Embedding</span><span class=p>,</span> <span class=n>Reshape</span><span class=p>,</span> <span class=n>Activation</span><span class=p>,</span> <span class=n>Input</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.layers</span> <span class=kn>import</span> <span class=n>Dot</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>embedding_dim</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 중심 단어를 위한 임베딩 테이블</span>
</span></span><span class=line><span class=cl><span class=n>w_inputs</span> <span class=o>=</span> <span class=n>Input</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=s1>&#39;int32&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>word_embedding</span> <span class=o>=</span> <span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>)(</span><span class=n>w_inputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 주변 단어를 위한 임베딩 테이블</span>
</span></span><span class=line><span class=cl><span class=n>c_inputs</span> <span class=o>=</span> <span class=n>Input</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=s1>&#39;int32&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>context_embedding</span>  <span class=o>=</span> <span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>)(</span><span class=n>c_inputs</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 두 임베딩 테이블에 대한 내적의 결과로 1 또는 0을 예측하기 위해 시그모이드 함수 사용</span>
</span></span><span class=line><span class=cl><span class=n>dot_product</span> <span class=o>=</span> <span class=n>Dot</span><span class=p>(</span><span class=n>axes</span><span class=o>=</span><span class=mi>2</span><span class=p>)([</span><span class=n>word_embedding</span><span class=p>,</span> <span class=n>context_embedding</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>dot_product</span> <span class=o>=</span> <span class=n>Reshape</span><span class=p>((</span><span class=mi>1</span><span class=p>,),</span> <span class=n>input_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>))(</span><span class=n>dot_product</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>Activation</span><span class=p>(</span><span class=s1>&#39;sigmoid&#39;</span><span class=p>)(</span><span class=n>dot_product</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Model</span><span class=p>(</span><span class=n>inputs</span><span class=o>=</span><span class=p>[</span><span class=n>w_inputs</span><span class=p>,</span> <span class=n>c_inputs</span><span class=p>],</span> <span class=n>outputs</span><span class=o>=</span><span class=n>output</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>summary</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;binary_crossentropy&#39;</span><span class=p>,</span> <span class=n>optimizer</span><span class=o>=</span><span class=s1>&#39;adam&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 5epochs 학습</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>6</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>_</span><span class=p>,</span> <span class=n>elem</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>skip_grams</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>first_elem</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=o>*</span><span class=n>elem</span><span class=p>[</span><span class=mi>0</span><span class=p>]))[</span><span class=mi>0</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=s1>&#39;int32&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>second_elem</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=o>*</span><span class=n>elem</span><span class=p>[</span><span class=mi>0</span><span class=p>]))[</span><span class=mi>1</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=s1>&#39;int32&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>labels</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>elem</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=s1>&#39;int32&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>X</span> <span class=o>=</span> <span class=p>[</span><span class=n>first_elem</span><span class=p>,</span> <span class=n>second_elem</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>Y</span> <span class=o>=</span> <span class=n>labels</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>+=</span> <span class=n>model</span><span class=o>.</span><span class=n>train_on_batch</span><span class=p>(</span><span class=n>X</span><span class=p>,</span><span class=n>Y</span><span class=p>)</span>  
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Epoch :&#39;</span><span class=p>,</span><span class=n>epoch</span><span class=p>,</span> <span class=s1>&#39;Loss :&#39;</span><span class=p>,</span><span class=n>loss</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=09-05-glove>09-05. GloVe<a hidden class=anchor aria-hidden=true href=#09-05-glove>#</a></h1><ul><li>카운트 기반과 예측 기반을 모두 사용하는 방법론으로, LSA와 Word2Vec의 단점 보완</li><li>LSA는 단어의 빈도수를 차원 축소하여 잠재된 의미를 끌어내지만, 같은 단어 의미의 유추 작업 성능은 떨어짐</li><li>Word2Vec는 예측 기반으로 단어 간 유추 작업에는 LSA보다 뛰어나지만,<br>window size 내 주변 단어만을 고려하여 전체적인 통계 정보를 반영하지 못함</li><li>임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것이 목적</li></ul><h2 id=window-based-co-occurrence-matrixhttpwebstanfordeduclasscs224nslidescs224n-2019-lecture02-wordvecs2pdf><a href=http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture02-wordvecs2.pdf target=_blank rel=noopener>Window based Co-occurrence Matrix</a><a hidden class=anchor aria-hidden=true href=#window-based-co-occurrence-matrixhttpwebstanfordeduclasscs224nslidescs224n-2019-lecture02-wordvecs2pdf>#</a></h2><ul><li>행과 열을 전체 단어 집합의 단어들로 구성하고,<br>i 단어의 window size 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬</li></ul><h2 id=co-occurence-probability>Co-occurence Probability<a hidden class=anchor aria-hidden=true href=#co-occurence-probability>#</a></h2><ul><li>동시 등장 확률 $P(k|i)$는 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고,<br>특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># pip install glove_python_binary</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>glove</span> <span class=kn>import</span> <span class=n>Corpus</span><span class=p>,</span> <span class=n>Glove</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>corpus</span> <span class=o>=</span> <span class=n>Corpus</span><span class=p>()</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성</span>
</span></span><span class=line><span class=cl><span class=n>corpus</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>result</span><span class=p>,</span> <span class=n>window</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>glove</span> <span class=o>=</span> <span class=n>Glove</span><span class=p>(</span><span class=n>no_components</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.05</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>glove</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>corpus</span><span class=o>.</span><span class=n>matrix</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=n>no_threads</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>glove</span><span class=o>.</span><span class=n>add_dictionary</span><span class=p>(</span><span class=n>corpus</span><span class=o>.</span><span class=n>dictionary</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>glove</span><span class=o>.</span><span class=n>most_similar</span><span class=p>(</span><span class=s2>&#34;man&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>[(</span><span class=s1>&#39;woman&#39;</span><span class=p>,</span> <span class=mf>0.9621753707315267</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;guy&#39;</span><span class=p>,</span> <span class=mf>0.8860281455579162</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;girl&#39;</span><span class=p>,</span> <span class=mf>0.8609057388487154</span><span class=p>),</span> <span class=p>(</span><span class=s1>&#39;kid&#39;</span><span class=p>,</span> <span class=mf>0.8383640509911114</span><span class=p>)]</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=09-06-fasttext>09-06. FastText<a hidden class=anchor aria-hidden=true href=#09-06-fasttext>#</a></h1><ul><li>Word2Vec가 단어를 쪼개질 수 없는 단위로 생각한다면,<br>FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주</li><li>각 단어를 글자 단위 n-gram의 구성으로 취급하여 n에 따라 단어들이 얼마나 분리되는지 결정</li><li>tri-gram의 경우 apple에 대해서 [&lt;ap, app, ppl, ple, le>]로 분리된 벡터 생성<br>(&lt;, >는 시작과 끝을 의미)</li><li>내부 단어들을 Word2Vec로 벡터화하고 apple의 벡터값은 내부 단어의 벡터값들의 총 합으로 구성</li></ul><h2 id=out-of-vocabulary>Out Of Vocabulary<a hidden class=anchor aria-hidden=true href=#out-of-vocabulary>#</a></h2><ul><li>FastText는 데이터셋만 충분하다면 내부 단어를 통해 모르는 단어에 대해서도 유사도 계산 가능</li><li>birthplace를 학습하지 않은 상태라도, birth와 place라는 내부 단어가 있다면 벡터를 얻을 수 있음</li></ul><h2 id=rare-word>Rare Word<a hidden class=anchor aria-hidden=true href=#rare-word>#</a></h2><ul><li>Word2Vec는 등장 빈도 수가 적은 단어에 대해서 임베딩의 정확도가 높지 않은 단점</li><li>FastText는 희귀 단어라도 n-gram이 다른 단어의 n-gram과 겹치는 경우라면,<br>Word2Vec보다 비교적 높은 임베딩 벡터값을 얻음</li><li>오타와 같은 노이즈가 많은 코퍼스에서도 일정 수준의 성능을 보임 (apple, appple)</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>gensim.models</span> <span class=kn>import</span> <span class=n>FastText</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>FastText</span><span class=p>(</span><span class=n>result</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>window</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>min_count</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>workers</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>sg</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=09-08-pre-trained-word-embedding>09-08. Pre-trained Word Embedding<a hidden class=anchor aria-hidden=true href=#09-08-pre-trained-word-embedding>#</a></h1><ul><li>사전 훈련된 워드 임베딩 <a href=https://wikidocs.net/33793 target=_blank rel=noopener>참고</a></li></ul><h1 id=09-09-elmo>09-09. ELMo<a hidden class=anchor aria-hidden=true href=#09-09-elmo>#</a></h1><ul><li>언어 모델로 하는 임베딩이라는 뜻으로, 사전 훈련된 언어 모델을 사용</li><li>Word2Vec는 Bank Account와 River Bank에서 Bank의 차이를 구분하지 못하지만,<br>ELMo는 문맥을 반영한 워드 임베딩을 수행</li><li>ELMo 표현을 기존 임베딩 벡터와 연결(concatenate)해서 입력으로 사용 가능</li></ul><h2 id=bilm>biLM<a hidden class=anchor aria-hidden=true href=#bilm>#</a></h2><ul><li>RNN 언어 모델에서 $h_t$는 시점이 지날수록 업데이트되기 때문에,<br>문장의 문맥 정보를 점차적으로 반영함</li><li>ELMo는 양쪽 방향의 언어 모델(biLM)을 학습하여 활용</li><li>biLM은 은닉층이 최소 2개 이상인 다층 구조를 전제로 함</li><li>양방향 RNN은 순방향 RNN의 hidden state와 역방향 RNN의 hidden state를 연결하는 것이지만,<br>biLM은 순방향 언어 모델과 역방향 언어 모델이라는 두 개의 언어 모델을 별개의 모델로 보고 학습</li><li>각 층(embedding, hidden state)의 출력값이 가진 정보가 서로 다른 것이므로,<br>이를 모두 활용하여 순방향 언어 모델과 역방향 언어 모델의 각 층의 출력값을 연결</li></ul><h2 id=elmo-representation>ELMo Representation<a hidden class=anchor aria-hidden=true href=#elmo-representation>#</a></h2><ol><li>각 층의 출력값을 연결(concatenate)</li><li>각 층의 출력값 별로 가중치($s_1, s_2, s_3$) 부여</li><li>각 층의 출력값을 모두 더함 (2번과 3번을 요약하여 가중합이라 표현)</li><li>벡터의 크기를 결정하는 스칼라 매개변수($\gamma$)를 곱함</li></ol><h2 id=elmo-활용httpswikidocsnet33930><a href=https://wikidocs.net/33930 target=_blank rel=noopener>ELMo 활용</a><a hidden class=anchor aria-hidden=true href=#elmo-활용httpswikidocsnet33930>#</a></h2><ul><li><a href=https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv target=_blank rel=noopener>스팸 메일 분류하기 데이터</a> 사용</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 텐서플로우 1버전에서 사용 가능</span>
</span></span><span class=line><span class=cl><span class=o>%</span><span class=n>tensorflow_version</span> <span class=mf>1.</span><span class=n>x</span>
</span></span><span class=line><span class=cl><span class=n>pip</span> <span class=n>install</span> <span class=n>tensorflow</span><span class=o>-</span><span class=n>hub</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorflow_hub</span> <span class=k>as</span> <span class=nn>hub</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 텐서플로우 허브로부터 ELMo를 다운로드</span>
</span></span><span class=line><span class=cl><span class=n>elmo</span> <span class=o>=</span> <span class=n>hub</span><span class=o>.</span><span class=n>Module</span><span class=p>(</span><span class=s2>&#34;https://tfhub.dev/google/elmo/1&#34;</span><span class=p>,</span> <span class=n>trainable</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>sess</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Session</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>K</span><span class=o>.</span><span class=n>set_session</span><span class=p>(</span><span class=n>sess</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>global_variables_initializer</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>tables_initializer</span><span class=p>())</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 데이터의 이동이 케라스 → 텐서플로우 → 케라스가 되도록 하는 함수</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>ELMoEmbedding</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>elmo</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>cast</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>string</span><span class=p>)),</span> <span class=n>as_dict</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>signature</span><span class=o>=</span><span class=s2>&#34;default&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>keras.models</span> <span class=kn>import</span> <span class=n>Model</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>keras.layers</span> <span class=kn>import</span> <span class=n>Dense</span><span class=p>,</span> <span class=n>Lambda</span><span class=p>,</span> <span class=n>Input</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>input_text</span> <span class=o>=</span> <span class=n>Input</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=mi>1</span><span class=p>,),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>string</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>embedding_layer</span> <span class=o>=</span> <span class=n>Lambda</span><span class=p>(</span><span class=n>ELMoEmbedding</span><span class=p>,</span> <span class=n>output_shape</span><span class=o>=</span><span class=p>(</span><span class=mi>1024</span><span class=p>,</span> <span class=p>))(</span><span class=n>input_text</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>hidden_layer</span> <span class=o>=</span> <span class=n>Dense</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>)(</span><span class=n>embedding_layer</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output_layer</span> <span class=o>=</span> <span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;sigmoid&#39;</span><span class=p>)(</span><span class=n>hidden_layer</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Model</span><span class=p>(</span><span class=n>inputs</span><span class=o>=</span><span class=p>[</span><span class=n>input_text</span><span class=p>],</span> <span class=n>outputs</span><span class=o>=</span><span class=n>output_layer</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;binary_crossentropy&#39;</span><span class=p>,</span> <span class=n>optimizer</span><span class=o>=</span><span class=s1>&#39;adam&#39;</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=09-10-embedding-visualization>09-10. Embedding Visualization<a hidden class=anchor aria-hidden=true href=#09-10-embedding-visualization>#</a></h1><ul><li>구글 <a href=https://projector.tensorflow.org/ target=_blank rel=noopener>embedding projector</a> 시각화 도구 (<a href=https://arxiv.org/pdf/1611.05469v1.pdf target=_blank rel=noopener>논문 참고</a>)</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># !python -m gensim.scripts.word2vec2tensor --input 모델이름 --output 모델이름</span>
</span></span><span class=line><span class=cl><span class=err>!</span><span class=n>python</span> <span class=o>-</span><span class=n>m</span> <span class=n>gensim</span><span class=o>.</span><span class=n>scripts</span><span class=o>.</span><span class=n>word2vec2tensor</span> <span class=o>--</span><span class=nb>input</span> <span class=n>eng_w2v</span> <span class=o>--</span><span class=n>output</span> <span class=n>eng_w2v</span>
</span></span><span class=line><span class=cl><span class=c1># 임베딩 프로젝트에 사용할 metadata.tsv와 tensor.tsv 파일 생성</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=09-11-document-embedding>09-11. Document Embedding<a hidden class=anchor aria-hidden=true href=#09-11-document-embedding>#</a></h1><ul><li>문서 벡터를 이용한 추천 시스템 <a href=https://wikidocs.net/102705 target=_blank rel=noopener>참고</a></li><li>문서 임베딩 : 워드 임베딩의 평균 <a href=https://wikidocs.net/103496 target=_blank rel=noopener>참고</a></li><li>Doc2Vec으로 공시 사업보고서 유사도 계산하기 <a href=https://wikidocs.net/155356 target=_blank rel=noopener>참고</a></li></ul><hr><h1 id=10-rnn-text-classification>10. RNN Text Classification<a hidden class=anchor aria-hidden=true href=#10-rnn-text-classification>#</a></h1><ul><li><a href=https://wikidocs.net/24873 target=_blank rel=noopener>케라스를 이용한 텍스트 분류 개요</a></li><li><a href=https://wikidocs.net/22894 target=_blank rel=noopener>스팸 메일 분류하기 (RNN)</a></li><li><a href=https://wikidocs.net/22933 target=_blank rel=noopener>로이터 뉴스 분류하기 (LSTM)</a></li><li><a href=https://wikidocs.net/24586 target=_blank rel=noopener>IMDB 리뷰 감성 분류하기 (GRU)</a></li><li><a href=https://wikidocs.net/22892 target=_blank rel=noopener>나이브 베이즈 분류기</a></li><li><a href=https://wikidocs.net/44249 target=_blank rel=noopener>네이버 영화 리뷰 감성 분류하기(LSTM)</a></li><li><a href=https://wikidocs.net/94600 target=_blank rel=noopener>네이버 쇼핑 리뷰 감성 분류하기(GRU)</a></li><li><a href=https://wikidocs.net/94748 target=_blank rel=noopener>BiLSTM으로 한국어 스팀 리뷰 감성 분류하기</a></li></ul><h2 id=bayes-theorem>Bayes&rsquo; Theorem<a hidden class=anchor aria-hidden=true href=#bayes-theorem>#</a></h2><p>$$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$</p><h2 id=naive-bayes-classifier>Naive Bayes Classifier<a hidden class=anchor aria-hidden=true href=#naive-bayes-classifier>#</a></h2><ul><li>베이즈 정리를 이용한 스팸 메일 확률 표현<br>P(정상 메일 | 입력 텍스트) = (P(입력 텍스트 | 정상 메일) x P(정상 메일)) / P(입력 텍스트)<br>P(스팸 메일 | 입력 텍스트) = (P(입력 텍스트 | 스팸 메일) x P(스팸 메일)) / P(입력 텍스트)</li><li>나이브 베이즈 분류기에서 토큰화 이전의 단어의 순서는 중요하지 않음<br>(BoW와 같이 단어의 순서를 무시하고 빈도수만 고려)</li><li>정상 메일에 입력 텍스트가 없어 확률이 0%가 되는 것을 방지하기 위해<br>각 단어에 대한 확률의 분모, 분자에 전부 숫자를 더해서 분자가 0이 되는 것을 방지하는 라플라스 스무딩 사용</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.feature_extraction.text</span> <span class=kn>import</span> <span class=n>TfidfTransformer</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.naive_bayes</span> <span class=kn>import</span> <span class=n>MultinomialNB</span> <span class=c1># 다항분포 나이브 베이즈 모델</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># alpha=1.0: 라플라스 스무딩 적용</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>MultinomialNB</span><span class=p>()</span> <span class=c1># MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>tfidfv</span><span class=p>,</span> <span class=n>newsdata</span><span class=o>.</span><span class=n>target</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><hr><h1 id=11-01-convolutional-neural-network>11-01. Convolutional Neural Network<a hidden class=anchor aria-hidden=true href=#11-01-convolutional-neural-network>#</a></h1><ul><li>이미지 처리에 탁월한 성능을 보이는 신경망</li><li>합성곱 신경망은 convolutional layer와 pooling layer로 구성</li><li>합성곱 연산(CONV)의 결과가 ReLU를 거쳐서 POOL 구간을 지나는 과정</li><li><strong>Channel</strong>: 이미지는 (높이, 너비, 채널)이라는 3차원 텐서로 구성, 채널은 색 성분을 의미</li><li>합성곱 신경망은 이미지의 모든 픽셀이 아닌, 커널과 맵핑되는 픽셀만을 입력으로 사용하여<br>다층 퍼셉트론보다 훨씬 적은 수의 가중치를 사용하여 공간적 구조 정보를 보존</li><li>편향을 추가할 경우 커널을 적용한 뒤에 더해지며, 단 하나의 편향이 커널이 적용된 결과의 모든 원소에 더해짐</li><li>다수의 채널을 가진 입력 데이터일 경우 커널의 채널 수도 입력의 채널 수만큼 존재,<br>각 채널 간 합성곱 연산을 마치고 그 결과를 모두 더해서 하나의 채널을 가지는 특성 맵 생성</li></ul><h2 id=convolution-operation>Convolution Operation<a hidden class=anchor aria-hidden=true href=#convolution-operation>#</a></h2><ul><li>이미지의 특징을 추출</li><li>Kernel(filter)라는 ${n}\times{m}$ 크기의 행렬로 각 이미지를 순차적으로 훑음</li><li><strong>Feature map</strong>: 합성곱 연산을 통해 나온 결과</li><li><strong>Stride</strong>: 커널의 이동 범위, 특성 맵의 크기</li><li><strong>Padding</strong>: 합성곱 연산 이후에도 특성 맵의 크기가 입력과 동일하도록 행과 열 추가</li></ul><h1 id=11-02-1d-cnn>11-02. 1D CNN<a hidden class=anchor aria-hidden=true href=#11-02-1d-cnn>#</a></h1><h2 id=1d-convolutions>1D Convolutions<a hidden class=anchor aria-hidden=true href=#1d-convolutions>#</a></h2><ul><li>LSTM과 동일하게 각 단어가 벡터로 변환된 문장 행렬을 입력으로 받음</li><li>커널의 너비는 임베딩 벡터의 차원과 동일, 커널의 높이만으로 해당 커널의 크기라 간주</li><li>커널의 너비가 임베딩 벡터의 차원이기 때문에 너비 방향으로 움직이지 못하고 높이 방향으로만 움직임</li></ul><h2 id=max-pooling>Max-pooling<a hidden class=anchor aria-hidden=true href=#max-pooling>#</a></h2><ul><li>1D CNN에서의 폴링 층</li><li>각 합성곱 연산으로부터 얻은 결과 벡터에서 가장 큰 값을 가진 스칼라 값을 빼내는 연산을 수행</li></ul><h2 id=1d-cnn-구현httpswikidocsnet80783><a href=https://wikidocs.net/80783 target=_blank rel=noopener>1D CNN 구현</a><a hidden class=anchor aria-hidden=true href=#1d-cnn-구현httpswikidocsnet80783>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.models</span> <span class=kn>import</span> <span class=n>Sequential</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.layers</span> <span class=kn>import</span> <span class=n>Embedding</span><span class=p>,</span> <span class=n>Dropout</span><span class=p>,</span> <span class=n>Conv1D</span><span class=p>,</span> <span class=n>GlobalMaxPooling1D</span><span class=p>,</span> <span class=n>Dense</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.keras.callbacks</span> <span class=kn>import</span> <span class=n>EarlyStopping</span><span class=p>,</span> <span class=n>ModelCheckpoint</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>embedding_dim</span> <span class=o>=</span> <span class=mi>256</span> <span class=c1># 임베딩 벡터의 차원</span>
</span></span><span class=line><span class=cl><span class=n>dropout_ratio</span> <span class=o>=</span> <span class=mf>0.3</span> <span class=c1># 드롭아웃 비율</span>
</span></span><span class=line><span class=cl><span class=n>num_filters</span> <span class=o>=</span> <span class=mi>256</span> <span class=c1># 커널의 수</span>
</span></span><span class=line><span class=cl><span class=n>kernel_size</span> <span class=o>=</span> <span class=mi>3</span> <span class=c1># 커널의 크기</span>
</span></span><span class=line><span class=cl><span class=n>hidden_units</span> <span class=o>=</span> <span class=mi>128</span> <span class=c1># 뉴런의 수</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>embedding_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout_ratio</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Conv1D</span><span class=p>(</span><span class=n>num_filters</span><span class=p>,</span> <span class=n>kernel_size</span><span class=p>,</span> <span class=n>padding</span><span class=o>=</span><span class=s1>&#39;valid&#39;</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>GlobalMaxPooling1D</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=n>hidden_units</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;relu&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout_ratio</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;sigmoid&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>es</span> <span class=o>=</span> <span class=n>EarlyStopping</span><span class=p>(</span><span class=n>monitor</span><span class=o>=</span><span class=s1>&#39;val_loss&#39;</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s1>&#39;min&#39;</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>patience</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>mc</span> <span class=o>=</span> <span class=n>ModelCheckpoint</span><span class=p>(</span><span class=s1>&#39;best_model.h5&#39;</span><span class=p>,</span> <span class=n>monitor</span><span class=o>=</span><span class=s1>&#39;val_acc&#39;</span><span class=p>,</span> <span class=n>mode</span><span class=o>=</span><span class=s1>&#39;max&#39;</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>save_best_only</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=s1>&#39;adam&#39;</span><span class=p>,</span> <span class=n>loss</span><span class=o>=</span><span class=s1>&#39;binary_crossentropy&#39;</span><span class=p>,</span> <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;acc&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>history</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=n>validation_data</span><span class=o>=</span><span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>),</span> <span class=n>callbacks</span><span class=o>=</span><span class=p>[</span><span class=n>es</span><span class=p>,</span> <span class=n>mc</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=11-06-intent-classification>11-06. Intent Classification<a hidden class=anchor aria-hidden=true href=#11-06-intent-classification>#</a></h1><ul><li>사전 훈련된 워드 임베딩을 이용한 의도 분류 <a href=https://wikidocs.net/86083 target=_blank rel=noopener>참고</a></li></ul><h1 id=11-07-character-embedding>11-07. Character Embedding<a hidden class=anchor aria-hidden=true href=#11-07-character-embedding>#</a></h1><ul><li>&lsquo;misunderstand&rsquo;의 의미를 &lsquo;mis-&lsquo;라는 접두사와 &lsquo;understand&rsquo;를 통해 추측하는 것과 같이,<br>사람의 이해 능력을 흉내내는 알고리즘</li><li>1D CNN에서는 단어를 문자 단위로 쪼개기만하면 되기 때문에 OOV라도 벡터를 얻을 수 있음</li><li>BiLSTM에서도 문자에 대한 임베딩을 통해 얻은 벡터를 단어에 대한 벡터로 사용</li></ul><hr><h1 id=12-tagging-task>12. Tagging Task<a hidden class=anchor aria-hidden=true href=#12-tagging-task>#</a></h1><ul><li><a href=https://wikidocs.net/33532 target=_blank rel=noopener>양방향 LSTM를 이용한 품사 태깅</a></li><li><a href=https://wikidocs.net/30682 target=_blank rel=noopener>개체명 인식</a></li><li><a href=https://wikidocs.net/24682 target=_blank rel=noopener>개체명 인식의 BIO 표현 이해하기</a></li><li><a href=https://wikidocs.net/147219 target=_blank rel=noopener>BiLSTM을 이용한 개체명 인식</a></li><li><a href=https://wikidocs.net/147234 target=_blank rel=noopener>BiLSTM-CRF를 이용한 개체명 인식</a></li><li><a href=https://wikidocs.net/147299 target=_blank rel=noopener>문자 임베딩 활용하기</a></li></ul><h2 id=bio-표현>BIO 표현<a hidden class=anchor aria-hidden=true href=#bio-표현>#</a></h2><ul><li>개체명이 시작되는 부분에 B(Begin), 개체명의 내부에 I(Inside), 나머지로 O(Outside) 태깅</li><li>개체명 태깅엔 LOC(location), ORG(organization), PER(person), MISC(miscellaneous)<br>태그가 추가로 붙음 (B-ORG 등)</li></ul><h2 id=crfconditional-random-field>CRF(Conditional Random Field)<a hidden class=anchor aria-hidden=true href=#crfconditional-random-field>#</a></h2><ul><li>LSTM 위에 CRF 층을 추가하면 모델은 예측 개체명(레이블 간 의존성)을 고려</li><li>기존 양방향 LSTM 모델은 활성화 함수를 지난 시점에서 개체명을 결정했지만,<br>CRF 층을 추가한 모델에서는 활성화 함수의 결과들이 CRF 층의 입력으로 전달</li><li>CRF 층은 [문장의 첫번쨰 단어에서는 I가 나오지 않는다, O-I 패턴은 나오지 않는다] 등의 제약사항을 학습</li><li>양방향 LSTM은 입력 단어에 대한 양방향 문맥을 반영하며, CRF는 출력 레이블에 대한 양방향 문맥을 반영</li><li>CRF 층은 one-hot encoding된 라벨을 지원하지 않음</li></ul><hr><h1 id=13-01-byte-pair-encoding>13-01. Byte Pair Encoding<a hidden class=anchor aria-hidden=true href=#13-01-byte-pair-encoding>#</a></h1><ul><li>UNK(Unknown Token) 등의 OOV 문제를 해결하기 위해 서브워드 분리 작업을 수행</li><li>BPE 알고리즘은 연속적으로 가장 많이 등장한 글자의 쌍을 찾아서 하나의 글자로 병합</li><li>BPE는 글자 단위에서 점차적으로 단어 집합을 만들어내는 Bottom up 방식의 접근 사용</li></ul><h2 id=wordpiece-tokenizer>WordPiece Tokenizer<a hidden class=anchor aria-hidden=true href=#wordpiece-tokenizer>#</a></h2><ul><li>BPE의 변형 알고리즘으로, 코퍼스의 likelihood를 가장 높이는 쌍을 병합</li><li>모든 단어의 맨 앞에 _를 붙이고, 단어는 subword로 통계에 기반하여 띄어쓰기로 분리</li><li>WordPiece TOkenizer 겨로가를 되돌리기 위해서는 모든 띄어쓰기를 제거하고 언더바를 띄어쓰기로 바꿈</li></ul><h2 id=unigram-language-model-tokenizer>Unigram Language Model Tokenizer<a hidden class=anchor aria-hidden=true href=#unigram-language-model-tokenizer>#</a></h2><ul><li>각각의 서브워드들에 대해서 손실(loss)을 계산</li><li>서브 단어의 손실은 해당 서브워드가 단어 집합에서 제거되었을 경우, 코퍼스의 likelihood가 감소하는 정도</li><li>서브워드들의 손실의 정도를 정렬하여, 최악의 영향을 주는 10~20%의 토큰을 제거</li></ul><h1 id=13-02-sentencepiece>13-02. SentencePiece<a hidden class=anchor aria-hidden=true href=#13-02-sentencepiece>#</a></h1><ul><li>내부 단어 분리를 위한 <a href=https://github.com/google/sentencepiece target=_blank rel=noopener>구글의 패키지</a></li><li>사전 토큰화 작업없이 단어 분리 토큰화를 수행하여 언어에 종속적이지 않음</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>sentencepiece</span> <span class=k>as</span> <span class=nn>spm</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># IMDB 리뷰 데이터 사용</span>
</span></span><span class=line><span class=cl><span class=n>spm</span><span class=o>.</span><span class=n>SentencePieceTrainer</span><span class=o>.</span><span class=n>Train</span><span class=p>(</span><span class=s1>&#39;--input=imdb_review.txt --model_prefix=imdb --vocab_size=5000 --model_type=bpe --max_sentence_length=9999&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><ul><li>input: 학습시킬 파일</li><li>model_prefix: 만들어질 모델 이름</li><li>vocab_size: 단어 집합의 크기</li><li>model_type: 사용할 모델 (unigram(default), bpe, char, word)</li><li>max_sentence_length: 문장의 최대 길이</li></ul><h1 id=13-03-subwordtextencoder>13-03. SubwordTextEncoder<a hidden class=anchor aria-hidden=true href=#13-03-subwordtextencoder>#</a></h1><ul><li>Wordpiece 모델을 채택한 텐서플로우의 서브워드 토크나이저</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorflow_datasets</span> <span class=k>as</span> <span class=nn>tfds</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>tfds</span><span class=o>.</span><span class=n>features</span><span class=o>.</span><span class=n>text</span><span class=o>.</span><span class=n>SubwordTextEncoder</span><span class=o>.</span><span class=n>build_from_corpus</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>train_df</span><span class=p>[</span><span class=s1>&#39;review&#39;</span><span class=p>],</span> <span class=n>target_vocab_size</span><span class=o>=</span><span class=mi>2</span><span class=o>**</span><span class=mi>13</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h1 id=13-04-huggingface-tokenizer>13-04. Huggingface Tokenizer<a hidden class=anchor aria-hidden=true href=#13-04-huggingface-tokenizer>#</a></h1><ul><li><strong>BertWordPieceTokenizer</strong>: BERT에서 사용된 워드피스 토크나이저(WordPiece Tokenizer)</li><li><strong>CharBPETokenizer</strong>: 오리지널 BPE</li><li><strong>ByteLevelBPETokenizer</strong>: BPE의 바이트 레벨 버전</li><li><strong>SentencePieceBPETokenizer</strong>: 앞서 본 패키지 센텐스피스(SentencePiece)와 호환되는 BPE 구현체</li></ul><h2 id=bertwordpiecetokenizer>BertWordPieceTokenizer<a hidden class=anchor aria-hidden=true href=#bertwordpiecetokenizer>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tokenizers</span> <span class=kn>import</span> <span class=n>BertWordPieceTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>BertWordPieceTokenizer</span><span class=p>(</span><span class=n>lowercase</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>trip_accents</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>data_file</span> <span class=o>=</span> <span class=s1>&#39;naver_review.txt&#39;</span>
</span></span><span class=line><span class=cl><span class=n>vocab_size</span> <span class=o>=</span> <span class=mi>30000</span>
</span></span><span class=line><span class=cl><span class=n>limit_alphabet</span> <span class=o>=</span> <span class=mi>6000</span>
</span></span><span class=line><span class=cl><span class=n>min_frequency</span> <span class=o>=</span> <span class=mi>5</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>files</span><span class=o>=</span><span class=n>data_file</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>vocab_size</span><span class=o>=</span><span class=n>vocab_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>limit_alphabet</span><span class=o>=</span><span class=n>limit_alphabet</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>min_frequency</span><span class=o>=</span><span class=n>min_frequency</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><hr><h1 id=14-01-sequence-to-sequenceseq2seq>14-01. Sequence-to-Sequence(seq2seq)<a hidden class=anchor aria-hidden=true href=#14-01-sequence-to-sequenceseq2seq>#</a></h1><ul><li>seq2seq는 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 다양한 분야에서 사용되는 모델</li><li>챗봇, 기계 번역, 내용 요약, STT(Speech to Text) 등에서 주로 사용</li><li>seq2seq는 인코더와 디코더로 나눠지며, 둘 다 LSTM 셀 또는 GRU 셀을 사용하는 RNN 아키텍처로 구성</li><li>softmax 함수를 통해 출력 시퀀스의 각 단어별 확률값을 반환하고, 디코더는 출력 단어를 결정</li></ul><h2 id=encoder>Encoder<a hidden class=anchor aria-hidden=true href=#encoder>#</a></h2><ul><li>입력 문장의 모든 단어들을 순차적으로 입력받고 모든 단어 정보들을 압축해서 하나의 벡터 생성</li><li>인코더 RNN 셀의 마지막 hidden state를 context vector로 디코더에 넘겨줌</li></ul><h2 id=decoder>Decoder<a hidden class=anchor aria-hidden=true href=#decoder>#</a></h2><ul><li>압축된 컨텍스트 벡터를 받아서 번역된 단어를 한 개씩 순차적으로 출력</li><li>기본적으로 RNNLM으로, 초기 입력으로 문장의 시작을 의미하는 심볼 <sos>가 들어감</li><li>첫번째 시점의 디코더 RNN 셀은 예측된 단어를 다음 시점의 RNN 셀 입력으로 넣으며,<br>문장의 끝을 의미하는 심볼인 <eos>가 다음 단어로 예측될 때까지 반복해서 예측</li><li>훈련 과정에서는 실제 정답 상황에서 <eos>가 나와야 된다고 정답을 알려줌<br>(교사 강요: 이전 시점의 디코더 셀의 예측이 틀릴 경우 연쇄 작용을 방지)</li></ul><h2 id=seq2seq-구현httpswikidocsnet24996><a href=https://wikidocs.net/24996 target=_blank rel=noopener>seq2seq 구현</a><a hidden class=anchor aria-hidden=true href=#seq2seq-구현httpswikidocsnet24996>#</a></h2><ul><li><a href=http://www.manythings.org/anki target=_blank rel=noopener>프랑스-영어 병렬 코퍼스 데이터</a> 사용</li><li>병렬 코퍼스 데이터에서 쌍이 되는 데이터의 길이가 같지 않음에 주의</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Encoder</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>encoder_inputs</span> <span class=o>=</span> <span class=n>Input</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=kc>None</span><span class=p>,</span> <span class=n>src_vocab_size</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>encoder_lstm</span> <span class=o>=</span> <span class=n>LSTM</span><span class=p>(</span><span class=n>units</span><span class=o>=</span><span class=mi>256</span><span class=p>,</span> <span class=n>return_state</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># encoder_outputs은 여기서는 불필요</span>
</span></span><span class=line><span class=cl><span class=n>encoder_outputs</span><span class=p>,</span> <span class=n>state_h</span><span class=p>,</span> <span class=n>state_c</span> <span class=o>=</span> <span class=n>encoder_lstm</span><span class=p>(</span><span class=n>encoder_inputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># LSTM은 바닐라 RNN과는 달리 상태가 두 개, 은닉 상태와 셀 상태</span>
</span></span><span class=line><span class=cl><span class=n>encoder_states</span> <span class=o>=</span> <span class=p>[</span><span class=n>state_h</span><span class=p>,</span> <span class=n>state_c</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Decoder</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>decoder_inputs</span> <span class=o>=</span> <span class=n>Input</span><span class=p>(</span><span class=n>shape</span><span class=o>=</span><span class=p>(</span><span class=kc>None</span><span class=p>,</span> <span class=n>tar_vocab_size</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>decoder_lstm</span> <span class=o>=</span> <span class=n>LSTM</span><span class=p>(</span><span class=n>units</span><span class=o>=</span><span class=mi>256</span><span class=p>,</span> <span class=n>return_sequences</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>return_state</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 디코더에게 인코더의 은닉 상태, 셀 상태를 전달</span>
</span></span><span class=line><span class=cl><span class=n>decoder_outputs</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>_</span><span class=o>=</span> <span class=n>decoder_lstm</span><span class=p>(</span><span class=n>decoder_inputs</span><span class=p>,</span> <span class=n>initial_state</span><span class=o>=</span><span class=n>encoder_states</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>decoder_softmax_layer</span> <span class=o>=</span> <span class=n>Dense</span><span class=p>(</span><span class=n>tar_vocab_size</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;softmax&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>decoder_outputs</span> <span class=o>=</span> <span class=n>decoder_softmax_layer</span><span class=p>(</span><span class=n>decoder_outputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Model</span><span class=p>([</span><span class=n>encoder_inputs</span><span class=p>,</span> <span class=n>decoder_inputs</span><span class=p>],</span> <span class=n>decoder_outputs</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>optimizer</span><span class=o>=</span><span class=s2>&#34;rmsprop&#34;</span><span class=p>,</span> <span class=n>loss</span><span class=o>=</span><span class=s2>&#34;categorical_crossentropy&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>x</span><span class=o>=</span><span class=p>[</span><span class=n>encoder_input</span><span class=p>,</span> <span class=n>decoder_input</span><span class=p>],</span> <span class=n>y</span><span class=o>=</span><span class=n>decoder_target</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>epochs</span><span class=o>=</span><span class=mi>40</span><span class=p>,</span> <span class=n>validation_split</span><span class=o>=</span><span class=mf>0.2</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=seq2seq-동작>seq2seq 동작<a hidden class=anchor aria-hidden=true href=#seq2seq-동작>#</a></h2><ol><li>번역하고자 하는 입력 문장이 인코더에 들어가서 은닉 상태와 셀 상태를 얻음</li><li>상태와 <sos>에 해당하는 &lsquo;\t&rsquo;를 디코더로 보냄</li><li>디코더가 <eos>에 해당하는 &lsquo;\n&rsquo;이 나올 때까지 다음 문자를 예측하는 행동을 반복</li></ol><h1 id=14-02-bleu-score>14-02. BLEU Score<a hidden class=anchor aria-hidden=true href=#14-02-bleu-score>#</a></h1><h2 id=bilingual-evaluation-understudybleu>Bilingual Evaluation Understudy(BLEU)<a hidden class=anchor aria-hidden=true href=#bilingual-evaluation-understudybleu>#</a></h2><ul><li>기계 번역 결과와 사람이 직접 번역한 결과가 얼마나 유사한지 비교하여 번역에 대한 성능을 측정하는 방법</li><li>측정 기준은 n-gram에 기반</li><li>언어에 구애받지 않고 사용할 수 있으며, 계산 속도가 빠른 이점</li><li>PPL과 달리 높을 수록 성능이 더 좋음을 의미</li></ul><h2 id=unigram-precision>Unigram Precision<a hidden class=anchor aria-hidden=true href=#unigram-precision>#</a></h2><ul><li>사람이 번역한 문장 중 어느 한 문장이라도 등장한 단어의 개수를 카운트하는 측정 방법</li><li>기계 번역기가 번역한 문장을 Ca, 사람이 번역한 문장을 Ref라 표현</li></ul><p>$$\text{Unigram Precision}=\frac{\text{Ref들 중에서 존재하는 Ca의 단어의 수}}{\text{Ca의 총 단어 수}}$$</p><h2 id=modified-unigram-precision>Modified Unigram Precision<a hidden class=anchor aria-hidden=true href=#modified-unigram-precision>#</a></h2><ul><li>하나의 단어가 여러번 반복되는 경우에서 정밀도가 1이 나오는 문제를 개선하기 위해<br>유니그램이 이미 매칭된 적이 있는지를 고려</li><li>$Max_Ref_Count$: 유니그램이 하나의 Ref에서 최대 몇 번 등장했는지 카운트</li><li>$Count_{dip}=min(Count,Max_Ref_Count)$</li></ul><p>$$\text{Modified Unigram Precision}=\frac{\text{Ca의 각 유니그램에 대해 }Count_{dip}\text{을 수행한 값의 총 합}}{\text{Ca의 총 유니그램 수}}$$</p><h2 id=bleu-score>BLEU Score<a hidden class=anchor aria-hidden=true href=#bleu-score>#</a></h2><ul><li>유니그램 정밀도는 단어의 빈도수로 접근하기 때문에 단어의 순서를 고려하기 위해 n-gram 이용</li><li>BLEU 최종 식은 보정된 정밀도 $p_1,p_2,&mldr;,p_n$을 모두 조합</li><li>해당 BLEU 식의 경우 문장의 길이가 짧을 때 높은 점수를 받는 문제가 있기 때문에,<br>길이가 짧은 문장에게 Brevity Penalty를 줄 필요가 있음</li><li>$BP$는 Ca와 가장 길이 차이가 작은 Ref의 길이 $r$을 기준으로 $e^{(1-r/c)}$ 값을 곱하며,<br>문장이 $r$보다 길어 패널티를 줄 필요가 없는 경우 1이어야 함</li></ul><p>$$\text{보정된 정밀도 } p_1=\frac{\Sigma_{{unigram}\in{Candidate}}Count_{dip}(unigram)}{\Sigma_{{unigram}\in{Candidate}}Count(unigram)}$$
$$\text{n-gram 일반화 } p_n=\frac{\Sigma_{{n\text{-}gram}\in{Candidate}}Count_{dip}(n\text{-}gram)}{\Sigma_{{n\text{-}gram}\in{Candidate}}Count(n\text{-}gram)}$$
$$BLEU={BP}\times{exp(\Sigma^N_{n=1}{w_n}{\log{p_n}})}$$</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>nltk.translate.bleu_score</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>bleu_score</span><span class=p>(</span><span class=n>candidate</span><span class=o>.</span><span class=n>split</span><span class=p>(),</span><span class=nb>list</span><span class=p>(</span><span class=nb>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>ref</span><span class=p>:</span> <span class=n>ref</span><span class=o>.</span><span class=n>split</span><span class=p>(),</span> <span class=n>references</span><span class=p>)))</span>
</span></span></code></pre></td></tr></table></div></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://minyeamer.github.io/tags/til/>TIL</a></li><li><a href=https://minyeamer.github.io/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=prev href=https://minyeamer.github.io/blog/2022-06-30/><span class=title>« Prev</span><br><span>2022-06-30 Log</span></a>
<a class=next href=https://minyeamer.github.io/blog/2022-06-28/><span class=title>Next »</span><br><span>2022-06-28 Log</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 2022-06-29 Log on twitter" href="https://twitter.com/intent/tweet/?text=2022-06-29%20Log&url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-29%2f&hashtags=TIL%2cNLP"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-06-29 Log on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-29%2f&title=2022-06-29%20Log&summary=2022-06-29%20Log&source=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-29%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-06-29 Log on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-29%2f&title=2022-06-29%20Log"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-06-29 Log on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-29%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-06-29 Log on whatsapp" href="https://api.whatsapp.com/send?text=2022-06-29%20Log%20-%20https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-29%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-06-29 Log on telegram" href="https://telegram.me/share/url?text=2022-06-29%20Log&url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-29%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=minyeamer/til issue-term=pathname label=comments theme=preferred-color-scheme crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://minyeamer.github.io/>Minystory</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>