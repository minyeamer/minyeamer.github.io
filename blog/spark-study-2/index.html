<!doctype html><html lang=ko-kr dir=ltr><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=author content><meta name=keywords content="Apache Spark,Spark Shell,JDK,Hadoop,HDFS,PySpark,데이터 엔지니어링,스파크,Study"><meta name=description content="Apache Spark의 설치와 PySpark 실행 과정을 다루며, JDK와 HDFS 설정부터 호스트명 오류 해결까지 단계별로 안내합니다. 또한, HDFS 실행 중 발생한 호스트명 …"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><link rel=icon href=https://minyeamer.github.io/images/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://minyeamer.github.io/images/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://minyeamer.github.io/images/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://minyeamer.github.io/images/favicons/apple-touch-icon.png><link rel=mask-icon href=https://minyeamer.github.io/images/favicons/apple-touch-icon.png><meta name=google-site-verification content="u1tWcHmHUZWfFT1cHaku6sqU-bK40N3WLR-C-4VUWN0"><meta name=naver-site-verification content="6eaf8e9da1a6104780f056f1a7797fe5a3a5a0da"><meta property="og:title" content="Apache Spark - 로컬 환경에서 설치하고 PySpark 실행하기"><meta property="og:description" content="Apache Spark의 설치와 PySpark 실행 과정을 다루며, JDK와 HDFS 설정부터 호스트명 오류 해결까지 단계별로 안내합니다. 또한, HDFS 실행 중 발생한 호스트명 …"><meta property="og:type" content="article"><meta property="og:url" content="https://minyeamer.github.io/blog/spark-study-2/"><meta property="og:image" content="https://dl.dropboxusercontent.com/scl/fi/iafnblb6k95kbw7bwn2xj/spark-00-cover.webp?rlkey=6995tacnu3mvr7s31akl5sca6&amp;raw=1"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-28T21:55:14+09:00"><meta property="article:modified_time" content="2025-06-28T21:55:14+09:00"><meta property="og:see_also" content="https://minyeamer.github.io/blog/spark-study-8/"><meta property="og:see_also" content="https://minyeamer.github.io/blog/spark-study-7/"><meta property="og:see_also" content="https://minyeamer.github.io/blog/spark-study-6/"><meta property="og:see_also" content="https://minyeamer.github.io/blog/spark-study-5/"><meta property="og:see_also" content="https://minyeamer.github.io/blog/spark-study-4/"><meta property="og:see_also" content="https://minyeamer.github.io/blog/spark-study-3/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dl.dropboxusercontent.com/scl/fi/iafnblb6k95kbw7bwn2xj/spark-00-cover.webp?rlkey=6995tacnu3mvr7s31akl5sca6&amp;raw=1"><meta name=twitter:title content="Apache Spark - 로컬 환경에서 설치하고 PySpark 실행하기"><meta name=twitter:description content="Apache Spark의 설치와 PySpark 실행 과정을 다루며, JDK와 HDFS 설정부터 호스트명 오류 해결까지 단계별로 안내합니다. 또한, HDFS 실행 중 발생한 호스트명 …"><meta name=twitter:site content="@https://x.com/minyeamer"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://minyeamer.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Apache Spark - 로컬 환경에서 설치하고 PySpark 실행하기","item":"https://minyeamer.github.io/blog/spark-study-2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Apache Spark - 로컬 환경에서 설치하고 PySpark 실행하기","name":"Apache Spark - 로컬 환경에서 설치하고 PySpark 실행하기","description":"Apache Spark의 설치와 PySpark 실행 과정을 다루며, JDK와 HDFS 설정부터 호스트명 오류 해결까지 단계별로 안내합니다. 또한, HDFS 실행 중 발생한 호스트명과 관련된 오류를 해결하는 과정을 공유합니다.\n","keywords":["Apache Spark, Spark Shell, JDK, Hadoop, HDFS, PySpark, 데이터 엔지니어링, 스파크, Study"],"articleBody":" Apache Spark 배우기 1. 스파크의 기본 개념과 아키텍처 2. 로컬 환경에서 설치하고 PySpark 실행하기 3. 스파크 애플리케이션 구조와 RDD 이해하기 4. DataFrame과 Dataset API 활용하기 5. 스파크 SQL과 테이블/뷰 관리 6. 다양한 데이터 소스 읽기/쓰기 (Parquet, JSON, CSV, Avro) 7. 외부 데이터베이스 연동 (PostgreSQL, MySQL) 8. 사용자 정의 함수(UDF)와 고차 함수 활용하기 숨기기 목록 보기 2/8 Spark Installation # Apple Silicon 환경에서 스파크 설치를 진행합니다.\n각 섹션의 이미지를 클릭하면 설치 페이지 또는 관련 문서로 이동합니다.\nSpark 설치 # 아파치 스파크 다운로드 페이지로 가서 최신 버전 4.0.0 및 \"Pre-built for Apache Hadoop\" 옵션을 선택하면 해당 버전의 다운로드 링크 spark-4.0.0-bin-hadoop3.tgz 가 나타난다. 해당 링크로 이동하면 아래와 같이 Hadoop 관련 바이너리 파일이 포함된 압축 파일의 설치 경로를 확인할 수 있다.\n브라우저 또는 curl, wget 등 명령어를 통해 압축 파일을 내려받을 수 있다. Copy bash wget https://dlcdn.apache.org/spark/spark-4.0.0/spark-4.0.0-bin-hadoop3.tgz 압축 해제 프로그램을 사용하거나, 터미널에서 아래 명령어를 입력하여 압축 해제한다. Copy bash tar zxvf spark-4.0.0-bin-hadoop3.tgz Spark 경로에 접근하기 위해 환경변수를 설정한다. Copy bash vi ~/.zshrc vi 편집기로 .zshrc 파일에 Spark 경로를 등록한다. SPARK_HOME 은 압축 해제한 Spark 경로를 입력한다. Copy bash export SPARK_HOME=/Users/{username}/spark-4.0.0 export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin 변경 사항을 적용하기 위해 터미널을 재시작하거나 아래 명령어를 실행한다. Copy text source ~/.zshrc주의할 점은 Spark를 실행하기 전에 Java와 Hadoop이 설치되어 있어야 한다. 보통은 Java 또는 Hadoop 버전에 맞춰서 Spark를 설치하지만, 어떤 것도 설치되어 있지 않기 때문에 스파크 버전에 맞춰서 Java와 Hadoop을 설치한다.\nHadoop은 다운로드할 때 지정한 것과 같은 3.4 버전을 설치하고, Java는 Spark 4.0.0에서 요구하는 최소 버전인 OpenJDK 17 버전을 설치한다. 이미 설치되어 있다면 Spark 실행 섹션으로 넘어간다.\nSpark runs on Java 17/21, Scala 2.13, Python 3.9+, and R 3.5+ (Deprecated)\nJava 설치 # Homebrew가 설치되었다는 전제 하에 OpenJDK 17 버전을 설치한다. Copy bash brew install openjdk@17 설치가 완료되면, 시스템에서 JDK를 찾을 수 있도록 심볼릭 링크로 연결한다. Copy bash sudo ln -sfn /opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk-17.jdk 환경변수에 OpenJDK 11의 bin 디렉터리를 추가한다. vi 편집기 등으로 직접 수정할 수도 있다. Copy bash echo 'export PATH=/opt/homebrew/opt/openjdk@17/bin:$PATH' \u003e\u003e ~/.zshrc 변경 사항을 적용하기 위해 터미널을 재시작하거나 아래 명령어를 실행한다. Copy bash source ~/.zshrc OpenJDK 11 버전이 정상적으로 설치되었는지 확인하기 위해 아래 명령어를 입력한다. Copy bash % java -version openjdk version \"17.0.15\" 2025-04-15 OpenJDK Runtime Environment Homebrew (build 17.0.15+0) OpenJDK 64-Bit Server VM Homebrew (build 17.0.15+0, mixed mode, sharing)Hadoop 설치 # Homebrew로 설치할 수도 있지만, Hadoop 3.4.0 버전을 맞추기 위해 압축 파일을 직접 내려받는다. 다운로드 버튼을 클릭하거나, curl, wget 등 명령어로 내려받을 수 있다. (ARM 아키텍처에서 설치할 때는 파일명에 aarch64 가 포함되어야 한다) Copy bash wget https://archive.apache.org/dist/hadoop/common/hadoop-3.4.0/hadoop-3.4.0-aarch64.tar.gz 압축 해제 프로그램을 사용하거나, 터미널에서 아래 명령어를 입력하여 압축 해제한다. Copy bash tar zxvf hadoop-3.4.0-aarch64.tar.gz -C ~/ Hadoop 명령어에 접근하기 위해 환경변수를 설정한다. Copy bash vi ~/.zshrc vi 편집기로 .zshrc 파일에 Hadoop 경로를 등록한다. HADOOP_HOME 은 압축 해제한 Hadoop 경로를 입력한다. Copy bash export HADOOP_HOME=/Users/{username}/hadoop-3.4.0 export PATH=$PATH:$HADOOP_HOME/bin 변경 사항을 적용하기 위해 터미널을 재시작하거나 아래 명령어를 실행한다. Copy text source ~/.zshrc Hadoop 환경 설정 파일을 수정한다. 파일들은 $HADOOP_HOME/etc/hadoop/ 경로에 있다. hadoop-env.sh\nCopy bash export JAVA_HOME=/Library/Java/JavaVirtualMachines/openjdk-17.jdk/Contents/Homecore-site.xml\nCopy xml fs.defaultFS hdfs://localhost:9000 hdfs-site.xml\nCopy xml dfs.replication 1 mapred-site.xml\nCopy xml mapreduce.framework.name yarn yarn-site.xml\nCopy xml yarn.nodemanager.aux-services mapreduce_shuffle HDFS은 자체적으로 SSH를 사용한다. SSH 키를 생성하고 본인 계정에 인증한다. Copy bash ssh-keygen -t rsa cat ~/.ssh/id_rsa.pub \u003e\u003e ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys ssh localhostHDFS 실행 #네임노드를 포맷하고 HDFS을 구성하는 모든 데몬을 실행한다.\nCopy bash hdfs namenode -format $HADOOP_HOME/sbin/start-dfs.sh먼저, 네임노드를 포맷하면 아래와 같은 로그가 발생한다. 호스트명으로 localhost 대신 다른 명칭을 사용하는데, 이로 인해 오류가 발생했다.\nCopy bash % sudo hdfs namenode -format Password: 2025-06-28 18:33:03,038 INFO namenode.NameNode: STARTUP_MSG: /************************************************************ STARTUP_MSG: Starting NameNode STARTUP_MSG: host = minyeamer/127.0.0.1 STARTUP_MSG: args = [-format] STARTUP_MSG: version = 3.4.0 STARTUP_MSG: classpath = /Users...정상적으로 HDFS이 실행된다면 아래와 같은 메시지를 조회할 수 있다.\nCopy bash % $HADOOP_HOME/sbin/start-dfs.sh Starting namenodes on [minyeamer] Starting datanodes minyeamer: datanode is running as process 21771. Stop it first and ensure /tmp/hadoop-cuz-datanode.pid file is empty before retry. Starting secondary namenodes [minyeamer] minyeamer: secondarynamenode is running as process 21906. Stop it first and ensure /tmp/hadoop-cuz-secondarynamenode.pid file is empty before retry. 2025-06-28 18:51:34,493 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicablejps 명령어를 입력하면 실행중인 노드를 확인할 수 있다.\nCopy bash % jps 21906 SecondaryNameNode 23016 Jps 22216 NameNode 21771 DataNodeHDFS을 종료하고 싶다면 start-dfs.sh 와 동일한 경로에서 stop-all.sh 스크립트를 실행하면 된다.\nCopy bash % $HADOOP_HOME/sbin/stop-all.sh WARNING: Stopping all Apache Hadoop daemons as cuz in 10 seconds. WARNING: Use CTRL-C to abort. Stopping namenodes on [minyeamer] Stopping datanodes Stopping secondary namenodes [minyeamer] Stopping nodemanagers Stopping resourcemanagerHDFS 실행 중 오류 처리 #localhost 가 아닌 minyeamer 라는 호스트명을 사용하는데, start-dfs.sh 실행 시 아래와 같은 에러 메시지가 발생했다. localhost 명칭을 사용한다면 해당 과정은 무시하고 Spark 실행 섹션으로 넘어간다.\nCopy bash minyeamer: ssh: Could not resolve hostname minyeamer: nodename nor servname provided, or not known첫 번째 에러는 SSH 연결 시 호스트명을 인식할 수 없다는 문제로, /etc/hosts 에 minyeamer 호스트명과 127.0.0.1 IP 주소가 매칭되지 않아서 발생한 문제다. 아래와 같이 추가할 수 있다.\nCopy text 127.0.0.1 localhost minyeamer 255.255.255.255 broadcasthost ::1 localhost minyeamer해당 호스트명으로 SSH 접속을 시도하면 정상적으로 접속할 수 있다.\nCopy bash ssh minyeamer또한, Hadoop 설정 파일도 일부 수정해주어야 한다. $HADOOP_HOME/etc/hadoop/ 경로의 workers 파일에는 localhost 한줄만 적혀있을 건데, minyeamer 호스트명으로 변경한다. 또한, 동일한 경로의 core-site.xml 파일의 hdfs://localhost:9000 부분도 변경해야 한다.\n그리고 나서 다시 start-dfs.sh 를 실행했는데 다른 에러가 발생했다.\nCopy bash minyeamer: ERROR: Cannot set priority of namenode process 19072이것만으로는 오류를 파악하기 어려워서 $HADOOP_HOME/logs/ 경로 아래 hadoop-*-namenode-*.log 형식의 로그 파일을 확인했다.\n로그 파일에서 에러가 발생한 부분에 아래와 같은 에러 메시지를 확인할 수 있었다.\nCopy bash 2025-06-28 18:44:47,867 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode. java.net.BindException: Problem binding to [minyeamer:9000] java.net.BindException: Address already in use; For more details see: http://wiki.apache.org/hadoop/BindException9000번 포트가 사용되고 있다는 건데, 확인해보니 localhost:cslistener 라는 이름의 프로세스가 실행 중에 있었다.\nCopy bash % lsof -i :9000 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME python3.1 1943 ...아마 localhost 호스트명으로 HDFS을 실행시켰을 때 프로세스가 중지되지 않고 남아있는 것 같아 강제로 중지했다.\nCopy bash kill -9 1943다시 네임노드를 포맷하고 HDFS을 실행하니 앞에서 보았던 정상적인 메시지를 확인할 수 있었다.\nCopy bash hdfs namenode -format $HADOOP_HOME/sbin/start-dfs.shSpark 실행 #spark-shell 을 실행하면 정상적으로 동작하는 것을 확인할 수 있다.\nCopy bash (main) cuz@minyeamer ~ % spark-shell WARNING: Using incubator modules: jdk.incubator.vector Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties 25/06/28 19:45:07 WARN Utils: Your hostname, minyeamer, resolves to a loopback address: 127.0.0.1; using 192.168.x.x instead (on interface en0) 25/06/28 19:45:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 4.0.0 /_/ Using Scala version 2.13.16 (OpenJDK 64-Bit Server VM, Java 17.0.15) Type in expressions to have them evaluated. Type :help for more information. Spark context Web UI available at http://localhost:4040 Spark context available as 'sc' (master = local[*], app id = local-1751107510852). Spark session available as 'spark'. scala\u003e 메시지에서 알려주는대로 http://localhost:4040 경로에 접근하니까 아래와 같은 웹 UI 화면을 조회할 수 있었다.\nSpark 디렉터리 구조 #Spark 경로 아래에는 다음과 같은 디렉터리 또는 파일이 존재한다.\nCopy bash % ls ~/spark-4.0.0 bin/\tconf/\tdata/\texamples/\thive-jackson/\tjars/\tkubernetes/\tlicenses/ python/\tR/\tsbin/\tyarn/\tLICENSE\tNOTICE\tREADME.md\tRELEASE READMD.md 스파크 셸을 어떻게 사용하는지에 대한 안내 및 스파크 문서의 링크와 설정 가이드 등이 기록되어 있다.\nbin/ spark-shell 을 포함한 대부분의 스크립트가 위치한다.\nsbin/ 다양한 배포 모드에서 클러스터의 스파크 컴포넌트들을 시작하고 중지하기 위한 관리 목적이다.\nkubernetes/ 쿠버네티스 클러스터에서 쓰는 스파크를 위해, 도커 이미지 제작을 위한 Dockerfile들을 담고 있다.\ndata/ MLlib, 정형화 프로그래밍, GraphX 등에서 입력으로 사용되는 .txt 파일이 있다.\nexamples/ Java, Python, R, Scala에 대한 예제들을 제공한다.\nPySpark Installation #PyPi 설치 # Copy bash pip install pyspark명령어를 입력해 PyPi 저장소에서 PySpark 라이브러리를 설치할 수 있다. 다운로드한 파일과 동일하게 25년 5월 23일 릴리즈된 4.0.0 버전을 설치한다. 다른 버전을 설치하고 싶다면 pip install pyspark=3.0.0 과 같이 입력할 수 있다.\nSQL, ML, MLlib 등 추가적인 라이브러리를 같이 설치하려면 pip install pyspark[sql,ml,mllib] 와 같이 입력할 수 있다.\n별도의 가상환경에서 PySpark를 설치하는 것을 추천한다. 4.0.0 버전 기준으로 Python 3.9 버전부터 지원한다. 개인적으로는 Python 3.10 버전을 사용한다.\nCopy bash (spark) % pip install pyspark Collecting pyspark Downloading pyspark-4.0.0.tar.gz (434.1 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 434.1/434.1 MB 3.9 MB/s eta 0:00:00 ... Successfully built pyspark Installing collected packages: py4j, pyspark Successfully installed py4j-0.10.9.9 pyspark-4.0.0pyspark 실행 #spark-shell 은 Scala 쉘을 실행한다. Scala가 아닌 Python을 사용하고 싶다면 pyspark 쉘을 실행할 수 있다.\nCopy bash (spark) % pyspark Python 3.10.18 | packaged by conda-forge | (main, Jun 4 2025, 14:46:00) [Clang 18.1.8 ] on darwin Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. WARNING: Using incubator modules: jdk.incubator.vector Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties 25/06/28 21:35:50 WARN Utils: Your hostname, minyeamer, resolves to a loopback address: 127.0.0.1; using 192.168.x.x instead (on interface en0) 25/06/28 21:35:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties Setting default log level to \"WARN\". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 25/06/28 21:35:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /__ / .__/\\_,_/_/ /_/\\_\\ version 4.0.0 /_/ Using Python version 3.10.18 (main, Jun 4 2025 14:46:00) Spark context Web UI available at http://localhost:4040 Spark context available as 'sc' (master = local[*], app id = local-1751114150819). SparkSession available as 'spark'. \u003e\u003e\u003e Python 쉘에서는 대화형으로 Python API를 사용할 수 있다. 현재 경로에 있는 README.md 파일을 첫 번째 10줄만 읽어보았다.\nCopy python \u003e\u003e\u003e strings = spark.read.text(\"README.md\") \u003e\u003e\u003e strings.show(10, truncate=False) +--------------------------------------------------------------------------------------------------+ |value | +--------------------------------------------------------------------------------------------------+ |# Apache Spark | | | |Spark is a unified analytics engine for large-scale data processing. It provides | |high-level APIs in Scala, Java, Python, and R (Deprecated), and an optimized engine that | |supports general computation graphs for data analysis. It also supports a | |rich set of higher-level tools including Spark SQL for SQL and DataFrames, | |pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing,| |and Structured Streaming for stream processing. | | | |- Official version: \u003chttps://spark.apache.org/\u003e | +--------------------------------------------------------------------------------------------------+ only showing top 10 rows \u003e\u003e\u003e \u003e\u003e\u003e strings.count() 125쉘을 나가고 싶다면 Ctrl+D 를 눌러 나갈 수도 있고, quit() 함수를 실행해 종료할 수도 있다.\nCopy python \u003e\u003e\u003e quit()","wordCount":"1687","inLanguage":"ko","image":"https:\/\/dl.dropboxusercontent.com\/scl\/fi\/iafnblb6k95kbw7bwn2xj\/spark-00-cover.webp?rlkey=6995tacnu3mvr7s31akl5sca6\u0026raw=1","datePublished":"2025-06-28T21:55:14+09:00","dateModified":"2025-06-28T21:55:14+09:00","author":{"@type":"Person","name":"minyeamer"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://minyeamer.github.io/blog/spark-study-2/"},"publisher":{"@type":"Person","name":"Minystory","logo":{"@type":"ImageObject","url":"https://minyeamer.github.io/images/favicons/favicon.ico"}}}</script><script>(function(){const e=256+768*1.3;window.innerWidth>e&&localStorage.getItem("menu-expanded")==="false"&&document.documentElement.classList.add("menu-initial-collapsed")})()</script><title>Apache Spark - 로컬 환경에서 설치하고 PySpark 실행하기|Minystory</title><link rel=icon href=/images/favicon.ico><link rel=manifest href=/data/manifest.json><link rel=canonical href=https://minyeamer.github.io/blog/spark-study-2/><link rel=stylesheet href=/main.min.bde103040955e62330088cfc990f437c722b645b0ee73b886f006f479c190e36.css integrity="sha256-veEDBAlV5iMwCIz8mQ9DfHIrZFsO5zuIbwBvR5wZDjY=" crossorigin=anonymous><script async src="https://www.googletagmanager.com/gtag/js?id=G-BJ8Z9RMBPJ"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BJ8Z9RMBPJ")</script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.1/css/all.min.css crossorigin=anonymous><script>(function(){const e=console.warn;console.warn=function(...n){const t=(new Error).stack||"";if(t.includes("highlight.min.js")||t.includes("highlightjs-line-numbers"))return;e.apply(console,n)}})()</script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlightjs-line-numbers.js/2.9.0/highlightjs-line-numbers.min.js crossorigin=anonymous></script><script defer src=/js/scroll-progress.min.841ade7e507a5f6d59c4e7bf2fe2b2ca034070677ff7957eec55610a024dd776.js integrity="sha256-hBreflB6X21ZxOe/L+KyygNAcGd/95V+7FVhCgJN13Y=" crossorigin=anonymous></script><script defer src=/js/menu-toggle.min.dd99cf31bd583912ea1bbd2c953c8c6b53d89da9846e8f05c9189de9369c97c9.js integrity="sha256-3ZnPMb1YORLqG70slTyMa1PYnamEbo8FyRid6Tacl8k=" crossorigin=anonymous></script><script defer src=/js/toc-toggle.min.48870f47596275987bbd50f04ba301b2b9469da2b3ed9d94e569774691851717.js integrity="sha256-SIcPR1lidZh7vVDwS6MBsrlGnaKz7Z2U5Wl3RpGFFxc=" crossorigin=anonymous></script><script defer src=/js/set-theme.min.986446861d7c48b101b4a6f53724b7fc97d6efec2a87906c6630cb9e503897c4.js integrity="sha256-mGRGhh18SLEBtKb1NyS3/JfW7+wqh5BsZjDLnlA4l8Q=" crossorigin=anonymous></script><script defer src=/js/copy-code.min.e18ffc44cf6beb1757c467945bb2897b0e1cb2ee52f08b460ee3ee57f0db6d60.js integrity="sha256-4Y/8RM9r6xdXxGeUW7KJew4csu5S8ItGDuPuV/DbbWA=" crossorigin=anonymous></script><script defer src=/js/toc-highlight.min.7917ead4ecb7378779bfbf3f988251ac76bd55ca7f12fd5919870777dbcd6095.js integrity="sha256-eRfq1Oy3N4d5v78/mIJRrHa9Vcp/Ev1ZGYcHd9vNYJU=" crossorigin=anonymous></script><script defer src=/js/reading-time.min.c8cf75fb9d8569d73b13b6a2bf6f52c235ff03ad7848e0386ccce2d7dcb8d9be.js integrity="sha256-yM91+52Fadc7E7aiv29SwjX/A614SOA4bMzi19y42b4=" crossorigin=anonymous></script><script defer src=/js/image-zoom.min.73ef1954212429af5df3e0386860f8f9b32748998c0ed1eb62ed800e348a2fab.js integrity="sha256-c+8ZVCEkKa9d8+A4aGD4+bMnSJmMDtHrYu2ADjSKL6s=" crossorigin=anonymous></script><script>window.siteSearch=window.siteSearch||{}</script><script>window.siteSearch.contentUrl="/data/content.min.af5361e8628f25c494b3aab53c6f0a5cd1500450dd2ef2d5c5c0c0b9b1c42f11.json"</script><script>window.siteSearch.categoriesUrl="/data/categories.min.249a3bfbb1ae8ee6e09c08bd2449462f8e161c8a98748da0169c2abe6f79598e.json"</script><script>window.siteSearch.tagsUrl="/data/tags.min.11b7d30fcd3cbb5269823c54a83c4889d580a62df2d1988344ddf158197da767.json"</script><script defer src=/fuse.min.js></script><script defer src=/js/search/init.min.3ec02e8ced439eef10e68b1663c5dab0c0ec89f63a79e154e33b0d62221bdbdb.js integrity="sha256-PsAujO1Dnu8Q5osWY8XasMDsifY6eeFU4zsNYiIb29s=" crossorigin=anonymous></script><script defer src=/js/search/input.min.a408f4459dfde324373a5993c754ed34738b88a238907346e0e5ec1a110bcf9b.js integrity="sha256-pAj0RZ394yQ3OlmTx1TtNHOLiKI4kHNG4OXsGhELz5s=" crossorigin=anonymous></script><script defer src=/js/search/list.min.985cf5b5f5b98133bffdd5b9f1d502cf1deb431146225583ca9d116ed6d737be.js integrity="sha256-mFz1tfW5gTO//dW58dUCzx3rQxFGIlWDyp0RbtbXN74=" crossorigin=anonymous></script></head><body dir=ltr class="site-kind-page site-type-posts site-layout-post"><div class=scroll-progress><div class=scroll-progress-bar></div></div><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><div class=menu-overlay></div><main class=container><aside class=site-menu aria-label=메뉴><div class=menu-content><nav><div class=menu-profile><div class=profile-image-wrap><a href=https://minyeamer.github.io/><img src=/images/profile/menu.jpg alt=Profile class=profile-image decoding=async></a></div><h2 class=profile-title><a class="flex align-center" href=https://minyeamer.github.io/><span>Minystory</span></a></h2></div><div class=menu-links><a href=https://github.com/minyeamer target=_blank id=github-link title=GitHub><i class="fa-brands fa-github"></i>
</a><a href=/categories/ id=categories-link title=Categories><i class="fa-solid fa-folder"></i>
</a><a href=/tags/ id=tags-link title=Tags><i class="fa-solid fa-tags"></i>
</a><button class=dark-mode-toggle id=theme-toggle-button aria-label="Toggle color scheme">
<i class="fa-solid fa-circle-half-stroke"></i></button></div><div class="site-search hidden"><div class=search-input-container><input type=text id=search-input placeholder=검색 aria-label=검색 maxlength=64 data-hotkeys=s/ onkeydown='event.key==="Enter"&&goToSearchPage()'>
<button type=button id=search-button class=search-button onclick=goToSearchPage()>
<i class="fa-solid fa-magnifying-glass"></i></button></div><div class="search-spinner hidden"></div><ul class=search-input-results id=search-input-results></ul></div><script>document.querySelector(".site-search").classList.remove("hidden");function goToSearchPage(){const t=document.getElementById("search-input"),e=t.value.trim();e&&(window.location.href="/search/?query="+encodeURIComponent(e))}</script><div class=menu-categories><input type=checkbox class="hidden toggle" id=categories-control checked>
<label for=categories-control class="categories-label categories-toggle"><a href=/categories/ class=categories-root><i class="fa-solid fa-folder"></i>
전체
<span class=category-count>(40)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li><input type=checkbox class="hidden toggle" id=cat-algorithm>
<label for=cat-algorithm class="categories-label categories-toggle"><a href="/search/?category1=Algorithm"><i class="fa-solid fa-folder"></i>Algorithm<span class=category-count>(4)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Algorithm&category2=Graph"><i class="fa-solid fa-file"></i>Graph<span class=category-count>(1)</span></a></li><li class=categories-label><a href="/search/?category1=Algorithm&category2=Python"><i class="fa-solid fa-file"></i>Python<span class=category-count>(2)</span></a></li><li class=categories-label><a href="/search/?category1=Algorithm&category2=SQL"><i class="fa-solid fa-file"></i>SQL<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-cloud>
<label for=cat-cloud class="categories-label categories-toggle"><a href="/search/?category1=Cloud"><i class="fa-solid fa-folder"></i>Cloud<span class=category-count>(2)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Cloud&category2=Kubernetes"><i class="fa-solid fa-file"></i>Kubernetes<span class=category-count>(2)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-data-analysis>
<label for=cat-data-analysis class="categories-label categories-toggle"><a href="/search/?category1=Data%20Analysis"><i class="fa-solid fa-folder"></i>Data Analysis<span class=category-count>(3)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Data%20Analysis&category2=Dacon"><i class="fa-solid fa-file"></i>Dacon<span class=category-count>(3)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-data-engineering>
<label for=cat-data-engineering class="categories-label categories-toggle"><a href="/search/?category1=Data%20Engineering"><i class="fa-solid fa-folder"></i>Data Engineering<span class=category-count>(19)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Data%20Engineering&category2=Apache%20Airflow"><i class="fa-solid fa-file"></i>Apache Airflow<span class=category-count>(7)</span></a></li><li class=categories-label><a href="/search/?category1=Data%20Engineering&category2=Apache%20Spark"><i class="fa-solid fa-file"></i>Apache Spark<span class=category-count>(8)</span></a></li><li class=categories-label><a href="/search/?category1=Data%20Engineering&category2=Crawling"><i class="fa-solid fa-file"></i>Crawling<span class=category-count>(4)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-frontend>
<label for=cat-frontend class="categories-label categories-toggle"><a href="/search/?category1=Frontend"><i class="fa-solid fa-folder"></i>Frontend<span class=category-count>(9)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Frontend&category2=Blog"><i class="fa-solid fa-file"></i>Blog<span class=category-count>(9)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-linux>
<label for=cat-linux class="categories-label categories-toggle"><a href="/search/?category1=Linux"><i class="fa-solid fa-folder"></i>Linux<span class=category-count>(1)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Linux&category2=Ubuntu"><i class="fa-solid fa-file"></i>Ubuntu<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-project>
<label for=cat-project class="categories-label categories-toggle"><a href="/search/?category1=Project"><i class="fa-solid fa-folder"></i>Project<span class=category-count>(2)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-label><a href="/search/?category1=Project&category2=Open%20Source"><i class="fa-solid fa-file"></i>Open Source<span class=category-count>(1)</span></a></li><li class=categories-label><a href="/search/?category1=Project&category2=Tools"><i class="fa-solid fa-file"></i>Tools<span class=category-count>(1)</span></a></li></ul></li></ul></div><div class=recent-posts><div class=recent-post-label><i class="fa-solid fa-clock"></i>
<span>최신글</span></div><ul class=recent-post-list><li class=recent-post-item><a href=/blog/hugo-blog-5/ title="Hugo 블로그 만들기 (5) - 본문 레이아웃 개선 (헤더와 푸터 및 Disqus 댓글 기능 구현)"><div class=recent-post-title>Hugo 블로그 만들기 (5) - 본문 레이아웃 개선 (헤더와 푸터 및 Disqus 댓글 기능 구현)</div><div class=recent-post-date><time datetime=2025-12-15>2025.12.15</time></div></a></li><li class=recent-post-item><a href=/blog/hugo-blog-4/ title="Hugo 블로그 만들기 (4) - 검색 기능 개선 및 검색 페이지 구현 (Fuse.js)"><div class=recent-post-title>Hugo 블로그 만들기 (4) - 검색 기능 개선 및 검색 페이지 구현 (Fuse.js)</div><div class=recent-post-date><time datetime=2025-12-14>2025.12.14</time></div></a></li><li class=recent-post-item><a href=/blog/hugo-blog-3/ title="Hugo 블로그 만들기 (3) - Taxonomies로 태그/카테고리 페이지 커스터마이징"><div class=recent-post-title>Hugo 블로그 만들기 (3) - Taxonomies로 태그/카테고리 페이지 커스터마이징</div><div class=recent-post-date><time datetime=2025-11-22>2025.11.22</time></div></a></li><li class=recent-post-item><a href=/blog/hugo-blog-2/ title="Hugo 블로그 만들기 (2) - 메인 레이아웃 커스터마이징 (메뉴, 목차, 헤더)"><div class=recent-post-title>Hugo 블로그 만들기 (2) - 메인 레이아웃 커스터마이징 (메뉴, 목차, 헤더)</div><div class=recent-post-date><time datetime=2025-11-04>2025.11.04</time></div></a></li><li class=recent-post-item><a href=/blog/hugo-blog-1/ title="Hugo 블로그 만들기 (1) - 프로젝트 구성과 GitHub Pages 배포 (Submodule 활용)"><div class=recent-post-title>Hugo 블로그 만들기 (1) - 프로젝트 구성과 GitHub Pages 배포 (Submodule 활용)</div><div class=recent-post-date><time datetime=2025-11-01>2025.11.01</time></div></a></li></ul></div></nav><script>(function(){var e=document.querySelector("aside .menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=main-wrap><header class=site-header><div class="flex align-center justify-between"><label for=menu-control aria-label="메뉴 접기/펼치기" title="메뉴 접기/펼치기"><i class="fa-solid fa-bars menu-icon" id=menu-icon></i></label><h3><a href=https://minyeamer.github.io/ class=mobile-title>Minystory</a></h3><label for=toc-control aria-label="목차 접기/펼치기" title="목차 접기/펼치기"><i class="fa-solid fa-list menu-icon" id=toc-icon></i></label></div></header><article class="content-wrap markdown"><header class=content-header><div class=content-category><a href="/search/?category1=Data%20Engineering&category2=Apache%20Spark" class=content-category-link>Data Engineering/Apache Spark</a></div><h1 class=content-title>Apache Spark - 로컬 환경에서 설치하고 PySpark 실행하기</h1><div class=content-datetime><time datetime=2025-06-28T21:55:14+09:00>2025. 6. 28. 21:55
</time><span id=reading-time></span></div></header><div class=content-cover-wrap><img src="https://dl.dropboxusercontent.com/scl/fi/iafnblb6k95kbw7bwn2xj/spark-00-cover.webp?rlkey=6995tacnu3mvr7s31akl5sca6&amp;raw=1" class=content-cover alt="Cover Image" decoding=async></div><div id=series-anchor></div><div class=sc-series><div class=series-bookmark><svg width="32" height="48" fill="currentColor" viewBox="0 0 32 48" class="series-corner-image"><path fill="currentColor" d="M32 0H0v48h.163l16-16L32 47.836V0z"/></svg></div><div class=series-header><h2 class=series-title>Apache Spark 배우기</h2></div><input type=checkbox id=series-toggle class=series-toggle-input hidden><div class=series-content><ol class=series-list><li class=series-item><span class=series-item-index>1.</span>
<a href=/blog/spark-study-1/#series-anchor>스파크의 기본 개념과 아키텍처</a></li><li class="series-item active"><span class=series-item-index>2.</span>
<a href=/blog/spark-study-2/#series-anchor>로컬 환경에서 설치하고 PySpark 실행하기</a></li><li class=series-item><span class=series-item-index>3.</span>
<a href=/blog/spark-study-3/#series-anchor>스파크 애플리케이션 구조와 RDD 이해하기</a></li><li class=series-item><span class=series-item-index>4.</span>
<a href=/blog/spark-study-4/#series-anchor>DataFrame과 Dataset API 활용하기</a></li><li class=series-item><span class=series-item-index>5.</span>
<a href=/blog/spark-study-5/#series-anchor>스파크 SQL과 테이블/뷰 관리</a></li><li class=series-item><span class=series-item-index>6.</span>
<a href=/blog/spark-study-6/#series-anchor>다양한 데이터 소스 읽기/쓰기 (Parquet, JSON, CSV, Avro)</a></li><li class=series-item><span class=series-item-index>7.</span>
<a href=/blog/spark-study-7/#series-anchor>외부 데이터베이스 연동 (PostgreSQL, MySQL)</a></li><li class=series-item><span class=series-item-index>8.</span>
<a href=/blog/spark-study-8/#series-anchor>사용자 정의 함수(UDF)와 고차 함수 활용하기</a></li></ol></div><div class=series-footer><label for=series-toggle class=series-toggle-label><span class=series-toggle-icon><i class="fas fa-caret-up"></i></span>
<span class=series-toggle-text-hide>숨기기</span>
<span class=series-toggle-text-show>목록 보기</span></label><div class=series-nav><span class=series-nav-info>2/8</span><div class=series-nav-buttons><a href=/blog/spark-study-1/#series-anchor class=series-nav-button><i class="fas fa-chevron-left"></i></a>
<a href=/blog/spark-study-3/#series-anchor class=series-nav-button><i class="fas fa-chevron-right"></i></a></div></div></div></div><script>(function(){const e=document.getElementById("series-toggle"),t="series-expanded",n=localStorage.getItem(t);n==="true"&&(e.checked=!0),e.addEventListener("change",function(){localStorage.setItem(t,this.checked)})})()</script><h2 id=spark-installation>Spark Installation
<a class=anchor href=#spark-installation>#</a></h2><blockquote class=sc-hint><p>Apple Silicon 환경에서 스파크 설치를 진행합니다.<br>각 섹션의 이미지를 클릭하면 설치 페이지 또는 관련 문서로 이동합니다.</p></blockquote><h3 id=spark-설치>Spark 설치
<a class=anchor href=#spark-%ec%84%a4%ec%b9%98>#</a></h3><label class=sc-image for=sc-image-toggle-1><a href=https://spark.apache.org/downloads.html target=_blank><img src="https://dl.dropboxusercontent.com/scl/fi/ziy6h7z1oxy7puilm5ml2/spark-03-download-spark.webp?rlkey=bv8ckikaauinh733icd2l9gem&amp;raw=1" alt="Download Apache Spark" loading=lazy decoding=async style=height:auto></a></label><p><a href=https://spark.apache.org/downloads.html target=_blank rel="noopener noreferrer">아파치 스파크 다운로드 페이지</a>로 가서
최신 버전 4.0.0 및 "Pre-built for Apache Hadoop" 옵션을 선택하면 해당 버전의 다운로드 링크
<a href=https://www.apache.org/dyn/closer.lua/spark/spark-4.0.0/spark-4.0.0-bin-hadoop3.tgz target=_blank rel="noopener noreferrer">spark-4.0.0-bin-hadoop3.tgz</a>
가 나타난다. 해당 링크로 이동하면 아래와 같이 Hadoop 관련 바이너리 파일이 포함된 압축 파일의 설치 경로를 확인할 수 있다.</p><label class=sc-image for=sc-image-toggle-2><a href=https://www.apache.org/dyn/closer.lua/spark/spark-4.0.0/spark-4.0.0-bin-hadoop3.tgz target=_blank><img src="https://dl.dropboxusercontent.com/scl/fi/20hbdfye2kl3jvmytp6bk/spark-04-download-spark-tgz.webp?rlkey=o7zeq22r59epazjwef0ck0s10&amp;raw=1" alt=https://www.apache.org/dyn/closer.lua/spark/spark-4.0.0/spark-4.0.0-bin-hadoop3.tgz loading=lazy decoding=async style=height:auto></a></label><ol><li>브라우저 또는 <code>curl</code>, <code>wget</code> 등 명령어를 통해 압축 파일을 내려받을 수 있다.</li></ol><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>wget https://dlcdn.apache.org/spark/spark-4.0.0/spark-4.0.0-bin-hadoop3.tgz</span></span></code></pre></div></div><ol start=2><li>압축 해제 프로그램을 사용하거나, 터미널에서 아래 명령어를 입력하여 압축 해제한다.</li></ol><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>tar zxvf spark-4.0.0-bin-hadoop3.tgz</span></span></code></pre></div></div><ol start=3><li>Spark 경로에 접근하기 위해 환경변수를 설정한다.</li></ol><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>vi ~/.zshrc</span></span></code></pre></div></div><ol start=4><li>vi 편집기로 .zshrc 파일에 Spark 경로를 등록한다. <code>SPARK_HOME</code> 은 압축 해제한 Spark 경로를 입력한다.</li></ol><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>SPARK_HOME</span><span class=o>=</span>/Users/<span class=o>{</span>username<span class=o>}</span>/spark-4.0.0
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span><span class=nv>$PATH</span>:<span class=nv>$SPARK_HOME</span>/bin:<span class=nv>$SPARK_HOME</span>/sbin</span></span></code></pre></div></div><ol start=5><li>변경 사항을 적용하기 위해 터미널을 재시작하거나 아래 명령어를 실행한다.</li></ol><div class=sc-codeblock data-lang><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">text</span></div><pre tabindex=0><code>source ~/.zshrc</code></pre></div><p>주의할 점은 Spark를 실행하기 전에 Java와 Hadoop이 설치되어 있어야 한다.
보통은 Java 또는 Hadoop 버전에 맞춰서 Spark를 설치하지만,
어떤 것도 설치되어 있지 않기 때문에 스파크 버전에 맞춰서 Java와 Hadoop을 설치한다.</p><p>Hadoop은 다운로드할 때 지정한 것과 같은 3.4 버전을 설치하고, Java는
<a href=https://spark.apache.org/docs/latest/ target=_blank rel="noopener noreferrer">Spark 4.0.0에서 요구하는 최소 버전</a>인 OpenJDK 17 버전을 설치한다.
이미 설치되어 있다면 <a href=#spark-%ec%8b%a4%ed%96%89>Spark 실행 섹션</a>으로 넘어간다.</p><blockquote class=sc-hint><p>Spark runs on Java 17/21, Scala 2.13, Python 3.9+, and R 3.5+ (Deprecated)</p></blockquote><h3 id=java-설치>Java 설치
<a class=anchor href=#java-%ec%84%a4%ec%b9%98>#</a></h3><label class=sc-image for=sc-image-toggle-3><a href=https://formulae.brew.sh/formula/openjdk@17 target=_blank><img src="https://dl.dropboxusercontent.com/scl/fi/ksrg6fvzag4uxt2foh65e/spark-05-homebrew-openjdk17.webp?rlkey=as293mym3jrl2097r83fqo4fp&amp;raw=1" alt="Homebrew Formulae > openjdk@17" loading=lazy decoding=async style=height:auto></a></label><ol><li><a href=https://brew.sh/ target=_blank rel="noopener noreferrer">Homebrew</a>가 설치되었다는 전제 하에 OpenJDK 17 버전을 설치한다.</li></ol><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>brew install openjdk@17</span></span></code></pre></div></div><ol start=2><li>설치가 완료되면, 시스템에서 JDK를 찾을 수 있도록 심볼릭 링크로 연결한다.</li></ol><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo ln -sfn /opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk-17.jdk</span></span></code></pre></div></div><ol start=3><li>환경변수에 OpenJDK 11의 <code>bin</code> 디렉터리를 추가한다. vi 편집기 등으로 직접 수정할 수도 있다.</li></ol><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>echo</span> <span class=s1>&#39;export PATH=/opt/homebrew/opt/openjdk@17/bin:$PATH&#39;</span> &gt;&gt; ~/.zshrc</span></span></code></pre></div></div><ol start=4><li>변경 사항을 적용하기 위해 터미널을 재시작하거나 아래 명령어를 실행한다.</li></ol><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>source</span> ~/.zshrc</span></span></code></pre></div></div><ol start=5><li>OpenJDK 11 버전이 정상적으로 설치되었는지 확인하기 위해 아래 명령어를 입력한다.</li></ol><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>% java -version
</span></span><span class=line><span class=cl>openjdk version <span class=s2>&#34;17.0.15&#34;</span> 2025-04-15
</span></span><span class=line><span class=cl>OpenJDK Runtime Environment Homebrew <span class=o>(</span>build 17.0.15+0<span class=o>)</span>
</span></span><span class=line><span class=cl>OpenJDK 64-Bit Server VM Homebrew <span class=o>(</span>build 17.0.15+0, mixed mode, sharing<span class=o>)</span></span></span></code></pre></div></div><h3 id=hadoop-설치>Hadoop 설치
<a class=anchor href=#hadoop-%ec%84%a4%ec%b9%98>#</a></h3><label class=sc-image for=sc-image-toggle-4><a href=https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.4.0/hadoop-3.4.0-aarch64.tar.gz target=_blank><img src="https://dl.dropboxusercontent.com/scl/fi/z3qvarefsly6np0p6y5y3/spark-06-download-hadoop-tgz.webp?rlkey=pp1nflw57fl14iur748zkp2td&amp;raw=1" alt=https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.4.0/hadoop-3.4.0-aarch64.tar.gz loading=lazy decoding=async style=height:auto></a></label><ol><li>Homebrew로 설치할 수도 있지만, Hadoop 3.4.0 버전을 맞추기 위해 압축 파일을 직접 내려받는다.
다운로드 버튼을 클릭하거나, <code>curl</code>, <code>wget</code> 등 명령어로 내려받을 수 있다.
(ARM 아키텍처에서 설치할 때는 파일명에 <code>aarch64</code> 가 포함되어야 한다)</li></ol><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>wget https://archive.apache.org/dist/hadoop/common/hadoop-3.4.0/hadoop-3.4.0-aarch64.tar.gz</span></span></code></pre></div></div><ol start=2><li>압축 해제 프로그램을 사용하거나, 터미널에서 아래 명령어를 입력하여 압축 해제한다.</li></ol><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>tar zxvf hadoop-3.4.0-aarch64.tar.gz -C ~/</span></span></code></pre></div></div><ol start=3><li>Hadoop 명령어에 접근하기 위해 환경변수를 설정한다.</li></ol><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>vi ~/.zshrc</span></span></code></pre></div></div><ol start=4><li>vi 편집기로 .zshrc 파일에 Hadoop 경로를 등록한다. <code>HADOOP_HOME</code> 은 압축 해제한 Hadoop 경로를 입력한다.</li></ol><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>HADOOP_HOME</span><span class=o>=</span>/Users/<span class=o>{</span>username<span class=o>}</span>/hadoop-3.4.0
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span><span class=nv>$PATH</span>:<span class=nv>$HADOOP_HOME</span>/bin</span></span></code></pre></div></div><ol start=5><li>변경 사항을 적용하기 위해 터미널을 재시작하거나 아래 명령어를 실행한다.</li></ol><div class=sc-codeblock data-lang><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">text</span></div><pre tabindex=0><code>source ~/.zshrc</code></pre></div><ol start=6><li>Hadoop 환경 설정 파일을 수정한다. 파일들은 <code>$HADOOP_HOME/etc/hadoop/</code> 경로에 있다.</li></ol><p><code>hadoop-env.sh</code></p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>JAVA_HOME</span><span class=o>=</span>/Library/Java/JavaVirtualMachines/openjdk-17.jdk/Contents/Home</span></span></code></pre></div></div><p><code>core-site.xml</code></p><div class=sc-codeblock data-lang=xml><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">xml</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-xml data-lang=xml><span class=line><span class=cl><span class=nt>&lt;configuration&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;property&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;name&gt;</span>fs.defaultFS<span class=nt>&lt;/name&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;value&gt;</span>hdfs://localhost:9000<span class=nt>&lt;/value&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;/property&gt;</span>
</span></span><span class=line><span class=cl><span class=nt>&lt;/configuration&gt;</span></span></span></code></pre></div></div><p><code>hdfs-site.xml</code></p><div class=sc-codeblock data-lang=xml><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">xml</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-xml data-lang=xml><span class=line><span class=cl><span class=nt>&lt;configuration&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;property&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;name&gt;</span>dfs.replication<span class=nt>&lt;/name&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;value&gt;</span>1<span class=nt>&lt;/value&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;/property&gt;</span>
</span></span><span class=line><span class=cl><span class=nt>&lt;/configuration&gt;</span></span></span></code></pre></div></div><p><code>mapred-site.xml</code></p><div class=sc-codeblock data-lang=xml><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">xml</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-xml data-lang=xml><span class=line><span class=cl><span class=nt>&lt;configuration&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;property&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;name&gt;</span>mapreduce.framework.name<span class=nt>&lt;/name&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;value&gt;</span>yarn<span class=nt>&lt;/value&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;/property&gt;</span>
</span></span><span class=line><span class=cl><span class=nt>&lt;/configuration&gt;</span></span></span></code></pre></div></div><p><code>yarn-site.xml</code></p><div class=sc-codeblock data-lang=xml><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">xml</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-xml data-lang=xml><span class=line><span class=cl><span class=nt>&lt;configuration&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;property&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;name&gt;</span>yarn.nodemanager.aux-services<span class=nt>&lt;/name&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nt>&lt;value&gt;</span>mapreduce_shuffle<span class=nt>&lt;/value&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nt>&lt;/property&gt;</span>
</span></span><span class=line><span class=cl><span class=nt>&lt;/configuration&gt;</span></span></span></code></pre></div></div><ol start=7><li>HDFS은 자체적으로 SSH를 사용한다. SSH 키를 생성하고 본인 계정에 인증한다.</li></ol><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ssh-keygen -t rsa
</span></span><span class=line><span class=cl>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
</span></span><span class=line><span class=cl>chmod <span class=m>600</span> ~/.ssh/authorized_keys
</span></span><span class=line><span class=cl>ssh localhost</span></span></code></pre></div></div><h3 id=hdfs-실행>HDFS 실행
<a class=anchor href=#hdfs-%ec%8b%a4%ed%96%89>#</a></h3><p>네임노드를 포맷하고 HDFS을 구성하는 모든 데몬을 실행한다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>hdfs namenode -format
</span></span><span class=line><span class=cl><span class=nv>$HADOOP_HOME</span>/sbin/start-dfs.sh</span></span></code></pre></div></div><p>먼저, 네임노드를 포맷하면 아래와 같은 로그가 발생한다. 호스트명으로 <code>localhost</code> 대신 다른 명칭을 사용하는데,
이로 인해 <a href=#hdfs-%ec%8b%a4%ed%96%89-%ec%a4%91-%ec%98%a4%eb%a5%98-%ec%b2%98%eb%a6%ac>오류</a>가 발생했다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>% sudo hdfs namenode -format
</span></span><span class=line><span class=cl>Password:
</span></span><span class=line><span class=cl>2025-06-28 18:33:03,038 INFO namenode.NameNode: STARTUP_MSG: 
</span></span><span class=line><span class=cl>/************************************************************
</span></span><span class=line><span class=cl>STARTUP_MSG: Starting NameNode
</span></span><span class=line><span class=cl>STARTUP_MSG:   <span class=nv>host</span> <span class=o>=</span> minyeamer/127.0.0.1
</span></span><span class=line><span class=cl>STARTUP_MSG:   <span class=nv>args</span> <span class=o>=</span> <span class=o>[</span>-format<span class=o>]</span>
</span></span><span class=line><span class=cl>STARTUP_MSG:   <span class=nv>version</span> <span class=o>=</span> 3.4.0
</span></span><span class=line><span class=cl>STARTUP_MSG:   <span class=nv>classpath</span> <span class=o>=</span> /Users...</span></span></code></pre></div></div><p>정상적으로 HDFS이 실행된다면 아래와 같은 메시지를 조회할 수 있다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>% <span class=nv>$HADOOP_HOME</span>/sbin/start-dfs.sh
</span></span><span class=line><span class=cl>Starting namenodes on <span class=o>[</span>minyeamer<span class=o>]</span>
</span></span><span class=line><span class=cl>Starting datanodes
</span></span><span class=line><span class=cl>minyeamer: datanode is running as process 21771.  Stop it first and ensure /tmp/hadoop-cuz-datanode.pid file is empty before retry.
</span></span><span class=line><span class=cl>Starting secondary namenodes <span class=o>[</span>minyeamer<span class=o>]</span>
</span></span><span class=line><span class=cl>minyeamer: secondarynamenode is running as process 21906.  Stop it first and ensure /tmp/hadoop-cuz-secondarynamenode.pid file is empty before retry.
</span></span><span class=line><span class=cl>2025-06-28 18:51:34,493 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class=k>for</span> your platform... using builtin-java classes where applicable</span></span></code></pre></div></div><p><code>jps</code> 명령어를 입력하면 실행중인 노드를 확인할 수 있다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>% jps
</span></span><span class=line><span class=cl><span class=m>21906</span> SecondaryNameNode
</span></span><span class=line><span class=cl><span class=m>23016</span> Jps
</span></span><span class=line><span class=cl><span class=m>22216</span> NameNode
</span></span><span class=line><span class=cl><span class=m>21771</span> DataNode</span></span></code></pre></div></div><p>HDFS을 종료하고 싶다면 <code>start-dfs.sh</code> 와 동일한 경로에서 <code>stop-all.sh</code> 스크립트를 실행하면 된다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>% <span class=nv>$HADOOP_HOME</span>/sbin/stop-all.sh
</span></span><span class=line><span class=cl>WARNING: Stopping all Apache Hadoop daemons as cuz in <span class=m>10</span> seconds.
</span></span><span class=line><span class=cl>WARNING: Use CTRL-C to abort.
</span></span><span class=line><span class=cl>Stopping namenodes on <span class=o>[</span>minyeamer<span class=o>]</span>
</span></span><span class=line><span class=cl>Stopping datanodes
</span></span><span class=line><span class=cl>Stopping secondary namenodes <span class=o>[</span>minyeamer<span class=o>]</span>
</span></span><span class=line><span class=cl>Stopping nodemanagers
</span></span><span class=line><span class=cl>Stopping resourcemanager</span></span></code></pre></div></div><h3 id=hdfs-실행-중-오류-처리>HDFS 실행 중 오류 처리
<a class=anchor href=#hdfs-%ec%8b%a4%ed%96%89-%ec%a4%91-%ec%98%a4%eb%a5%98-%ec%b2%98%eb%a6%ac>#</a></h3><p><code>localhost</code> 가 아닌 <code>minyeamer</code> 라는 호스트명을 사용하는데,
<code>start-dfs.sh</code> 실행 시 아래와 같은 에러 메시지가 발생했다. <code>localhost</code> 명칭을 사용한다면
해당 과정은 무시하고 <a href=#spark-%ec%8b%a4%ed%96%89>Spark 실행 섹션</a>으로 넘어간다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>minyeamer: ssh: Could not resolve hostname minyeamer: nodename nor servname provided, or not known</span></span></code></pre></div></div><p>첫 번째 에러는 SSH 연결 시 호스트명을 인식할 수 없다는 문제로, <code>/etc/hosts</code> 에
<code>minyeamer</code> 호스트명과 <code>127.0.0.1</code> IP 주소가 매칭되지 않아서 발생한 문제다. 아래와 같이 추가할 수 있다.</p><div class=sc-codeblock data-lang><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">text</span></div><pre tabindex=0><code>127.0.0.1       localhost minyeamer
255.255.255.255 broadcasthost
::1             localhost minyeamer</code></pre></div><p>해당 호스트명으로 SSH 접속을 시도하면 정상적으로 접속할 수 있다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ssh minyeamer</span></span></code></pre></div></div><p>또한, Hadoop 설정 파일도 일부 수정해주어야 한다. <code>$HADOOP_HOME/etc/hadoop/</code> 경로의
<code>workers</code> 파일에는 <code>localhost</code> 한줄만 적혀있을 건데, <code>minyeamer</code> 호스트명으로 변경한다.
또한, 동일한 경로의 <code>core-site.xml</code> 파일의 <code>hdfs://localhost:9000</code> 부분도 변경해야 한다.</p><p>그리고 나서 다시 <code>start-dfs.sh</code> 를 실행했는데 다른 에러가 발생했다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>minyeamer: ERROR: Cannot <span class=nb>set</span> priority of namenode process <span class=m>19072</span></span></span></code></pre></div></div><p>이것만으로는 오류를 파악하기 어려워서 <code>$HADOOP_HOME/logs/</code> 경로
아래 <code>hadoop-*-namenode-*.log</code> 형식의 로그 파일을 확인했다.</p><p>로그 파일에서 에러가 발생한 부분에 아래와 같은 에러 메시지를 확인할 수 있었다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>2025-06-28 18:44:47,867 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.
</span></span><span class=line><span class=cl>java.net.BindException: Problem binding to <span class=o>[</span>minyeamer:9000<span class=o>]</span> java.net.BindException: Address already in use<span class=p>;</span> For more details see:  http://wiki.apache.org/hadoop/BindException</span></span></code></pre></div></div><p>9000번 포트가 사용되고 있다는 건데, 확인해보니 <code>localhost:cslistener</code> 라는 이름의 프로세스가 실행 중에 있었다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>% lsof -i :9000
</span></span><span class=line><span class=cl>COMMAND   PID USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
</span></span><span class=line><span class=cl>python3.1   <span class=m>1943</span> ...</span></span></code></pre></div></div><p>아마 <code>localhost</code> 호스트명으로 HDFS을 실행시켰을 때 프로세스가 중지되지 않고 남아있는 것 같아 강제로 중지했다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>kill</span> -9 <span class=m>1943</span></span></span></code></pre></div></div><p>다시 네임노드를 포맷하고 HDFS을 실행하니 앞에서 보았던 <a href=#hdfs-%ec%8b%a4%ed%96%89>정상적인 메시지</a>를 확인할 수 있었다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>hdfs namenode -format
</span></span><span class=line><span class=cl><span class=nv>$HADOOP_HOME</span>/sbin/start-dfs.sh</span></span></code></pre></div></div><h3 id=spark-실행>Spark 실행
<a class=anchor href=#spark-%ec%8b%a4%ed%96%89>#</a></h3><p><code>spark-shell</code> 을 실행하면 정상적으로 동작하는 것을 확인할 수 있다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>(</span>main<span class=o>)</span> cuz@minyeamer ~ % spark-shell
</span></span><span class=line><span class=cl>WARNING: Using incubator modules: jdk.incubator.vector
</span></span><span class=line><span class=cl>Using Spark<span class=s1>&#39;s default log4j profile: org/apache/spark/log4j2-defaults.properties
</span></span></span><span class=line><span class=cl><span class=s1>25/06/28 19:45:07 WARN Utils: Your hostname, minyeamer, resolves to a loopback address: 127.0.0.1; using 192.168.x.x instead (on interface en0)
</span></span></span><span class=line><span class=cl><span class=s1>25/06/28 19:45:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
</span></span></span><span class=line><span class=cl><span class=s1>Using Spark&#39;</span>s default log4j profile: org/apache/spark/log4j2-defaults.properties
</span></span><span class=line><span class=cl>Setting default log level to <span class=s2>&#34;WARN&#34;</span>.
</span></span><span class=line><span class=cl>To adjust logging level use sc.setLogLevel<span class=o>(</span>newLevel<span class=o>)</span>. For SparkR, use setLogLevel<span class=o>(</span>newLevel<span class=o>)</span>.
</span></span><span class=line><span class=cl>Welcome to
</span></span><span class=line><span class=cl>      ____              __
</span></span><span class=line><span class=cl>     / __/__  ___ _____/ /__
</span></span><span class=line><span class=cl>    _<span class=se>\ \/</span> _ <span class=se>\/</span> _ <span class=sb>`</span>/ __/  <span class=s1>&#39;_/
</span></span></span><span class=line><span class=cl><span class=s1>   /___/ .__/\_,_/_/ /_/\_\   version 4.0.0
</span></span></span><span class=line><span class=cl><span class=s1>      /_/
</span></span></span><span class=line><span class=cl><span class=s1>         
</span></span></span><span class=line><span class=cl><span class=s1>Using Scala version 2.13.16 (OpenJDK 64-Bit Server VM, Java 17.0.15)
</span></span></span><span class=line><span class=cl><span class=s1>Type in expressions to have them evaluated.
</span></span></span><span class=line><span class=cl><span class=s1>Type :help for more information.
</span></span></span><span class=line><span class=cl><span class=s1>Spark context Web UI available at http://localhost:4040
</span></span></span><span class=line><span class=cl><span class=s1>Spark context available as &#39;</span>sc<span class=s1>&#39; (master = local[*], app id = local-1751107510852).
</span></span></span><span class=line><span class=cl><span class=s1>Spark session available as &#39;</span>spark<span class=err>&#39;</span>.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>scala&gt; </span></span></code></pre></div></div><p>메시지에서 알려주는대로 <code>http://localhost:4040</code> 경로에 접근하니까 아래와 같은 웹 UI 화면을 조회할 수 있었다.</p><p><label class=md-image for=md-image-toggle-0><input class="hidden toggle" type=checkbox id=md-image-toggle-0>
<img src="https://dl.dropboxusercontent.com/scl/fi/pmbfig1sp0s22xa387bie/spark-07-spark-jobs.webp?rlkey=y10mey04y7rpfm7nacplypcie&amp;raw=1" alt="Spark Jobs" loading=lazy decoding=async></label></p><h3 id=spark-디렉터리-구조>Spark 디렉터리 구조
<a class=anchor href=#spark-%eb%94%94%eb%a0%89%ed%84%b0%eb%a6%ac-%ea%b5%ac%ec%a1%b0>#</a></h3><p>Spark 경로 아래에는 다음과 같은 디렉터리 또는 파일이 존재한다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>% ls ~/spark-4.0.0
</span></span><span class=line><span class=cl>bin/		conf/	data/	examples/	hive-jackson/	jars/	kubernetes/		licenses/
</span></span><span class=line><span class=cl>python/		R/		sbin/	yarn/		LICENSE			NOTICE	README.md		RELEASE</span></span></code></pre></div></div><ul><li><code>READMD.md</code></li></ul><p>스파크 셸을 어떻게 사용하는지에 대한 안내 및 스파크 문서의 링크와 설정 가이드 등이 기록되어 있다.</p><ul><li><code>bin/</code></li></ul><p><code>spark-shell</code> 을 포함한 대부분의 스크립트가 위치한다.</p><ul><li><code>sbin/</code></li></ul><p>다양한 배포 모드에서 클러스터의 스파크 컴포넌트들을 시작하고 중지하기 위한 관리 목적이다.</p><ul><li><code>kubernetes/</code></li></ul><p>쿠버네티스 클러스터에서 쓰는 스파크를 위해, 도커 이미지 제작을 위한 Dockerfile들을 담고 있다.</p><ul><li><code>data/</code></li></ul><p>MLlib, 정형화 프로그래밍, GraphX 등에서 입력으로 사용되는 <code>.txt</code> 파일이 있다.</p><ul><li><code>examples/</code></li></ul><p>Java, Python, R, Scala에 대한 예제들을 제공한다.</p><h2 id=pyspark-installation>PySpark Installation
<a class=anchor href=#pyspark-installation>#</a></h2><h3 id=pypi-설치>PyPi 설치
<a class=anchor href=#pypi-%ec%84%a4%ec%b9%98>#</a></h3><label class=sc-image for=sc-image-toggle-5><a href=https://pypi.org/project/pyspark/ target=_blank><img src="https://dl.dropboxusercontent.com/scl/fi/svtj2gv0qt6yg4r4440kq/spark-08-pyspark-pypi.webp?rlkey=jsyxby5oeh4ty6dvzo40phtly&amp;raw=1" alt="Apache Spark Python API > Project description" loading=lazy decoding=async style=height:auto></a></label><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install pyspark</span></span></code></pre></div></div><p>명령어를 입력해 PyPi 저장소에서 PySpark 라이브러리를 설치할 수 있다.
다운로드한 파일과 동일하게 25년 5월 23일 릴리즈된 4.0.0 버전을 설치한다.
다른 버전을 설치하고 싶다면 <code>pip install pyspark=3.0.0</code> 과 같이 입력할 수 있다.</p><p>SQL, ML, MLlib 등 추가적인 라이브러리를 같이 설치하려면 <code>pip install pyspark[sql,ml,mllib]</code> 와 같이 입력할 수 있다.</p><p>별도의 가상환경에서 PySpark를 설치하는 것을 추천한다. 4.0.0 버전 기준으로
Python 3.9 버전부터 지원한다. 개인적으로는 Python 3.10 버전을 사용한다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>(</span>spark<span class=o>)</span> % pip install pyspark
</span></span><span class=line><span class=cl>Collecting pyspark
</span></span><span class=line><span class=cl>  Downloading pyspark-4.0.0.tar.gz <span class=o>(</span>434.1 MB<span class=o>)</span>
</span></span><span class=line><span class=cl>     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 434.1/434.1 MB 3.9 MB/s eta 0:00:00
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>Successfully built pyspark
</span></span><span class=line><span class=cl>Installing collected packages: py4j, pyspark
</span></span><span class=line><span class=cl>Successfully installed py4j-0.10.9.9 pyspark-4.0.0</span></span></code></pre></div></div><h3 id=pyspark-실행>pyspark 실행
<a class=anchor href=#pyspark-%ec%8b%a4%ed%96%89>#</a></h3><p><code>spark-shell</code> 은 Scala 쉘을 실행한다.
Scala가 아닌 Python을 사용하고 싶다면 <code>pyspark</code> 쉘을 실행할 수 있다.</p><div class=sc-codeblock data-lang=bash><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">bash</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>(</span>spark<span class=o>)</span> % pyspark             
</span></span><span class=line><span class=cl>Python 3.10.18 <span class=p>|</span> packaged by conda-forge <span class=p>|</span> <span class=o>(</span>main, Jun  <span class=m>4</span> 2025, 14:46:00<span class=o>)</span> <span class=o>[</span>Clang 18.1.8 <span class=o>]</span> on darwin
</span></span><span class=line><span class=cl>Type <span class=s2>&#34;help&#34;</span>, <span class=s2>&#34;copyright&#34;</span>, <span class=s2>&#34;credits&#34;</span> or <span class=s2>&#34;license&#34;</span> <span class=k>for</span> more information.
</span></span><span class=line><span class=cl>WARNING: Using incubator modules: jdk.incubator.vector
</span></span><span class=line><span class=cl>Using Spark<span class=s1>&#39;s default log4j profile: org/apache/spark/log4j2-defaults.properties
</span></span></span><span class=line><span class=cl><span class=s1>25/06/28 21:35:50 WARN Utils: Your hostname, minyeamer, resolves to a loopback address: 127.0.0.1; using 192.168.x.x instead (on interface en0)
</span></span></span><span class=line><span class=cl><span class=s1>25/06/28 21:35:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
</span></span></span><span class=line><span class=cl><span class=s1>Using Spark&#39;</span>s default log4j profile: org/apache/spark/log4j2-defaults.properties
</span></span><span class=line><span class=cl>Setting default log level to <span class=s2>&#34;WARN&#34;</span>.
</span></span><span class=line><span class=cl>To adjust logging level use sc.setLogLevel<span class=o>(</span>newLevel<span class=o>)</span>. For SparkR, use setLogLevel<span class=o>(</span>newLevel<span class=o>)</span>.
</span></span><span class=line><span class=cl>25/06/28 21:35:50 WARN NativeCodeLoader: Unable to load native-hadoop library <span class=k>for</span> your platform... using builtin-java classes where applicable
</span></span><span class=line><span class=cl>Welcome to
</span></span><span class=line><span class=cl>      ____              __
</span></span><span class=line><span class=cl>     / __/__  ___ _____/ /__
</span></span><span class=line><span class=cl>    _<span class=se>\ \/</span> _ <span class=se>\/</span> _ <span class=sb>`</span>/ __/  <span class=s1>&#39;_/
</span></span></span><span class=line><span class=cl><span class=s1>   /__ / .__/\_,_/_/ /_/\_\   version 4.0.0
</span></span></span><span class=line><span class=cl><span class=s1>      /_/
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>Using Python version 3.10.18 (main, Jun  4 2025 14:46:00)
</span></span></span><span class=line><span class=cl><span class=s1>Spark context Web UI available at http://localhost:4040
</span></span></span><span class=line><span class=cl><span class=s1>Spark context available as &#39;</span>sc<span class=s1>&#39; (master = local[*], app id = local-1751114150819).
</span></span></span><span class=line><span class=cl><span class=s1>SparkSession available as &#39;</span>spark<span class=err>&#39;</span>.
</span></span><span class=line><span class=cl>&gt;&gt;&gt; </span></span></code></pre></div></div><p>Python 쉘에서는 대화형으로 Python API를 사용할 수 있다.
현재 경로에 있는 <code>README.md</code> 파일을 첫 번째 10줄만 읽어보았다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>strings</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>read</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=s2>&#34;README.md&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>strings</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=n>truncate</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>+--------------------------------------------------------------------------------------------------+</span>
</span></span><span class=line><span class=cl><span class=o>|</span><span class=n>value</span>                                                                                             <span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>+--------------------------------------------------------------------------------------------------+</span>
</span></span><span class=line><span class=cl><span class=o>|</span><span class=c1># Apache Spark                                                                                    |</span>
</span></span><span class=line><span class=cl><span class=o>|</span>                                                                                                  <span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>|</span><span class=n>Spark</span> <span class=ow>is</span> <span class=n>a</span> <span class=n>unified</span> <span class=n>analytics</span> <span class=n>engine</span> <span class=k>for</span> <span class=n>large</span><span class=o>-</span><span class=n>scale</span> <span class=n>data</span> <span class=n>processing</span><span class=o>.</span> <span class=n>It</span> <span class=n>provides</span>                  <span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>|</span><span class=n>high</span><span class=o>-</span><span class=n>level</span> <span class=n>APIs</span> <span class=ow>in</span> <span class=n>Scala</span><span class=p>,</span> <span class=n>Java</span><span class=p>,</span> <span class=n>Python</span><span class=p>,</span> <span class=ow>and</span> <span class=n>R</span> <span class=p>(</span><span class=n>Deprecated</span><span class=p>),</span> <span class=ow>and</span> <span class=n>an</span> <span class=n>optimized</span> <span class=n>engine</span> <span class=n>that</span>          <span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>|</span><span class=n>supports</span> <span class=n>general</span> <span class=n>computation</span> <span class=n>graphs</span> <span class=k>for</span> <span class=n>data</span> <span class=n>analysis</span><span class=o>.</span> <span class=n>It</span> <span class=n>also</span> <span class=n>supports</span> <span class=n>a</span>                         <span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>|</span><span class=n>rich</span> <span class=nb>set</span> <span class=n>of</span> <span class=n>higher</span><span class=o>-</span><span class=n>level</span> <span class=n>tools</span> <span class=n>including</span> <span class=n>Spark</span> <span class=n>SQL</span> <span class=k>for</span> <span class=n>SQL</span> <span class=ow>and</span> <span class=n>DataFrames</span><span class=p>,</span>                        <span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>|</span><span class=n>pandas</span> <span class=n>API</span> <span class=n>on</span> <span class=n>Spark</span> <span class=k>for</span> <span class=n>pandas</span> <span class=n>workloads</span><span class=p>,</span> <span class=n>MLlib</span> <span class=k>for</span> <span class=n>machine</span> <span class=n>learning</span><span class=p>,</span> <span class=n>GraphX</span> <span class=k>for</span> <span class=n>graph</span> <span class=n>processing</span><span class=p>,</span><span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>|</span><span class=ow>and</span> <span class=n>Structured</span> <span class=n>Streaming</span> <span class=k>for</span> <span class=n>stream</span> <span class=n>processing</span><span class=o>.</span>                                                   <span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>|</span>                                                                                                  <span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>|-</span> <span class=n>Official</span> <span class=n>version</span><span class=p>:</span> <span class=o>&lt;</span><span class=n>https</span><span class=p>:</span><span class=o>//</span><span class=n>spark</span><span class=o>.</span><span class=n>apache</span><span class=o>.</span><span class=n>org</span><span class=o>/&gt;</span>                                                   <span class=o>|</span>
</span></span><span class=line><span class=cl><span class=o>+--------------------------------------------------------------------------------------------------+</span>
</span></span><span class=line><span class=cl><span class=n>only</span> <span class=n>showing</span> <span class=n>top</span> <span class=mi>10</span> <span class=n>rows</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>strings</span><span class=o>.</span><span class=n>count</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=mi>125</span></span></span></code></pre></div></div><p>쉘을 나가고 싶다면 <code>Ctrl+D</code> 를 눌러 나갈 수도 있고, <code>quit()</code> 함수를 실행해 종료할 수도 있다.</p><div class=sc-codeblock data-lang=python><div class=code-actions><button class="code-copy-button code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>quit</span><span class=p>()</span></span></span></code></pre></div></div></article><footer class=site-footer><div class=tags-wrap><a href="/search/?tags=Apache%20Spark" class=tag>#Apache Spark</a>
<a href="/search/?tags=Spark%20Shell" class=tag>#Spark Shell</a>
<a href="/search/?tags=JDK" class=tag>#JDK</a>
<a href="/search/?tags=Hadoop" class=tag>#Hadoop</a>
<a href="/search/?tags=HDFS" class=tag>#HDFS</a>
<a href="/search/?tags=PySpark" class=tag>#PySpark</a>
<a href="/search/?tags=%eb%8d%b0%ec%9d%b4%ed%84%b0%20%ec%97%94%ec%a7%80%eb%8b%88%ec%96%b4%eb%a7%81" class=tag>#데이터 엔지니어링</a>
<a href="/search/?tags=%ec%8a%a4%ed%8c%8c%ed%81%ac" class=tag>#스파크</a>
<a href="/search/?tags=Study" class=tag>#Study</a></div><div class=prev-next-wrap><a href=/blog/spark-study-1/ class=prev-link><span class=prev-next-label><i class="fa-solid fa-backward"></i> PREV</span>
<span class=prev-next-title>Apache Spark - 스파크의 기본 개념과 아키텍처</span>
</a><a href=/blog/spark-study-3/ class=next-link><span class=prev-next-label>NEXT <i class="fa-solid fa-forward"></i></span>
<span class=prev-next-title>Apache Spark - 스파크 애플리케이션 구조와 RDD 이해하기</span></a></div><div class=comments-wrap><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://minyeamer.github.io/blog/spark-study-2/",this.page.identifier="https://minyeamer.github.io/blog/spark-study-2/"};(function(){var e=document,t=e.createElement("script");t.src="https://minyeamer.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})();function reloadDisqus(){window.DISQUS&&DISQUS.reset({reload:!0,config:function(){this.page.url="https://minyeamer.github.io/blog/spark-study-2/",this.page.identifier="https://minyeamer.github.io/blog/spark-study-2/"}})}</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div><div class="copyright-wrap flex justify-center"><small>Minystory - <a href=https://creativecommons.org/licenses/by/4.0/legalcode target=_blank rel="noopener noreferrer">© CC BY 4.0</a></small></div></footer><label for=menu-control class="hidden site-menu-overlay"></label></div><aside class=site-toc aria-label=목차><div class=toc-content><nav id=TableOfContents><ul><li><a href=#spark-installation>Spark Installation</a><ul><li><a href=#spark-설치>Spark 설치</a></li><li><a href=#java-설치>Java 설치</a></li><li><a href=#hadoop-설치>Hadoop 설치</a></li><li><a href=#hdfs-실행>HDFS 실행</a></li><li><a href=#hdfs-실행-중-오류-처리>HDFS 실행 중 오류 처리</a></li><li><a href=#spark-실행>Spark 실행</a></li><li><a href=#spark-디렉터리-구조>Spark 디렉터리 구조</a></li></ul></li><li><a href=#pyspark-installation>PySpark Installation</a><ul><li><a href=#pypi-설치>PyPi 설치</a></li><li><a href=#pyspark-실행>pyspark 실행</a></li></ul></li></ul></nav></div><div class=hidden id=toc-config data-start=2 data-end=3></div></aside><div class=site-toolbar role=toolbar aria-label="툴바'"><input type=checkbox id=toolbar-toggle class="hidden toggle"><div class=toolbar-items><button class=toolbar-button onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=toolbar-button onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
</button>
<button class=toolbar-button onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div><label for=toolbar-toggle class="toolbar-button toolbar-toggle-label"><i class="fa-solid fa-plus icon-expand"></i>
<i class="fa-solid fa-xmark icon-collapse"></i></label></div></main></body></html>