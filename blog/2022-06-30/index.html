<!doctype html><html lang=ko-kr dir=ltr><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="index, follow"><meta name=keywords content="TIL,NLP"><meta name=description content="딥 러닝을 이용한 자연어 처리 입문 3"><meta name=author content="minyeamer"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><link rel=canonical href=https://minyeamer.github.io/blog/2022-06-30/><meta name=google-site-verification content="u1tWcHmHUZWfFT1cHaku6sqU-bK40N3WLR-C-4VUWN0"><meta name=naver-site-verification content="6eaf8e9da1a6104780f056f1a7797fe5a3a5a0da"><meta property="og:url" content="https://minyeamer.github.io/blog/2022-06-30/"><meta property="og:site_name" content="Minystory"><meta property="og:title" content="2022-06-30 Log"><meta property="og:description" content="딥 러닝을 이용한 자연어 처리 입문 3"><meta property="og:locale" content="ko_kr"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-06-30T20:00:00+09:00"><meta property="article:modified_time" content="2022-06-30T20:00:00+09:00"><meta property="article:tag" content="TIL"><meta property="article:tag" content="NLP"><title>2022-06-30 Log | Minystory</title><link rel=icon href=/favicon.png><link rel=manifest href=/manifest.json><link rel=canonical href=https://minyeamer.github.io/blog/2022-06-30/><link rel=stylesheet href=/book.min.b806db2fcd7252f443c9368f219d473f25aaf586f28798bf9f651f41a7225060.css integrity="sha256-uAbbL81yUvRDyTaPIZ1HPyWq9Ybyh5i/n2UfQaciUGA=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/search-input.min.97bd2c69bb66aba393499c7ad9ff319905e14e001ef050bf45d8b47a9c6d9278.js integrity="sha256-l70sabtmq6OTSZx62f8xmQXhTgAe8FC/Rdi0epxtkng=" crossorigin=anonymous></script><link rel=preload href=/search-data.min.7cb06c5a504171147e5c23f8fa923c5413dc6769b4d635e327bb3c3acb570140.json as=fetch crossorigin><script>window.SEARCH_DATA_URL="/search-data.min.7cb06c5a504171147e5c23f8fa923c5413dc6769b4d635e327bb3c3acb570140.json"</script><script defer src=/search.min.f30f9834d4764fd9751da64098c954d01085f648ba9ca421a3c97582f8c47253.js integrity="sha256-8w+YNNR2T9l1HaZAmMlU0BCF9ki6nKQho8l1gvjEclM=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V")</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.1/css/all.min.css crossorigin=anonymous><script defer src=/scroll-progress.min.841ade7e507a5f6d59c4e7bf2fe2b2ca034070677ff7957eec55610a024dd776.js integrity="sha256-hBreflB6X21ZxOe/L+KyygNAcGd/95V+7FVhCgJN13Y=" crossorigin=anonymous></script><script defer src=/dark-mode.min.e41c6440ffd9967d6f6a419ff3ce09b862009fe1646ab265f5cb2817d2a508e3.js integrity="sha256-5BxkQP/Zln1vakGf884JuGIAn+FkarJl9csoF9KlCOM=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlightjs-line-numbers.js/2.9.0/highlightjs-line-numbers.min.js></script><script defer src=/copy-code.min.aaeef965f0b4992e55f976edaecb34a89d414e1791caa18c3f4f4376c6d8b5a8.js integrity="sha256-qu75ZfC0mS5V+Xbtrss0qJ1BTheRyqGMP09DdsbYtag=" crossorigin=anonymous></script><script defer src=/toc-highlightjs.093016f0ef312174ad862fdcf5792e88ab5442bd39beecc38d15643f71ab5c31.min integrity="sha256-CTAW8O8xIXSthi/c9XkuiKtUQr05vuzDjRVkP3GrXDE=" crossorigin=anonymous></script></head><body dir=ltr class="book-kind-page book-type-post book-layout-post"><div class=scroll-progress><div class=scroll-progress-bar></div></div><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><div class=sidebar-profile><div class=profile-img-wrap><a href=https://minyeamer.github.io/><img src="https://avatars.githubusercontent.com/u/17109173?v=4" alt=Profile class=profile-img></a></div><div class=sidebar-social><a href=https://github.com/minyeamer target=_blank title=GitHub><i class="fa-brands fa-github"></i>
</a><a href=/categories/ title=Categories><i class="fa-solid fa-folder"></i>
</a><a href=/tags/ title=Tags><i class="fa-solid fa-tags"></i>
</a><button id=dark-mode-toggle class=dark-mode-toggle aria-label="Toggle dark mode">
<i class="fa-solid fa-circle-half-stroke"></i></button></div></div><h2 class=book-brand><a class="flex align-center" href=/><span>Minystory</span></a></h2><div class="book-search hidden"><div class=search-input-container><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/ onkeydown='event.key==="Enter"&&goToSearchPage()'>
<button type=button id=book-search-button class=book-search-btn onclick=goToSearchPage()>
<i class="fa-solid fa-magnifying-glass"></i></button></div><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden");function goToSearchPage(){const t=document.getElementById("book-search-input"),e=t.value.trim();e&&(window.location.href="/search/?q="+encodeURIComponent(e))}</script><div class=book-categories><input type=checkbox class="hidden toggle" id=categories-control checked>
<label for=categories-control class="categories-toggle categories-link"><a href=/categories/><i class="fa-solid fa-folder"></i>
<span>전체</span>
<span class=category-count>(126)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul class=categories-menu id=categories-menu><li><input type=checkbox class="hidden toggle" id=cat-algorithm>
<label for=cat-algorithm class="categories-toggle categories-link"><a href=/categories/algorithm/><i class="fa-solid fa-folder"></i>
Algorithm
<span class=category-count>(51)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/algorithm/baekjoon/><i class="fa-solid fa-file"></i>
Baekjoon
<span class=category-count>(31)</span></a></li><li class=categories-link><a href=/categories/algorithm/leetcode/><i class="fa-solid fa-file"></i>
LeetCode
<span class=category-count>(2)</span></a></li><li class=categories-link><a href=/categories/algorithm/programmers/><i class="fa-solid fa-file"></i>
Programmers
<span class=category-count>(17)</span></a></li><li class=categories-link><a href=/categories/algorithm/references/><i class="fa-solid fa-file"></i>
References
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-blog>
<label for=cat-blog class="categories-toggle categories-link"><a href=/categories/blog/><i class="fa-solid fa-folder"></i>
Blog
<span class=category-count>(5)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/blog/review/><i class="fa-solid fa-file"></i>
Review
<span class=category-count>(1)</span></a></li><li class=categories-link><a href=/categories/blog/tech/><i class="fa-solid fa-file"></i>
Tech
<span class=category-count>(4)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-book>
<label for=cat-book class="categories-toggle categories-link"><a href=/categories/book/><i class="fa-solid fa-folder"></i>
Book
<span class=category-count>(1)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/book/finance/><i class="fa-solid fa-file"></i>
Finance
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-data>
<label for=cat-data class="categories-toggle categories-link"><a href=/categories/data/><i class="fa-solid fa-folder"></i>
Data
<span class=category-count>(4)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/data/crawling/><i class="fa-solid fa-file"></i>
Crawling
<span class=category-count>(4)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-diary>
<label for=cat-diary class="categories-toggle categories-link"><a href=/categories/diary/><i class="fa-solid fa-folder"></i>
Diary
<span class=category-count>(3)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/diary/2022/><i class="fa-solid fa-file"></i>
2022
<span class=category-count>(2)</span></a></li><li class=categories-link><a href=/categories/diary/2023/><i class="fa-solid fa-file"></i>
2023
<span class=category-count>(1)</span></a></li></ul></li><li><input type=checkbox class="hidden toggle" id=cat-study>
<label for=cat-study class="categories-toggle categories-link"><a href=/categories/study/><i class="fa-solid fa-folder"></i>
Study
<span class=category-count>(61)</span>
</a><i class="fa-solid fa-chevron-down categories-arrow"></i></label><ul><li class=categories-link><a href=/categories/study/2022/><i class="fa-solid fa-file"></i>
2022
<span class=category-count>(16)</span></a></li><li class=categories-link><a href=/categories/study/2023/><i class="fa-solid fa-file"></i>
2023
<span class=category-count>(10)</span></a></li><li class=categories-link><a href=/categories/study/ai-school/><i class="fa-solid fa-file"></i>
AI SCHOOL
<span class=category-count>(34)</span></a></li><li class=categories-link><a href=/categories/study/datacamp/><i class="fa-solid fa-file"></i>
DataCamp
<span class=category-count>(1)</span></a></li></ul></li></ul></div><div class=recent-posts><div class=recent-posts-header><i class="fa-solid fa-clock"></i>
<span>최신글</span></div><ul class=recent-posts-list><li class=recent-post-item><a href=/blog/2023-04-02/ title="2023-04-02 Log"><div class=recent-post-title>2023-04-02 Log</div><div class=recent-post-date><time datetime=2023-04-02>2023.04.02</time></div></a></li><li class=recent-post-item><a href=/blog/10000-recipe/ title="[Python] 만개의 레시피 데이터 수집"><div class=recent-post-title>[Python] 만개의 레시피 데이터 수집</div><div class=recent-post-date><time datetime=2023-03-26>2023.03.26</time></div></a></li><li class=recent-post-item><a href=/blog/2023-03-25/ title="2023-03-25 Log"><div class=recent-post-title>2023-03-25 Log</div><div class=recent-post-date><time datetime=2023-03-25>2023.03.25</time></div></a></li><li class=recent-post-item><a href=/blog/2023-03-21/ title="2023-03-21 Log"><div class=recent-post-title>2023-03-21 Log</div><div class=recent-post-date><time datetime=2023-03-21>2023.03.21</time></div></a></li><li class=recent-post-item><a href=/blog/2023-02-19/ title="2023-02-19 Log"><div class=recent-post-title>2023-02-19 Log</div><div class=recent-post-date><time datetime=2023-02-19>2023.02.19</time></div></a></li></ul></div></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><i class="fa-solid fa-bars book-icon" id=menu-icon></i></label><h3><a href=https://minyeamer.github.io/ class=site-title>Minystory</a></h3><label for=toc-control><i class="fa-solid fa-list book-icon" id=toc-icon></i></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#attention-function>Attention Function</a></li><li><a href=#dot-product-attention>Dot-Product Attention</a></li><li><a href=#1-attention-score>1. Attention Score</a></li><li><a href=#2-attention-distribution>2. Attention Distribution</a></li><li><a href=#3-attention-value>3. Attention Value</a></li></ul><ul><li><a href=#transformer-hyperparameter>Transformer Hyperparameter</a></li><li><a href=#positional-encoding>Positional Encoding</a></li><li><a href=#self-attention>Self-Attention</a></li><li><a href=#scaled-dot-product-attention>Scaled dot-product Attention</a></li><li><a href=#multi-head-attention>Multi-head Attention</a></li><li><a href=#padding-mask>Padding Mask</a></li><li><a href=#position-wise-ffnn><code>Position-wise</code> FFNN</a></li><li><a href=#residual-connection>Residual Connection</a></li><li><a href=#layer-normalization>Layer Normalization</a></li><li><a href=#look-ahead-mask>Look-ahead Mask</a></li><li><a href=#endocer-decoder-attention>Endocer-Decoder Attention</a></li></ul><ul><li><a href=#사전-훈련된-워드-임베딩>사전 훈련된 워드 임베딩</a></li><li><a href=#사전-훈련된-언어-모델>사전 훈련된 언어 모델</a></li><li><a href=#masked-language-model>Masked Language Model</a></li></ul><ul><li><a href=#contextual-embedding>Contextual Embedding</a></li><li><a href=#subword-tokenizer>Subword Tokenizer</a></li><li><a href=#position-embedding>Position Embedding</a></li><li><a href=#mlm-pre-training>MLM (Pre-training)</a></li><li><a href=#nsp-pre-training>NSP (Pre-training)</a></li><li><a href=#segment-embedding>Segment Embedding</a></li><li><a href=#find-tuning>Find-tuning</a></li><li><a href=#attention-mask>Attention Mask</a></li></ul><ul><li><a href=#sentence-embedding>Sentence Embedding</a></li></ul><ul><li><a href=#svd>SVD</a></li><li><a href=#truncated-svd>Truncated SVD</a></li><li><a href=#latent-semantic-analysislsa>Latent Semantic Analysis(LSA)</a></li></ul><ul><li><a href=#latent-dirichlet-allocationlda>Latent Dirichlet Allocation(LDA)</a></li><li><a href=#lda의-가정>LDA의 가정</a></li><li><a href=#lda-수행>LDA 수행</a></li></ul><ul><li><a href=#extractive-summarization>Extractive Summarization</a></li><li><a href=#abstractive-summarization>Abstractive Summarization</a></li><li><a href=#추상적-요약-구현>추상적 요약 구현</a></li><li><a href=#textrank>TextRank</a></li></ul><ul><li><a href=#babi-데이터셋>Babi 데이터셋</a></li><li><a href=#메모리-네트워크-구조>메모리 네트워크 구조</a></li><li><a href=#qa-태스크-풀기>QA 태스크 풀기</a></li><li><a href=#mean으로-한국어-qa>MeaN으로 한국어 QA</a></li></ul><ul><li><a href=#generative-pre-trained-transformergpt>Generative Pre-trained Transformer(GPT)</a></li><li><a href=#gpt-실습>GPT 실습</a></li></ul></nav><div class=book-nav><button class=book-nav-btn3 onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=book-nav-btn3 onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
<button class=book-nav-btn3 onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div></aside></header><article class="markdown book-article"><header class=post-header><div class=post-header-category><a href=/categories/study/2022/ class=post-header-category-link>Study/2022</a></div><h1 class=post-header-title>2022-06-30 Log</h1><div class=post-header-date><time datetime=2022-06-30T20:00:00+09:00>2022. 6. 30. 20:00</time></div></header><h1 id=15-01-attention-mechanism>15-01. Attention Mechanism
<a class=anchor href=#15-01-attention-mechanism>#</a></h1><ul><li>seq2seq 모델은 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 해서 정보 손실이 발생하며,<br>RNN의 고질적인 문제인 기울기 소실 문제도 존재
기계 번역 분야에서 입력 문장이 길면 번역 품질이 떨어지는 것을 보정하기 위해 어텐션 기법 활용</li><li>어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시접마다,<br>인코더에서 전체 입력 문장을 다시 한 번 참고한다는 점</li></ul><h2 id=attention-function>Attention Function
<a class=anchor href=#attention-function>#</a></h2><ul><li>Attention(Q, K, V) = Attention Value</li><li>어텐션 함수는 주어진 Query에 대해서 모든 Key와의 유사도를 각각 구하고,<br>유사도를 키와 맵핑되어있는 각각의 Value에 반영, 이후 유사도가 반영된 Value를 모두 더해 리턴</li></ul><blockquote class=book-hint><p><strong>Q(Query)</strong>: t 시점의 디코더 셀에서의 은닉 상태<br><strong>K(Keys)</strong>: 모든 시점의 인코더 셀의 은닉 상태들<br><strong>V(Values)</strong>: 모든 시점의 인코더 셀의 은닉 상태들</p></blockquote><h2 id=dot-product-attention>Dot-Product Attention
<a class=anchor href=#dot-product-attention>#</a></h2><ul><li>어텐션 메커니즘에서 출력 단어를 예측하기 위해 디코더 셀은<br>t-1의 hidden state, t-1에 나온 출력 단어, Attention Value $a_t$를 필요</li><li>제안자의 이름을 따서 루옹(Luong) 어텐션이라고도 함</li></ul><h2 id=1-attention-score>1. Attention Score
<a class=anchor href=#1-attention-score>#</a></h2><ul><li>$a_t$를 구하기 위해서는 Attention Score를 구해야 함<br>(인코더의 모든 은닉 상태 각각의 디코더의 현 시점의 은닉 상태 $s_t$와 얼마나 유사한지 판단하는 스코어)</li><li>Dot-Product Attention에서는 스코어 값을 구하기 위해 $s_t$를 전치하고 각 은닉 상태와 내적을 수행</li><li>스코어 함수 $score(s_t,h_i)={s^T_t}{h_i}$</li><li>$s_t$와 인코더의 모든 은닉 상태의 어텐션 스코어 모음값 $e^t=[{s^T_t}{h_1},&mldr;,{s^T_t}{h_N}]$</li><li>스코어 함수에 따라 scaled dot, general, concat, location-base 어텐션 등이 존재</li></ul><h2 id=2-attention-distribution>2. Attention Distribution
<a class=anchor href=#2-attention-distribution>#</a></h2><ul><li>$e^t$에 softmax 함수를 적용하여 모든 값을 합하면 1이 되는 확률 분포,<br>Attention Distribution을 얻으며, 분포 각각의 값을 Attention Weight라 함</li><li>어텐션 분포 $\alpha^t=softmax(e^t)$</li></ul><h2 id=3-attention-value>3. Attention Value
<a class=anchor href=#3-attention-value>#</a></h2><ul><li>어텐션의 최종 결과값을 얻기 위해서 각 인코더의 은닉 상태와 어텐션 가중치들을 곱하고,<br>최종적으로 모두 더하는 Weighted Sum을 진행</li><li>어텐션 값이 구해지면 어텐션 메커니즘은 $a_t$를 $s_t$와 결합(concatenate)하여 하나의 벡터 $v_t$를 생성</li><li>$v_t\text{를 }\hat{y}$ 예측 연산의 입력으로 사용해 인코더로부터 얻은 정보를 활용하여 $\hat{y}$를 예측</li></ul><p>$$a_t=\Sigma^N_{i=1}{\alpha^t_i}{h_i}$$</p><h1 id=15-02-bahdanau-attention>15-02. Bahdanau Attention
<a class=anchor href=#15-02-bahdanau-attention>#</a></h1><ul><li>바다나우 어텐션 함수의 Query는 t 시점의 은닉 상태가 아니라 t-1 시점의 은닉 상태를 사용</li><li>$score(s_{t-1},h_i)={W^T_\alpha}\tanh{({W_b}{s_{t-1}}+{W_c}{h_i})}$</li><li>$W_a,W_b,W_c$는 학습 가능한 가중치 행렬을 의미하며, $s_{t-1}$와 $h_1,h_2,h_3,h_4$의 어텐션 스코어를<br>각각 구하는 병렬 연산을 위해 $h_1,h_2,h_3,h_4$를 하나의 행렬 $H$로 변환</li><li>$e^t={W^T_\alpha}\tanh{({W_b}{s_{t-1}}+{W_c}H)}$</li><li>컨텍스트 벡터를 구하면, 현재 시점의 입력인 단어의 임베딩 벡터와 연결(concatenate)하고,<br>현재 시점의 새로운 입력으로 사용</li></ul><hr><h1 id=16-01-transformer>16-01. Transformer
<a class=anchor href=#16-01-transformer>#</a></h1><ul><li>어텐션을 RNN의 보정을 위한 용도로 사용하지 않고, 어텐션만으로 인코더와 디코더를 생성</li></ul><h2 id=transformer-hyperparameter>Transformer Hyperparameter
<a class=anchor href=#transformer-hyperparameter>#</a></h2><p>$d_{model}=152$</p><blockquote class=book-hint><p>트랜스 포머의 인코더와 디코더에서의 정해진 입력과 출력의 크기, 임베딩 벡터의 차원</p></blockquote><p>$num_{layers}=6$</p><blockquote class=book-hint><p>트랜스포머 모델에서 인코더와 디코더가 총 몇 층으로 구성되었는지</p></blockquote><p>$num_{heads}=8$</p><blockquote class=book-hint><p>어텐션을 여러 개로 분할해서 병렬로 수행하고 결과값을 다시 하나로 합치는데, 이때 이 병렬의 개수</p></blockquote><p>$d_{ff}=2048$</p><blockquote class=book-hint><p>피드 포워드 신경망의 은닉층의 크기, 입력층과 출력층의 크기는 $d_{model}$</p></blockquote><h2 id=positional-encoding>Positional Encoding
<a class=anchor href=#positional-encoding>#</a></h2><ul><li>RNN은 단어의 위치에 따라 단어를 순차적으로 입력받아 처리하는 특성으로 인해 각 단어의 위치 정보를 가짐</li><li>트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니기 때문에<br>단어의 위치 정보를 얻기 위해 각 단어의 임베딩 벡터에 위치 정보들을 더해 모델의 입력으로 사용(포지셔널 인코딩)</li><li>트랜스포머는 위치 정보를 가진 값을 만들기 위해 아래 함수를 사용</li></ul><p>$$PE_{(pos,2i)}=\sin{(pos/10000^{2i/d_{model}})}$$
$$PE_{(pos,2i+1)}=\cos{(pos/10000^{2i/d_{model}})}$$</p><ul><li>사인 함수와 코사인 함수의 값을 임베딩 벡터에 더해 단어에 순서 정보를 부여</li><li>$pos$는 입력 문장에서의 임베딩 벡터의 위치, $i$는 임데빙 벡터 내의 차원의 인덱스 의미</li><li>$d_{model}$은 트랜스포머의 모든 층의 출력 차원을 의미하는 트랜스포머의 하이퍼파라미터</li></ul><h2 id=self-attention>Self-Attention
<a class=anchor href=#self-attention>#</a></h2><ul><li>어텐션을 자기 자신에게 수행하는 것</li><li>Q, K, V는 모두 입력 문장의 모든 단어 벡터들을 의미</li><li>셀프 어텐션은 입력 문장 내의 단어들끼리 유사도를 구해 it이 어떤 단어와 연관되었는지 확률을 찾아냄</li><li>셀프 어텐션은 인코더의 초기 입력인 $d_{model}$의 차원을 가지는 단어 벡터들을 사용하여 수행하지 않고,<br>각 단어 벡터들로부터 Q벡터, K벡터, V벡터를 얻는 작업을 거침</li><li>$d_{model}=512$의 차원을 가졌던 각 단어 벡터들은 Q벡터, K벡터, V벡터로 변환되면서<br>$d_{model}$을 $num_{heads}$로 나눈 값 64를 차원으로 갖게 됨</li></ul><h2 id=scaled-dot-product-attention>Scaled dot-product Attention
<a class=anchor href=#scaled-dot-product-attention>#</a></h2><ul><li>트랜스포머에서는 스케일드 닷-프로적트 어텐션을 사용</li><li>벡터마다 일일히 컨텍스트 벡터를 구하는 벡터 연산을 하지 않고,<br>문장 행렬에 가중치 행렬을 곱하여 구한 Q행렬, K행렬, V행렬에 행렬 연산을 수행</li><li>행렬 연산에서 어텐션 스코어는 행렬의 값에 전체적으로 $\sqrt{d_k}$를 나누어 스코어 값을 가지는 행렬을 구함</li></ul><p>$$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$$</p><h2 id=multi-head-attention>Multi-head Attention
<a class=anchor href=#multi-head-attention>#</a></h2><ul><li>한 번의 어텐션을 하는 것보다 여러번의 어텐션을 병렬로 사용하는 것이 더 효과적이기 때문에<br>$d_{model}$의 차원을 $num_{heads}$개로 나누어 Q, K, V에 대해서 $num_{heads}$개의 병렬 어텐션을 수행</li><li>각각의 어텐션 값 행렬을 어텐션 헤드라고 불는데, 이때 가중치 행렬 $W^Q, W^K, W^V$의 값은 어텐션 헤드마다 전부 다름</li><li>멀티 헤드 어텐션은 어텐션을 병렬로 수행하여 다른 시각으로 정보들을 수집</li><li>벙렬 어텐션 수행 후 모든 어텐션 헤드를 연결(concatenate)하여 $(seq_{len}, d_{model})$ 크기의 행렬 생성</li><li>연결한 행렬에 가중치 행렬 $W^O$를 곱한 것이 멀티 헤드 어텐션의 최종 결과물이며, 인코더의 입력이었던 문장 행렬과 동일<br>(인코더에서의 입력의 킉가 출력에서도 동일 크기로 계쏙 유지되어야만 다음 인코더에서도 다시 입력이 될 수 있음)</li></ul><h2 id=padding-mask>Padding Mask
<a class=anchor href=#padding-mask>#</a></h2><ul><li>입력 문장에 <pad>토큰이 있을 경우 어텐션에서 제외하기 위해 -1e9라는 아주 작은 음수 값을 곱함</li><li><strong>Masking</strong>: 어텐션에서 제외하기 위해 값을 가리는 것</li><li>어텐션 스코어 행렬에서 행에 해당하는 문장은 Query, 열에 해당하는 문장은 Key이며,<br>Key에 <pad>가 있는 경우 해당 열 전체를 마스킹</li></ul><h2 id=position-wise-ffnn><code>Position-wise</code> FFNN
<a class=anchor href=#position-wise-ffnn>#</a></h2><ul><li>포지션 와이즈 FFNN은 인코더와 디코더에서 공통적으로 가지고 있는 서브층으로, 완전 연결 FFNN을 의미</li><li>$FFNN(x)=MAX(0,x{W_1}+b_1){W_2}+{b_2}$</li><li>$x$는 멀티 헤드 어텐션의 결과로 나온 $(seq_{len}, d_{model})$ 크기의 행렬을 의미,<br>가중치 행렬 $W_1\text{은 }(d_{model},d_{ff})\text{, }W_2\text{은 }(d_{ff},d_{model})$의 크기를 가짐</li><li>서브층을 지난 인코더의 최종 출력은 여전히 인코더의 입력의 크기였던 $(seq_{len}, d_{model})$의 크기가 보존</li></ul><h2 id=residual-connection>Residual Connection
<a class=anchor href=#residual-connection>#</a></h2><ul><li>트랜스포머에서 두 개의 서브층을 가진 인코더에 추가적으로 사용하는 Add & Norm 기법 중 Add에 해당</li><li>잔차 연결은 서브층의 입력과 출력을 더하는 것으로, 컴퓨터 비전 분야에서 주로 사용되는 모델의 학습을 돕는 기법</li><li>$x+Sublayer(x)$로 표현할 수 있으며, 멀티 헤드 어텐션이라면 $H(x)=x+Multi\text{-}head\ Attention(x)$과 같음</li></ul><h2 id=layer-normalization>Layer Normalization
<a class=anchor href=#layer-normalization>#</a></h2><ul><li>잔차 연결의 입력을 $x$, 잔차 연결과 층 정규화 두 가지 연산을 모두 수행한 후의 결과 행렬을 $LN$이라 할 때,<br>잔차 연결 후 정규화 연산을 수식으로 표현하면 $LN=LayerNorm(x+Sublayer(x))$와 같음</li><li>총 정규화는 텐서의 마지막 차원에 대해서 평균과 분산을 구하고, 이를 가지고 어떤 수식을 통해 값을 정규화하여 학습을 도움</li><li>총 정규화를 평균과 분산을 통한 정규화, 감마와 베타를 도입하는 것으로 나누었을 때,<br>우선, 평균과 분산을 통해 벡터 $x_i$를 정규화</li><li>$x_i$는 벡터인 반면, 평균 $\mu_i\text{과 분산 }\sigma^2_i$은 스칼라이기 때문에,<br>벡터 $x_i$의 각 $k$차원의 값은 다음과 같이 정규화 ($\epsilon$은 분모가 0이 되는 것을 방지하는 값)</li></ul><p>$$\hat{x_{i,k}}=\frac{x_{i,k}-\mu_i}{\sqrt{\sigma^2_i+\epsilon}}$$</p><h2 id=look-ahead-mask>Look-ahead Mask
<a class=anchor href=#look-ahead-mask>#</a></h2><ul><li>입력 단어를 매 시점마다 순차적으로 입력받는 RNN 계열의 신경망에 반해,<br>트랜스포머는 문장 행렬을 한 번에 받기 때문에 미래 시점의 단어까지 참고할 수 있는 현상 발생</li><li>룩-어헤드 마스크는 디코더의 첫번째 서브층에서 이루어지며,<br>자기 자신보다 미래에 있는 단어들은 참고하지 못하도록 마스킹함</li><li>룩-어헤드 마스크를 한다고해서 패딩 마스크가 불필요한 것이 아니므로 패딩 마스크를 포함하도록 구현</li></ul><h2 id=endocer-decoder-attention>Endocer-Decoder Attention
<a class=anchor href=#endocer-decoder-attention>#</a></h2><ul><li>디코더의 두번째 서브층은 멀티 헤드 어텐션을 수행한다는 점에서 이전의 어텐션들과 공통점이 있지만,<br>Query와 Key, Value가 달라 셀프 어텐션이 아님</li></ul><blockquote class=book-hint><p>인코더의 첫번째 서브층 Query = Key = Value<br>디코더의 첫번째 서브층 Query = Key = Value<br>디코더의 두번째 서브층 Query: 디코더 행렬 / Key = Value: 인코더 행렬</p></blockquote><h1 id=16-02-transformer-chatbot>16-02. Transformer Chatbot
<a class=anchor href=#16-02-transformer-chatbot>#</a></h1><ul><li>트랜스포머를 이용한 한국어 챗봇 <a href=https://wikidocs.net/89786>참고</a></li><li><a href=https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv>챗본 데이터</a> 사용</li></ul><hr><h1 id=17-01-pre-training>17-01. Pre-training
<a class=anchor href=#17-01-pre-training>#</a></h1><h2 id=사전-훈련된-워드-임베딩>사전 훈련된 워드 임베딩
<a class=anchor href=#%ec%82%ac%ec%a0%84-%ed%9b%88%eb%a0%a8%eb%90%9c-%ec%9b%8c%eb%93%9c-%ec%9e%84%eb%b2%a0%eb%94%a9>#</a></h2><ul><li>Word2Vec나 GloVe 등의 워드 임베딩은 하나의 단어가 하나의 벡터값으로 맵핑되므로,<br>문맥을 고려하지 못하여 다의어나 동음이의어를 구분하지 못하는 문제</li></ul><h2 id=사전-훈련된-언어-모델>사전 훈련된 언어 모델
<a class=anchor href=#%ec%82%ac%ec%a0%84-%ed%9b%88%eb%a0%a8%eb%90%9c-%ec%96%b8%ec%96%b4-%eb%aa%a8%eb%8d%b8>#</a></h2><ul><li>언어 모델은 주어진 텍스트로부터 다음 단어를 예측하도록 학습하여 별도의 라벨 없이 학습 가능</li><li>다의어를 구분할 수 없었던 문제점을 해결하고, RNN 계열의 신경망에서 탈피하기 위해 트랜스포머로 학습 시도</li><li>트랜스포머로 데이터를 학습시킨 언어 모델 GPT-1 등은 다양한 태스크에서 높은 성능을 얻음</li><li>이전 단어들로부터 다음 단어를 예측하는 언어 모델의 특성으로는 양방향 언어 모델을 사용할 수 없으므로,<br>ELMo에서는 두 개의 단방향 언어 모델을 따로 준비해 학습하는 방법을 사용했는데 여기서 발전된 마스크드 언어 모델이 등장</li></ul><h2 id=masked-language-model>Masked Language Model
<a class=anchor href=#masked-language-model>#</a></h2><ul><li>마스크드 언어 모델은 입력 텍스트의 단어 집합의 15%의 단어를 랜덤으로 Masking</li><li>빈칸 채우기 형식으로 마스킹된 단어들을 예측하게 함</li></ul><h1 id=17-02-bert>17-02. BERT
<a class=anchor href=#17-02-bert>#</a></h1><ul><li>구글이 공개한 사전 훈련된 모델로 수많은 NLP 태스크에서 최고 성능을 보임</li><li>트랜스포머를 이용하여 구현되었으며, 위키피디아와 BooksCorpus 같이 라벨이 없는 텍스트 데이터로 사전 훈련된 언어 모델</li><li>사전 훈련된 모델을 라벨이 있는 다른 작업에서 추가 훈련과 함께 하이퍼파라미터를 재조정,<br>다른 작업에 대해서 파라미터 재조정을 위한 추가 훈련 과정을 Fine-tuning이라고 함</li><li>BERT-Base: L=12, D=768, A=12: 110M개의 파라미터</li><li>BERT-Large: L=24, D=1024, A=16: 340M개의 파라미터</li></ul><h2 id=contextual-embedding>Contextual Embedding
<a class=anchor href=#contextual-embedding>#</a></h2><ul><li>BERT의 입력은 기존 모델들과 마찬가지로 임베딩 층을 지난 임베딩 벡터</li><li>BERT의 연산을 거친 후의 출력 임베딩은 문장의 문맥을 모두 참고한 문맥을 반영한 임베딩이 됨</li><li>BERT는 트랜스포머 인코더를 12번 쌓은 구조로, 셀프 어텐션을 통해 문맥을 반영</li></ul><h2 id=subword-tokenizer>Subword Tokenizer
<a class=anchor href=#subword-tokenizer>#</a></h2><ul><li>BERT는 서브워드 토크나이저로 WordPiece 토크나이저를 사용</li><li>자주 등장하는 단어는 그대로 단어 집합에 추가하지만, 자주 등장하지 않는 단어는 서브워드로 분리해 집합에 추가하며,<br>집합이 만들어지고 나면 단어 집합을 기반으로 토큰화 수행</li><li>BERT에서 토큰이 단어 집합에 존재할 경우 해당 토큰을 분리하지 않지만,<br>존재하지 않으면 토큰을 서브워드로 분리하고, 첫번째 서브워드를 제외한 나머지 서브워드들은 앞에 #를 붙인 것을 토큰으로 함</li><li>#은 서브워드들이 단어의 중간부터 등장하는 것을 알려주기 위해 표시해둔 기호</li></ul><div class=book-codeblock data-lang=python><div class=code-actions><button class="code-copy-btn code-action" onclick=copyCode(this)>
<i class="fa-solid fa-copy"></i>Copy
</button>
<span class="code-language code-action">python</span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>BertTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>BertTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;bert-base-uncased&#34;</span><span class=p>)</span> <span class=c1># BERT-Base의 토크나이저</span></span></span></code></pre></div></div><h2 id=position-embedding>Position Embedding
<a class=anchor href=#position-embedding>#</a></h2><ul><li>포지셔널 인코딩과 유사하지만, 위치 정보를 사인 함수와 코사인 함수가 아닌 학습을 통해서 얻는 방법</li><li>위치 정보를 위한 임베딩 층을 하나 더 사용하고, 입력마다 포지션 임베딩 벡터를 더해줌</li></ul><h2 id=mlm-pre-training>MLM (Pre-training)
<a class=anchor href=#mlm-pre-training>#</a></h2><ul><li>BERT는 사전 훈련을 위해서 입력 텍스트의 15%의 단어를 랜덤으로 마스킹</li><li>[MASK] 토큰이 파인 튜닝 단계에서 나타나지 않아 사전 학습 단계와 파인 튜닝 단계에서의 불일치 문제가 생기는데,<br>이를 완화하기 위해 마스킹 단어 중 80%는 [MASK]로 변경, 10%는 랜덤으로 단어가 변경, 10%는 동일하게 둠</li></ul><h2 id=nsp-pre-training>NSP (Pre-training)
<a class=anchor href=#nsp-pre-training>#</a></h2><ul><li>BERT는 두 개의 문장을 준 후에 이어지는 문장인지 아닌지를 맞추는 방식으로 훈련</li><li>BERT의 입력에서 [SEP]라는 특별 토큰을 사용해서 문장을 구분</li><li>두 문장이 실제 이어지는 문장인지 아닌지에 대한 이진 분류 문제를 [CLS] 토큰의 위치로 결정</li></ul><h2 id=segment-embedding>Segment Embedding
<a class=anchor href=#segment-embedding>#</a></h2><ul><li>WordPiece Embedding, Position Embedding과 함께 두 개의 문장을 구분하기 위한 목적으로 사용되는 임베딩 층</li><li>세그먼트 임베딩으로 구분되는 BERT의 입력에서 두 개의 문장은 두 종류의 텍스트, 두 개의 문서일 수 있음</li></ul><h2 id=find-tuning>Find-tuning
<a class=anchor href=#find-tuning>#</a></h2><ul><li>영화 리뷰 감성 분류, 로이터 뉴스 분류 등 <strong>Single Text Classification</strong>을 위해,<br>문서의 시작에 [CLS] 토큰을 입력해 분류에 대해 예측</li><li><strong>태깅 작업</strong>을 위해 각 토큰의 위치에 밀집층을 사용하여 분류에 대해 예측</li><li>자연어 추론 등의 <strong>Text Pair Classification</strong>을 위해,<br>텍스트 사이에 [SEP] 토큰을 집어넣고 두 종류의 세그먼트 임베딩을 모두 사용하여 문서를 구분</li><li>QA(Question Answering)를 풀기 위해 질문과 본문이라는 두 개의 텍스트의 쌍을 입력 (SQuAD v1.1)</li></ul><h2 id=attention-mask>Attention Mask
<a class=anchor href=#attention-mask>#</a></h2><ul><li>BERT가 어텐션 연산을 할 때, 불필요하게 패딩 토큰에 대해서 어텐션을 하지 않도록 실제 단어와 패딩 토큰을 구분하는 입력</li><li>숫자 1은 실제 단어로 마스킹을 하지 않고, 숫자 0은 패딩 토큰으로 마스킹을 함</li></ul><h1 id=17-03-pre-training-실습>17-03. Pre-training 실습
<a class=anchor href=#17-03-pre-training-%ec%8b%a4%ec%8a%b5>#</a></h1><ul><li>구글 BERT의 마스크드 언어 모델 실습 <a href=https://wikidocs.net/153992>참고</a></li><li>한국어 BERT의 마스크드 언어 모델 실습 <a href=https://wikidocs.net/152922>참고</a></li><li>구글 BERT의 다음 문장 예측 <a href=https://wikidocs.net/156767>참고</a></li><li>한국어 BERT의 다음 문장 예측 <a href=https://wikidocs.net/156774>참고</a></li></ul><h1 id=17-07-sentence-bertsbert>17-07. Sentence BERT(SBERT)
<a class=anchor href=#17-07-sentence-bertsbert>#</a></h1><ul><li>BERT의 문장 임베딩의 성능을 우수하게 개선시킨 모델</li><li>문장 쌍 분류 태스크 또는 문장 쌍 회귀 태스크로 파인 튜닝</li></ul><h2 id=sentence-embedding>Sentence Embedding
<a class=anchor href=#sentence-embedding>#</a></h2><ul><li>[CLS] 토큰은 입력된 문장에 대한 총체적 표현으로, [CLS] 토큰 자체를 입력 문장의 벡터로 간주</li><li>문장 벡터를 얻기 위해 [CLS] 토큰뿐 아니라, BERT의 모든 출력 벡터들을 평균냄</li><li>출력 벡터들의 평균을 pooling이라 하며, mean pooling, max pooling 등이 있음</li></ul><hr><h1 id=18-bert-실습>18. BERT 실습
<a class=anchor href=#18-bert-%ec%8b%a4%ec%8a%b5>#</a></h1><ul><li><a href=https://wikidocs.net/119990>Colab에서 TPU 사용하기</a></li><li><a href=https://wikidocs.net/158085>Transformers의 모델 클래스 불러오기</a></li><li><a href=https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/18.%20Fine-tuning%20BERT%20%28Cls%2C%20NER%2C%20NLI%29/18-3.%20kor_bert_nsmc_tpu.ipynb>KoBERT를 이용한 네이버 영화 리뷰 분류하기</a></li><li><a href=https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/18.%20Fine-tuning%20BERT%20%28Cls%2C%20NER%2C%20NLI%29/18-4.%20kor_bert_nsmc_model_from_transformers_gpu.ipynb>TFBertForSequenceClassification</a></li><li><a href=https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/18.%20Fine-tuning%20BERT%20%28Cls%2C%20NER%2C%20NLI%29/18-5.%20kor_bert_kornli_model_from_transformers_tpu.ipynb>KoBERT를 이용한 KorNLI 풀어보기 (다중 클래스 분류)</a></li><li><a href=https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/18.%20Fine-tuning%20BERT%20%28Cls%2C%20NER%2C%20NLI%29/18-6.%20kor_bert_ner_model_from_transformers_tpu.ipynb>KoBERT를 이용한 개체명 인식</a></li><li><a href=https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/18.%20Fine-tuning%20BERT%20%28Cls%2C%20NER%2C%20NLI%29/18-7.%20kor_bert_question_answering_tpu.ipynb>KoBERT를 이용한 기계 독해</a></li><li><a href=https://wikidocs.net/154530>BERT의 문장 임베딩(SBERT)을 이용한 한국어 챗봇</a></li><li><a href=https://wikidocs.net/162007>Faiss와 SBERT를 이용한 시맨틱 검색기</a></li></ul><hr><h1 id=19-topic-modelling>19. Topic Modelling
<a class=anchor href=#19-topic-modelling>#</a></h1><ul><li>토픽이라는 문서 집합의 추상적인 주제를 발견하기 위한 통계적 모델 중 하나</li><li>텍스트 본문의 숨겨진 의미 구조를 발견하기 위해 사용되는 텍스트 마이닝 기법</li></ul><h1 id=19-01-lsa>19-01. LSA
<a class=anchor href=#19-01-lsa>#</a></h1><h2 id=svd>SVD
<a class=anchor href=#svd>#</a></h2><ul><li>특이값 분해(Singular Value Decomposition)는 $A$가 ${m}\times{m}$ 행렬일 때,<br>다음과 같이 3개의 행렬의 곱으로 분해(decomposition)하는 것</li></ul><p>$$A={U}\Sigma{V^T}$$</p><ul><li>각 3개 행렬은 다음과 같은 조건을 만족<br>${U}\text{: }{m}\times{m}\text{ 직교행렬 }(AA^T=U(\Sigma\Sigma^T)U^T)$<br>$V\text{: }{n}\times{n}\text{ 직교행렬 }(A^TA=U(\Sigma^T\Sigma)V^T)$<br>$\Sigma\text{: }{m}\times{n}\text{ 직사각 대각행렬}$</li></ul><h2 id=truncated-svd>Truncated SVD
<a class=anchor href=#truncated-svd>#</a></h2><ul><li>Full SVD에서 나온 3개의 행렬에서 일부 벡터들을 삭제시킨 절단된 SVD</li><li>대각 행렬 $\Sigma$의 대각 원소의 값 중에서 상위값 t개만 남기고,<br>U행렬과 V행렬의 t열까지만 남김</li><li>일부 벡터들을 삭제해 데이터의 차원을 줄이는 것으로 계산 비용이 낮아지는 효과를 얻음</li><li>또한 상대적으로 중요하지 않은 정보(노이즈)를 삭제해 기존의 행렬에서 드러나지 않았던 심층적인 의미 확인</li></ul><h2 id=latent-semantic-analysislsa>Latent Semantic Analysis(LSA)
<a class=anchor href=#latent-semantic-analysislsa>#</a></h2><ul><li>BoW에 기반한 DTM이나 TF-IDF는 단어의 빈도 수를 이용한 수치화 방법으로 단어의 의미를 고려하지 못함</li><li>LSA는 DTM이나 TF-IDF 행렬에 절단된 SVD를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌어냄</li><li>문서의 유사도 계산 등에서 좋은 성능을 보여줌</li><li>SVD의 특성상 이미 계산된 LSA에 새로운 데이터를 추가하려면 처음부터 다시 계산해야해 업데이트가 어려움</li></ul><h1 id=19-02-lda>19-02. LDA
<a class=anchor href=#19-02-lda>#</a></h1><h2 id=latent-dirichlet-allocationlda>Latent Dirichlet Allocation(LDA)
<a class=anchor href=#latent-dirichlet-allocationlda>#</a></h2><ul><li>문서의 집합으로부터 어떤 토픽이 존재하는지를 알아내기 위한 알고리즘</li><li>문서들은 토픽들의 혼합으로 구성되어져 있으며, 토픽들은 확률 분포에 기반하여 단어들을 생성한다 가정</li><li>문서가 작성되었다는 가정 하에 토픽을 뽑아내기 위해 아래 과정을 역으로 추적하는 역공학을 수행</li></ul><h2 id=lda의-가정>LDA의 가정
<a class=anchor href=#lda%ec%9d%98-%ea%b0%80%ec%a0%95>#</a></h2><ol><li>문서에 사용할 단어의 개수 N을 정합</li><li>문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정</li><li>문서에 사용할 각 단어를 (아래와 같이) 정함<br>3-1. 토픽 분포에서 토픽 T를 확률적으로 고름<br>3-2. 선택한 토픽 T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어를 고름</li></ol><h2 id=lda-수행>LDA 수행
<a class=anchor href=#lda-%ec%88%98%ed%96%89>#</a></h2><ol><li>사용자는 알고리즘에게 토픽의 개수 k를 알려줌</li><li>모든 단어를 k개 중 하나의 토픽에 할당</li><li>모든 문서의 모든 단어에 대해서 아래의 사항을 반복 진행<br>3-1. 어떤 문서의 단어 w는 자신의 잘못된 토픽에 할당되어 있지만,<br>다른 단어들은 올바른 토픽에 할당되어 있다는 가정 하에 단어 w의 토픽을 재할당</li></ol><table><thead><tr><th style=text-align:center></th><th></th></tr></thead><tbody><tr><td style=text-align:center>LSA</td><td style=text-align:left>DTM을 차원 축소하여 축소 차원에서 근접 단어들을 토픽으로 묶음</td></tr><tr><td style=text-align:center>LDA</td><td style=text-align:left>단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽 추출</td></tr></tbody></table><h1 id=19-08-bertopic>19-08. BERTopic
<a class=anchor href=#19-08-bertopic>#</a></h1><ul><li>BERT embeddings과 클래스 기반 TF-IDF를 활용하여<br>주제 설명에서 중요한 단어를 유지하면서도 쉽게 해석할 수 있는 조밀한 클러스터를 만드는 토픽 모델링 기술</li></ul><ol><li>텍스트 데이터를 SBERT로 임베딩</li><li>문서를 군집화 (UMAP을 사용해 임베딩의 차원을 줄이고 HDBSCAN 기술을 사용해 클러스터링)</li><li>토픽 표현을 생성 (클래스 기반 TF-IDF 토픽 추출)</li></ol><hr><h1 id=20-text-summarization>20. Text Summarization
<a class=anchor href=#20-text-summarization>#</a></h1><h2 id=extractive-summarization>Extractive Summarization
<a class=anchor href=#extractive-summarization>#</a></h2><ul><li>원문에서 중요한 핵심 문장 또는 단어구 몇 개를 뽑아서 이들로 구성된 요약문을 만드는 방법</li><li>추출적 요약의 결과로 나온 요약문의 문장이나 단어구들은 전부 원문에 있는 문장들</li><li>대표적인 알고리즘으로 머신 러닝 알고리즘 TextRank가 있음</li></ul><h2 id=abstractive-summarization>Abstractive Summarization
<a class=anchor href=#abstractive-summarization>#</a></h2><ul><li>원문에 없던 문장이라도 핵심 문맥을 반영한 새로운 문장을 생성해서 원문을 요약하는 방법</li><li>주로 seq2seq 같은 인공 신경망을 이용하지만, 지도 학습이기 때문에 라벨 데이터가 있어야함</li></ul><h2 id=추상적-요약-구현><a href=https://wikidocs.net/72820>추상적 요약 구현</a>
<a class=anchor href=#%ec%b6%94%ec%83%81%ec%a0%81-%ec%9a%94%ec%95%bd-%ea%b5%ac%ed%98%84>#</a></h2><ul><li><a href=https://www.kaggle.com/snap/amazon-fine-food-reviews>아마존 리뷰 데이터</a> 사용</li></ul><h2 id=textrank>TextRank
<a class=anchor href=#textrank>#</a></h2><ul><li>페이지랭크를 기반으로 한 텍스트 요약 알고리즘으로,<br>텍스트랭크에서 그래프의 노드들은 문장들이며 각 간선의 가중치는 문장들 간의 유사도를 의미</li><li>사전 훈련된 GloVe 및 <a href=https://raw.githubusercontent.com/prateekjoshi565/textrank_text_summarization/master/tennis_articles_v4.csv>테니스 관련 기사 데이터</a> 사용</li></ul><hr><h1 id=21-question-answeringqa>21. Question Answering(QA)
<a class=anchor href=#21-question-answeringqa>#</a></h1><h2 id=babi-데이터셋>Babi 데이터셋
<a class=anchor href=#babi-%eb%8d%b0%ec%9d%b4%ed%84%b0%ec%85%8b>#</a></h2><ul><li>ID는 각 문장의 번호를 의미, 스토리가 시작될 때는 1번으로 시작</li><li>라벨의 supporting fact는 실제 정답이 주어진 스토리에서 몇 번 ID 문장에 있었는지를 알려줌</li></ul><h2 id=메모리-네트워크-구조>메모리 네트워크 구조
<a class=anchor href=#%eb%a9%94%eb%aa%a8%eb%a6%ac-%eb%84%a4%ed%8a%b8%ec%9b%8c%ed%81%ac-%ea%b5%ac%ec%a1%b0>#</a></h2><ul><li>두 개의 문장, 스토리 문장과 질문 문장이 입력으로 들어오며, 두 문장은 각각 임베딩 과정을 거침</li><li>Embedding C를 통해서 임베딩 된 스토리 문장과 Embedding B를 통해서 임베딩 된 질문 문장은<br>내적을 통해 각 단어 간 유사도를 구하고, 그 결과가 softmax 함수를 지나서<br>Embedding A로 임베딩이 된 스토리 문장에 더해짐<br>(Embedding A, B, C는 각각 별개의 임베딩 층)</li><li>Query(질문 문장)와 Key(스토리 문장)의 유사도를 구하고 softmax 함수를 통해 정규화해<br>Value(스토리 문장)에 더하는 어텐션 메커니즘과 유사</li><li>어텐션 메커니즘을 통해서 질문 문장과의 유사도를 반영한 스토리 문장 표현을 얻고,<br>이를 질문 표현과 연결(concatenate)하여 LSTM과 밀집층의 입력으로 사용</li></ul><h2 id=qa-태스크-풀기><a href=https://wikidocs.net/82475>QA 태스크 풀기</a>
<a class=anchor href=#qa-%ed%83%9c%ec%8a%a4%ed%81%ac-%ed%92%80%ea%b8%b0>#</a></h2><ul><li><a href=https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz>Babi 데이터셋</a> 사용</li></ul><h2 id=mean으로-한국어-qa><a href=https://wikidocs.net/85470>MeaN으로 한국어 QA</a>
<a class=anchor href=#mean%ec%9c%bc%eb%a1%9c-%ed%95%9c%ea%b5%ad%ec%96%b4-qa>#</a></h2><ul><li>한국어 Babi 데이터셋 사용 (<a href=%28https://bit.ly/31SqtHy%29>훈련 데이터</a>, <a href=https://bit.ly/3f7rH5g>테스트 데이터</a>)</li></ul><hr><h1 id=22-gpt>22. GPT
<a class=anchor href=#22-gpt>#</a></h1><h2 id=generative-pre-trained-transformergpt>Generative Pre-trained Transformer(GPT)
<a class=anchor href=#generative-pre-trained-transformergpt>#</a></h2><ul><li>GPT 설명 <a href=https://huggingface.co/blog/how-to-generate>참고</a></li></ul><h2 id=gpt-실습>GPT 실습
<a class=anchor href=#gpt-%ec%8b%a4%ec%8a%b5>#</a></h2><ul><li><a href=https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/22.%20Fine-tuning%20GPT-2%20%28Cls%2C%20Chatbot%2C%20NLI%29/22-2.%20kogpt2_text_generation_gpu.ipynb>KoGPT-2를 이용한 문장 생성</a></li><li><a href=https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/22.%20Fine-tuning%20GPT-2%20%28Cls%2C%20Chatbot%2C%20NLI%29/22-3.%20kogpt2_chatbot_gpu.ipynb>KoGPT-2 텍스트 생성을 이용한 한국어 챗봇</a></li><li><a href=https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/22.%20Fine-tuning%20GPT-2%20%28Cls%2C%20Chatbot%2C%20NLI%29/22-4.%20kogpt2_nsmc_tpu.ipynb>KoGPT-2를 이용한 네이버 영화 리뷰 분류</a></li><li><a href=https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/22.%20Fine-tuning%20GPT-2%20%28Cls%2C%20Chatbot%2C%20NLI%29/22-5.%20kogpt2_kornli_tpu.ipynb>KoGPT-2를 이용한 KorNLI 분류</a></li></ul></article><div class=book-mobile-nav><button class=book-nav-btn3 onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=book-nav-btn3 onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
<button class=book-nav-btn3 onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div><footer class=book-footer><div class="flex flex-wrap justify-between"><div></div><div></div></div><div class=post-tags><a href=/tags/til/ class=tag>#TIL</a>
<a href=/tags/nlp/ class=tag>#NLP</a></div><div class=post-navigation><a href=/blog/2022-07-04/ class="post-nav-link post-nav-prev"><span class=post-nav-direction><i class="fa-solid fa-backward"></i> PREV</span>
<span class=post-nav-title>2022-07-04 Log</span>
</a><a href=/blog/2022-06-29/ class="post-nav-link post-nav-next"><span class=post-nav-direction>NEXT <i class="fa-solid fa-forward"></i></span>
<span class=post-nav-title>2022-06-29 Log</span></a></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script><div class=book-comments><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://minyeamer.github.io/blog/2022-06-30/",this.page.identifier="https://minyeamer.github.io/blog/2022-06-30/"};(function(){var e=document,t=e.createElement("script");t.src="https://minyeamer.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})();function reloadDisqus(){window.DISQUS&&DISQUS.reset({reload:!0,config:function(){this.page.url="https://minyeamer.github.io/blog/2022-06-30/",this.page.identifier="https://minyeamer.github.io/blog/2022-06-30/"}})}</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#attention-function>Attention Function</a></li><li><a href=#dot-product-attention>Dot-Product Attention</a></li><li><a href=#1-attention-score>1. Attention Score</a></li><li><a href=#2-attention-distribution>2. Attention Distribution</a></li><li><a href=#3-attention-value>3. Attention Value</a></li></ul><ul><li><a href=#transformer-hyperparameter>Transformer Hyperparameter</a></li><li><a href=#positional-encoding>Positional Encoding</a></li><li><a href=#self-attention>Self-Attention</a></li><li><a href=#scaled-dot-product-attention>Scaled dot-product Attention</a></li><li><a href=#multi-head-attention>Multi-head Attention</a></li><li><a href=#padding-mask>Padding Mask</a></li><li><a href=#position-wise-ffnn><code>Position-wise</code> FFNN</a></li><li><a href=#residual-connection>Residual Connection</a></li><li><a href=#layer-normalization>Layer Normalization</a></li><li><a href=#look-ahead-mask>Look-ahead Mask</a></li><li><a href=#endocer-decoder-attention>Endocer-Decoder Attention</a></li></ul><ul><li><a href=#사전-훈련된-워드-임베딩>사전 훈련된 워드 임베딩</a></li><li><a href=#사전-훈련된-언어-모델>사전 훈련된 언어 모델</a></li><li><a href=#masked-language-model>Masked Language Model</a></li></ul><ul><li><a href=#contextual-embedding>Contextual Embedding</a></li><li><a href=#subword-tokenizer>Subword Tokenizer</a></li><li><a href=#position-embedding>Position Embedding</a></li><li><a href=#mlm-pre-training>MLM (Pre-training)</a></li><li><a href=#nsp-pre-training>NSP (Pre-training)</a></li><li><a href=#segment-embedding>Segment Embedding</a></li><li><a href=#find-tuning>Find-tuning</a></li><li><a href=#attention-mask>Attention Mask</a></li></ul><ul><li><a href=#sentence-embedding>Sentence Embedding</a></li></ul><ul><li><a href=#svd>SVD</a></li><li><a href=#truncated-svd>Truncated SVD</a></li><li><a href=#latent-semantic-analysislsa>Latent Semantic Analysis(LSA)</a></li></ul><ul><li><a href=#latent-dirichlet-allocationlda>Latent Dirichlet Allocation(LDA)</a></li><li><a href=#lda의-가정>LDA의 가정</a></li><li><a href=#lda-수행>LDA 수행</a></li></ul><ul><li><a href=#extractive-summarization>Extractive Summarization</a></li><li><a href=#abstractive-summarization>Abstractive Summarization</a></li><li><a href=#추상적-요약-구현>추상적 요약 구현</a></li><li><a href=#textrank>TextRank</a></li></ul><ul><li><a href=#babi-데이터셋>Babi 데이터셋</a></li><li><a href=#메모리-네트워크-구조>메모리 네트워크 구조</a></li><li><a href=#qa-태스크-풀기>QA 태스크 풀기</a></li><li><a href=#mean으로-한국어-qa>MeaN으로 한국어 QA</a></li></ul><ul><li><a href=#generative-pre-trained-transformergpt>Generative Pre-trained Transformer(GPT)</a></li><li><a href=#gpt-실습>GPT 실습</a></li></ul></nav><div class=book-nav><button class=book-nav-btn3 onclick='window.scrollTo({top:0,behavior:"smooth"})' title="Go to top">
<i class="fa fa-chevron-up"></i>
</button>
<button class=book-nav-btn3 onclick='window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})' title="Go to bottom">
<i class="fa fa-chevron-down"></i>
<button class=book-nav-btn3 onclick=history.back() title="Go back">
<i class="fa-solid fa-arrow-left"></i></button></div></div></aside></main></body></html>