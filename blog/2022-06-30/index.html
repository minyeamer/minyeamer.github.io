<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>2022-06-30 Log | Minystory</title><meta name=keywords content="TIL,NLP"><meta name=description content="딥 러닝을 이용한 자연어 처리 입문 3"><meta name=author content="minyeamer"><link rel=canonical href=https://minyeamer.github.io/blog/2022-06-30/><meta name=google-site-verification content="u1tWcHmHUZWfFT1cHaku6sqU-bK40N3WLR-C-4VUWN0"><meta name=naver-site-verification content="6eaf8e9da1a6104780f056f1a7797fe5a3a5a0da"><link crossorigin=anonymous href=/assets/css/stylesheet.78a14cf8249250820b49e9dc59e58b846a9beea6d16d50612c68b121ddf02146.css integrity="sha256-eKFM+CSSUIILSencWeWLhGqb7qbRbVBhLGixId3wIUY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://minyeamer.github.io/img/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://minyeamer.github.io/img/favicons/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://minyeamer.github.io/img/favicons/favicon-32x32.png><link rel=apple-touch-icon href=https://minyeamer.github.io/img/favicons/apple-touch-icon.png><link rel=mask-icon href=https://minyeamer.github.io/img/favicons/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V")</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BWECRMSX3V"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BWECRMSX3V",{anonymize_ip:!1})}</script><meta property="og:title" content="2022-06-30 Log"><meta property="og:description" content="딥 러닝을 이용한 자연어 처리 입문 3"><meta property="og:type" content="article"><meta property="og:url" content="https://minyeamer.github.io/blog/2022-06-30/"><meta property="og:image" content="https://minyeamer.github.io/calendar.jpg"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-06-30T20:00:00+09:00"><meta property="article:modified_time" content="2022-06-30T20:00:00+09:00"><meta property="og:site_name" content="Minystory"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://minyeamer.github.io/calendar.jpg"><meta name=twitter:title content="2022-06-30 Log"><meta name=twitter:description content="딥 러닝을 이용한 자연어 처리 입문 3"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://minyeamer.github.io/post/"},{"@type":"ListItem","position":2,"name":"2022-06-30 Log","item":"https://minyeamer.github.io/blog/2022-06-30/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"2022-06-30 Log","name":"2022-06-30 Log","description":"딥 러닝을 이용한 자연어 처리 입문 3","keywords":["TIL","NLP"],"articleBody":"15-01. Attention Mechanism seq2seq 모델은 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 해서 정보 손실이 발생하며,\nRNN의 고질적인 문제인 기울기 소실 문제도 존재 기계 번역 분야에서 입력 문장이 길면 번역 품질이 떨어지는 것을 보정하기 위해 어텐션 기법 활용 어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시접마다,\n인코더에서 전체 입력 문장을 다시 한 번 참고한다는 점 Attention Function Attention(Q, K, V) = Attention Value 어텐션 함수는 주어진 Query에 대해서 모든 Key와의 유사도를 각각 구하고,\n유사도를 키와 맵핑되어있는 각각의 Value에 반영, 이후 유사도가 반영된 Value를 모두 더해 리턴 Q(Query): t 시점의 디코더 셀에서의 은닉 상태\nK(Keys): 모든 시점의 인코더 셀의 은닉 상태들\nV(Values): 모든 시점의 인코더 셀의 은닉 상태들\nDot-Product Attention 어텐션 메커니즘에서 출력 단어를 예측하기 위해 디코더 셀은\nt-1의 hidden state, t-1에 나온 출력 단어, Attention Value $a_t$를 필요 제안자의 이름을 따서 루옹(Luong) 어텐션이라고도 함 1. Attention Score $a_t$를 구하기 위해서는 Attention Score를 구해야 함\n(인코더의 모든 은닉 상태 각각의 디코더의 현 시점의 은닉 상태 $s_t$와 얼마나 유사한지 판단하는 스코어) Dot-Product Attention에서는 스코어 값을 구하기 위해 $s_t$를 전치하고 각 은닉 상태와 내적을 수행 스코어 함수 $score(s_t,h_i)={s^T_t}{h_i}$ $s_t$와 인코더의 모든 은닉 상태의 어텐션 스코어 모음값 $e^t=[{s^T_t}{h_1},…,{s^T_t}{h_N}]$ 스코어 함수에 따라 scaled dot, general, concat, location-base 어텐션 등이 존재 2. Attention Distribution $e^t$에 softmax 함수를 적용하여 모든 값을 합하면 1이 되는 확률 분포,\nAttention Distribution을 얻으며, 분포 각각의 값을 Attention Weight라 함 어텐션 분포 $\\alpha^t=softmax(e^t)$ 3. Attention Value 어텐션의 최종 결과값을 얻기 위해서 각 인코더의 은닉 상태와 어텐션 가중치들을 곱하고,\n최종적으로 모두 더하는 Weighted Sum을 진행 어텐션 값이 구해지면 어텐션 메커니즘은 $a_t$를 $s_t$와 결합(concatenate)하여 하나의 벡터 $v_t$를 생성 $v_t\\text{를 }\\hat{y}$ 예측 연산의 입력으로 사용해 인코더로부터 얻은 정보를 활용하여 $\\hat{y}$를 예측 $$a_t=\\Sigma^N_{i=1}{\\alpha^t_i}{h_i}$$\n15-02. Bahdanau Attention 바다나우 어텐션 함수의 Query는 t 시점의 은닉 상태가 아니라 t-1 시점의 은닉 상태를 사용 $score(s_{t-1},h_i)={W^T_\\alpha}\\tanh{({W_b}{s_{t-1}}+{W_c}{h_i})}$ $W_a,W_b,W_c$는 학습 가능한 가중치 행렬을 의미하며, $s_{t-1}$와 $h_1,h_2,h_3,h_4$의 어텐션 스코어를\n각각 구하는 병렬 연산을 위해 $h_1,h_2,h_3,h_4$를 하나의 행렬 $H$로 변환 $e^t={W^T_\\alpha}\\tanh{({W_b}{s_{t-1}}+{W_c}H)}$ 컨텍스트 벡터를 구하면, 현재 시점의 입력인 단어의 임베딩 벡터와 연결(concatenate)하고,\n현재 시점의 새로운 입력으로 사용 16-01. Transformer 어텐션을 RNN의 보정을 위한 용도로 사용하지 않고, 어텐션만으로 인코더와 디코더를 생성 Transformer Hyperparameter $d_{model}=152$\n트랜스 포머의 인코더와 디코더에서의 정해진 입력과 출력의 크기, 임베딩 벡터의 차원\n$num_{layers}=6$\n트랜스포머 모델에서 인코더와 디코더가 총 몇 층으로 구성되었는지\n$num_{heads}=8$\n어텐션을 여러 개로 분할해서 병렬로 수행하고 결과값을 다시 하나로 합치는데, 이때 이 병렬의 개수\n$d_{ff}=2048$\n피드 포워드 신경망의 은닉층의 크기, 입력층과 출력층의 크기는 $d_{model}$\nPositional Encoding RNN은 단어의 위치에 따라 단어를 순차적으로 입력받아 처리하는 특성으로 인해 각 단어의 위치 정보를 가짐 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니기 때문에\n단어의 위치 정보를 얻기 위해 각 단어의 임베딩 벡터에 위치 정보들을 더해 모델의 입력으로 사용(포지셔널 인코딩) 트랜스포머는 위치 정보를 가진 값을 만들기 위해 아래 함수를 사용 $$PE_{(pos,2i)}=\\sin{(pos/10000^{2i/d_{model}})}$$ $$PE_{(pos,2i+1)}=\\cos{(pos/10000^{2i/d_{model}})}$$\n사인 함수와 코사인 함수의 값을 임베딩 벡터에 더해 단어에 순서 정보를 부여 $pos$는 입력 문장에서의 임베딩 벡터의 위치, $i$는 임데빙 벡터 내의 차원의 인덱스 의미 $d_{model}$은 트랜스포머의 모든 층의 출력 차원을 의미하는 트랜스포머의 하이퍼파라미터 Self-Attention 어텐션을 자기 자신에게 수행하는 것 Q, K, V는 모두 입력 문장의 모든 단어 벡터들을 의미 셀프 어텐션은 입력 문장 내의 단어들끼리 유사도를 구해 it이 어떤 단어와 연관되었는지 확률을 찾아냄 셀프 어텐션은 인코더의 초기 입력인 $d_{model}$의 차원을 가지는 단어 벡터들을 사용하여 수행하지 않고,\n각 단어 벡터들로부터 Q벡터, K벡터, V벡터를 얻는 작업을 거침 $d_{model}=512$의 차원을 가졌던 각 단어 벡터들은 Q벡터, K벡터, V벡터로 변환되면서\n$d_{model}$을 $num_{heads}$로 나눈 값 64를 차원으로 갖게 됨 Scaled dot-product Attention 트랜스포머에서는 스케일드 닷-프로적트 어텐션을 사용 벡터마다 일일히 컨텍스트 벡터를 구하는 벡터 연산을 하지 않고,\n문장 행렬에 가중치 행렬을 곱하여 구한 Q행렬, K행렬, V행렬에 행렬 연산을 수행 행렬 연산에서 어텐션 스코어는 행렬의 값에 전체적으로 $\\sqrt{d_k}$를 나누어 스코어 값을 가지는 행렬을 구함 $$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\nMulti-head Attention 한 번의 어텐션을 하는 것보다 여러번의 어텐션을 병렬로 사용하는 것이 더 효과적이기 때문에\n$d_{model}$의 차원을 $num_{heads}$개로 나누어 Q, K, V에 대해서 $num_{heads}$개의 병렬 어텐션을 수행 각각의 어텐션 값 행렬을 어텐션 헤드라고 불는데, 이때 가중치 행렬 $W^Q, W^K, W^V$의 값은 어텐션 헤드마다 전부 다름 멀티 헤드 어텐션은 어텐션을 병렬로 수행하여 다른 시각으로 정보들을 수집 벙렬 어텐션 수행 후 모든 어텐션 헤드를 연결(concatenate)하여 $(seq_{len}, d_{model})$ 크기의 행렬 생성 연결한 행렬에 가중치 행렬 $W^O$를 곱한 것이 멀티 헤드 어텐션의 최종 결과물이며, 인코더의 입력이었던 문장 행렬과 동일\n(인코더에서의 입력의 킉가 출력에서도 동일 크기로 계쏙 유지되어야만 다음 인코더에서도 다시 입력이 될 수 있음) Padding Mask 입력 문장에 토큰이 있을 경우 어텐션에서 제외하기 위해 -1e9라는 아주 작은 음수 값을 곱함 Masking: 어텐션에서 제외하기 위해 값을 가리는 것 어텐션 스코어 행렬에서 행에 해당하는 문장은 Query, 열에 해당하는 문장은 Key이며,\nKey에 가 있는 경우 해당 열 전체를 마스킹 Position-wise FFNN 포지션 와이즈 FFNN은 인코더와 디코더에서 공통적으로 가지고 있는 서브층으로, 완전 연결 FFNN을 의미 $FFNN(x)=MAX(0,x{W_1}+b_1){W_2}+{b_2}$ $x$는 멀티 헤드 어텐션의 결과로 나온 $(seq_{len}, d_{model})$ 크기의 행렬을 의미,\n가중치 행렬 $W_1\\text{은 }(d_{model},d_{ff})\\text{, }W_2\\text{은 }(d_{ff},d_{model})$의 크기를 가짐 서브층을 지난 인코더의 최종 출력은 여전히 인코더의 입력의 크기였던 $(seq_{len}, d_{model})$의 크기가 보존 Residual Connection 트랜스포머에서 두 개의 서브층을 가진 인코더에 추가적으로 사용하는 Add \u0026 Norm 기법 중 Add에 해당 잔차 연결은 서브층의 입력과 출력을 더하는 것으로, 컴퓨터 비전 분야에서 주로 사용되는 모델의 학습을 돕는 기법 $x+Sublayer(x)$로 표현할 수 있으며, 멀티 헤드 어텐션이라면 $H(x)=x+Multi\\text{-}head\\ Attention(x)$과 같음 Layer Normalization 잔차 연결의 입력을 $x$, 잔차 연결과 층 정규화 두 가지 연산을 모두 수행한 후의 결과 행렬을 $LN$이라 할 때,\n잔차 연결 후 정규화 연산을 수식으로 표현하면 $LN=LayerNorm(x+Sublayer(x))$와 같음 총 정규화는 텐서의 마지막 차원에 대해서 평균과 분산을 구하고, 이를 가지고 어떤 수식을 통해 값을 정규화하여 학습을 도움 총 정규화를 평균과 분산을 통한 정규화, 감마와 베타를 도입하는 것으로 나누었을 때,\n우선, 평균과 분산을 통해 벡터 $x_i$를 정규화 $x_i$는 벡터인 반면, 평균 $\\mu_i\\text{과 분산 }\\sigma^2_i$은 스칼라이기 때문에,\n벡터 $x_i$의 각 $k$차원의 값은 다음과 같이 정규화 ($\\epsilon$은 분모가 0이 되는 것을 방지하는 값) $$\\hat{x_{i,k}}=\\frac{x_{i,k}-\\mu_i}{\\sqrt{\\sigma^2_i+\\epsilon}}$$\nLook-ahead Mask 입력 단어를 매 시점마다 순차적으로 입력받는 RNN 계열의 신경망에 반해,\n트랜스포머는 문장 행렬을 한 번에 받기 때문에 미래 시점의 단어까지 참고할 수 있는 현상 발생 룩-어헤드 마스크는 디코더의 첫번째 서브층에서 이루어지며,\n자기 자신보다 미래에 있는 단어들은 참고하지 못하도록 마스킹함 룩-어헤드 마스크를 한다고해서 패딩 마스크가 불필요한 것이 아니므로 패딩 마스크를 포함하도록 구현 Endocer-Decoder Attention 디코더의 두번째 서브층은 멀티 헤드 어텐션을 수행한다는 점에서 이전의 어텐션들과 공통점이 있지만,\nQuery와 Key, Value가 달라 셀프 어텐션이 아님 인코더의 첫번째 서브층 Query = Key = Value\n디코더의 첫번째 서브층 Query = Key = Value\n디코더의 두번째 서브층 Query: 디코더 행렬 / Key = Value: 인코더 행렬\n16-02. Transformer Chatbot 트랜스포머를 이용한 한국어 챗봇 참고 챗본 데이터 사용 17-01. Pre-training 사전 훈련된 워드 임베딩 Word2Vec나 GloVe 등의 워드 임베딩은 하나의 단어가 하나의 벡터값으로 맵핑되므로,\n문맥을 고려하지 못하여 다의어나 동음이의어를 구분하지 못하는 문제 사전 훈련된 언어 모델 언어 모델은 주어진 텍스트로부터 다음 단어를 예측하도록 학습하여 별도의 라벨 없이 학습 가능 다의어를 구분할 수 없었던 문제점을 해결하고, RNN 계열의 신경망에서 탈피하기 위해 트랜스포머로 학습 시도 트랜스포머로 데이터를 학습시킨 언어 모델 GPT-1 등은 다양한 태스크에서 높은 성능을 얻음 이전 단어들로부터 다음 단어를 예측하는 언어 모델의 특성으로는 양방향 언어 모델을 사용할 수 없으므로,\nELMo에서는 두 개의 단방향 언어 모델을 따로 준비해 학습하는 방법을 사용했는데 여기서 발전된 마스크드 언어 모델이 등장 Masked Language Model 마스크드 언어 모델은 입력 텍스트의 단어 집합의 15%의 단어를 랜덤으로 Masking 빈칸 채우기 형식으로 마스킹된 단어들을 예측하게 함 17-02. BERT 구글이 공개한 사전 훈련된 모델로 수많은 NLP 태스크에서 최고 성능을 보임 트랜스포머를 이용하여 구현되었으며, 위키피디아와 BooksCorpus 같이 라벨이 없는 텍스트 데이터로 사전 훈련된 언어 모델 사전 훈련된 모델을 라벨이 있는 다른 작업에서 추가 훈련과 함께 하이퍼파라미터를 재조정,\n다른 작업에 대해서 파라미터 재조정을 위한 추가 훈련 과정을 Fine-tuning이라고 함 BERT-Base: L=12, D=768, A=12: 110M개의 파라미터 BERT-Large: L=24, D=1024, A=16: 340M개의 파라미터 Contextual Embedding BERT의 입력은 기존 모델들과 마찬가지로 임베딩 층을 지난 임베딩 벡터 BERT의 연산을 거친 후의 출력 임베딩은 문장의 문맥을 모두 참고한 문맥을 반영한 임베딩이 됨 BERT는 트랜스포머 인코더를 12번 쌓은 구조로, 셀프 어텐션을 통해 문맥을 반영 Subword Tokenizer BERT는 서브워드 토크나이저로 WordPiece 토크나이저를 사용 자주 등장하는 단어는 그대로 단어 집합에 추가하지만, 자주 등장하지 않는 단어는 서브워드로 분리해 집합에 추가하며,\n집합이 만들어지고 나면 단어 집합을 기반으로 토큰화 수행 BERT에서 토큰이 단어 집합에 존재할 경우 해당 토큰을 분리하지 않지만,\n존재하지 않으면 토큰을 서브워드로 분리하고, 첫번째 서브워드를 제외한 나머지 서브워드들은 앞에 #를 붙인 것을 토큰으로 함 #은 서브워드들이 단어의 중간부터 등장하는 것을 알려주기 위해 표시해둔 기호 1 2 3 from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") # BERT-Base의 토크나이저 Position Embedding 포지셔널 인코딩과 유사하지만, 위치 정보를 사인 함수와 코사인 함수가 아닌 학습을 통해서 얻는 방법 위치 정보를 위한 임베딩 층을 하나 더 사용하고, 입력마다 포지션 임베딩 벡터를 더해줌 MLM (Pre-training) BERT는 사전 훈련을 위해서 입력 텍스트의 15%의 단어를 랜덤으로 마스킹 [MASK] 토큰이 파인 튜닝 단계에서 나타나지 않아 사전 학습 단계와 파인 튜닝 단계에서의 불일치 문제가 생기는데,\n이를 완화하기 위해 마스킹 단어 중 80%는 [MASK]로 변경, 10%는 랜덤으로 단어가 변경, 10%는 동일하게 둠 NSP (Pre-training) BERT는 두 개의 문장을 준 후에 이어지는 문장인지 아닌지를 맞추는 방식으로 훈련 BERT의 입력에서 [SEP]라는 특별 토큰을 사용해서 문장을 구분 두 문장이 실제 이어지는 문장인지 아닌지에 대한 이진 분류 문제를 [CLS] 토큰의 위치로 결정 Segment Embedding WordPiece Embedding, Position Embedding과 함께 두 개의 문장을 구분하기 위한 목적으로 사용되는 임베딩 층 세그먼트 임베딩으로 구분되는 BERT의 입력에서 두 개의 문장은 두 종류의 텍스트, 두 개의 문서일 수 있음 Find-tuning 영화 리뷰 감성 분류, 로이터 뉴스 분류 등 Single Text Classification을 위해,\n문서의 시작에 [CLS] 토큰을 입력해 분류에 대해 예측 태깅 작업을 위해 각 토큰의 위치에 밀집층을 사용하여 분류에 대해 예측 자연어 추론 등의 Text Pair Classification을 위해,\n텍스트 사이에 [SEP] 토큰을 집어넣고 두 종류의 세그먼트 임베딩을 모두 사용하여 문서를 구분 QA(Question Answering)를 풀기 위해 질문과 본문이라는 두 개의 텍스트의 쌍을 입력 (SQuAD v1.1) Attention Mask BERT가 어텐션 연산을 할 때, 불필요하게 패딩 토큰에 대해서 어텐션을 하지 않도록 실제 단어와 패딩 토큰을 구분하는 입력 숫자 1은 실제 단어로 마스킹을 하지 않고, 숫자 0은 패딩 토큰으로 마스킹을 함 17-03. Pre-training 실습 구글 BERT의 마스크드 언어 모델 실습 참고 한국어 BERT의 마스크드 언어 모델 실습 참고 구글 BERT의 다음 문장 예측 참고 한국어 BERT의 다음 문장 예측 참고 17-07. Sentence BERT(SBERT) BERT의 문장 임베딩의 성능을 우수하게 개선시킨 모델 문장 쌍 분류 태스크 또는 문장 쌍 회귀 태스크로 파인 튜닝 Sentence Embedding [CLS] 토큰은 입력된 문장에 대한 총체적 표현으로, [CLS] 토큰 자체를 입력 문장의 벡터로 간주 문장 벡터를 얻기 위해 [CLS] 토큰뿐 아니라, BERT의 모든 출력 벡터들을 평균냄 출력 벡터들의 평균을 pooling이라 하며, mean pooling, max pooling 등이 있음 18. BERT 실습 Colab에서 TPU 사용하기 Transformers의 모델 클래스 불러오기 KoBERT를 이용한 네이버 영화 리뷰 분류하기 TFBertForSequenceClassification KoBERT를 이용한 KorNLI 풀어보기 (다중 클래스 분류) KoBERT를 이용한 개체명 인식 KoBERT를 이용한 기계 독해 BERT의 문장 임베딩(SBERT)을 이용한 한국어 챗봇 Faiss와 SBERT를 이용한 시맨틱 검색기 19. Topic Modelling 토픽이라는 문서 집합의 추상적인 주제를 발견하기 위한 통계적 모델 중 하나 텍스트 본문의 숨겨진 의미 구조를 발견하기 위해 사용되는 텍스트 마이닝 기법 19-01. LSA SVD 특이값 분해(Singular Value Decomposition)는 $A$가 ${m}\\times{m}$ 행렬일 때,\n다음과 같이 3개의 행렬의 곱으로 분해(decomposition)하는 것 $$A={U}\\Sigma{V^T}$$\n각 3개 행렬은 다음과 같은 조건을 만족\n${U}\\text{: }{m}\\times{m}\\text{ 직교행렬 }(AA^T=U(\\Sigma\\Sigma^T)U^T)$\n$V\\text{: }{n}\\times{n}\\text{ 직교행렬 }(A^TA=U(\\Sigma^T\\Sigma)V^T)$\n$\\Sigma\\text{: }{m}\\times{n}\\text{ 직사각 대각행렬}$ Truncated SVD Full SVD에서 나온 3개의 행렬에서 일부 벡터들을 삭제시킨 절단된 SVD 대각 행렬 $\\Sigma$의 대각 원소의 값 중에서 상위값 t개만 남기고,\nU행렬과 V행렬의 t열까지만 남김 일부 벡터들을 삭제해 데이터의 차원을 줄이는 것으로 계산 비용이 낮아지는 효과를 얻음 또한 상대적으로 중요하지 않은 정보(노이즈)를 삭제해 기존의 행렬에서 드러나지 않았던 심층적인 의미 확인 Latent Semantic Analysis(LSA) BoW에 기반한 DTM이나 TF-IDF는 단어의 빈도 수를 이용한 수치화 방법으로 단어의 의미를 고려하지 못함 LSA는 DTM이나 TF-IDF 행렬에 절단된 SVD를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌어냄 문서의 유사도 계산 등에서 좋은 성능을 보여줌 SVD의 특성상 이미 계산된 LSA에 새로운 데이터를 추가하려면 처음부터 다시 계산해야해 업데이트가 어려움 19-02. LDA Latent Dirichlet Allocation(LDA) 문서의 집합으로부터 어떤 토픽이 존재하는지를 알아내기 위한 알고리즘 문서들은 토픽들의 혼합으로 구성되어져 있으며, 토픽들은 확률 분포에 기반하여 단어들을 생성한다 가정 문서가 작성되었다는 가정 하에 토픽을 뽑아내기 위해 아래 과정을 역으로 추적하는 역공학을 수행 LDA의 가정 문서에 사용할 단어의 개수 N을 정합 문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정 문서에 사용할 각 단어를 (아래와 같이) 정함\n3-1. 토픽 분포에서 토픽 T를 확률적으로 고름\n3-2. 선택한 토픽 T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어를 고름 LDA 수행 사용자는 알고리즘에게 토픽의 개수 k를 알려줌 모든 단어를 k개 중 하나의 토픽에 할당 모든 문서의 모든 단어에 대해서 아래의 사항을 반복 진행\n3-1. 어떤 문서의 단어 w는 자신의 잘못된 토픽에 할당되어 있지만,\n다른 단어들은 올바른 토픽에 할당되어 있다는 가정 하에 단어 w의 토픽을 재할당 LSA DTM을 차원 축소하여 축소 차원에서 근접 단어들을 토픽으로 묶음 LDA 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽 추출 19-08. BERTopic BERT embeddings과 클래스 기반 TF-IDF를 활용하여\n주제 설명에서 중요한 단어를 유지하면서도 쉽게 해석할 수 있는 조밀한 클러스터를 만드는 토픽 모델링 기술 텍스트 데이터를 SBERT로 임베딩 문서를 군집화 (UMAP을 사용해 임베딩의 차원을 줄이고 HDBSCAN 기술을 사용해 클러스터링) 토픽 표현을 생성 (클래스 기반 TF-IDF 토픽 추출) 20. Text Summarization Extractive Summarization 원문에서 중요한 핵심 문장 또는 단어구 몇 개를 뽑아서 이들로 구성된 요약문을 만드는 방법 추출적 요약의 결과로 나온 요약문의 문장이나 단어구들은 전부 원문에 있는 문장들 대표적인 알고리즘으로 머신 러닝 알고리즘 TextRank가 있음 Abstractive Summarization 원문에 없던 문장이라도 핵심 문맥을 반영한 새로운 문장을 생성해서 원문을 요약하는 방법 주로 seq2seq 같은 인공 신경망을 이용하지만, 지도 학습이기 때문에 라벨 데이터가 있어야함 추상적 요약 구현 아마존 리뷰 데이터 사용 TextRank 페이지랭크를 기반으로 한 텍스트 요약 알고리즘으로,\n텍스트랭크에서 그래프의 노드들은 문장들이며 각 간선의 가중치는 문장들 간의 유사도를 의미 사전 훈련된 GloVe 및 테니스 관련 기사 데이터 사용 21. Question Answering(QA) Babi 데이터셋 ID는 각 문장의 번호를 의미, 스토리가 시작될 때는 1번으로 시작 라벨의 supporting fact는 실제 정답이 주어진 스토리에서 몇 번 ID 문장에 있었는지를 알려줌 메모리 네트워크 구조 두 개의 문장, 스토리 문장과 질문 문장이 입력으로 들어오며, 두 문장은 각각 임베딩 과정을 거침 Embedding C를 통해서 임베딩 된 스토리 문장과 Embedding B를 통해서 임베딩 된 질문 문장은\n내적을 통해 각 단어 간 유사도를 구하고, 그 결과가 softmax 함수를 지나서\nEmbedding A로 임베딩이 된 스토리 문장에 더해짐\n(Embedding A, B, C는 각각 별개의 임베딩 층) Query(질문 문장)와 Key(스토리 문장)의 유사도를 구하고 softmax 함수를 통해 정규화해\nValue(스토리 문장)에 더하는 어텐션 메커니즘과 유사 어텐션 메커니즘을 통해서 질문 문장과의 유사도를 반영한 스토리 문장 표현을 얻고,\n이를 질문 표현과 연결(concatenate)하여 LSTM과 밀집층의 입력으로 사용 QA 태스크 풀기 Babi 데이터셋 사용 MeaN으로 한국어 QA 한국어 Babi 데이터셋 사용 (훈련 데이터, 테스트 데이터) 22. GPT Generative Pre-trained Transformer(GPT) GPT 설명 참고 GPT 실습 KoGPT-2를 이용한 문장 생성 KoGPT-2 텍스트 생성을 이용한 한국어 챗봇 KoGPT-2를 이용한 네이버 영화 리뷰 분류 KoGPT-2를 이용한 KorNLI 분류 ","wordCount":"2319","inLanguage":"en","image":"https://minyeamer.github.io/calendar.jpg","datePublished":"2022-06-30T20:00:00+09:00","dateModified":"2022-06-30T20:00:00+09:00","author":{"@type":"Person","name":"minyeamer"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://minyeamer.github.io/blog/2022-06-30/"},"publisher":{"@type":"Organization","name":"Minystory","logo":{"@type":"ImageObject","url":"https://minyeamer.github.io/img/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://minyeamer.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://minyeamer.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://minyeamer.github.io/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://minyeamer.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://minyeamer.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://minyeamer.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://minyeamer.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://minyeamer.github.io/post/>Posts</a></div><h1 class=post-title>2022-06-30 Log</h1><div class=post-meta><span title='2022-06-30 20:00:00 +0900 KST'>June 30, 2022</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;2319 words&nbsp;·&nbsp;minyeamer&nbsp;|&nbsp;<a href=https://github.com/minyeamer/til/edit/main/2022/2022-06/2022-06-30.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><figure class=entry-cover><img loading=lazy src="https://github.com/minyeamer/til/blob/main/.media/covers/calendar.jpg?raw=true" alt></figure><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>&nbsp;Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#15-01-attention-mechanism>15-01. Attention Mechanism</a><ul><li><a href=#attention-function>Attention Function</a></li><li><a href=#dot-product-attention>Dot-Product Attention</a></li><li><a href=#1-attention-score>1. Attention Score</a></li><li><a href=#2-attention-distribution>2. Attention Distribution</a></li><li><a href=#3-attention-value>3. Attention Value</a></li></ul></li><li><a href=#15-02-bahdanau-attention>15-02. Bahdanau Attention</a></li><li><a href=#16-01-transformer>16-01. Transformer</a><ul><li><a href=#transformer-hyperparameter>Transformer Hyperparameter</a></li><li><a href=#positional-encoding>Positional Encoding</a></li><li><a href=#self-attention>Self-Attention</a></li><li><a href=#scaled-dot-product-attention>Scaled dot-product Attention</a></li><li><a href=#multi-head-attention>Multi-head Attention</a></li><li><a href=#padding-mask>Padding Mask</a></li><li><a href=#position-wise-ffnn><code>Position-wise</code> FFNN</a></li><li><a href=#residual-connection>Residual Connection</a></li><li><a href=#layer-normalization>Layer Normalization</a></li><li><a href=#look-ahead-mask>Look-ahead Mask</a></li><li><a href=#endocer-decoder-attention>Endocer-Decoder Attention</a></li></ul></li><li><a href=#16-02-transformer-chatbot>16-02. Transformer Chatbot</a></li><li><a href=#17-01-pre-training>17-01. Pre-training</a><ul><li><a href=#사전-훈련된-워드-임베딩>사전 훈련된 워드 임베딩</a></li><li><a href=#사전-훈련된-언어-모델>사전 훈련된 언어 모델</a></li><li><a href=#masked-language-model>Masked Language Model</a></li></ul></li><li><a href=#17-02-bert>17-02. BERT</a><ul><li><a href=#contextual-embedding>Contextual Embedding</a></li><li><a href=#subword-tokenizer>Subword Tokenizer</a></li><li><a href=#position-embedding>Position Embedding</a></li><li><a href=#mlm-pre-training>MLM (Pre-training)</a></li><li><a href=#nsp-pre-training>NSP (Pre-training)</a></li><li><a href=#segment-embedding>Segment Embedding</a></li><li><a href=#find-tuning>Find-tuning</a></li><li><a href=#attention-mask>Attention Mask</a></li></ul></li><li><a href=#17-03-pre-training-실습>17-03. Pre-training 실습</a></li><li><a href=#17-07-sentence-bertsbert>17-07. Sentence BERT(SBERT)</a><ul><li><a href=#sentence-embedding>Sentence Embedding</a></li></ul></li><li><a href=#18-bert-실습>18. BERT 실습</a></li><li><a href=#19-topic-modelling>19. Topic Modelling</a></li><li><a href=#19-01-lsa>19-01. LSA</a><ul><li><a href=#svd>SVD</a></li><li><a href=#truncated-svd>Truncated SVD</a></li><li><a href=#latent-semantic-analysislsa>Latent Semantic Analysis(LSA)</a></li></ul></li><li><a href=#19-02-lda>19-02. LDA</a><ul><li><a href=#latent-dirichlet-allocationlda>Latent Dirichlet Allocation(LDA)</a></li><li><a href=#lda의-가정>LDA의 가정</a></li><li><a href=#lda-수행>LDA 수행</a></li></ul></li><li><a href=#19-08-bertopic>19-08. BERTopic</a></li><li><a href=#20-text-summarization>20. Text Summarization</a><ul><li><a href=#extractive-summarization>Extractive Summarization</a></li><li><a href=#abstractive-summarization>Abstractive Summarization</a></li><li><a href=#추상적-요약-구현httpswikidocsnet72820><a href=https://wikidocs.net/72820>추상적 요약 구현</a></a></li><li><a href=#textrank>TextRank</a></li></ul></li><li><a href=#21-question-answeringqa>21. Question Answering(QA)</a><ul><li><a href=#babi-데이터셋>Babi 데이터셋</a></li><li><a href=#메모리-네트워크-구조>메모리 네트워크 구조</a></li><li><a href=#qa-태스크-풀기httpswikidocsnet82475><a href=https://wikidocs.net/82475>QA 태스크 풀기</a></a></li><li><a href=#mean으로-한국어-qahttpswikidocsnet85470><a href=https://wikidocs.net/85470>MeaN으로 한국어 QA</a></a></li></ul></li><li><a href=#22-gpt>22. GPT</a><ul><li><a href=#generative-pre-trained-transformergpt>Generative Pre-trained Transformer(GPT)</a></li><li><a href=#gpt-실습>GPT 실습</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h1 id=15-01-attention-mechanism>15-01. Attention Mechanism<a hidden class=anchor aria-hidden=true href=#15-01-attention-mechanism>#</a></h1><ul><li>seq2seq 모델은 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 해서 정보 손실이 발생하며,<br>RNN의 고질적인 문제인 기울기 소실 문제도 존재
기계 번역 분야에서 입력 문장이 길면 번역 품질이 떨어지는 것을 보정하기 위해 어텐션 기법 활용</li><li>어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시접마다,<br>인코더에서 전체 입력 문장을 다시 한 번 참고한다는 점</li></ul><h2 id=attention-function>Attention Function<a hidden class=anchor aria-hidden=true href=#attention-function>#</a></h2><ul><li>Attention(Q, K, V) = Attention Value</li><li>어텐션 함수는 주어진 Query에 대해서 모든 Key와의 유사도를 각각 구하고,<br>유사도를 키와 맵핑되어있는 각각의 Value에 반영, 이후 유사도가 반영된 Value를 모두 더해 리턴</li></ul><blockquote><p><strong>Q(Query)</strong>: t 시점의 디코더 셀에서의 은닉 상태<br><strong>K(Keys)</strong>: 모든 시점의 인코더 셀의 은닉 상태들<br><strong>V(Values)</strong>: 모든 시점의 인코더 셀의 은닉 상태들</p></blockquote><h2 id=dot-product-attention>Dot-Product Attention<a hidden class=anchor aria-hidden=true href=#dot-product-attention>#</a></h2><ul><li>어텐션 메커니즘에서 출력 단어를 예측하기 위해 디코더 셀은<br>t-1의 hidden state, t-1에 나온 출력 단어, Attention Value $a_t$를 필요</li><li>제안자의 이름을 따서 루옹(Luong) 어텐션이라고도 함</li></ul><h2 id=1-attention-score>1. Attention Score<a hidden class=anchor aria-hidden=true href=#1-attention-score>#</a></h2><ul><li>$a_t$를 구하기 위해서는 Attention Score를 구해야 함<br>(인코더의 모든 은닉 상태 각각의 디코더의 현 시점의 은닉 상태 $s_t$와 얼마나 유사한지 판단하는 스코어)</li><li>Dot-Product Attention에서는 스코어 값을 구하기 위해 $s_t$를 전치하고 각 은닉 상태와 내적을 수행</li><li>스코어 함수 $score(s_t,h_i)={s^T_t}{h_i}$</li><li>$s_t$와 인코더의 모든 은닉 상태의 어텐션 스코어 모음값 $e^t=[{s^T_t}{h_1},&mldr;,{s^T_t}{h_N}]$</li><li>스코어 함수에 따라 scaled dot, general, concat, location-base 어텐션 등이 존재</li></ul><h2 id=2-attention-distribution>2. Attention Distribution<a hidden class=anchor aria-hidden=true href=#2-attention-distribution>#</a></h2><ul><li>$e^t$에 softmax 함수를 적용하여 모든 값을 합하면 1이 되는 확률 분포,<br>Attention Distribution을 얻으며, 분포 각각의 값을 Attention Weight라 함</li><li>어텐션 분포 $\alpha^t=softmax(e^t)$</li></ul><h2 id=3-attention-value>3. Attention Value<a hidden class=anchor aria-hidden=true href=#3-attention-value>#</a></h2><ul><li>어텐션의 최종 결과값을 얻기 위해서 각 인코더의 은닉 상태와 어텐션 가중치들을 곱하고,<br>최종적으로 모두 더하는 Weighted Sum을 진행</li><li>어텐션 값이 구해지면 어텐션 메커니즘은 $a_t$를 $s_t$와 결합(concatenate)하여 하나의 벡터 $v_t$를 생성</li><li>$v_t\text{를 }\hat{y}$ 예측 연산의 입력으로 사용해 인코더로부터 얻은 정보를 활용하여 $\hat{y}$를 예측</li></ul><p>$$a_t=\Sigma^N_{i=1}{\alpha^t_i}{h_i}$$</p><h1 id=15-02-bahdanau-attention>15-02. Bahdanau Attention<a hidden class=anchor aria-hidden=true href=#15-02-bahdanau-attention>#</a></h1><ul><li>바다나우 어텐션 함수의 Query는 t 시점의 은닉 상태가 아니라 t-1 시점의 은닉 상태를 사용</li><li>$score(s_{t-1},h_i)={W^T_\alpha}\tanh{({W_b}{s_{t-1}}+{W_c}{h_i})}$</li><li>$W_a,W_b,W_c$는 학습 가능한 가중치 행렬을 의미하며, $s_{t-1}$와 $h_1,h_2,h_3,h_4$의 어텐션 스코어를<br>각각 구하는 병렬 연산을 위해 $h_1,h_2,h_3,h_4$를 하나의 행렬 $H$로 변환</li><li>$e^t={W^T_\alpha}\tanh{({W_b}{s_{t-1}}+{W_c}H)}$</li><li>컨텍스트 벡터를 구하면, 현재 시점의 입력인 단어의 임베딩 벡터와 연결(concatenate)하고,<br>현재 시점의 새로운 입력으로 사용</li></ul><hr><h1 id=16-01-transformer>16-01. Transformer<a hidden class=anchor aria-hidden=true href=#16-01-transformer>#</a></h1><ul><li>어텐션을 RNN의 보정을 위한 용도로 사용하지 않고, 어텐션만으로 인코더와 디코더를 생성</li></ul><h2 id=transformer-hyperparameter>Transformer Hyperparameter<a hidden class=anchor aria-hidden=true href=#transformer-hyperparameter>#</a></h2><p>$d_{model}=152$</p><blockquote><p>트랜스 포머의 인코더와 디코더에서의 정해진 입력과 출력의 크기, 임베딩 벡터의 차원</p></blockquote><p>$num_{layers}=6$</p><blockquote><p>트랜스포머 모델에서 인코더와 디코더가 총 몇 층으로 구성되었는지</p></blockquote><p>$num_{heads}=8$</p><blockquote><p>어텐션을 여러 개로 분할해서 병렬로 수행하고 결과값을 다시 하나로 합치는데, 이때 이 병렬의 개수</p></blockquote><p>$d_{ff}=2048$</p><blockquote><p>피드 포워드 신경망의 은닉층의 크기, 입력층과 출력층의 크기는 $d_{model}$</p></blockquote><h2 id=positional-encoding>Positional Encoding<a hidden class=anchor aria-hidden=true href=#positional-encoding>#</a></h2><ul><li>RNN은 단어의 위치에 따라 단어를 순차적으로 입력받아 처리하는 특성으로 인해 각 단어의 위치 정보를 가짐</li><li>트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니기 때문에<br>단어의 위치 정보를 얻기 위해 각 단어의 임베딩 벡터에 위치 정보들을 더해 모델의 입력으로 사용(포지셔널 인코딩)</li><li>트랜스포머는 위치 정보를 가진 값을 만들기 위해 아래 함수를 사용</li></ul><p>$$PE_{(pos,2i)}=\sin{(pos/10000^{2i/d_{model}})}$$
$$PE_{(pos,2i+1)}=\cos{(pos/10000^{2i/d_{model}})}$$</p><ul><li>사인 함수와 코사인 함수의 값을 임베딩 벡터에 더해 단어에 순서 정보를 부여</li><li>$pos$는 입력 문장에서의 임베딩 벡터의 위치, $i$는 임데빙 벡터 내의 차원의 인덱스 의미</li><li>$d_{model}$은 트랜스포머의 모든 층의 출력 차원을 의미하는 트랜스포머의 하이퍼파라미터</li></ul><h2 id=self-attention>Self-Attention<a hidden class=anchor aria-hidden=true href=#self-attention>#</a></h2><ul><li>어텐션을 자기 자신에게 수행하는 것</li><li>Q, K, V는 모두 입력 문장의 모든 단어 벡터들을 의미</li><li>셀프 어텐션은 입력 문장 내의 단어들끼리 유사도를 구해 it이 어떤 단어와 연관되었는지 확률을 찾아냄</li><li>셀프 어텐션은 인코더의 초기 입력인 $d_{model}$의 차원을 가지는 단어 벡터들을 사용하여 수행하지 않고,<br>각 단어 벡터들로부터 Q벡터, K벡터, V벡터를 얻는 작업을 거침</li><li>$d_{model}=512$의 차원을 가졌던 각 단어 벡터들은 Q벡터, K벡터, V벡터로 변환되면서<br>$d_{model}$을 $num_{heads}$로 나눈 값 64를 차원으로 갖게 됨</li></ul><h2 id=scaled-dot-product-attention>Scaled dot-product Attention<a hidden class=anchor aria-hidden=true href=#scaled-dot-product-attention>#</a></h2><ul><li>트랜스포머에서는 스케일드 닷-프로적트 어텐션을 사용</li><li>벡터마다 일일히 컨텍스트 벡터를 구하는 벡터 연산을 하지 않고,<br>문장 행렬에 가중치 행렬을 곱하여 구한 Q행렬, K행렬, V행렬에 행렬 연산을 수행</li><li>행렬 연산에서 어텐션 스코어는 행렬의 값에 전체적으로 $\sqrt{d_k}$를 나누어 스코어 값을 가지는 행렬을 구함</li></ul><p>$$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$$</p><h2 id=multi-head-attention>Multi-head Attention<a hidden class=anchor aria-hidden=true href=#multi-head-attention>#</a></h2><ul><li>한 번의 어텐션을 하는 것보다 여러번의 어텐션을 병렬로 사용하는 것이 더 효과적이기 때문에<br>$d_{model}$의 차원을 $num_{heads}$개로 나누어 Q, K, V에 대해서 $num_{heads}$개의 병렬 어텐션을 수행</li><li>각각의 어텐션 값 행렬을 어텐션 헤드라고 불는데, 이때 가중치 행렬 $W^Q, W^K, W^V$의 값은 어텐션 헤드마다 전부 다름</li><li>멀티 헤드 어텐션은 어텐션을 병렬로 수행하여 다른 시각으로 정보들을 수집</li><li>벙렬 어텐션 수행 후 모든 어텐션 헤드를 연결(concatenate)하여 $(seq_{len}, d_{model})$ 크기의 행렬 생성</li><li>연결한 행렬에 가중치 행렬 $W^O$를 곱한 것이 멀티 헤드 어텐션의 최종 결과물이며, 인코더의 입력이었던 문장 행렬과 동일<br>(인코더에서의 입력의 킉가 출력에서도 동일 크기로 계쏙 유지되어야만 다음 인코더에서도 다시 입력이 될 수 있음)</li></ul><h2 id=padding-mask>Padding Mask<a hidden class=anchor aria-hidden=true href=#padding-mask>#</a></h2><ul><li>입력 문장에 <pad>토큰이 있을 경우 어텐션에서 제외하기 위해 -1e9라는 아주 작은 음수 값을 곱함</li><li><strong>Masking</strong>: 어텐션에서 제외하기 위해 값을 가리는 것</li><li>어텐션 스코어 행렬에서 행에 해당하는 문장은 Query, 열에 해당하는 문장은 Key이며,<br>Key에 <pad>가 있는 경우 해당 열 전체를 마스킹</li></ul><h2 id=position-wise-ffnn><code>Position-wise</code> FFNN<a hidden class=anchor aria-hidden=true href=#position-wise-ffnn>#</a></h2><ul><li>포지션 와이즈 FFNN은 인코더와 디코더에서 공통적으로 가지고 있는 서브층으로, 완전 연결 FFNN을 의미</li><li>$FFNN(x)=MAX(0,x{W_1}+b_1){W_2}+{b_2}$</li><li>$x$는 멀티 헤드 어텐션의 결과로 나온 $(seq_{len}, d_{model})$ 크기의 행렬을 의미,<br>가중치 행렬 $W_1\text{은 }(d_{model},d_{ff})\text{, }W_2\text{은 }(d_{ff},d_{model})$의 크기를 가짐</li><li>서브층을 지난 인코더의 최종 출력은 여전히 인코더의 입력의 크기였던 $(seq_{len}, d_{model})$의 크기가 보존</li></ul><h2 id=residual-connection>Residual Connection<a hidden class=anchor aria-hidden=true href=#residual-connection>#</a></h2><ul><li>트랜스포머에서 두 개의 서브층을 가진 인코더에 추가적으로 사용하는 Add & Norm 기법 중 Add에 해당</li><li>잔차 연결은 서브층의 입력과 출력을 더하는 것으로, 컴퓨터 비전 분야에서 주로 사용되는 모델의 학습을 돕는 기법</li><li>$x+Sublayer(x)$로 표현할 수 있으며, 멀티 헤드 어텐션이라면 $H(x)=x+Multi\text{-}head\ Attention(x)$과 같음</li></ul><h2 id=layer-normalization>Layer Normalization<a hidden class=anchor aria-hidden=true href=#layer-normalization>#</a></h2><ul><li>잔차 연결의 입력을 $x$, 잔차 연결과 층 정규화 두 가지 연산을 모두 수행한 후의 결과 행렬을 $LN$이라 할 때,<br>잔차 연결 후 정규화 연산을 수식으로 표현하면 $LN=LayerNorm(x+Sublayer(x))$와 같음</li><li>총 정규화는 텐서의 마지막 차원에 대해서 평균과 분산을 구하고, 이를 가지고 어떤 수식을 통해 값을 정규화하여 학습을 도움</li><li>총 정규화를 평균과 분산을 통한 정규화, 감마와 베타를 도입하는 것으로 나누었을 때,<br>우선, 평균과 분산을 통해 벡터 $x_i$를 정규화</li><li>$x_i$는 벡터인 반면, 평균 $\mu_i\text{과 분산 }\sigma^2_i$은 스칼라이기 때문에,<br>벡터 $x_i$의 각 $k$차원의 값은 다음과 같이 정규화 ($\epsilon$은 분모가 0이 되는 것을 방지하는 값)</li></ul><p>$$\hat{x_{i,k}}=\frac{x_{i,k}-\mu_i}{\sqrt{\sigma^2_i+\epsilon}}$$</p><h2 id=look-ahead-mask>Look-ahead Mask<a hidden class=anchor aria-hidden=true href=#look-ahead-mask>#</a></h2><ul><li>입력 단어를 매 시점마다 순차적으로 입력받는 RNN 계열의 신경망에 반해,<br>트랜스포머는 문장 행렬을 한 번에 받기 때문에 미래 시점의 단어까지 참고할 수 있는 현상 발생</li><li>룩-어헤드 마스크는 디코더의 첫번째 서브층에서 이루어지며,<br>자기 자신보다 미래에 있는 단어들은 참고하지 못하도록 마스킹함</li><li>룩-어헤드 마스크를 한다고해서 패딩 마스크가 불필요한 것이 아니므로 패딩 마스크를 포함하도록 구현</li></ul><h2 id=endocer-decoder-attention>Endocer-Decoder Attention<a hidden class=anchor aria-hidden=true href=#endocer-decoder-attention>#</a></h2><ul><li>디코더의 두번째 서브층은 멀티 헤드 어텐션을 수행한다는 점에서 이전의 어텐션들과 공통점이 있지만,<br>Query와 Key, Value가 달라 셀프 어텐션이 아님</li></ul><blockquote><p>인코더의 첫번째 서브층 Query = Key = Value<br>디코더의 첫번째 서브층 Query = Key = Value<br>디코더의 두번째 서브층 Query: 디코더 행렬 / Key = Value: 인코더 행렬</p></blockquote><h1 id=16-02-transformer-chatbot>16-02. Transformer Chatbot<a hidden class=anchor aria-hidden=true href=#16-02-transformer-chatbot>#</a></h1><ul><li>트랜스포머를 이용한 한국어 챗봇 <a href=https://wikidocs.net/89786 target=_blank rel=noopener>참고</a></li><li><a href=https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv target=_blank rel=noopener>챗본 데이터</a> 사용</li></ul><hr><h1 id=17-01-pre-training>17-01. Pre-training<a hidden class=anchor aria-hidden=true href=#17-01-pre-training>#</a></h1><h2 id=사전-훈련된-워드-임베딩>사전 훈련된 워드 임베딩<a hidden class=anchor aria-hidden=true href=#사전-훈련된-워드-임베딩>#</a></h2><ul><li>Word2Vec나 GloVe 등의 워드 임베딩은 하나의 단어가 하나의 벡터값으로 맵핑되므로,<br>문맥을 고려하지 못하여 다의어나 동음이의어를 구분하지 못하는 문제</li></ul><h2 id=사전-훈련된-언어-모델>사전 훈련된 언어 모델<a hidden class=anchor aria-hidden=true href=#사전-훈련된-언어-모델>#</a></h2><ul><li>언어 모델은 주어진 텍스트로부터 다음 단어를 예측하도록 학습하여 별도의 라벨 없이 학습 가능</li><li>다의어를 구분할 수 없었던 문제점을 해결하고, RNN 계열의 신경망에서 탈피하기 위해 트랜스포머로 학습 시도</li><li>트랜스포머로 데이터를 학습시킨 언어 모델 GPT-1 등은 다양한 태스크에서 높은 성능을 얻음</li><li>이전 단어들로부터 다음 단어를 예측하는 언어 모델의 특성으로는 양방향 언어 모델을 사용할 수 없으므로,<br>ELMo에서는 두 개의 단방향 언어 모델을 따로 준비해 학습하는 방법을 사용했는데 여기서 발전된 마스크드 언어 모델이 등장</li></ul><h2 id=masked-language-model>Masked Language Model<a hidden class=anchor aria-hidden=true href=#masked-language-model>#</a></h2><ul><li>마스크드 언어 모델은 입력 텍스트의 단어 집합의 15%의 단어를 랜덤으로 Masking</li><li>빈칸 채우기 형식으로 마스킹된 단어들을 예측하게 함</li></ul><h1 id=17-02-bert>17-02. BERT<a hidden class=anchor aria-hidden=true href=#17-02-bert>#</a></h1><ul><li>구글이 공개한 사전 훈련된 모델로 수많은 NLP 태스크에서 최고 성능을 보임</li><li>트랜스포머를 이용하여 구현되었으며, 위키피디아와 BooksCorpus 같이 라벨이 없는 텍스트 데이터로 사전 훈련된 언어 모델</li><li>사전 훈련된 모델을 라벨이 있는 다른 작업에서 추가 훈련과 함께 하이퍼파라미터를 재조정,<br>다른 작업에 대해서 파라미터 재조정을 위한 추가 훈련 과정을 Fine-tuning이라고 함</li><li>BERT-Base: L=12, D=768, A=12: 110M개의 파라미터</li><li>BERT-Large: L=24, D=1024, A=16: 340M개의 파라미터</li></ul><h2 id=contextual-embedding>Contextual Embedding<a hidden class=anchor aria-hidden=true href=#contextual-embedding>#</a></h2><ul><li>BERT의 입력은 기존 모델들과 마찬가지로 임베딩 층을 지난 임베딩 벡터</li><li>BERT의 연산을 거친 후의 출력 임베딩은 문장의 문맥을 모두 참고한 문맥을 반영한 임베딩이 됨</li><li>BERT는 트랜스포머 인코더를 12번 쌓은 구조로, 셀프 어텐션을 통해 문맥을 반영</li></ul><h2 id=subword-tokenizer>Subword Tokenizer<a hidden class=anchor aria-hidden=true href=#subword-tokenizer>#</a></h2><ul><li>BERT는 서브워드 토크나이저로 WordPiece 토크나이저를 사용</li><li>자주 등장하는 단어는 그대로 단어 집합에 추가하지만, 자주 등장하지 않는 단어는 서브워드로 분리해 집합에 추가하며,<br>집합이 만들어지고 나면 단어 집합을 기반으로 토큰화 수행</li><li>BERT에서 토큰이 단어 집합에 존재할 경우 해당 토큰을 분리하지 않지만,<br>존재하지 않으면 토큰을 서브워드로 분리하고, 첫번째 서브워드를 제외한 나머지 서브워드들은 앞에 #를 붙인 것을 토큰으로 함</li><li>#은 서브워드들이 단어의 중간부터 등장하는 것을 알려주기 위해 표시해둔 기호</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>BertTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>BertTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;bert-base-uncased&#34;</span><span class=p>)</span> <span class=c1># BERT-Base의 토크나이저</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=position-embedding>Position Embedding<a hidden class=anchor aria-hidden=true href=#position-embedding>#</a></h2><ul><li>포지셔널 인코딩과 유사하지만, 위치 정보를 사인 함수와 코사인 함수가 아닌 학습을 통해서 얻는 방법</li><li>위치 정보를 위한 임베딩 층을 하나 더 사용하고, 입력마다 포지션 임베딩 벡터를 더해줌</li></ul><h2 id=mlm-pre-training>MLM (Pre-training)<a hidden class=anchor aria-hidden=true href=#mlm-pre-training>#</a></h2><ul><li>BERT는 사전 훈련을 위해서 입력 텍스트의 15%의 단어를 랜덤으로 마스킹</li><li>[MASK] 토큰이 파인 튜닝 단계에서 나타나지 않아 사전 학습 단계와 파인 튜닝 단계에서의 불일치 문제가 생기는데,<br>이를 완화하기 위해 마스킹 단어 중 80%는 [MASK]로 변경, 10%는 랜덤으로 단어가 변경, 10%는 동일하게 둠</li></ul><h2 id=nsp-pre-training>NSP (Pre-training)<a hidden class=anchor aria-hidden=true href=#nsp-pre-training>#</a></h2><ul><li>BERT는 두 개의 문장을 준 후에 이어지는 문장인지 아닌지를 맞추는 방식으로 훈련</li><li>BERT의 입력에서 [SEP]라는 특별 토큰을 사용해서 문장을 구분</li><li>두 문장이 실제 이어지는 문장인지 아닌지에 대한 이진 분류 문제를 [CLS] 토큰의 위치로 결정</li></ul><h2 id=segment-embedding>Segment Embedding<a hidden class=anchor aria-hidden=true href=#segment-embedding>#</a></h2><ul><li>WordPiece Embedding, Position Embedding과 함께 두 개의 문장을 구분하기 위한 목적으로 사용되는 임베딩 층</li><li>세그먼트 임베딩으로 구분되는 BERT의 입력에서 두 개의 문장은 두 종류의 텍스트, 두 개의 문서일 수 있음</li></ul><h2 id=find-tuning>Find-tuning<a hidden class=anchor aria-hidden=true href=#find-tuning>#</a></h2><ul><li>영화 리뷰 감성 분류, 로이터 뉴스 분류 등 <strong>Single Text Classification</strong>을 위해,<br>문서의 시작에 [CLS] 토큰을 입력해 분류에 대해 예측</li><li><strong>태깅 작업</strong>을 위해 각 토큰의 위치에 밀집층을 사용하여 분류에 대해 예측</li><li>자연어 추론 등의 <strong>Text Pair Classification</strong>을 위해,<br>텍스트 사이에 [SEP] 토큰을 집어넣고 두 종류의 세그먼트 임베딩을 모두 사용하여 문서를 구분</li><li>QA(Question Answering)를 풀기 위해 질문과 본문이라는 두 개의 텍스트의 쌍을 입력 (SQuAD v1.1)</li></ul><h2 id=attention-mask>Attention Mask<a hidden class=anchor aria-hidden=true href=#attention-mask>#</a></h2><ul><li>BERT가 어텐션 연산을 할 때, 불필요하게 패딩 토큰에 대해서 어텐션을 하지 않도록 실제 단어와 패딩 토큰을 구분하는 입력</li><li>숫자 1은 실제 단어로 마스킹을 하지 않고, 숫자 0은 패딩 토큰으로 마스킹을 함</li></ul><h1 id=17-03-pre-training-실습>17-03. Pre-training 실습<a hidden class=anchor aria-hidden=true href=#17-03-pre-training-실습>#</a></h1><ul><li>구글 BERT의 마스크드 언어 모델 실습 <a href=https://wikidocs.net/153992 target=_blank rel=noopener>참고</a></li><li>한국어 BERT의 마스크드 언어 모델 실습 <a href=https://wikidocs.net/152922 target=_blank rel=noopener>참고</a></li><li>구글 BERT의 다음 문장 예측 <a href=https://wikidocs.net/156767 target=_blank rel=noopener>참고</a></li><li>한국어 BERT의 다음 문장 예측 <a href=https://wikidocs.net/156774 target=_blank rel=noopener>참고</a></li></ul><h1 id=17-07-sentence-bertsbert>17-07. Sentence BERT(SBERT)<a hidden class=anchor aria-hidden=true href=#17-07-sentence-bertsbert>#</a></h1><ul><li>BERT의 문장 임베딩의 성능을 우수하게 개선시킨 모델</li><li>문장 쌍 분류 태스크 또는 문장 쌍 회귀 태스크로 파인 튜닝</li></ul><h2 id=sentence-embedding>Sentence Embedding<a hidden class=anchor aria-hidden=true href=#sentence-embedding>#</a></h2><ul><li>[CLS] 토큰은 입력된 문장에 대한 총체적 표현으로, [CLS] 토큰 자체를 입력 문장의 벡터로 간주</li><li>문장 벡터를 얻기 위해 [CLS] 토큰뿐 아니라, BERT의 모든 출력 벡터들을 평균냄</li><li>출력 벡터들의 평균을 pooling이라 하며, mean pooling, max pooling 등이 있음</li></ul><hr><h1 id=18-bert-실습>18. BERT 실습<a hidden class=anchor aria-hidden=true href=#18-bert-실습>#</a></h1><ul><li><a href=https://wikidocs.net/119990 target=_blank rel=noopener>Colab에서 TPU 사용하기</a></li><li><a href=https://wikidocs.net/158085 target=_blank rel=noopener>Transformers의 모델 클래스 불러오기</a></li><li><a href=https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/18.%20Fine-tuning%20BERT%20%28Cls%2C%20NER%2C%20NLI%29/18-3.%20kor_bert_nsmc_tpu.ipynb target=_blank rel=noopener>KoBERT를 이용한 네이버 영화 리뷰 분류하기</a></li><li><a href=https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/18.%20Fine-tuning%20BERT%20%28Cls%2C%20NER%2C%20NLI%29/18-4.%20kor_bert_nsmc_model_from_transformers_gpu.ipynb target=_blank rel=noopener>TFBertForSequenceClassification</a></li><li><a href=https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/18.%20Fine-tuning%20BERT%20%28Cls%2C%20NER%2C%20NLI%29/18-5.%20kor_bert_kornli_model_from_transformers_tpu.ipynb target=_blank rel=noopener>KoBERT를 이용한 KorNLI 풀어보기 (다중 클래스 분류)</a></li><li><a href=https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/18.%20Fine-tuning%20BERT%20%28Cls%2C%20NER%2C%20NLI%29/18-6.%20kor_bert_ner_model_from_transformers_tpu.ipynb target=_blank rel=noopener>KoBERT를 이용한 개체명 인식</a></li><li><a href=https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/18.%20Fine-tuning%20BERT%20%28Cls%2C%20NER%2C%20NLI%29/18-7.%20kor_bert_question_answering_tpu.ipynb target=_blank rel=noopener>KoBERT를 이용한 기계 독해</a></li><li><a href=https://wikidocs.net/154530 target=_blank rel=noopener>BERT의 문장 임베딩(SBERT)을 이용한 한국어 챗봇</a></li><li><a href=https://wikidocs.net/162007 target=_blank rel=noopener>Faiss와 SBERT를 이용한 시맨틱 검색기</a></li></ul><hr><h1 id=19-topic-modelling>19. Topic Modelling<a hidden class=anchor aria-hidden=true href=#19-topic-modelling>#</a></h1><ul><li>토픽이라는 문서 집합의 추상적인 주제를 발견하기 위한 통계적 모델 중 하나</li><li>텍스트 본문의 숨겨진 의미 구조를 발견하기 위해 사용되는 텍스트 마이닝 기법</li></ul><h1 id=19-01-lsa>19-01. LSA<a hidden class=anchor aria-hidden=true href=#19-01-lsa>#</a></h1><h2 id=svd>SVD<a hidden class=anchor aria-hidden=true href=#svd>#</a></h2><ul><li>특이값 분해(Singular Value Decomposition)는 $A$가 ${m}\times{m}$ 행렬일 때,<br>다음과 같이 3개의 행렬의 곱으로 분해(decomposition)하는 것</li></ul><p>$$A={U}\Sigma{V^T}$$</p><ul><li>각 3개 행렬은 다음과 같은 조건을 만족<br>${U}\text{: }{m}\times{m}\text{ 직교행렬 }(AA^T=U(\Sigma\Sigma^T)U^T)$<br>$V\text{: }{n}\times{n}\text{ 직교행렬 }(A^TA=U(\Sigma^T\Sigma)V^T)$<br>$\Sigma\text{: }{m}\times{n}\text{ 직사각 대각행렬}$</li></ul><h2 id=truncated-svd>Truncated SVD<a hidden class=anchor aria-hidden=true href=#truncated-svd>#</a></h2><ul><li>Full SVD에서 나온 3개의 행렬에서 일부 벡터들을 삭제시킨 절단된 SVD</li><li>대각 행렬 $\Sigma$의 대각 원소의 값 중에서 상위값 t개만 남기고,<br>U행렬과 V행렬의 t열까지만 남김</li><li>일부 벡터들을 삭제해 데이터의 차원을 줄이는 것으로 계산 비용이 낮아지는 효과를 얻음</li><li>또한 상대적으로 중요하지 않은 정보(노이즈)를 삭제해 기존의 행렬에서 드러나지 않았던 심층적인 의미 확인</li></ul><h2 id=latent-semantic-analysislsa>Latent Semantic Analysis(LSA)<a hidden class=anchor aria-hidden=true href=#latent-semantic-analysislsa>#</a></h2><ul><li>BoW에 기반한 DTM이나 TF-IDF는 단어의 빈도 수를 이용한 수치화 방법으로 단어의 의미를 고려하지 못함</li><li>LSA는 DTM이나 TF-IDF 행렬에 절단된 SVD를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌어냄</li><li>문서의 유사도 계산 등에서 좋은 성능을 보여줌</li><li>SVD의 특성상 이미 계산된 LSA에 새로운 데이터를 추가하려면 처음부터 다시 계산해야해 업데이트가 어려움</li></ul><h1 id=19-02-lda>19-02. LDA<a hidden class=anchor aria-hidden=true href=#19-02-lda>#</a></h1><h2 id=latent-dirichlet-allocationlda>Latent Dirichlet Allocation(LDA)<a hidden class=anchor aria-hidden=true href=#latent-dirichlet-allocationlda>#</a></h2><ul><li>문서의 집합으로부터 어떤 토픽이 존재하는지를 알아내기 위한 알고리즘</li><li>문서들은 토픽들의 혼합으로 구성되어져 있으며, 토픽들은 확률 분포에 기반하여 단어들을 생성한다 가정</li><li>문서가 작성되었다는 가정 하에 토픽을 뽑아내기 위해 아래 과정을 역으로 추적하는 역공학을 수행</li></ul><h2 id=lda의-가정>LDA의 가정<a hidden class=anchor aria-hidden=true href=#lda의-가정>#</a></h2><ol><li>문서에 사용할 단어의 개수 N을 정합</li><li>문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정</li><li>문서에 사용할 각 단어를 (아래와 같이) 정함<br>3-1. 토픽 분포에서 토픽 T를 확률적으로 고름<br>3-2. 선택한 토픽 T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어를 고름</li></ol><h2 id=lda-수행>LDA 수행<a hidden class=anchor aria-hidden=true href=#lda-수행>#</a></h2><ol><li>사용자는 알고리즘에게 토픽의 개수 k를 알려줌</li><li>모든 단어를 k개 중 하나의 토픽에 할당</li><li>모든 문서의 모든 단어에 대해서 아래의 사항을 반복 진행<br>3-1. 어떤 문서의 단어 w는 자신의 잘못된 토픽에 할당되어 있지만,<br>다른 단어들은 올바른 토픽에 할당되어 있다는 가정 하에 단어 w의 토픽을 재할당</li></ol><table><thead><tr><th style=text-align:center></th><th></th></tr></thead><tbody><tr><td style=text-align:center>LSA</td><td style=text-align:left>DTM을 차원 축소하여 축소 차원에서 근접 단어들을 토픽으로 묶음</td></tr><tr><td style=text-align:center>LDA</td><td style=text-align:left>단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽 추출</td></tr></tbody></table><h1 id=19-08-bertopic>19-08. BERTopic<a hidden class=anchor aria-hidden=true href=#19-08-bertopic>#</a></h1><ul><li>BERT embeddings과 클래스 기반 TF-IDF를 활용하여<br>주제 설명에서 중요한 단어를 유지하면서도 쉽게 해석할 수 있는 조밀한 클러스터를 만드는 토픽 모델링 기술</li></ul><ol><li>텍스트 데이터를 SBERT로 임베딩</li><li>문서를 군집화 (UMAP을 사용해 임베딩의 차원을 줄이고 HDBSCAN 기술을 사용해 클러스터링)</li><li>토픽 표현을 생성 (클래스 기반 TF-IDF 토픽 추출)</li></ol><hr><h1 id=20-text-summarization>20. Text Summarization<a hidden class=anchor aria-hidden=true href=#20-text-summarization>#</a></h1><h2 id=extractive-summarization>Extractive Summarization<a hidden class=anchor aria-hidden=true href=#extractive-summarization>#</a></h2><ul><li>원문에서 중요한 핵심 문장 또는 단어구 몇 개를 뽑아서 이들로 구성된 요약문을 만드는 방법</li><li>추출적 요약의 결과로 나온 요약문의 문장이나 단어구들은 전부 원문에 있는 문장들</li><li>대표적인 알고리즘으로 머신 러닝 알고리즘 TextRank가 있음</li></ul><h2 id=abstractive-summarization>Abstractive Summarization<a hidden class=anchor aria-hidden=true href=#abstractive-summarization>#</a></h2><ul><li>원문에 없던 문장이라도 핵심 문맥을 반영한 새로운 문장을 생성해서 원문을 요약하는 방법</li><li>주로 seq2seq 같은 인공 신경망을 이용하지만, 지도 학습이기 때문에 라벨 데이터가 있어야함</li></ul><h2 id=추상적-요약-구현httpswikidocsnet72820><a href=https://wikidocs.net/72820 target=_blank rel=noopener>추상적 요약 구현</a><a hidden class=anchor aria-hidden=true href=#추상적-요약-구현httpswikidocsnet72820>#</a></h2><ul><li><a href=https://www.kaggle.com/snap/amazon-fine-food-reviews target=_blank rel=noopener>아마존 리뷰 데이터</a> 사용</li></ul><h2 id=textrank>TextRank<a hidden class=anchor aria-hidden=true href=#textrank>#</a></h2><ul><li>페이지랭크를 기반으로 한 텍스트 요약 알고리즘으로,<br>텍스트랭크에서 그래프의 노드들은 문장들이며 각 간선의 가중치는 문장들 간의 유사도를 의미</li><li>사전 훈련된 GloVe 및 <a href=https://raw.githubusercontent.com/prateekjoshi565/textrank_text_summarization/master/tennis_articles_v4.csv target=_blank rel=noopener>테니스 관련 기사 데이터</a> 사용</li></ul><hr><h1 id=21-question-answeringqa>21. Question Answering(QA)<a hidden class=anchor aria-hidden=true href=#21-question-answeringqa>#</a></h1><h2 id=babi-데이터셋>Babi 데이터셋<a hidden class=anchor aria-hidden=true href=#babi-데이터셋>#</a></h2><ul><li>ID는 각 문장의 번호를 의미, 스토리가 시작될 때는 1번으로 시작</li><li>라벨의 supporting fact는 실제 정답이 주어진 스토리에서 몇 번 ID 문장에 있었는지를 알려줌</li></ul><h2 id=메모리-네트워크-구조>메모리 네트워크 구조<a hidden class=anchor aria-hidden=true href=#메모리-네트워크-구조>#</a></h2><ul><li>두 개의 문장, 스토리 문장과 질문 문장이 입력으로 들어오며, 두 문장은 각각 임베딩 과정을 거침</li><li>Embedding C를 통해서 임베딩 된 스토리 문장과 Embedding B를 통해서 임베딩 된 질문 문장은<br>내적을 통해 각 단어 간 유사도를 구하고, 그 결과가 softmax 함수를 지나서<br>Embedding A로 임베딩이 된 스토리 문장에 더해짐<br>(Embedding A, B, C는 각각 별개의 임베딩 층)</li><li>Query(질문 문장)와 Key(스토리 문장)의 유사도를 구하고 softmax 함수를 통해 정규화해<br>Value(스토리 문장)에 더하는 어텐션 메커니즘과 유사</li><li>어텐션 메커니즘을 통해서 질문 문장과의 유사도를 반영한 스토리 문장 표현을 얻고,<br>이를 질문 표현과 연결(concatenate)하여 LSTM과 밀집층의 입력으로 사용</li></ul><h2 id=qa-태스크-풀기httpswikidocsnet82475><a href=https://wikidocs.net/82475 target=_blank rel=noopener>QA 태스크 풀기</a><a hidden class=anchor aria-hidden=true href=#qa-태스크-풀기httpswikidocsnet82475>#</a></h2><ul><li><a href=https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz target=_blank rel=noopener>Babi 데이터셋</a> 사용</li></ul><h2 id=mean으로-한국어-qahttpswikidocsnet85470><a href=https://wikidocs.net/85470 target=_blank rel=noopener>MeaN으로 한국어 QA</a><a hidden class=anchor aria-hidden=true href=#mean으로-한국어-qahttpswikidocsnet85470>#</a></h2><ul><li>한국어 Babi 데이터셋 사용 (<a href=%28https://bit.ly/31SqtHy%29>훈련 데이터</a>, <a href=https://bit.ly/3f7rH5g target=_blank rel=noopener>테스트 데이터</a>)</li></ul><hr><h1 id=22-gpt>22. GPT<a hidden class=anchor aria-hidden=true href=#22-gpt>#</a></h1><h2 id=generative-pre-trained-transformergpt>Generative Pre-trained Transformer(GPT)<a hidden class=anchor aria-hidden=true href=#generative-pre-trained-transformergpt>#</a></h2><ul><li>GPT 설명 <a href=https://huggingface.co/blog/how-to-generate target=_blank rel=noopener>참고</a></li></ul><h2 id=gpt-실습>GPT 실습<a hidden class=anchor aria-hidden=true href=#gpt-실습>#</a></h2><ul><li><a href=https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/22.%20Fine-tuning%20GPT-2%20%28Cls%2C%20Chatbot%2C%20NLI%29/22-2.%20kogpt2_text_generation_gpu.ipynb target=_blank rel=noopener>KoGPT-2를 이용한 문장 생성</a></li><li><a href=https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/22.%20Fine-tuning%20GPT-2%20%28Cls%2C%20Chatbot%2C%20NLI%29/22-3.%20kogpt2_chatbot_gpu.ipynb target=_blank rel=noopener>KoGPT-2 텍스트 생성을 이용한 한국어 챗봇</a></li><li><a href=https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/22.%20Fine-tuning%20GPT-2%20%28Cls%2C%20Chatbot%2C%20NLI%29/22-4.%20kogpt2_nsmc_tpu.ipynb target=_blank rel=noopener>KoGPT-2를 이용한 네이버 영화 리뷰 분류</a></li><li><a href=https://github.com/ukairia777/tensorflow-nlp-tutorial/blob/main/22.%20Fine-tuning%20GPT-2%20%28Cls%2C%20Chatbot%2C%20NLI%29/22-5.%20kogpt2_kornli_tpu.ipynb target=_blank rel=noopener>KoGPT-2를 이용한 KorNLI 분류</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://minyeamer.github.io/tags/til/>TIL</a></li><li><a href=https://minyeamer.github.io/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=prev href=https://minyeamer.github.io/blog/2022-07-04/><span class=title>« Prev</span><br><span>2022-07-04 Log</span></a>
<a class=next href=https://minyeamer.github.io/blog/2022-06-29/><span class=title>Next »</span><br><span>2022-06-29 Log</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 2022-06-30 Log on twitter" href="https://twitter.com/intent/tweet/?text=2022-06-30%20Log&url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-30%2f&hashtags=TIL%2cNLP"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-06-30 Log on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-30%2f&title=2022-06-30%20Log&summary=2022-06-30%20Log&source=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-30%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-06-30 Log on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-30%2f&title=2022-06-30%20Log"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-06-30 Log on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-30%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-06-30 Log on whatsapp" href="https://api.whatsapp.com/send?text=2022-06-30%20Log%20-%20https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-30%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 2022-06-30 Log on telegram" href="https://telegram.me/share/url?text=2022-06-30%20Log&url=https%3a%2f%2fminyeamer.github.io%2fblog%2f2022-06-30%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=minyeamer/minyeamer.github.io issue-term=pathname label=comments theme=preferred-color-scheme crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://minyeamer.github.io/>Minystory</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>