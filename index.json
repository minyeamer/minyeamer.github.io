[{"content":"블로그를 구성할 때 기술적, 시간적 한계 때문에 이미 만들어진 테마를 사용하게 됩니다.\n제가 Hugo 블로그를 만들 때도 이러한 문제 때문에 PaperMod 테마를 사용했지만,\n블로그를 보다보면 만족스럽지 못한 부분이 발견됩니다.\n이번 포스트에서는 제가 PaperMod 테마를 커스터마이징한 과정을 안내해드리겠습니다.\nArchive, Search 추가하기 PaperMod 테마를 가져오면서 가장 신경쓰였던 부분은\n메인 메뉴가 Categories, Tags 두 개 뿐이었단 점입니다.\nArchive는 그렇다쳐도 Search 기능은 빼먹을 수 없는 부분이라 생각하기 때문에,\nHugo 및 PaperMod 내 이슈를 참고하여 관련된 내용을 탐색했습니다.\n다행히 PaperMod 테마에서 해당 기능을 연결하지 않았을 뿐,\n기능에 대한 레이아웃은 존재하기 때문에 content/ 디렉토리 아래 다음과 같은 파일을 추가했습니다.\n1 2 3 4 5 6 7 # content/archive.md --- title: \u0026#34;Archive\u0026#34; layout: \u0026#34;archives\u0026#34; url: \u0026#34;/archive\u0026#34; summary: \u0026#34;archive\u0026#34; --- 1 2 3 4 5 6 7 # content/search.md --- title: \u0026#34;Search\u0026#34; layout: \u0026#34;search\u0026#34; url: \u0026#34;/search\u0026#34; summary: \u0026#34;search\u0026#34; --- 추가로, 설정에서도 해당 파일을 인식해야되기 때문에 다음과 같은 설정을 추가했습니다.\npost/ 외에 다른 디렉토리를 등록하고 싶은 경우에도 해당 키값을 활용할 수 있습니다.\n1 2 params: mainsections: [\u0026#34;page\u0026#34;, \u0026#34;post\u0026#34;, \u0026#34;archive\u0026#34;, \u0026#34;search\u0026#34;] 마지막으로, 메인 메뉴에서 해당 링크로 이동하기 위한 바로가기를 추가했습니다.\n여기에는 카테고리, 태그 등이 있을건데 weight 값을 통해 적절하게 위치를 조정할 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 menu: main: - identifier: archive name: Archive url: /archive/ weight: 10 - identifier: search name: Search url: /search/ weight: 20 위와 같은 과정을 통해 Archive, Search 기능을 추가했습니다.\n검색 엔진 등록하기 검색 엔진에 등록하기 위한 과정은 해당 영상을 참고해주시기 바랍니다.\n저는 위 과정에서 블로그 내에 추가해야 할 Site Verification Tag를 추가하는 법을 전달드리겠습니다.\nPaperMod 테마에서는 아래처럼 해당 부분이 만들어져 있기 때문에 크게 걱정할 필요는 없습니다.\n아래는 layouts/partials/ 내에 head.html 파일에서 가져왔습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 {{- if site.Params.analytics.google.SiteVerificationTag }} \u0026lt;meta name=\u0026#34;google-site-verification\u0026#34; content=\u0026#34;{{ site.Params.analytics.google.SiteVerificationTag }}\u0026#34;\u0026gt; {{- end }} {{- if site.Params.analytics.yandex.SiteVerificationTag }} \u0026lt;meta name=\u0026#34;yandex-verification\u0026#34; content=\u0026#34;{{ site.Params.analytics.yandex.SiteVerificationTag }}\u0026#34;\u0026gt; {{- end }} {{- if site.Params.analytics.bing.SiteVerificationTag }} \u0026lt;meta name=\u0026#34;msvalidate.01\u0026#34; content=\u0026#34;{{ site.Params.analytics.bing.SiteVerificationTag }}\u0026#34;\u0026gt; {{- end }} {{- if site.Params.analytics.naver.SiteVerificationTag }} \u0026lt;meta name=\u0026#34;naver-site-verification\u0026#34; content=\u0026#34;{{ site.Params.analytics.naver.SiteVerificationTag }}\u0026#34;\u0026gt; {{- end }} 구글, 네이버 외에 Bing, Yandex를 지원하며 저는 다음과 같이 구글과 네이버만 설정했습니다.\n1 2 3 4 5 6 params: analytics: google: SiteVerificationTag: \u0026lt;YOUR-VERIFICATION-TAG\u0026gt; naver: SiteVerificationTag: \u0026lt;YOUR-VERIFICATION-TAG\u0026gt; 번외로 Google Tag 등 head에 추가로 입력할 부분이 있다면,\n동일한 위치에 extend_head.html을 사용할 수 있습니다.\n아래는 제가 extend_head.html 내에 Google Tag를 위한 스크립트를 추가한 부분입니다.\n1 2 3 4 5 6 7 8 9 10 {{- if site.GoogleAnalytics }} {{- /* Google tag (gtag.js) */}} \u0026lt;script async src=\u0026#34;https://www.googletagmanager.com/gtag/js?id={{ site.GoogleAnalytics }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\u0026#39;js\u0026#39;, new Date()); gtag(\u0026#39;config\u0026#39;, \u0026#39;{{ site.GoogleAnalytics }}\u0026#39;); \u0026lt;/script\u0026gt; {{- end }} KaTex 추가하기 KaTex는 웹에서 수식을 표현하기 위한 방식입니다.\n제 과거 게시글엔 KaTex 표기법을 사용한 것이 존재하는데 이것이 제대로 표시되지 않는 문제를 발견했습니다.\n저는 공식 문서 대신 Stack Overflow 등을 참고해 아래 코드를 extend_head.html에 추가했는데,\n아쉽게도 출처는 남겨두지 못했습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \u0026lt;script\u0026gt; MathJax = { tex: { inlineMath: [[\u0026#39;$\u0026#39;, \u0026#39;$\u0026#39;], [\u0026#39;\\\\(\u0026#39;, \u0026#39;\\\\)\u0026#39;]], displayMath: [[\u0026#39;$$\u0026#39;,\u0026#39;$$\u0026#39;], [\u0026#39;\\\\[\u0026#39;, \u0026#39;\\\\]\u0026#39;]], processEscapes: true, processEnvironments: true }, options: { skipHtmlTags: [\u0026#39;script\u0026#39;, \u0026#39;noscript\u0026#39;, \u0026#39;style\u0026#39;, \u0026#39;textarea\u0026#39;, \u0026#39;pre\u0026#39;] } }; window.addEventListener(\u0026#39;load\u0026#39;, (event) =\u0026gt; { document.querySelectorAll(\u0026#34;mjx-container\u0026#34;).forEach(function(x){ x.parentElement.classList += \u0026#39;has-jax\u0026#39;}) }); \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://polyfill.io/v3/polyfill.min.js?features=es6\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; id=\u0026#34;MathJax-script\u0026#34; async src=\u0026#34;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; Cover 간편하게 지정하기 저는 Github 저장소 내에 업로드한 이미지 주소를 속성값에 연결해 블로그 이미지를 표시하는데,\n게시글을 작성할 때마다 지정하게 되는 Cover 이미지의 경우 매번 전체 링크를 지정하는게 불편했습니다.\n대표적으로 해당 게시글의 Cover 이미지 주소는 다음과 같습니다.\n1 https://github.com/minyeamer/til/blob/main/.media/covers/hugo-logo.png?raw=true 저는 여기서 hugo-logo.png를 제외한 앞뒤의 요소가 불필요하다는 것을 인식했고\n설정 파일에 다음과 같이 prefix, suffix라는 키값으로 지정하게 처리했습니다.\n1 2 3 4 params: cover: prefix: \u0026#34;https://github.com/minyeamer/til/blob/main/.media/covers/\u0026#34; suffix: \u0026#34;?raw=true\u0026#34; 그리고 해당 설정을 적용시키기 위해 실질적으로 Cover 이미지를 표시하는 layouts/partials/ 아래 cover.html 파일을 수정했습니다.\n주석으로 지정된 부분이 원본이며, image 키값의 앞뒤로 prefix와 suffix를 덧붙였습니다.\n1 2 3 4 5 6 7 \u0026lt;!-- {{- if $addLink }}\u0026lt;a href=\u0026#34;{{ (.Params.cover.image) | absURL }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34;\u0026gt;{{ end -}} \u0026lt;img loading=\u0026#34;lazy\u0026#34; src=\u0026#34;{{ (.Params.cover.image) | absURL }}\u0026#34; alt=\u0026#34;{{ $alt }}\u0026#34;\u0026gt; --\u0026gt; {{- if $addLink }}\u0026lt;a href=\u0026#34;{{ if site.Params.cover.prefix }}{{ site.Params.cover.prefix }}{{ end }}{{ .Params.cover.image }}{{ if site.Params.cover.suffix }}{{ site.Params.cover.suffix }}{{ end }}\u0026#34; target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener noreferrer\u0026#34;\u0026gt;{{ end -}} \u0026lt;img loading=\u0026#34;lazy\u0026#34; src=\u0026#34;{{ if site.Params.cover.prefix }}{{ site.Params.cover.prefix }}{{ end }}{{ .Params.cover.image }}{{ if site.Params.cover.suffix }}{{ site.Params.cover.suffix }}{{ end }}\u0026#34; alt=\u0026#34;{{ $alt }}\u0026#34;\u0026gt; 기타 설정 너비 설정 초기에 PaperMod 테마를 사용할 때 너비가 좁아 불편한 느낌이 있었습니다.\n해당 설정은 css 파일로 지정할 것이라 생각했고,\nassets/css/core/ 경로에 있는 theme-vars.css 파일을 발견해 다음과 같이 수정했습니다.\n기존 720px에서 900px로 늘어나 쾌적하게 블로그를 볼 수 있게 되었습니다.\n1 2 :root { --main-width: 900px; 새 탭에서 링크 열기 다음으로 관심을 가진 건 깃허브에서 매번 불편하게 생각했던 링크 오픈 방식인데,\n개인적으로는 현재 탭이 아닌 새 탭에서 열리는 방식을 선호하기 때문에 해당 부분의 수정이 필요했습니다.\n다행히 Hugo 이슈 내용 중 다음과 같은 답변을 참고해 파일을 추가했습니다.\n아래는 layouts/_default/_markup/ 경로에 추가한 render-link.html 파일입니다.\n1 \u0026lt;a href=\u0026#34;{{ .Destination | safeURL }}\u0026#34;{{ with .Title}} title=\u0026#34;{{ . }}\u0026#34;{{ end }}{{ if strings.HasPrefix .Destination \u0026#34;http\u0026#34; }} target=\u0026#34;_blank\u0026#34; rel=\u0026#34;noopener\u0026#34;{{ end }}\u0026gt;{{ .Text | safeHTML }}\u0026lt;/a\u0026gt; 포스트 수정 마지막으로 포스트 수정 버튼에 문제를 인식했습니다.\n물론, 모든 포스트는 로컬에서 작성하고 수정하지만, 오류가 발생하는 버튼을 그냥 놔둘 수는 없습니다.\nGo에 대해 잘 알지 못해 최선의 기능이라고 생각하지는 않지만,\n검색을 통해 발견한 replace 함수를 사용해 기존 경로에서 오류를 일으키는 부분을 제거했습니다.\n1 2 3 4 5 6 7 8 {{- if or .Params.editPost.URL site.Params.editPost.URL -}} {{- $fileUrlPath := path.Join .File.Path }} {{- if or .Params.author site.Params.author (.Param \u0026#34;ShowReadingTime\u0026#34;) (not .Date.IsZero) .IsTranslated }}\u0026amp;nbsp;|\u0026amp;nbsp;{{- end -}} \u0026lt;a href=\u0026#39;{{ .Params.editPost.URL | default site.Params.editPost.URL }}{{ if .Params.editPost.appendFilePath | default ( site.Params.editPost.appendFilePath | default false ) }}/{{ replace $fileUrlPath site.Params.editPost.ignoreFilePath \u0026#34;\u0026#34; 1 }}{{ end }}\u0026#39; rel=\u0026#34;noopener noreferrer\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt; {{- .Params.editPost.Text | default (site.Params.editPost.Text | default (i18n \u0026#34;edit_post\u0026#34; | default \u0026#34;Edit\u0026#34;)) -}} \u0026lt;/a\u0026gt; {{- end }} 개선사항 현재 PaperMod 테마의 카테고리는 아래 그림처럼 태그와 동일한 리스트 템플릿을 사용하는데,\n개인적으로는 트리 형태의 계층식 카테고리를 선호합니다.\n언제나처럼 PaperMod 이슈를 탐색하던 중 해당 이슈를 발견했는데,\n아래 그림처럼 제가 머릿속에 그리던 방식을 그대로 표현하여 큰 관심을 가졌습니다.\n해당 기능을 구현한 분께 메일을 보내 참고 자료를 얻었지만,\n아직까진 시간적 여유가 부족해 해당 작업을 처리하지 못한 상태입니다.\n향후 개선되기를 희망하는 부분입니다.\n마치며 Hugo 블로그 만들기 시리즈의 마지막으로 커스터마이징 과정을 소개했습니다.\n커스터마이징은 그때그때 필요하다고 생각하는 부분을 수정하는 것이기 때문에\n본인의 입맛에 맛는 블로그를 만들기 위해서는 테마의 구조를 이해해야 합니다.\n아직 Go에 대해서도 잘 몰라 검색을 통해 요령껏 찾아내는 수준이지만,\nGo에 익숙해지게 된다면 동적 TOC 등 기능의 개선을 기대해 볼 수 있을 것입니다.\n해당 게시글을 통해 Hugo 블로그 만들기에 도움이 되었으면 좋겠습니다.\n참고 자료 EP09. 구글, 네이버 검색엔진 등록하기 KaTex Simple way to open in a new tab [Feature][Discussion] Tree-style category list page ","permalink":"https://minyeamer.github.io/blog/hugo-blog-3/","summary":"블로그를 구성할 때 기술적, 시간적 한계 때문에 이미 만들어진 테마를 사용하게 됩니다.\n제가 Hugo 블로그를 만들 때도 이러한 문제 때문에 PaperMod 테마를 사용했지만,\n블로그를 보다보면 만족스럽지 못한 부분이 발견됩니다.\n이번 포스트에서는 제가 PaperMod 테마를 커스터마이징한 과정을 안내해드리겠습니다.\nArchive, Search 추가하기 PaperMod 테마를 가져오면서 가장 신경쓰였던 부분은\n메인 메뉴가 Categories, Tags 두 개 뿐이었단 점입니다.\nArchive는 그렇다쳐도 Search 기능은 빼먹을 수 없는 부분이라 생각하기 때문에,\nHugo 및 PaperMod 내 이슈를 참고하여 관련된 내용을 탐색했습니다.","title":"Hugo 블로그 만들기 (3) - 테마 커스터마이징"},{"content":"Hugo 블로그는 기본적으로 댓글 기능을 제공하지는 않습니다.\n제가 사용하는 PaperMod 테마에서는 서드파티 서비스인 Disqus를 위한 레이아웃이 존재하지만,\n저는 기본적인 블로그 운영을 Github 플랫폼 내에서 구성하고 싶기 때문에 다른 기능을 사용해보려 합니다.\n이번 포스트에서는 Utterances 댓글 기능을 추가하는 방법을 안내해드리겠습니다.\nUtterances 설치하기 Utterances는 Github issues 기반으로 댓글을 관리하는 기능입니다.\n무료 플랜에서 광고가 붙는 Disqus와 다르게 별도의 유료 플랜이 없어 간편하게 사용할 수 있습니다.\nUtterances 설치는 단순히 레이아웃 상에서 댓글이 위치할 곳에 자바스크립트 코드를 삽입하면 됩니다.\n하지만, 선행적으로 해당 링크를 통해 Utterances와 연동시킬 저장소를 등록해야 합니다.\n무료 플랜 선택 후 Utterances를 적용할 저장소를 선택하게 되는데\n모든 저장소를 지정해도 되지만, 저는 댓글을 관리할 저장소만 지정하겠습니다.\n간단하게 Utterances 적용이 완료되면 아래 공식 문서 페이지로 이동합니다.\nhttps://utteranc.es/ 공식 문서에서 저장소 이름, 이슈 맵핑 방식 등을 지정하면 해당하는 스크립트가 생성됩니다.\n저는 포스트 제목이 변경될 수 있기 때문에 pathname을 기준으로 이슈를 생성하고,\n사용자 시스템 설정에 호환되는 Preferred Color Scheme 테마를 사용합니다.\n1 2 3 4 5 6 7 \u0026lt;script src=\u0026#34;https://utteranc.es/client.js\u0026#34; repo=\u0026#34;[ENTER REPO HERE]\u0026#34; issue-term=\u0026#34;pathname\u0026#34; theme=\u0026#34;github-light\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; 스크립트 삽입하기 PaperMod 테마에는 layouts/partials/ 위치에 comments.html이라는 레이아웃이 존재합니다.\n테마 별로 레이아웃이 다르기 때문에 다른 테마의 경우 이슈 등을 참고하여 구조를 파악할 필요가 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 {{- /* Comments area start */ -}} {{- /* to add comments read =\u0026gt; https://gohugo.io/content-management/comments/ */ -}} {{- if $.Site.Params.utteranc.enable -}} \u0026lt;script src=\u0026#34;https://utteranc.es/client.js\u0026#34; repo=\u0026#34;{{ .Site.Params.utteranc.repo }}\u0026#34; issue-term=\u0026#34;{{ .Site.Params.utteranc.issueTerm }}\u0026#34; {{- if $.Site.Params.utteranc.label -}}label=\u0026#34;{{ .Site.Params.utteranc.label }}\u0026#34;{{- end }} theme=\u0026#34;{{ .Site.Params.utteranc.theme }}\u0026#34; crossorigin=\u0026#34;{{ .Site.Params.utteranc.crossorigin }}\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; {{- end }} {{- /* Comments area end */ -}} 단순하게 레이아웃에 스크립트를 붙여넣어도 되지만,\n향후 속성값을 변경하기 위해 불필요하게 테마를 수정하는 경우를 방지하기 위해\n설정 파일을 통해 동적으로 속성값을 집어넣도록 설정했습니다.\nHugo HTML 코드 내에 이중 중괄호({{ }})는 Go 템플릿을 코딩하는 부분으로,\n아래와 같은 설정 파일을 읽어서 각각의 키에 해당하는 값을 할당합니다.\n이에 대한 자세한 사용법은 Hugo 공식 문서를 참조할 수 있습니다.\n1 2 3 4 5 6 7 8 params: utteranc: enable: true repo: \u0026#34;minyeamer/minyeamer.github.io\u0026#34; issueTerm: \u0026#34;pathname\u0026#34; label: \u0026#34;comments\u0026#34; theme: \u0026#34;preferred-color-scheme\u0026#34; crossorigin: \u0026#34;anonymous\u0026#34; 정상적으로 스크립트가 삽입되었다면 아래와 같이 댓글을 입력하는 부분이 표시됩니다.\n댓글 기능이 정상적으로 적용되는지 확인하기 위해 실험적으로 댓글을 작성해봅니다.\n저도 과거 게시글에 댓글을 작성하여 아래와 같이 올라온 이슈를 확인했습니다.\n마치며 Hugo 블로그를 통한 소통을 기대하여 댓글 기능을 추가해보았습니다.\n생각보다 간단하기 때문에 깃허브 블로그를 꾸미면서 댓글 기능을 희망하시는 분들이라면\nUtterances를 적극 활용해보시기를 추천드립니다.\n마지막 포스트로는 PaperMod 테마를 수정한 과정을 안내해드리겠습니다.\nHugo 테마끼리 공통적인 부분이 있기 때문에 다른 테마를 사용하시더라도 도움이 될 것입니다.\n참고 자료 Utterances Documents Introduction to Hugo Templating ","permalink":"https://minyeamer.github.io/blog/hugo-blog-2/","summary":"Hugo 블로그는 기본적으로 댓글 기능을 제공하지는 않습니다.\n제가 사용하는 PaperMod 테마에서는 서드파티 서비스인 Disqus를 위한 레이아웃이 존재하지만,\n저는 기본적인 블로그 운영을 Github 플랫폼 내에서 구성하고 싶기 때문에 다른 기능을 사용해보려 합니다.\n이번 포스트에서는 Utterances 댓글 기능을 추가하는 방법을 안내해드리겠습니다.\nUtterances 설치하기 Utterances는 Github issues 기반으로 댓글을 관리하는 기능입니다.\n무료 플랜에서 광고가 붙는 Disqus와 다르게 별도의 유료 플랜이 없어 간편하게 사용할 수 있습니다.\nUtterances 설치는 단순히 레이아웃 상에서 댓글이 위치할 곳에 자바스크립트 코드를 삽입하면 됩니다.","title":"Hugo 블로그 만들기 (2) - Utterances 댓글 적용"},{"content":"얼마 전, 티스토리 블로그에서 Jekyll 블로그로 이동했는데,\n처음 기대했던 submodule을 활용한 효율적인 저장소 연동에서 어려움을 겪고 다른 대안을 탐색하게 되었습니다.\nJekyll 블로그를 사용함에 있어서, Ruby 언어로 구성된 블로그 구조에 대해 이해하기 어려운데다가\n로컬 환경에서 Jekyll 블로그를 실행하면서 발생하는 에러를 처리하는데도 난항을 겪었는데,\n웹상에서 자동 배포가 이루어지는 과정에서 submodule인 TIL 저장소를 포스트로 인식하지 못하는 문제가 있었습니다.\nJekyll 블로그의 대안으로 Hexo 및 Hugo 프레임워크에 주목했고,\n두 제품의 장단점을 비교하여 상대적으로 배포가 빠르고 현재까지도 업데이트가 이루어지는 Hugo를 선택했습니다.\n이번 포스트에서는 제가 Hugo 블로그를 구성한 과정을 간략한 설명과 함께 안내해드리겠습니다.\n테마 선택하기 블로그의 모든 페이지 레이아웃을 만들 계획이 아니라면 블로그 선택에 있어 테마 선정이 필요합니다.\nHugo는 아래 페이지에서 다양한 테마를 제공하며, 태그를 통해 블로그 외에도 목적에 맞는 테마를 찾아볼 수 있습니다.\n미리보기만으로 알기 어렵다면 제작자가 제공하는 데모 사이트를 방문해볼 수 있고,\n아래 안내드릴 Hugo 설치를 통해 로컬에서 exampleSite를 확인해 볼 수도 있습니다.\nhttps://themes.gohugo.io/ Jekyll 블로그를 사용했을 당시 적용했던 Chirpy 테마는 사이드 메뉴, 계층식 카테고리, 동적 TOC 등\n제가 추구하는 모든 기능을 가지고 있었는데, Hugo에는 저의 취향을 완벽히 만족시키는 테마가 없었습니다.\n그나마 괜찮았던 LoveIt 테마의 경우 설정 곳곳에 중국어가 포함되어 있어 이해하기 어렵겠다는 생각이 들었습니다.\n결국, 저는 모든 테마를 둘러본 후 다루기 쉬워보이면서 외적으로도 괜찮았던 PaperMod 테마를 선택했습니다.\nHugo 블로그 구성하기 이번 Hugo 블로그 구성은 Mac 환경에서 진행되었으며, 다른 환경의 구성 방식은 제공되지 않습니다.\nHugo 설치 Mac 사용자라면 Homebrew를 통해 쉽게 Hugo를 설치하여 사용할 수 있습니다.\n터미널에 아래 명령어를 입력해 설치가 가능합니다.\n1 brew install hugo 설치가 완료되면, 버전 정보를 출력해서 정상 설치 여부를 확인합니다.\n1 2 % hugo version hugo v0.102.2+extended darwin/arm64 BuildDate=unknown Github 저장소 생성 Hugo는 원본 데이터 및 설정 파일이 포함될 공간과, 렌더링된 페이지가 저장될 공간이 필요합니다.\n일반적으로는 분리된 저장소를 통해 구현하지만, 앞서 Jekyll 블로그를 구성해보면서\n브랜치를 통해 하나의 저장소에서 두 개의 공간을 관리할 수 있을 것이라 판단했습니다.\n하나의 저장소를 main과 gh-pages, 두 개의 브랜치로 나누어 구성할 계획이며,\n우선적으로 \u0026lt;USERNAME\u0026gt;.github.io 명칭의 저장소를 생성합니다.\nHugo 프로젝트 생성 일반적인 웹 프레임워크에서 프로젝트를 시작하는 것처럼, Hugo에서도 기본 템플릿을 제공합니다. 아래 명령어를 통해 프로젝트를 생성할 수 있고, 이름은 자유롭게 지정해도 됩니다.\n1 % hugo new site \u0026lt;NAME\u0026gt; 만들어진 프로젝트 구조는 다음과 같습니다.\n만들어진 테마를 사용한다면 대부분의 구성요소들이 themes/ 디렉토리 내에 위치하게 되며,\n포스트를 위한 content/, 이미지 등을 위한 static/ 디렉토리 외엔 거의 사용하지 않습니다.\n1 2 3 4 5 6 7 8 9 10 . ├── archetypes │ └── default.md ├── config.toml ├── content ├── data ├── layouts ├── public ├── static └── themes 저장소 연동 테마를 불러오기에 앞서 git 설정이 필요합니다.\n프로젝트 디렉토리로 이동한 후, 아래 명령어를 통해 원격 저장소와 연동합니다.\n1 2 3 4 5 6 % git init % git add . % git commit -m \u0026#34;feat: new site\u0026#34; % git branch -M main % git remote add origin https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git % git push -u origin main 추가적으로, 렌더링된 페이지가 저장되고 실질적인 배포가 이루어지는 브랜치를 생성합니다.\n1 2 3 4 % git branch gh-pages main % git checkout gh-pages % git push origin gh-pages % git checkout main Hugo에서 페이지를 렌더링한 결과는 public/ 디렉토리에 저장되며, 이를 gh-pages 브랜치와 연결해야 합니다.\n기존에 존재하는 빈 디렉토리를 제거하고 gh-pages 브랜치를 main 브랜치의 submodule로 연결합니다.\nsubmodule에 대한 개념은 해당 영상을 참고해주시기 바라며, 단순하게 설명하자면 동기화 기능입니다.\n1 2 3 4 5 6 % rm -rf public % git submodule add -b gh-pages https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git public % git add public % git add .gitmodules % git commit -m \u0026#34;feat: add submodule for github pages\u0026#34; % git push 테마 불러오기 git 설정을 완료한 후, 미리 정해두었던 테마를 themes/ 디렉토리 내에 위치시킵니다.\n마찬가지로 submodule을 활용하며, 테마의 디렉토리명은 반드시 테마 설정에 명시된 것과 동일한 이름이어야 합니다.\n커스터마이징을 고려하면 원본 저장소가 아닌 별도로 fork한 저장소를 연결시키는게 좋습니다.\n1 2 3 4 % git submodule add https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod % git add themes/PaperMod % git add .gitmodules % git commit -m \u0026#34;feat: import hugo theme\u0026#34; 만약 fork 저장소를 사용하면서 원본 저장소의 변경사항을 업데이트하고 싶다면,\n원본 저장소를 새로운 원격 저장소로 등록해 pull 작업을 수행합니다.\n1 2 3 4 git remote add upstream https://github.com/adityatelange/hugo-PaperMod git fetch upstream git merge upstream/master git commit -m \u0026#34;update: pull upstream\u0026#34; 아래는 PaperMod 테마의 디렉토리 구조입니다.\n테마를 수정할 일이 있다면 아래 구조를 참고해 필요한 파일에 접근해 볼 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 themes/PaperMod ├── LICENSE ├── README.md ├── assets │ ├── css │ │ ├── common │ │ ├── core │ │ ├── extended │ │ ├── hljs │ │ └── includes │ └── js ├── go.mod ├── i18n ├── images ├── layouts │ ├── 404.html │ ├── _default │ │ └── _markup │ ├── partials │ ├── robots.txt │ └── shortcodes └── theme.toml Hugo 설정 Hugo 블로그 설정은 config 파일에서 지정할 수 있고, toml, yaml, json 형식을 지원합니다.\n테마를 사용할 경우 커스텀 키가 존재할 수 있어 별도의 문서를 참조하는게 좋습니다.\nHugo 공식 설정에 관한 문서와 PaperMod 설정에 관한 문서는 아래를 참고해주시기 바랍니다.\nhttps://gohugo.io/getting-started/configuration/ https://github.com/adityatelange/hugo-PaperMod/wiki/Installation 제 설정 파일의 경우 커스터마이징을 통해 호환되지 않는 키가 존재할 수 있지만,\n동일한 테마를 사용한다면 일부분을 참고해 도움을 받을 수 있을거라 기대합니다.\nHugo 배포 Hugo는 hugo -t \u0026lt;THEMES\u0026gt; 명령어를 통해 로컬에서 페이지 렌더링을 진행할 수 있고,\n그 결과인 public/ 디렉토리 내 내용을 gh-pages에 push하여 배포를 수행합니다.\n배포에 앞서, 깃허브에서 제공하는 Github Pages가 gh-pages 브랜치를 참고하도록\n아래 그림과 같이 저장소 설정에서 빌드 및 배포 대상 브랜치를 지정해주어야 합니다.\n위와 같이 수동으로 배포할 경우 두 번의 push 과정을 거쳐야 합니다.\n매번 이 과정을 수행하는 것은 불편하기 때문에 쉘 스크립트를 작성하여 작업을 단순화합니다.\n해당 스크립트는 다른 포스트를 참고해 작성했습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #!/bin/bash echo -e \u0026#34;\\033[0;32mDeploying updates to GitHub...\\033[0m\u0026#34; # Build the project. # hugo -t \u0026lt;your theme\u0026gt; hugo -t PaperMod # Go to public folder, submodule commit cd public # Add changes to git. git add . # Commit changes. msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; # Push source and build repos. git push origin gh-pages # Come back up to the project root cd .. # Commit and push to main branch git add . msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; git push origin main 스크립트 파일에 실행 권한을 부여하고 실행해 볼 수 있습니다.\n1 2 % chmod 777 deploy.sh % ./deploy.sh 배포가 완료되면, https://.github.io 주소로 접속해 블로그를 확인할 수 있습니다.\n포스트 작성하기 Hugo 포스트는 아래 명령어를 통해 생성할 수 있고,\n별도의 markdown 파일을 content/post/ 경로 내에 추가할 수도 있습니다.\n1 % hugo new post/\u0026lt;FILENAME\u0026gt;.md Front Matter 제목, 작성일자 등을 지정하기 위해 포스트 상단에 Front Matter라고 하는 토큰을 작성해야 합니다. Front Matter는 설정 파일과 동일하게 toml, yaml, json 형식을 지원하며,\nHugo 공식 문서 또는 PaperMod에서 안내하는 아래형식을 참고할 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 --- title: \u0026#34;My 1st post\u0026#34; date: 2020-09-15T11:30:03+00:00 # weight: 1 # aliases: [\u0026#34;/first\u0026#34;] tags: [\u0026#34;first\u0026#34;] author: \u0026#34;Me\u0026#34; # author: [\u0026#34;Me\u0026#34;, \u0026#34;You\u0026#34;] # multiple authors showToc: true TocOpen: false draft: false hidemeta: false comments: false description: \u0026#34;Desc Text.\u0026#34; canonicalURL: \u0026#34;https://canonical.url/to/page\u0026#34; disableHLJS: true # to disable highlightjs disableShare: false disableHLJS: false hideSummary: false searchHidden: true ShowReadingTime: true ShowBreadCrumbs: true ShowPostNavLinks: true ShowWordCount: true ShowRssButtonInSectionTermList: true UseHugoToc: true cover: image: \u0026#34;\u0026lt;image path/url\u0026gt;\u0026#34; # image path/url alt: \u0026#34;\u0026lt;alt text\u0026gt;\u0026#34; # alt text caption: \u0026#34;\u0026lt;text\u0026gt;\u0026#34; # display caption under cover relative: false # when using page bundles set this to true hidden: true # only hide on current single page editPost: URL: \u0026#34;https://github.com/\u0026lt;path_to_repo\u0026gt;/content\u0026#34; Text: \u0026#34;Suggest Changes\u0026#34; # edit text appendFilePath: true # to append file path to Edit link --- 게시글 저장소 연동 저는 기존 TIL 저장소를 게시글로 활용할 예정이었기에,\ncontent/post/ 디렉토리를 TIL 저장소의 submodule로 대체했습니다.\n1 2 3 4 % git submodule add https://github.com/minyeamer/til.git content/post/ % git add content/post/ % git add .gitmodules % git commit -m \u0026#34;feat: add til repository as post\u0026#34; 이렇게 설정했을 때 장점은 TIL 저장소에 변경사항이 발생했을 경우,\n아래와 같은 단 한 줄의 명령어로 블로그 저장소에서 업데이트할 수 있습니다.\n해당 명령어는 물론, 테마와 같은 다른 submodule에도 적용할 수 있습니다.\n1 % git submodule update --remote 마치며 Jekyll 블로그와 며칠간 씨름하다 Hugo로 이동해 기존의 목표를 달성할 수 있었습니다.\nChirpy 테마를 활용하지 못하는 것이 아쉽지만, PaperMod의 코드는 알기 쉽게 작성되어 있어\n시간적 여유만 있다면 커스터마이징에서 어려움이 없을 것이라 판단합니다.\n이번 포스트에서는 Hugo 블로그를 구성하고 포스트를 작성하는 과정을 전달했습니다.\n다음엔 Utterances 위젯을 활용해 댓글 기능을 추가하는 방법을 안내해드리겠습니다.\n참고 자료 Hugo Documents PaperMod Documents 블로그 구축기 (1) Hugo + Github으로 개인 블로그 만들기 저장소 안에 저장소 - git submodule ","permalink":"https://minyeamer.github.io/blog/hugo-blog-1/","summary":"얼마 전, 티스토리 블로그에서 Jekyll 블로그로 이동했는데,\n처음 기대했던 submodule을 활용한 효율적인 저장소 연동에서 어려움을 겪고 다른 대안을 탐색하게 되었습니다.\nJekyll 블로그를 사용함에 있어서, Ruby 언어로 구성된 블로그 구조에 대해 이해하기 어려운데다가\n로컬 환경에서 Jekyll 블로그를 실행하면서 발생하는 에러를 처리하는데도 난항을 겪었는데,\n웹상에서 자동 배포가 이루어지는 과정에서 submodule인 TIL 저장소를 포스트로 인식하지 못하는 문제가 있었습니다.\nJekyll 블로그의 대안으로 Hexo 및 Hugo 프레임워크에 주목했고,\n두 제품의 장단점을 비교하여 상대적으로 배포가 빠르고 현재까지도 업데이트가 이루어지는 Hugo를 선택했습니다.","title":"Hugo 블로그 만들기 (1) - Hugo 기본 구성"},{"content":"이제까지 2022년 3월 코로나19 수혜로 조기전역한 후,\n멋쟁이사자처럼에서 운영하는 AI SCHOOL 교육 과정에 참여했습니다.\n비록 군대 이슈로 머리가 리셋된 상태지만, 몇달 뒤 서류상으로 전역하게 되면\n결국 일자리를 구해야 했기에 평소 목표로 했던 개발 관련 교육을 들을 필요성을 느꼈고,\n흔히 말하는 국비 지원 교육\u0026hellip;보다는 조금 특별한 K-Digital Training을 들었습니다.\n비전공자(유사전공자)도 3개월의 과정을 거쳐 딥러닝 모델이 탑재된 웹 서비스를 만들 수 있다는\n희망에 가득찬 상태로 교육을 수료했지만,\n관심있었던 NLP 분야를 중심으로 독학하면서 스스로의 부족함을 크게 실감했습니다.\n그나마 다행인 것은 교육 과정을 통해, 그리고 이후 스터디 그룹을 통해 만난 인연이 있었습니다.\n무인도에서 일년을 혼자 살아도 큰 문제가 없을 정도로 외로움을 느끼는 성향은 아니지만,\n자신의 공부 진도가 어느 정도 수준인지 감이 잡히지 않는 불확실성은 독학에 있어 가장 큰 불안요소 입니다.\n다행히 하고 싶은 것만큼은 명확했기에 방향성이 흔들릴 일은 없었지만,\n머신러닝 엔지니어를 목적으로 실무적인 부분에 집중하다보니 기초 수학, 데이터 분석 기법 등\n기반지식의 부족으로 언젠가는 무너져 버릴 수 있겠다는 생각을 했습니다.\n교육 과정 수료 후 약 3개월이 다되가는 시점에서 저에게 두 가지 선택지 중 하나를 골라야 했습니다.\n첫 번째 선택지는 현재의 방식을 고수하면서 \u0026ldquo;속아줄\u0026rdquo; 회사를 찾는 것.\n두 번째 선택지는 바닥, 선형대수와 통계학부터 기반을 쌓아가는 것.\n이때, 우연한 기회가 찾아왔습니다.\n첫 면접 기존에 참여하던 단톡방 커뮤니티에서 평소 관심을 두던 회사의 CSO 분께 연락을 드릴 기회를 잡았습니다.\n많지는 않지만 앞선 회사 지원 과정에서 서류광탈을 느꼈던 저는 첫 서류합격을 경험했습니다.\n해당 회사는 AI 작곡 스타트업으로 저는 NLP 개발자 직무에 지원했는데,\n일반적인 NLP 과제를 기대했던 제가 받아본 과제는 도메인 특화된 새로운 과제였습니다.\n다행히 이틀의 기간 동안 논문과 코드를 뜯어보며 성공적으로 과제를 마무리했고,\n이는 제 개인적으로도 크게 성장할 수 있는 계기였습니다.\n하지만, 처음 보는 면접이었기에 도저히 감이 잡히지 않았고,\n오직 제가 경험한 프로젝트 위주로만 준비하는 실수를 저질렀습니다.\n실제 면접에서는 과제 20, 프로젝트 40, 딥러닝 지식 40으로 질문을 받았었는데,\n전혀 예상하지 못했던 질문들이었기에 제대로 답변할 수 없었습니다.\n그런데 이후 생각해보니 쏘카의 엔지니어분께서 수집한 데이터 사이언스 질문이 대부분이었습니다.\n사실 과제를 받아볼 때부터 느꼈던 것이지만 해당 회사는 저의 수준에 맞지 않은 뛰어난 기업이었습니다.\n우연한 기회를 놓치지 않기 위해 자신의 부족함을 감수하고 지원한 것이었는데,\n역시 예상은 빗나가지 않았습니다.\n비록 결과는 좋지 않았지만, 당시 저는 오히려 \u0026ldquo;불확실성 해소\u0026quot;를 느끼고 만족했습니다.\n그리고, 이 경험이 뒷받침 되지는 않았지만 다음 면접에서 합격할 수 있었습니다.\n취업 첫 면접 이후 제 수준에서는 인턴부터 시작하는 것이 맞다고 판단하여 관련된 공고에 지원했습니다.\n그리 많은 공고가 올라와 있지는 않아서 당첨을 기대하지는 않았지만,\n다행히 스포츠 브랜드 쇼핑몰을 운영하는 기업인 거성디지털에서 데이터 분석가 직무로 연락을 주었습니다.\n저는 거성디지털 자체보다는 해당 기업에서 운영하는 멡킨스포츠 라는 브랜드에 대해 조금 알고 있었고,\n매일 웨이트 트레이닝을 할 정도로 스포츠 용품에 애정을 갖고 있었기 때문에 흥미가 있었습니다.\n연락 후 주말을 거쳐 바로 보게된 1차 면접에서는 이전 면접에서의 과오를 되풀이하지 않기 위해\n질문 리스트를 보고 하나하나 답변을 기록해가면서 준비했는데,\n실제로 면접을 진행하니 이번에도 전혀 예상치 못하게 실무적인 부분을 말씀주셨습니다.\n하지만, 해당 기업의 경우 개발팀이란 것이 존재하지 않는 마케팅 중심 기업이었고,\n실험적으로 데이터 수집 자동화 및 시각화를 위해 관련 전문가를 채용해보는 것이었습니다.\n제가 지금까지 준비하지 않았던 내용들이지만, 어떻게 구현할지에 대한 방향성은 금방 잡을 수 있었습니다.\n이것은 제가 머신러닝 기술 뿐 아니라 전반적인 데이터 관련 기술에 흥미를 가졌던 덕분입니다.\n1차 면접을 무사히 합격하고 바로 이틀 뒤 실장님과 대표님을 만나 2차 면접을 보았습니다.\n실장님과의 대화에서 이전에 들었던 업무에 더해 DB 설계라는 새로운 과제를 인식했지만,\n이것이 High-Risk High-Return의 기회라 생각했습니다.\n합격 통보를 듣고 현재는 향후 주어진 과제를 어떻게 구현할 것인지에 대해 고민하고 있습니다.\n크게 데이터 분석가, 데이터 엔지니어, DB 개발자의 3가지 분야의 역량을 가지고\n3개월 동안 혼자서 성과를 보여야 하는데 머신러닝 엔지니어 직무를 준비했던 제게는 굉장히 도전적인 선택입니다.\n현재 앞선 분야 별 직무와 관련해 도입을 계획하고 있는 기술은 태블로, MySQL, Airflow 입니다.\n사실 세 가지 서비스 모두 얕은 수준에서만 사용해본게 전부라서 선택에 있어서 중압감을 느끼지만,\n최소한 태블로 만큼은 첫 출근까지 남은 5일의 시간 동안 마스터할 각오로 준비해야 합니다.\n어떻게 보면 해당 기업은 저를 \u0026ldquo;속아서\u0026rdquo; 뽑아준 것이지만,\n저는 현재의 자신을 속일지라도 미래의 자신을 속일 생각은 없습니다.\n단기적으로는 저라는 개발자의 위엄과 자존심을 지키기 위해,\n장기적으로는 거성디지털의 디지털 트랜스포메이션을 성공시키기 위해 분골쇄신 해야겠다는 마음가짐입니다.\n누군가에게 밝혀서 좋은 글은 아니지만, 적어도 과거를 돌아볼 미래의 자신에게,\n그리고 저와 같은 상황에 놓인 분들에게 작은 도움이 되고자 이 글을 남깁니다.\n","permalink":"https://minyeamer.github.io/blog/2022-09-07/","summary":"이제까지 2022년 3월 코로나19 수혜로 조기전역한 후,\n멋쟁이사자처럼에서 운영하는 AI SCHOOL 교육 과정에 참여했습니다.\n비록 군대 이슈로 머리가 리셋된 상태지만, 몇달 뒤 서류상으로 전역하게 되면\n결국 일자리를 구해야 했기에 평소 목표로 했던 개발 관련 교육을 들을 필요성을 느꼈고,\n흔히 말하는 국비 지원 교육\u0026hellip;보다는 조금 특별한 K-Digital Training을 들었습니다.\n비전공자(유사전공자)도 3개월의 과정을 거쳐 딥러닝 모델이 탑재된 웹 서비스를 만들 수 있다는\n희망에 가득찬 상태로 교육을 수료했지만,\n관심있었던 NLP 분야를 중심으로 독학하면서 스스로의 부족함을 크게 실감했습니다.","title":"데이터 분석가의 첫 스텝"},{"content":"구매 계기 매번 딥러닝 모델을 학습하는 실험을 하면서 Colab에 의존하는 방식에 불편함을 느꼈는데,\n결국 190발을 사용해서 RTX 3060이 포함된 조립 PC를 구매했습니다.\n이번 기회에 혼자서 컴퓨터를 조립해봤으면 좋았겠지만,\n여기에 많은 시간을 쏟을만한 상황도 아니었고 이쪽이 고장날 일도 없어서 맡겼습니다.\n컴퓨터 조립 업체로 다나와, 피씨팩토리 등을 고려했는데,\n현금 결제를 권장하는 부분이 미심쩍었고 컴퓨존의 평이 좋아 믿고 맡겼습니다.\n부품 선정 그래픽 카드 처음엔 약 100만원을 얹어서 RTX 3090Ti을 구매할 생각이었지만,\n당시 소득이 없는 상황이라 저렴한 가격의 RTX 3060을 구매했습니다.\nColab에서 RoBERTa 등의 모델을 돌리면서 15GB의 VRAM이 고갈되어 배치 사이즈를 줄인 경험이 있어\n상대적으로 적은 12GB VRAM의 RTX 3060 제품이 불안하긴 했습니다.\n하지만, 당장엔 큰 모델을 돌릴 예정이 없고 그래픽 카드를 교체하는게 어렵지는 않기 때문에\n향후 확장성을 고려하는 방향으로 저사양의 제품을 구매했습니다.\n파워 다만, 파워의 경우 교체가 쉽지 않기 때문에 향후 그래픽 카드를 교체할 것을 고려해 850W로 선택했고,\nWiFi 보드, 강화유리가 없는 케이스를 필수 요건으로 삼아\n퀘이사존을 통해 부품에 대한 정보를 탐색했습니다.\n메인보드 메인보드로는 인텔용 B660 보드를 선택했는데,\n구매하고 보니 썬더볼트 독과 연결할만한 고속 포트가 존재하지 않아 아쉬웠습니다.\n이점은 충분히 고려하지 못하고 결정한 문제입니다.\n케이스 케이스는 프랙탈 디자인 사의 Torrent Compact 제품을 구매했는데,\n강화유리가 안달린 제품들의 가격이 대체로 비싼 편이어서 이왕이면 취향에 맞는 제품으로 선택했습니다.\n기타 부품 게임이 목적은 아니었기 때문에 CPU는 적당한 인텔 사 제품을 골랐고,\n관리의 불편함 때문에 CPU용 쿨러로 DEEPCOOL 사의 공랭 쿨러를 선택했습니다.\n램, SSD, HDD는 각각 마이크론, 하이닉스, WD 사의 제품을 선택했습니다.\n마찬가지로 크게 중요한 부품은 아니었기 때문에 유명한 것으로 골랐습니다.\n컴퓨존 조립 컴퓨존에 견적 신청을 한 후 당일날 조립이 완료되었다는 답변을 받았습니다.\n용산에서 택배가 발송되고 다음날 아래와 같은 사진을 전달받았습니다.\n제품 배송도 주문 후 하루 내지 이틀 사이에 도착한 것으로 기억합니다.\n컴퓨터 자체도 스티로폼이랑 완충재로 단단히 감싸져 있어 배송 상태에 만족했습니다.\n제품 수령 및 확인 각 제품의 박스는 찌그러짐 없이 잘 배송되었는데, 겉박스가 살짝 과대포장이 아닌가 생각했습니다.\n컴퓨터는 아래 사진과 같이 내부까지 완충재로 보호된 상태로 배송되었습니다.\n완충재를 걷어내니 그래픽 카드의 아름다운 자태가 눈에 들어왔습니다.\n개인적으로 RGB를 선호하지는 않기 때문에 제품 그 자체를 보는 것만으로 행복감을 느꼈습니다.\n케이스도 개인 취향에 살짝 비싼 제품을 구매했는데 이렇게 다시 보니 잘 샀다고 생각합니다.\n특히 케이스 정면의 Y자 형태의 디자인이 매력적입니다.\n별도의 수작업 없이 바로 컴퓨터를 실행시켰는데 한가지 아쉬웠던 점은 쿨러 소리가 너무 시끄러웠던 것입니다.\n특별히 불량이 있는 것은 아니고 단순히 쿨러의 RPM이 높게 느껴졌던 건데\n메인보드에서 CPU 쿨러의 속도를 조절해도 변화가 없어 각각의 쿨러를 확인해보니\n케이스 쿨러에서 제어가 이루어지지 않는 문제가 있었습니다.\n제가 조립한 제품이 아니어서 다 뜯어보기도 난감했는데\n다행히 가장 의심쩍었던 팬 허브와 관련해 찾아보면서 메인보드와 연결 문제가 있을 것이라 짐작하게 되었고,\n팬 허브의 PWM 포트가 메인보드가 아닌, 동일 허브의 FAN4 포트에 연결되어 있었습니다.\n황당하긴 하지만 정상적으로 연결 후 조용히 돌아가는 모습을 보니 만족했습니다.\n현재는 우분투를 탑재해 장남감으로서 재밌게 가지고 놀고 있습니다.\nM1 맥북을 유일한 PC로 사용하면서 가끔씩 호환성에 불편함을 느꼈었는데\n리눅스 시스템과 함께 VMware 상에서 윈도우를 운영하면서 더이상 그러한 걱정을 할 필요가 없어졌습니다.\n무엇보다 가장 만족스러운건 터미널에서 nvidia-smi 명령어를 입력했을 때\n표시되는 그래픽 카드의 상태를 확인하는 것입니다.\n적지않은 지출이었지만 그 이상으로 만족한 경험이었습니다.\n","permalink":"https://minyeamer.github.io/blog/desktop-settings/","summary":"구매 계기 매번 딥러닝 모델을 학습하는 실험을 하면서 Colab에 의존하는 방식에 불편함을 느꼈는데,\n결국 190발을 사용해서 RTX 3060이 포함된 조립 PC를 구매했습니다.\n이번 기회에 혼자서 컴퓨터를 조립해봤으면 좋았겠지만,\n여기에 많은 시간을 쏟을만한 상황도 아니었고 이쪽이 고장날 일도 없어서 맡겼습니다.\n컴퓨터 조립 업체로 다나와, 피씨팩토리 등을 고려했는데,\n현금 결제를 권장하는 부분이 미심쩍었고 컴퓨존의 평이 좋아 믿고 맡겼습니다.\n부품 선정 그래픽 카드 처음엔 약 100만원을 얹어서 RTX 3090Ti을 구매할 생각이었지만,\n당시 소득이 없는 상황이라 저렴한 가격의 RTX 3060을 구매했습니다.","title":"가벼운 딥러닝용 조립PC 후기"},{"content":"문제 링크 https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-tree/ 문제 해설 Idea root부터 조건을 만족하는 깊이까지 재귀적으로 자식 노드를 탐색하면서\np 또는 q 노드를 발견한 경우 해당 노드를 호출한 함수에게 반환 최종적으로 좌,우에 각각 p와 q가 있을 경우 깊이가 가장 깊은 부모 노드를 반환하고,\n한쪽 방향에 p와 q가 몰려있을 경우 둘 중 부모 관계에 있는 노드를 반환 Time Complexity O(N) = 10^5 Data Size nodes: [2, 10^5] val: -10^9 \u0026lt;= int \u0026lt;= 10^9 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Definition for a binary tree node. # class TreeNode(object): # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution(object): def lowestCommonAncestor(self, root, p, q): \u0026#34;\u0026#34;\u0026#34; :type root: TreeNode :type p: TreeNode :type q: TreeNode :rtype: TreeNode \u0026#34;\u0026#34;\u0026#34; if root: if root == p or root == q: return root left = self.lowestCommonAncestor(root.left, p, q) right = self.lowestCommonAncestor(root.right, p, q) if left and right: return root return left if left else right ","permalink":"https://minyeamer.github.io/blog/leetcode-problems-236/","summary":"문제 링크 https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-tree/ 문제 해설 Idea root부터 조건을 만족하는 깊이까지 재귀적으로 자식 노드를 탐색하면서\np 또는 q 노드를 발견한 경우 해당 노드를 호출한 함수에게 반환 최종적으로 좌,우에 각각 p와 q가 있을 경우 깊이가 가장 깊은 부모 노드를 반환하고,\n한쪽 방향에 p와 q가 몰려있을 경우 둘 중 부모 관계에 있는 노드를 반환 Time Complexity O(N) = 10^5 Data Size nodes: [2, 10^5] val: -10^9 \u0026lt;= int \u0026lt;= 10^9 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # Definition for a binary tree node.","title":"[LeetCode 236] Lowest Common Ancestor of a Binary Tree (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/10026 문제 해설 Idea 모든 방문하지 않은 칸에 대해 BFS 탐색하면서 같은 구역을 방문 적록색약의 경우 R과 G를 같은 구역으로 판단하고 탐색 각각의 경우에 대한 BFS 호출 횟수를 서로 다른 구역의 수로 판단하여 출력 Time Complexity BFS: O(N^2) = 10,000 Data Size N: 1 \u0026lt;= int \u0026lt;= 100 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from collections import deque import sys input = sys.stdin.readline N = int(input()) grid = [list(input().strip()) for _ in range(N)] visited = [[[False] * N for _ in range(N)] for _ in range(2)] answer = [0, 0] def bfs(start, visited, st): queue = deque([start]) dy = [0,1,0,-1] dx = [-1,0,1,0] while queue: y,x = queue.popleft() for i in range(4): ny, nx = y+dy[i], x+dx[i] if 0\u0026lt;=ny\u0026lt;N and 0\u0026lt;=nx\u0026lt;N and not visited[ny][nx]: if st[grid[y][x]] == st[grid[ny][nx]]: queue.append((ny,nx)) visited[ny][nx] = True return 1 for i in range(N): for j in range(N): if not visited[0][i][j]: answer[0] += bfs((i,j), visited[0], {\u0026#39;R\u0026#39;:0,\u0026#39;G\u0026#39;:1,\u0026#39;B\u0026#39;:2}) if not visited[1][i][j]: answer[1] += bfs((i,j), visited[1], {\u0026#39;R\u0026#39;:0,\u0026#39;G\u0026#39;:0,\u0026#39;B\u0026#39;:2}) print(answer[0], answer[1]) ","permalink":"https://minyeamer.github.io/blog/boj-problems-10026/","summary":"문제 링크 https://www.acmicpc.net/problem/10026 문제 해설 Idea 모든 방문하지 않은 칸에 대해 BFS 탐색하면서 같은 구역을 방문 적록색약의 경우 R과 G를 같은 구역으로 판단하고 탐색 각각의 경우에 대한 BFS 호출 횟수를 서로 다른 구역의 수로 판단하여 출력 Time Complexity BFS: O(N^2) = 10,000 Data Size N: 1 \u0026lt;= int \u0026lt;= 100 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from collections import deque import sys input = sys.","title":"[백준 10026] 적록색약 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/21758 문제 해설 Idea 벌이 같은 방향을 향하는 경우 상자까지의 총합에서 두 벌의 시작 위치에 있는 값을 제외 벌이 다른 방향을 향하는 경우 상자까지의 총합에 절댓값을 취해서 더함 Data Size N: 3 \u0026lt;= int \u0026lt;= 100,000 arr[i]: 1 \u0026lt;= int \u0026lt;= 10,000 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 N = int(input()) arr = list(map(int, input().split())) forward, backward = [arr[0]]+[0]*(N-1), [0]*(N-1)+[arr[-1]] for i in range(1,N): forward[i] = forward[i-1] + arr[i] backward[N-i-1] = backward[N-i] + arr[N-i-1] answer = 0 for i in range(1,N-1): answer = max(answer, forward[N-1]*2-forward[0]-forward[i-1]-arr[i]*2) answer = max(answer, backward[0]*2-backward[N-1]-backward[N-i]-arr[N-i-1]*2) answer = max(answer, forward[i]-arr[0]+backward[i]-arr[-1]) print(answer) ","permalink":"https://minyeamer.github.io/blog/boj-problems-21758/","summary":"문제 링크 https://www.acmicpc.net/problem/21758 문제 해설 Idea 벌이 같은 방향을 향하는 경우 상자까지의 총합에서 두 벌의 시작 위치에 있는 값을 제외 벌이 다른 방향을 향하는 경우 상자까지의 총합에 절댓값을 취해서 더함 Data Size N: 3 \u0026lt;= int \u0026lt;= 100,000 arr[i]: 1 \u0026lt;= int \u0026lt;= 10,000 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 N = int(input()) arr = list(map(int, input().split())) forward, backward = [arr[0]]+[0]*(N-1), [0]*(N-1)+[arr[-1]] for i in range(1,N): forward[i] = forward[i-1] + arr[i] backward[N-i-1] = backward[N-i] + arr[N-i-1] answer = 0 for i in range(1,N-1): answer = max(answer, forward[N-1]*2-forward[0]-forward[i-1]-arr[i]*2) answer = max(answer, backward[0]*2-backward[N-1]-backward[N-i]-arr[N-i-1]*2) answer = max(answer, forward[i]-arr[0]+backward[i]-arr[-1]) print(answer) ","title":"[백준 21758] 꿀 따기 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/5547 문제 해설 Idea 전체 좌표 평면의 외곽에 1만큼의 여백을 추가하고 x,y 좌표가 0부터 시작한다고 판단 y가 홀수 일 때, 인접한 좌표는 상하좌우와 함께 우상단,우하단을 포함 y가 짝수 일 때, 인접한 좌표는 상하좌우와 함께 좌상단, 좌하단을 포함 건물이 없는 좌표를 BFS 탐색하면서 건물과 만나는 지점을 카운트 Time Complexity BFS: O(N^2) = 10,000 Data Size W, H: 1 \u0026lt;= int \u0026lt;= 100 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from collections import deque import sys input = sys.stdin.readline W, H = map(lambda x: int(x)+2, input().split()) maps = [[0] * W for _ in range(H)] for i in range(1,H-1): maps[i] = [0]+list(map(int, input().split()))+[0] visited = [[False] * W for _ in range(H)] def bfs(start): queue = deque([start]) dy = [0,1,0,-1,1,-1] cnt = 0 while queue: y,x = queue.popleft() dx = [-1,0,1,0,-1,-1] if y%2==0 else [-1,0,1,0,1,1] for i in range(6): ny, nx = y+dy[i], x+dx[i] if 0\u0026lt;=ny\u0026lt;H and 0\u0026lt;=nx\u0026lt;W: if maps[ny][nx] == 0 and not visited[ny][nx]: queue.append((ny,nx)) visited[ny][nx] = True elif maps[ny][nx] == 1: cnt += 1 return cnt visited[0][0] = True print(bfs((0,0))) ","permalink":"https://minyeamer.github.io/blog/boj-problems-5547/","summary":"문제 링크 https://www.acmicpc.net/problem/5547 문제 해설 Idea 전체 좌표 평면의 외곽에 1만큼의 여백을 추가하고 x,y 좌표가 0부터 시작한다고 판단 y가 홀수 일 때, 인접한 좌표는 상하좌우와 함께 우상단,우하단을 포함 y가 짝수 일 때, 인접한 좌표는 상하좌우와 함께 좌상단, 좌하단을 포함 건물이 없는 좌표를 BFS 탐색하면서 건물과 만나는 지점을 카운트 Time Complexity BFS: O(N^2) = 10,000 Data Size W, H: 1 \u0026lt;= int \u0026lt;= 100 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from collections import deque import sys input = sys.","title":"[백준 5547] 일루미네이션 (Python)"},{"content":"블로그를 처음 시작함에 있어서 모든 것이 준비된 호스팅 서비스의 편의성은 무시할 수 없습니다.\n저도 처음엔 코드를 직접 건드리는 자유도 높은 방식의 블로그에 진입 장벽을 느끼고\n가볍게 시작할 수 있는 티스토리를 통해 블로그에 입문했습니다.\n하지만, 개발적 지식을 학습하면서 깃허브에 마크다운 문서를 올리는 빈도가 늘어났고,\n깃허브에 올린 문서를 굳이 티스토리로 다시 옮겨 담는 것에 불편함을 느끼게 되었습니다.\n마크다운 문서를 자주 작성하고 깃허브 저장소를 학습 노트로 활용한다면,\n깃허브 블로그를 구성해보는 것이 문서를 통합적으로 관리할 수 있다는 점에서 매력적이라 생각합니다.\n현재는 막 깃허브 블로그를 꾸려서 적응해가는 단계에 불과하지만,\n웹에서 정적 파일을 수집하는 기술을 적용할 수 있다면 중복된 자료를 생성할 필요 없이\nTIL 저장소 자체를 블로그 포스트 저장소로도 활용할 수 있을 것이라 기대합니다.\n블로그를 개설하고 처음 작성하는 이번 포스트에서는 깃허브 블로그를 만든 과정을 소개해드리겠습니다.\n테마 선택 및 가져오기 깃허브 블로그를 생성하는데 있어 주로 사용되는 기술이 Jekyll이라는 사이트 생성 엔진 입니다.\nJekyll을 구성하는 Ruby와 쉘 스크립트 작성에 대한 이해가 있다면 더욱 자유도 높은 작업을 할 수 있지만,\n다행히 이를 모를지라도 다른 사용자들이 만든 테마를 가져와 블로그를 구성해 볼 수 있습니다. Jekyll 테마는 아래와 같은 사이트를 참조하여 마음에 드는 UI를 확인할 수 있습니다.\nhttps://jekyllthemes.io http://jekyllthemes.org 무료로 가져다 사용할 수 있는 여러 테마 중 개인적으로 마음에 드는 Chirpy 테마를 활용해 보겠습니다.\n테마 별로 적용 및 활용하는 방식에 다소 차이가 있지만,\nChirpy 같은 경우 아래 튜토리얼 사이트가 만들어져 있어 비교적 쉽게 블로그를 구성할 수 있습니다.\nhttps://chirpy.cotes.page 블로그 배포하기 Chirpy 테마를 설치하고 배포하는 방법엔 두 가지 방식이 있습니다.\nChirpy Starter를 통해 간단하게 설치하기\n튜토리얼에서는 Jekyll을 전혀 모르는 사용자도 쉽게 테마를 활용할 수 있는 프로젝트 파일이 마련되어 있습니다.\n깃허브 저장소를 생성하는 것과 같은 단순한 버튼 클릭만으로 완성된 사이트를 배포할 수 있습니다.\nGithub에서 소스코드를 fork 받아 직접 설치하기\n스크립트를 실행하는 등 다소의 작업이 추가되지만, 블로그 커스터마이징에 유리한 방식입니다.\nJekyll을 다뤄볼 줄 안다면 직접 설치를 진행하는 것이 취향에 맞는 방식일 수 있습니다.\n저 같은 경우 Jekyll에 친숙한 편이 아니기 때문에 1번째 방법을 통해 설치를 진행했습니다.\n이때, 저장소 이름은 \u0026lt;GH_USERNAME\u0026gt;.github.io 형식으로 지정해야 하며,\n\u0026lt;GH_USERNAME\u0026gt;에는 깃허브 아이디를 입력하면 됩니다.\n위 방식으로 저장소를 생성하면 자동으로 배포가 수행되는데, Actions 탭을 통해 아래처럼 진행사항을 확인할 수 있습니다.\n빌드 및 배포가 완료되면 https://\u0026lt;저장소 이름\u0026gt; 주소를 통해 블로그 페이지에 접근할 수 있는데,\n2022년 8월 기준에서 해당 테마를 가져온 직후엔\n--- layout: home # Index page --- 텍스트만 존재하는 화면을 마주하게 됩니다.\n이것은 현재 Github Pages가 스타일이 적용되지 않는 main 브랜치를 대상으로 하고 있는 것이 원인으로,\nSettings 탭 아래 Pages 메뉴를 클릭했을 때 보이는 Branch 부분을 gh-pages로 수정하면 됩니다.\n블로그 설정하기 향후 블로그 호스팅 및 사이트 제목을 수정하는 등의 설정을 위해 _config.yml 파일을 수정할 필요가 있습니다.\n제가 블로그 세팅에 도움을 받은 게시글로부터 일부 항목에 대한 설명을 가져왔습니다.\n항목 값 설명 timezone Asia/Seoul 시간대를 설정하는 부분으로 서울 표준시로 설정합니다. title 블로그 제목 프로필 사진 아래 큰 글씨로 제목이 표시됩니다. tagline 프로필 설명 블로그 제목 아래에 작은 글씨로 부연설명을 넣을 수 있습니다. description SEO 구글 검색에 어떤 키워드로 내 블로그를 검색하게 할 것인가를 정의하는 부분입니다. url https://*.github.io 블로그와 연결된 url을 입력합니다. github Github ID 본인의 github 아이디를 입력합니다. twitter.username Twitter ID 트위터를 사용한다면 아이디를 입력합니다. social.name 이름 포스트 등에 작성자로 표시할 나의 이름을 입력합니다. social.email 이메일 나의 이메일 계정을 입력합니다. social.links 소셜 링크들 트위터, 페이스북 등 내가 사용하고 있는 소셜 서비스의 나의 홈 url을 입력합니다. avatar 프로필 사진 블로그 왼쪽 상단에 표시될 프로필 사진의 경로를 설정합니다. toc true 포스트 오른쪽에 목차를 표시합니다. paginate 10 한 목록에 몇 개의 글을 표시할 것인지 지정합니다. 이 부분은 저의 설정 파일 _config.yml 또는 github 내 검색을 통해 접근할 수 있는\n다른 사용자 분들의 설정 파일을 참고하면 원하는 부분을 수정하는데 도움이 될 것입니다.\n_config.yml 파일이 수정 등 저장소에 변화가 발생하면 자동으로 빌드 및 배포 과정이 수행되며,\n변경사항이 적용되는데 약간 시간이 걸릴 수 있습니다.\n포스트 작성하기 Jekyll은 마크다운 문법으로 글을 작성할 수 있습니다.\n마크다운 문법에 익숙하지 않다면 해당 게시글을 참고해 주시기 바랍니다.\nVS Code 또는 기타 웹 편집기를 활용하면 마크다운 작성 내용을 실시간으로 렌더링해 확인할 수 있습니다.\n게시글에 대한 마크다운 파일은 _posts 디렉토리 내에 위치시키고,\nyyyy-mm-dd-제목.md의 형식으로 파일 이름을 지정해야 합니다.\n제목에 해당하는 부분은 실제 포스트 제목이 아닌, url로 활용되는 부분이기 때문에\n게시글의 내용을 짐작하게 하는 간단한 단어나 문장을 활용하는게 좋습니다.\n마크다운 파일의 상단엔 Front Matter라고 하는 Jekyll 게시글에서 허용하는 규칙을 통해\n게시글 제목, 작성일자, 카테고리, 태그 등을 지정할 수 있습니다.\n자세한 내용은 튜토리얼을 참조할 수도 있고,\n해당 게시글에 대한 raw 파일을 확인해보셔도 좋습니다.\n마치며 과거 깃허브 블로그를 만들려고 했을 때는 Jekyll을 직접 다뤄야 해서 쉽게 접근하지 못했는데,\n이제는 그럴 필요 없이 완성된 패키지를 가져다 쓸 수 있게 되어서 많이 편해졌다고 생각합니다.\n참고 자료 Chirpy Documents 깃헙(GitHub) 블로그 10분안에 완성하기 Jekyll Chirpy 테마 사용하여 블로그 만들기 Github 블로그 테마적용하기(Chirpy) ","permalink":"https://minyeamer.github.io/blog/jekyll-blog/","summary":"블로그를 처음 시작함에 있어서 모든 것이 준비된 호스팅 서비스의 편의성은 무시할 수 없습니다.\n저도 처음엔 코드를 직접 건드리는 자유도 높은 방식의 블로그에 진입 장벽을 느끼고\n가볍게 시작할 수 있는 티스토리를 통해 블로그에 입문했습니다.\n하지만, 개발적 지식을 학습하면서 깃허브에 마크다운 문서를 올리는 빈도가 늘어났고,\n깃허브에 올린 문서를 굳이 티스토리로 다시 옮겨 담는 것에 불편함을 느끼게 되었습니다.\n마크다운 문서를 자주 작성하고 깃허브 저장소를 학습 노트로 활용한다면,\n깃허브 블로그를 구성해보는 것이 문서를 통합적으로 관리할 수 있다는 점에서 매력적이라 생각합니다.","title":"깃허브 블로그 시작하기"},{"content":"문제 링크 https://www.acmicpc.net/problem/1308 문제 해설 Idea 각각의 날짜에 대한 문자열을 date 타입으로 변환하고, today 기준 1000년 후 날짜와 dday를 비교 조건이 맞을 경우 \u0026lsquo;gg\u0026rsquo;를 출력하고, 아니면 두 날짜의 차이를 출력 Data Size y,m,d: 1,1,1 \u0026lt;= int*3 \u0026lt;= 9999,12,31 해설 코드 1 2 3 4 5 6 7 from datetime import date strptime = lambda: date(**{k:int(v) for k,v in zip([\u0026#39;year\u0026#39;,\u0026#39;month\u0026#39;,\u0026#39;day\u0026#39;],input().split())}) today, dday = strptime(), strptime() if dday \u0026gt;= today.replace(today.year+1000): print(\u0026#39;gg\u0026#39;) else: print(\u0026#39;D-\u0026#39;+str((dday-today).days)) ","permalink":"https://minyeamer.github.io/blog/boj-problems-1308/","summary":"문제 링크 https://www.acmicpc.net/problem/1308 문제 해설 Idea 각각의 날짜에 대한 문자열을 date 타입으로 변환하고, today 기준 1000년 후 날짜와 dday를 비교 조건이 맞을 경우 \u0026lsquo;gg\u0026rsquo;를 출력하고, 아니면 두 날짜의 차이를 출력 Data Size y,m,d: 1,1,1 \u0026lt;= int*3 \u0026lt;= 9999,12,31 해설 코드 1 2 3 4 5 6 7 from datetime import date strptime = lambda: date(**{k:int(v) for k,v in zip([\u0026#39;year\u0026#39;,\u0026#39;month\u0026#39;,\u0026#39;day\u0026#39;],input().split())}) today, dday = strptime(), strptime() if dday \u0026gt;= today.replace(today.year+1000): print(\u0026#39;gg\u0026#39;) else: print(\u0026#39;D-\u0026#39;+str((dday-today).","title":"[백준 1308] D-Day (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/7569 문제 해설 Idea BFS 7569번 토마토 문제에서 하나의 차원이 추가된 버전입니다. 차원이 늘어난만큼 N의 최대 길이가 감소했기 때문에 여전히 BFS로 해결할 수 있습니다. 익은 토마토의 기준에서 전체 상자를 BFS로 완전탐색하면서 안익은 토마토까지의 최소 거리를 기록합니다. 최소 거리의 최댓값이 곧 토마토들이 모두 익는 최소 일수이며,\n모든 토마토가 다 익었을 경우에 최소 일수를 출력하고, 그렇지 않은 경우엔 -1을 출력합니다. Time Complexity O(N^3) = 1,000,000 Data Size M,N: 2 \u0026lt;= int \u0026lt;= 100 H: 1 \u0026lt;= int \u0026lt;= 100 t in [1,0,-1] 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from collections import deque import sys input = sys.stdin.readline M, N, H = map(int, input().split()) box = [[list(map(int, input().split())) for _ in range(N)] for _ in range(H)] queue = deque([(h,r,c) for h in range(H) for r in range(N) for c in range(M) if box[h][r][c]==1]) days = 0 dz = [0,0,0,0,1,-1] dy = [0,1,0,-1,0,0] dx = [-1,0,1,0,0,0] while queue: z,y,x = queue.popleft() days = max(days, box[z][y][x]) for i in range(6): nz,ny,nx = z+dz[i],y+dy[i],x+dx[i] if 0\u0026lt;=nz\u0026lt;H and 0\u0026lt;=ny\u0026lt;N and 0\u0026lt;=nx\u0026lt;M and box[nz][ny][nx]==0: box[nz][ny][nx] = box[z][y][x] + 1 queue.append((nz,ny,nx)) if 0 in {cell for layer in box for row in layer for cell in row}: print(-1) else: print(days-1) ","permalink":"https://minyeamer.github.io/blog/boj-problems-7569/","summary":"문제 링크 https://www.acmicpc.net/problem/7569 문제 해설 Idea BFS 7569번 토마토 문제에서 하나의 차원이 추가된 버전입니다. 차원이 늘어난만큼 N의 최대 길이가 감소했기 때문에 여전히 BFS로 해결할 수 있습니다. 익은 토마토의 기준에서 전체 상자를 BFS로 완전탐색하면서 안익은 토마토까지의 최소 거리를 기록합니다. 최소 거리의 최댓값이 곧 토마토들이 모두 익는 최소 일수이며,\n모든 토마토가 다 익었을 경우에 최소 일수를 출력하고, 그렇지 않은 경우엔 -1을 출력합니다. Time Complexity O(N^3) = 1,000,000 Data Size M,N: 2 \u0026lt;= int \u0026lt;= 100 H: 1 \u0026lt;= int \u0026lt;= 100 t in [1,0,-1] 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from collections import deque import sys input = sys.","title":"[백준 7569] 토마토 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/7576 문제 해설 Idea BFS를 활용한 시뮬레이션을 통해 모든 토마토가 익을 떄까지 걸리는 최소 기간을 계산 초기엔 안익은 토마토의 기준에서 매번 익은 토마토까지의 최단거리를 탐색하여,\nO(N^4)의 시간 복잡도로 시간 초과가 발생 이후 익은 토마토의 기준에서 시뮬레이션을 단 한번만 수행하여 각각의 칸에 도달하는데 걸리는 거리값을 갱신 모두 익지 못하는 상황에 대해 1안에선 에러를 발생시켜 처리했고, 2안에선 종료 코드를 실행해 처리 Time Complexity O(N^2) = 1,000,000 Data Size M,N: 2 \u0026lt;= int \u0026lt;= 1,000 t in [1,0,-1] 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 from collections import deque import sys input = sys.stdin.readline M, N = map(int, input().split()) box = [list(map(int, input().split())) for _ in range(N)] days = 0 # ============== 1안 (시간초과) ============= def bfs(graph, start, n, m, vistied): queue = deque([start]) dist = 0 dy = [0,1,0,-1] dx = [-1,0,1,0] while queue: for _ in range(len(queue)): y,x = queue.popleft() if graph[y][x] == 1: return dist for i in range(4): ny,nx = y+dy[i],x+dx[i] if 0\u0026lt;=ny\u0026lt;n and 0\u0026lt;=nx\u0026lt;m and not vistied[ny][nx] and graph[ny][nx]!=-1: queue.append((ny,nx)) vistied[ny][nx] = True dist += 1 raise Exception() try: for i in range(N): for j in range(M): if box[i][j] == 0: vistied = [[False] * M for _ in range(N)] vistied[i][j] = True days = max(days, bfs(box, (i,j), N, M, vistied)) print(days) except Exception: print(-1) # =============== 2안 (통과) =============== queue = deque() for i in range(N): for j in range(M): if box[i][j] == 1: queue.append((i,j)) def bfs(graph, queue, n, m): dy = [0,1,0,-1] dx = [-1,0,1,0] while queue: y,x = queue.popleft() for i in range(4): ny,nx = y+dy[i],x+dx[i] if 0\u0026lt;=ny\u0026lt;n and 0\u0026lt;=nx\u0026lt;m and graph[ny][nx]==0: graph[ny][nx] = graph[y][x] + 1 queue.append((ny,nx)) bfs(box, queue, N, M) for i in range(N): for j in range(M): if box[i][j] == 0: print(-1) exit(0) days = max(days, box[i][j]) print(days-1) ","permalink":"https://minyeamer.github.io/blog/boj-problems-7576/","summary":"문제 링크 https://www.acmicpc.net/problem/7576 문제 해설 Idea BFS를 활용한 시뮬레이션을 통해 모든 토마토가 익을 떄까지 걸리는 최소 기간을 계산 초기엔 안익은 토마토의 기준에서 매번 익은 토마토까지의 최단거리를 탐색하여,\nO(N^4)의 시간 복잡도로 시간 초과가 발생 이후 익은 토마토의 기준에서 시뮬레이션을 단 한번만 수행하여 각각의 칸에 도달하는데 걸리는 거리값을 갱신 모두 익지 못하는 상황에 대해 1안에선 에러를 발생시켜 처리했고, 2안에선 종료 코드를 실행해 처리 Time Complexity O(N^2) = 1,000,000 Data Size M,N: 2 \u0026lt;= int \u0026lt;= 1,000 t in [1,0,-1] 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 from collections import deque import sys input = sys.","title":"[백준 7576] 토마토 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/18870 문제 해설 Idea Sort 집합을 통해 압축한 unique한 좌표 목록을 정렬시키고,\n정렬된 리스트 내에서 좌표와 인덱스를 딕셔너리로 맵핑 Time Complexity O(N Log N) = 13,000,000 Data Size N: 1 \u0026lt;= int \u0026lt;= 1,000,000 X: -10^9 \u0026lt;= int \u0026lt;= 10^9 해설 코드 1 2 3 4 N = int(input()) X = list(map(int, input().split())) xtoi = {x:i for i,x in enumerate(sorted(set(X)))} print(\u0026#39; \u0026#39;.join(map(lambda x: str(xtoi[x]), X))) ","permalink":"https://minyeamer.github.io/blog/boj-problems-18870/","summary":"문제 링크 https://www.acmicpc.net/problem/18870 문제 해설 Idea Sort 집합을 통해 압축한 unique한 좌표 목록을 정렬시키고,\n정렬된 리스트 내에서 좌표와 인덱스를 딕셔너리로 맵핑 Time Complexity O(N Log N) = 13,000,000 Data Size N: 1 \u0026lt;= int \u0026lt;= 1,000,000 X: -10^9 \u0026lt;= int \u0026lt;= 10^9 해설 코드 1 2 3 4 N = int(input()) X = list(map(int, input().split())) xtoi = {x:i for i,x in enumerate(sorted(set(X)))} print(\u0026#39; \u0026#39;.join(map(lambda x: str(xtoi[x]), X))) ","title":"[백준 18870] 좌표 압축 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/1931 문제 해설 Idea Sliding Window 슬라이딩 윈도우의 전형적인 문제로, 끝 시간을 기준으로 시간을 정렬해서 겹치지 않는 수를 계산 Time Complexity O(N) = 100,000 Data Size N: 1 \u0026lt;= int \u0026lt;= 100,000 t1,t2: 0 \u0026lt;= int \u0026lt;= 2^31-1 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 import sys input = sys.stdin.readline N = int(input()) times = sorted([tuple(map(int, input().split())) for _ in range(N)], key=lambda x: [x[1],x[0]]) count, end_time = 0, 0 for t1,t2 in times: if t1 \u0026gt;= end_time: count += 1 end_time = t2 print(count) ","permalink":"https://minyeamer.github.io/blog/boj-problems-1931/","summary":"문제 링크 https://www.acmicpc.net/problem/1931 문제 해설 Idea Sliding Window 슬라이딩 윈도우의 전형적인 문제로, 끝 시간을 기준으로 시간을 정렬해서 겹치지 않는 수를 계산 Time Complexity O(N) = 100,000 Data Size N: 1 \u0026lt;= int \u0026lt;= 100,000 t1,t2: 0 \u0026lt;= int \u0026lt;= 2^31-1 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 import sys input = sys.stdin.readline N = int(input()) times = sorted([tuple(map(int, input().split())) for _ in range(N)], key=lambda x: [x[1],x[0]]) count, end_time = 0, 0 for t1,t2 in times: if t1 \u0026gt;= end_time: count += 1 end_time = t2 print(count) ","title":"[백준 1931] 회의실 배정 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/15686 문제 해설 Idea Combinations 최대 집의 개수가 100, 최대 치킨집의 개수가 13으로 매우 적은 경우의 수를 가지고 있기 때문에,\n모든 조합에 대한 완전탐색을 통해 최소 거리를 계산 초기에는 집에 대한 치킨 거리가 작은 치킨집을 우선적으로 선발해서,\n폐업하지 않은 치킨집에 대한 치킨 거리의 최소 합을 계산했지만 틀림 이후 combinations 모듈을 활용한 완전탐색을 통해 통과 Time Complexity O(N * nCr) ~ 100,000 Data Size N: 2 \u0026lt;= int \u0026lt;= 50 M: 1 \u0026lt;= int \u0026lt;= 13 cell in (0, 1, 2) count(house): 1 \u0026lt;= int \u0026lt; 2N count(chicken): M \u0026lt;= int \u0026lt;= 13 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 from itertools import combinations import sys input = sys.stdin.readline N, M = map(int, input().split()) city = {\u0026#39;0\u0026#39;:list(),\u0026#39;1\u0026#39;:list(),\u0026#39;2\u0026#39;:list()} for r in range(N): for c,i in enumerate(input().split()): city[i].append((r,c)) houses, chickens = city[\u0026#39;1\u0026#39;], city[\u0026#39;2\u0026#39;] diff = lambda c1,c2: abs(c1[0]-c2[0])+abs(c1[1]-c2[1]) # =============== 1안 (틀림) =============== house_cost = {chicken:0 for chicken in chickens} chicken_cost = {house:house_cost.copy() for house in houses} for house in houses: for chicken in chickens: cost = diff(house,chicken) chicken_cost[house][chicken] = cost house_cost[chicken] += cost city_cost = 0 open = [chicken for chicken,cost in sorted(house_cost.items(), key=lambda x: x[1])[:M]] for house, costs in chicken_cost.items(): city_cost += min([cost for chicken,cost in costs.items() if chicken in open]) # =============== 2안 (통과) =============== city_cost = sys.maxsize for comb in combinations(chickens, M): cost = 0 for house in houses: cost += min([diff(house,chicken) for chicken in comb]) city_cost = min(city_cost,cost) print(city_cost) ","permalink":"https://minyeamer.github.io/blog/boj-problems-15686/","summary":"문제 링크 https://www.acmicpc.net/problem/15686 문제 해설 Idea Combinations 최대 집의 개수가 100, 최대 치킨집의 개수가 13으로 매우 적은 경우의 수를 가지고 있기 때문에,\n모든 조합에 대한 완전탐색을 통해 최소 거리를 계산 초기에는 집에 대한 치킨 거리가 작은 치킨집을 우선적으로 선발해서,\n폐업하지 않은 치킨집에 대한 치킨 거리의 최소 합을 계산했지만 틀림 이후 combinations 모듈을 활용한 완전탐색을 통해 통과 Time Complexity O(N * nCr) ~ 100,000 Data Size N: 2 \u0026lt;= int \u0026lt;= 50 M: 1 \u0026lt;= int \u0026lt;= 13 cell in (0, 1, 2) count(house): 1 \u0026lt;= int \u0026lt; 2N count(chicken): M \u0026lt;= int \u0026lt;= 13 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 from itertools import combinations import sys input = sys.","title":"[백준 15686] 치킨 배달 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/1927 문제 해설 Idea Heapq 파이썬 heapq 모듈 자체가 최소힙이기 때문에 해당하는 기능을 활용하여 구현 Time Complexity O(Log N) = 16 Data Size N: 1 \u0026lt;= int \u0026lt;= 100,000 x: 0 \u0026lt;= int \u0026lt; 2^31 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import heapq import sys input = sys.stdin.readline N = int(input()) arr = list() for _ in range(N): x = int(input()) if x \u0026gt; 0: heapq.heappush(arr, x) else: if arr: print(heapq.heappop(arr)) else: print(0) ","permalink":"https://minyeamer.github.io/blog/boj-problems-1927/","summary":"문제 링크 https://www.acmicpc.net/problem/1927 문제 해설 Idea Heapq 파이썬 heapq 모듈 자체가 최소힙이기 때문에 해당하는 기능을 활용하여 구현 Time Complexity O(Log N) = 16 Data Size N: 1 \u0026lt;= int \u0026lt;= 100,000 x: 0 \u0026lt;= int \u0026lt; 2^31 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import heapq import sys input = sys.stdin.readline N = int(input()) arr = list() for _ in range(N): x = int(input()) if x \u0026gt; 0: heapq.","title":"[백준 1927] 최소 힙 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/1780 문제 해설 Idea Divide and Conquer 2차원 배열의 요소를 완전탐색하면서 동일한 값으로 구성되지 않을 경우,\n행렬을 9등분하여 재귀적 호출 수행 처음 시도에서는 행렬을 매번 슬라이싱하면서 전달하여 시간 초과가 발생 행렬의 시작 인덱스 번호를 전달하고 길이만큼 참조하는 방식으로 시간 복잡도 개선 Data Size N: 1 \u0026lt;= int \u0026lt;= 3^7 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import sys input = sys.stdin.readline value2id = {-1:0,0:1,1:2} # ============== 1안 (시간초과) ============= from itertools import chain mat_slice = lambda mat,r1,r2,c1,c2: list(map(lambda x: x[c1:c2], mat[r1:r2])) def nona_comp(n, arr, answer): values = set(chain.from_iterable(arr)) if len(values) == 1: answer[value2id[values.pop()]] += 1 return div = n//3 for i in range(3): for j in range(3): nona_comp(div, mat_slice(arr,div*i,div*(i+1),div*j,div*(j+1)), answer) # =============== 2안 (통과) =============== def nona_comp(n, arr, r, c, answer): start = arr[r][c] for row in range(r,r+n): for col in range(c,c+n): if arr[row][col] != start: div = n//3 for i in range(3): for j in range(3): nona_comp(div, arr, r+div*i, c+div*j, answer) return answer[value2id[start]] += 1 N = int(input()) arr = [list(map(int, input().split())) for _ in range(N)] answer = [0, 0, 0] nona_comp(len(arr), arr, 0, 0, answer) for num in answer: print(num) ","permalink":"https://minyeamer.github.io/blog/boj-problems-1780/","summary":"문제 링크 https://www.acmicpc.net/problem/1780 문제 해설 Idea Divide and Conquer 2차원 배열의 요소를 완전탐색하면서 동일한 값으로 구성되지 않을 경우,\n행렬을 9등분하여 재귀적 호출 수행 처음 시도에서는 행렬을 매번 슬라이싱하면서 전달하여 시간 초과가 발생 행렬의 시작 인덱스 번호를 전달하고 길이만큼 참조하는 방식으로 시간 복잡도 개선 Data Size N: 1 \u0026lt;= int \u0026lt;= 3^7 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 import sys input = sys.","title":"[백준 1780] 종이의 개수 (Python)"},{"content":"문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/77486 문제 해설 Idea Union-Find 알고리즘의 Find() 함수를 사용하여 부모 노드에 대해 재귀적으로 접근 최악의 경우 O(NM)=10^10으로 시간 초과가 발생하지만, 매 탐색마다 최대 10,000원을 10씩 나눠 0이 되는 순간에 재귀가 종료되기 때문에 최대 깊이가 5로 좁혀짐 Time Complexity O(N) = 100,000 Data Size enroll, referral: str(10) * 10,000 seller: str(10) * 100,000 amount: int(100) * 100,000 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def find(parents, cur, income, answer): alloc = income//10 if parents[cur] == cur or alloc == 0: answer[cur] += income-alloc return answer[cur] += income-alloc find(parents, parents[cur], alloc, answer) return def solution(enroll, referral, seller, amount): N = len(enroll) answer = [0] * N name2id = {name:i for i,name in enumerate(enroll)} parents = [i if referral[i]==\u0026#39;-\u0026#39; else name2id[referral[i]] for i in range(N)] for i in range(len(seller)): find(parents, name2id[seller[i]], amount[i]*100, answer) return answer ","permalink":"https://minyeamer.github.io/blog/programmers-problems-77486/","summary":"문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/77486 문제 해설 Idea Union-Find 알고리즘의 Find() 함수를 사용하여 부모 노드에 대해 재귀적으로 접근 최악의 경우 O(NM)=10^10으로 시간 초과가 발생하지만, 매 탐색마다 최대 10,000원을 10씩 나눠 0이 되는 순간에 재귀가 종료되기 때문에 최대 깊이가 5로 좁혀짐 Time Complexity O(N) = 100,000 Data Size enroll, referral: str(10) * 10,000 seller: str(10) * 100,000 amount: int(100) * 100,000 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def find(parents, cur, income, answer): alloc = income//10 if parents[cur] == cur or alloc == 0: answer[cur] += income-alloc return answer[cur] += income-alloc find(parents, parents[cur], alloc, answer) return def solution(enroll, referral, seller, amount): N = len(enroll) answer = [0] * N name2id = {name:i for i,name in enumerate(enroll)} parents = [i if referral[i]==\u0026#39;-\u0026#39; else name2id[referral[i]] for i in range(N)] for i in range(len(seller)): find(parents, name2id[seller[i]], amount[i]*100, answer) return answer ","title":"[프로그래머스 77486] 다단계 칫솔 판매 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/1182 문제 해설 Idea Brute Force 전체 배열에서 1부터 N개의 부분 조합을 완전탐색하면서 합이 S와 같은 경우를 카운트하고 출력 Data Size N: 1 \u0026lt;= int \u0026lt;= 20 S: abs(int) \u0026lt;= 1,000,000 arr: int(100,000) * N 해설 코드 1 2 3 4 5 6 7 8 9 10 11 from itertools import combinations N, S = map(int, input().split()) arr = list(map(int, input().split())) count = 0 for i in range(1,N+1): comb = combinations(arr, i) count += sum(map(lambda x: sum(x)==S, comb)) print(count) ","permalink":"https://minyeamer.github.io/blog/boj-problems-1182/","summary":"문제 링크 https://www.acmicpc.net/problem/1182 문제 해설 Idea Brute Force 전체 배열에서 1부터 N개의 부분 조합을 완전탐색하면서 합이 S와 같은 경우를 카운트하고 출력 Data Size N: 1 \u0026lt;= int \u0026lt;= 20 S: abs(int) \u0026lt;= 1,000,000 arr: int(100,000) * N 해설 코드 1 2 3 4 5 6 7 8 9 10 11 from itertools import combinations N, S = map(int, input().split()) arr = list(map(int, input().split())) count = 0 for i in range(1,N+1): comb = combinations(arr, i) count += sum(map(lambda x: sum(x)==S, comb)) print(count) ","title":"[백준 1182] 부분수열의 합 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/11725 문제 해설 Idea BFS 1번 노드부터 BFS를 수행하면서 다음 노드에 순차적으로 접근 다음 노드가 이미 방문한 노드의 경우 부모 노드라 판단하여 배열에 저장 부모 노드가 저장된 배열에 대해 2번 노드부터 순차적으로 부모 노드를 출력 Time Complexity O(N+E) = 200,000 Data Size N: 2 \u0026lt;= int \u0026lt;= 100,000 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 from collections import deque import sys input = sys.stdin.readline N = int(input()) graph = [[] for _ in range(N+1)] visited = [False] * (N+1) parents = [1] * (N+1) for _ in range(N-1): u, v = map(int, input().split()) graph[u].append(v) graph[v].append(u) queue = deque([1]) visited[1] = True while queue: node = queue.popleft() for next in graph[node]: if not visited[next]: queue.append(next) visited[next] = True else: parents[node] = next for parent in parents[2:]: print(parent) ","permalink":"https://minyeamer.github.io/blog/boj-problems-11725/","summary":"문제 링크 https://www.acmicpc.net/problem/11725 문제 해설 Idea BFS 1번 노드부터 BFS를 수행하면서 다음 노드에 순차적으로 접근 다음 노드가 이미 방문한 노드의 경우 부모 노드라 판단하여 배열에 저장 부모 노드가 저장된 배열에 대해 2번 노드부터 순차적으로 부모 노드를 출력 Time Complexity O(N+E) = 200,000 Data Size N: 2 \u0026lt;= int \u0026lt;= 100,000 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 from collections import deque import sys input = sys.","title":"[백준 11725] 트리의 부모 찾기 (Python)"},{"content":"문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/68936 문제 해설 Idea Divide and Conquer 2차원 배열을 4등분씩 나눠 재귀함수를 호출하고 동일한 값으로 채워져 있는지 판단하여 값의 개수 증가 2^n 형태의 정수에 대해 NumPy를 활용해 행렬 인덱싱을 간단히 구현 Time Complexity O(N^2 Log N^2) = 20,000,000 Data Size arr: [[int(1)]], shape(2^n, 2^n) 1 \u0026lt;= 2^n \u0026lt;= 1024 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import numpy as np def quad_comp(n, arr, answer): values = np.unique(arr) if len(values) == 1: answer[values[0]] += 1 return div = n//2 quad_comp(div, arr[:div, :div], answer) quad_comp(div, arr[:div, div:], answer) quad_comp(div, arr[div:, :div], answer) quad_comp(div, arr[div:, div:], answer) def solution(arr): arr = np.array(arr) answer = [0,0] quad_comp(len(arr), arr, answer) return answer ","permalink":"https://minyeamer.github.io/blog/programmers-problems-68936/","summary":"문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/68936 문제 해설 Idea Divide and Conquer 2차원 배열을 4등분씩 나눠 재귀함수를 호출하고 동일한 값으로 채워져 있는지 판단하여 값의 개수 증가 2^n 형태의 정수에 대해 NumPy를 활용해 행렬 인덱싱을 간단히 구현 Time Complexity O(N^2 Log N^2) = 20,000,000 Data Size arr: [[int(1)]], shape(2^n, 2^n) 1 \u0026lt;= 2^n \u0026lt;= 1024 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import numpy as np def quad_comp(n, arr, answer): values = np.","title":"[프로그래머스 68936] 쿼드압축 후 개수 세기 (Python)"},{"content":"문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/87390 문제 해설 Idea Greedy n의 크기가 굉장히 크기 때문에 2차원 배열을 만드는 것만으로 시간 초과가 발생할 것을 예상 r행 c열의 값은 max(r,c)+1과 같고 1차원 배열의 인덱스 i에 대해 r은 i//n, c는 i%n와 동일 left부터 right까지의 인덱스를 규칙에 맞는 값으로 변환하여 반환 Time Complexity O(N) = 10^5 Data Size n: 1 \u0026lt;= int \u0026lt;= 10^7 left, right: 0 \u0026lt;= long \u0026lt;= n^2 right - left \u0026lt; 10^5 해설 코드 1 2 def solution(n, left, right): return [max(divmod(i,n))+1 for i in range(left,right+1)] ","permalink":"https://minyeamer.github.io/blog/programmers-problems-87390/","summary":"문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/87390 문제 해설 Idea Greedy n의 크기가 굉장히 크기 때문에 2차원 배열을 만드는 것만으로 시간 초과가 발생할 것을 예상 r행 c열의 값은 max(r,c)+1과 같고 1차원 배열의 인덱스 i에 대해 r은 i//n, c는 i%n와 동일 left부터 right까지의 인덱스를 규칙에 맞는 값으로 변환하여 반환 Time Complexity O(N) = 10^5 Data Size n: 1 \u0026lt;= int \u0026lt;= 10^7 left, right: 0 \u0026lt;= long \u0026lt;= n^2 right - left \u0026lt; 10^5 해설 코드 1 2 def solution(n, left, right): return [max(divmod(i,n))+1 for i in range(left,right+1)] ","title":"[프로그래머스 87390] n^2 배열 자르기 (Python)"},{"content":"문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/17687 문제 해설 Idea Math 0부터 시작해 t*m의 길이를 만족하는 N진법 배열을 생성 매 순서마다 p 위치에 해당하는 값을 추출해 문자열로 반환 Data Size n: 2 \u0026lt;= int \u0026lt;= 16 t: 0 \u0026lt; int \u0026lt;= 1,000 m: 2 \u0026lt;= int \u0026lt;= 100 p: 1 \u0026lt;= int \u0026lt;= m 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 alpha = {10:\u0026#39;A\u0026#39;,11:\u0026#39;B\u0026#39;,12:\u0026#39;C\u0026#39;,13:\u0026#39;D\u0026#39;,14:\u0026#39;E\u0026#39;,15:\u0026#39;F\u0026#39;} def n_base(num, base): result = str() while num \u0026gt; 0: num, mod = divmod(num, base) result += str(mod) if mod \u0026lt; 10 else alpha[mod] return result[::-1] def solution(n, t, m, p): arr = \u0026#39;01\u0026#39; total = t*m p = p%m i = 2 while len(arr) \u0026lt; total: arr += n_base(i, n) i += 1 answer = [t for i,t in enumerate(arr[:total]) if (i+1)%m==p] return \u0026#39;\u0026#39;.join(answer) ","permalink":"https://minyeamer.github.io/blog/programmers-problems-17687/","summary":"문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/17687 문제 해설 Idea Math 0부터 시작해 t*m의 길이를 만족하는 N진법 배열을 생성 매 순서마다 p 위치에 해당하는 값을 추출해 문자열로 반환 Data Size n: 2 \u0026lt;= int \u0026lt;= 16 t: 0 \u0026lt; int \u0026lt;= 1,000 m: 2 \u0026lt;= int \u0026lt;= 100 p: 1 \u0026lt;= int \u0026lt;= m 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 alpha = {10:\u0026#39;A\u0026#39;,11:\u0026#39;B\u0026#39;,12:\u0026#39;C\u0026#39;,13:\u0026#39;D\u0026#39;,14:\u0026#39;E\u0026#39;,15:\u0026#39;F\u0026#39;} def n_base(num, base): result = str() while num \u0026gt; 0: num, mod = divmod(num, base) result += str(mod) if mod \u0026lt; 10 else alpha[mod] return result[::-1] def solution(n, t, m, p): arr = \u0026#39;01\u0026#39; total = t*m p = p%m i = 2 while len(arr) \u0026lt; total: arr += n_base(i, n) i += 1 answer = [t for i,t in enumerate(arr[:total]) if (i+1)%m==p] return \u0026#39;\u0026#39;.","title":"[프로그래머스/카카오 17687] n진수 게임 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/1676 문제 해설 Idea Math 팩토리얼 수를 구하고 문자열로 변환해 연속되는 0의 개수를 출력 Data Size N: 0 \u0026lt;= int \u0026lt;= 500 해설 코드 1 2 3 4 5 6 7 8 9 from math import factorial import re N = int(input()) zeros = re.findall(\u0026#39;0+\u0026#39;, str(factorial(N))) if zeros: print(len(zeros[-1])) else: print(0) ","permalink":"https://minyeamer.github.io/blog/boj-problems-1676/","summary":"문제 링크 https://www.acmicpc.net/problem/1676 문제 해설 Idea Math 팩토리얼 수를 구하고 문자열로 변환해 연속되는 0의 개수를 출력 Data Size N: 0 \u0026lt;= int \u0026lt;= 500 해설 코드 1 2 3 4 5 6 7 8 9 from math import factorial import re N = int(input()) zeros = re.findall(\u0026#39;0+\u0026#39;, str(factorial(N))) if zeros: print(len(zeros[-1])) else: print(0) ","title":"[백준 1676] 팩토리얼 0의 개수 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/1541 문제 해설 Idea Greedy 최솟값을 만들기 위해서는 \u0026lsquo;-\u0026lsquo;를 기준으로 괄호를 치는 것이 최선 \u0026lsquo;-\u0026lsquo;를 기준으로 식을 나누고 구분된 식을 계산하여 결과를 출력 Data Size arr: str(50) 해설 코드 1 2 3 4 5 arr = input().split(\u0026#39;-\u0026#39;) answer = sum(map(int,arr[0].split(\u0026#39;+\u0026#39;))) for i in arr[1:]: answer -= sum(map(int,i.split(\u0026#39;+\u0026#39;))) print(answer) ","permalink":"https://minyeamer.github.io/blog/boj-problems-1541/","summary":"문제 링크 https://www.acmicpc.net/problem/1541 문제 해설 Idea Greedy 최솟값을 만들기 위해서는 \u0026lsquo;-\u0026lsquo;를 기준으로 괄호를 치는 것이 최선 \u0026lsquo;-\u0026lsquo;를 기준으로 식을 나누고 구분된 식을 계산하여 결과를 출력 Data Size arr: str(50) 해설 코드 1 2 3 4 5 arr = input().split(\u0026#39;-\u0026#39;) answer = sum(map(int,arr[0].split(\u0026#39;+\u0026#39;))) for i in arr[1:]: answer -= sum(map(int,i.split(\u0026#39;+\u0026#39;))) print(answer) ","title":"[백준 1541] 잃어버린 괄호 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/1389 문제 해설 Idea BFS 1부터 N까지의 번호에 대해 매번 BFS를 수행하면서 다른 모든 노드와의 거리를 파악 가장 작은 거리의 합을 가진 노드의 인덱스 번호를 출력 Time Complexity O(N^2+NM) = 510,000 Data Size N: 2 \u0026lt;= int \u0026lt;= 100 M: 1 \u0026lt;= int \u0026lt;= 5,000 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from collections import deque import sys input = sys.stdin.readline def bfs(target, nodes): num = [0] * (N+1) queue = deque([target]) visited = [False] * (N+1) visited[target] = True while queue: node = queue.popleft() for next in nodes[node]: if not visited[next]: num[next] = num[node]+1 visited[next] = True queue.append(next) return sum(num) N, M = map(int, input().split()) rels = [list() for _ in range(N+1)] kevin = [0] * (N+1) for _ in range(M): A, B = map(int, input().split()) rels[A].append(B) rels[B].append(A) for i in range(1,N+1): kevin[i] = bfs(i, rels) print(kevin.index(min(kevin[1:]))) ","permalink":"https://minyeamer.github.io/blog/boj-problems-1389/","summary":"문제 링크 https://www.acmicpc.net/problem/1389 문제 해설 Idea BFS 1부터 N까지의 번호에 대해 매번 BFS를 수행하면서 다른 모든 노드와의 거리를 파악 가장 작은 거리의 합을 가진 노드의 인덱스 번호를 출력 Time Complexity O(N^2+NM) = 510,000 Data Size N: 2 \u0026lt;= int \u0026lt;= 100 M: 1 \u0026lt;= int \u0026lt;= 5,000 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from collections import deque import sys input = sys.","title":"[백준 1389] 케빈 베이컨의 6단계 법칙 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/5430 문제 해설 Idea Implementation, Deque 문제에서 주어진대로 매번 배열을 뒤집으면 O(N^2)의 시간 복잡도로 시간 초과가 발생 배열에 영향을 주지 않으면서 R 함수를 처리하기 위해 상태 변수를 정의하고,\nD 함수가 호출될 경우 배열의 상태에 따라 첫 번째 수를 버릴지 마지막 수를 버릴지 결정 마지막에 배열의 상태를 업데이트하고 정해진 형태로 결과를 출력 Time Complexity O(N) = 100,000 Data Size T: 1 \u0026lt;= int \u0026lt;= 100 p: 1 \u0026lt;= int \u0026lt;= 100,000 n: 1 \u0026lt;= int \u0026lt;= 100,000 arr: int(100) * n (like [x_1,\u0026hellip;,x_n]) 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from collections import deque for _ in range(int(input())): p = input() n = int(input()) arr = deque(eval(input())) forward = True try: for cmd in p: if cmd == \u0026#39;R\u0026#39;: forward = not forward elif cmd == \u0026#39;D\u0026#39;: if forward: arr.popleft() else: arr.pop() arr = map(str,arr) if forward else map(str,reversed(arr)) print(f\u0026#39;[{\u0026#34;,\u0026#34;.join(map(str,arr))}]\u0026#39;) except IndexError: print(\u0026#39;error\u0026#39;) ","permalink":"https://minyeamer.github.io/blog/boj-problems-5430/","summary":"문제 링크 https://www.acmicpc.net/problem/5430 문제 해설 Idea Implementation, Deque 문제에서 주어진대로 매번 배열을 뒤집으면 O(N^2)의 시간 복잡도로 시간 초과가 발생 배열에 영향을 주지 않으면서 R 함수를 처리하기 위해 상태 변수를 정의하고,\nD 함수가 호출될 경우 배열의 상태에 따라 첫 번째 수를 버릴지 마지막 수를 버릴지 결정 마지막에 배열의 상태를 업데이트하고 정해진 형태로 결과를 출력 Time Complexity O(N) = 100,000 Data Size T: 1 \u0026lt;= int \u0026lt;= 100 p: 1 \u0026lt;= int \u0026lt;= 100,000 n: 1 \u0026lt;= int \u0026lt;= 100,000 arr: int(100) * n (like [x_1,\u0026hellip;,x_n]) 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from collections import deque for _ in range(int(input())): p = input() n = int(input()) arr = deque(eval(input())) forward = True try: for cmd in p: if cmd == \u0026#39;R\u0026#39;: forward = not forward elif cmd == \u0026#39;D\u0026#39;: if forward: arr.","title":"[백준 5430] AC (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/1463 문제 해설 Idea Dynamic Programming N에 대해 조건을 만족하는 경우에서 3으로 나누기, 2로 나누기, 1을 빼는 연산을 반복 수행하고\n각각의 연산횟수 별로 도출할 수 있는 값을 모두 저장 앞선 결과를 모두 활용해 다음 결과에 대한 모든 경우를 탐색하고 결과 집합에 1이 있을 시 탐색을 종료 1이 포함된 마지막 집합의 인덱스 번호를 최소 연산횟수로 출력 Data Size N: 1 \u0026lt;= int \u0026lt;= 10^6 해설 코드 1 2 3 4 5 6 7 8 9 10 11 N = int(input()) dp = [{N,}] while 1 not in dp[-1]: dp.append(set()) for n in dp[-2]: if n % 3 == 0: dp[-1].add(n//3) if n % 2 == 0: dp[-1].add(n//2) dp[-1].add(n-1) print(len(dp)-1) ","permalink":"https://minyeamer.github.io/blog/boj-problems-1463/","summary":"문제 링크 https://www.acmicpc.net/problem/1463 문제 해설 Idea Dynamic Programming N에 대해 조건을 만족하는 경우에서 3으로 나누기, 2로 나누기, 1을 빼는 연산을 반복 수행하고\n각각의 연산횟수 별로 도출할 수 있는 값을 모두 저장 앞선 결과를 모두 활용해 다음 결과에 대한 모든 경우를 탐색하고 결과 집합에 1이 있을 시 탐색을 종료 1이 포함된 마지막 집합의 인덱스 번호를 최소 연산횟수로 출력 Data Size N: 1 \u0026lt;= int \u0026lt;= 10^6 해설 코드 1 2 3 4 5 6 7 8 9 10 11 N = int(input()) dp = [{N,}] while 1 not in dp[-1]: dp.","title":"[백준 1463] 1로 만들기 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/1697 문제 해설 Idea BFS N에서 시작해 K에 도달할 때까지 x-1, x+1, x*2에 대한 최단거리를 탐색 두 점이 위치할 수 있는 범위 내에서 가까운 거리의 점부터 탐색을 수행 K에 대한 거리를 출력 N이 K보다 클 경우 x-1 외에는 이동수단이 없기 때문에 시간 단축을 위해 예외로 처리 Time Complexity O(N) = 100,000 Data Size N: 0 \u0026lt;= int \u0026lt;= 100,000 K: 0 \u0026lt;= int \u0026lt;= 100,000 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from collections import deque def bfs(start, target): MAX = 10**5 count = [0] * (MAX+1) queue = deque([start]) while queue: x = queue.popleft() if x == target: return count[x] for nx in (x-1,x+1,x*2): if 0 \u0026lt;= nx \u0026lt;= MAX and not count[nx]: count[nx] = count[x] + 1 queue.append(nx) N, K = map(int, input().split()) if N \u0026gt;= K: print(N - K) else: print(bfs(N, K)) ","permalink":"https://minyeamer.github.io/blog/boj-problems-1697/","summary":"문제 링크 https://www.acmicpc.net/problem/1697 문제 해설 Idea BFS N에서 시작해 K에 도달할 때까지 x-1, x+1, x*2에 대한 최단거리를 탐색 두 점이 위치할 수 있는 범위 내에서 가까운 거리의 점부터 탐색을 수행 K에 대한 거리를 출력 N이 K보다 클 경우 x-1 외에는 이동수단이 없기 때문에 시간 단축을 위해 예외로 처리 Time Complexity O(N) = 100,000 Data Size N: 0 \u0026lt;= int \u0026lt;= 100,000 K: 0 \u0026lt;= int \u0026lt;= 100,000 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from collections import deque def bfs(start, target): MAX = 10**5 count = [0] * (MAX+1) queue = deque([start]) while queue: x = queue.","title":"[백준 1697] 숨바꼭질 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/20922 문제 해설 Idea Two Pointer 수열의 시작과 끝 지점에 대한 두 개의 포인터 지정 끝 지점에 대한 포인터를 확장하면서 탐색되는 원소의 수를 카운트 원소의 수가 K개와 같아지는 시점부터 시작 지점에 대한 포인터를 확장하여 범위 축소 최종적으로 두 포인터 간 거리의 최대치를 출력 Time Complexity O(N) = 200,000 Data Size N: 1 \u0026lt;= int \u0026lt;= 200,000 K: 1 \u0026lt;= int \u0026lt;= 100 a: int(100,000) * N 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 N, K = map(int, input().split()) a = list(map(int, input().split())) answer = 0 start, end = 0, 0 counter = [0] * (max(a)+1) while end \u0026lt; N: if counter[a[end]] \u0026lt; K: counter[a[end]] += 1 end += 1 else: counter[a[start]] -= 1 start += 1 answer = max(end-start, answer) print(answer) ","permalink":"https://minyeamer.github.io/blog/boj-problems-20922/","summary":"문제 링크 https://www.acmicpc.net/problem/20922 문제 해설 Idea Two Pointer 수열의 시작과 끝 지점에 대한 두 개의 포인터 지정 끝 지점에 대한 포인터를 확장하면서 탐색되는 원소의 수를 카운트 원소의 수가 K개와 같아지는 시점부터 시작 지점에 대한 포인터를 확장하여 범위 축소 최종적으로 두 포인터 간 거리의 최대치를 출력 Time Complexity O(N) = 200,000 Data Size N: 1 \u0026lt;= int \u0026lt;= 200,000 K: 1 \u0026lt;= int \u0026lt;= 100 a: int(100,000) * N 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 N, K = map(int, input().","title":"[백준 20922] 겹치는 건 싫어 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/22859 문제 해설 Idea Implementation, String , , 태그 등을 구분 의 attribute인 title을 우선 출력하고 안에 있는 를 한 줄씩 출력 안에 있는 태그와 시작과 끝에 있는 공백을 지우고 2개 이상의 공백을 하나로 변경 제목은 무조건 존재하고 태그 사이에는 공백이 없으며 태그는 올바른 쌍으로만 주어짐을 보장 정규 표현식을 활용해 조건에 맞는 문장을 파싱하고 불필요한 문자를 제거해 출력 Data Size source: str(1,000,000) 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import re source = input() main = re.findall(\u0026#39;\u0026lt;main\u0026gt;(.*)\u0026lt;/main\u0026gt;\u0026#39;, source)[0] div_list = re.findall(\u0026#39;\u0026lt;div title=\u0026#34;(.*?)\u0026#34;\u0026gt;(.*?)\u0026lt;/div\u0026gt;\u0026#39;, main) for title, paragraph in div_list: print(\u0026#39;title :\u0026#39;, title) p_list = re.findall(\u0026#39;\u0026lt;p\u0026gt;(.*?)\u0026lt;/p\u0026gt;\u0026#39;, paragraph) for p in p_list: p = re.sub(\u0026#39;(\u0026lt;.*?\u0026gt;)\u0026#39;, \u0026#39;\u0026#39;, p) p = re.sub(\u0026#39;\\s+\u0026#39;, \u0026#39; \u0026#39;, p.strip()) print(p) ","permalink":"https://minyeamer.github.io/blog/boj-problems-22859/","summary":"문제 링크 https://www.acmicpc.net/problem/22859 문제 해설 Idea Implementation, String , , 태그 등을 구분 의 attribute인 title을 우선 출력하고 안에 있는 를 한 줄씩 출력 안에 있는 태그와 시작과 끝에 있는 공백을 지우고 2개 이상의 공백을 하나로 변경 제목은 무조건 존재하고 태그 사이에는 공백이 없으며 태그는 올바른 쌍으로만 주어짐을 보장 정규 표현식을 활용해 조건에 맞는 문장을 파싱하고 불필요한 문자를 제거해 출력 Data Size source: str(1,000,000) 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import re source = input() main = re.","title":"[백준 22859] HTML 파싱 (Python)"},{"content":"문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/43238 문제 해설 Idea Binary Search answer에 대한 이진탐색 수행 (1 \u0026lt;= answer \u0026lt;= max(times)*n) 매 탐색마다 answer 시간 동안 심사관들이 심사할 수 있는 사람의 수를 계산 심사한 사람의 수가 n명 이상일 경우 최대 범위를 조정하고 재탐색 심사한 사람의 수가 n명 미만일 경우 최소 범위를 조정하고 재탐색 n명 이상의 사람을 심사할 수 있는 가장 작은 answer를 반환 Time Complexity Binary Search: O(M Log N^N) = 6,000,000 Data Size n: 1 \u0026lt;= int \u0026lt;= 1,000,000,000 times: int(1,000,000,000) * 100,000 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def solution(n, times): answer = 0 start, end = 1, max(times)*n while start \u0026lt;= end: mid = (start+end)//2 passed = 0 for time in times: passed += mid // time if passed \u0026gt;= n: break if passed \u0026gt;= n: answer = mid end = mid-1 elif passed \u0026lt; n: start = mid+1 return answer ","permalink":"https://minyeamer.github.io/blog/programmers-problems-43238/","summary":"문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/43238 문제 해설 Idea Binary Search answer에 대한 이진탐색 수행 (1 \u0026lt;= answer \u0026lt;= max(times)*n) 매 탐색마다 answer 시간 동안 심사관들이 심사할 수 있는 사람의 수를 계산 심사한 사람의 수가 n명 이상일 경우 최대 범위를 조정하고 재탐색 심사한 사람의 수가 n명 미만일 경우 최소 범위를 조정하고 재탐색 n명 이상의 사람을 심사할 수 있는 가장 작은 answer를 반환 Time Complexity Binary Search: O(M Log N^N) = 6,000,000 Data Size n: 1 \u0026lt;= int \u0026lt;= 1,000,000,000 times: int(1,000,000,000) * 100,000 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def solution(n, times): answer = 0 start, end = 1, max(times)*n while start \u0026lt;= end: mid = (start+end)//2 passed = 0 for time in times: passed += mid // time if passed \u0026gt;= n: break if passed \u0026gt;= n: answer = mid end = mid-1 elif passed \u0026lt; n: start = mid+1 return answer ","title":"[프로그래머스 43238] 입국심사 (Python)"},{"content":"문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/42895 문제 해설 Idea Dynamic Programming S[1] = {N} S[2] = {NN, N+N, N-N, N*N, N/N} S[3] = {NNN, S[2] (+,-,*,/) S[1][y], \u0026hellip;} 2부터 8까지의 범위를 가진 i와 1부터 i-1까지의 범위를 가진 j에 대해,\nS[j]와 S[i-j]의 사칙연산 결과를 S[i]에 추가하고 해당 집합이 number를 포함하는지 검증 Time Complexity DP: O(1) Data Size N: 1 \u0026lt;= int \u0026lt;= 9 number: 1 \u0026lt;= int \u0026lt;= 32,000 answer: int \u0026lt;= 8 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from itertools import product def solution(N, number): S = [set() for _ in range(9)] if N == number: return 1 else: S[1].add(N) for i in range(2,9): S[i].add(int(str(N)*i)) for j in range(1,i): for x,y in product(S[j],S[i-j]): S[i].update({x+y,x-y,x*y}) if y != 0: S[i].add(x//y) if number in S[i]: return i return -1 ","permalink":"https://minyeamer.github.io/blog/programmers-problems-42895/","summary":"문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/42895 문제 해설 Idea Dynamic Programming S[1] = {N} S[2] = {NN, N+N, N-N, N*N, N/N} S[3] = {NNN, S[2] (+,-,*,/) S[1][y], \u0026hellip;} 2부터 8까지의 범위를 가진 i와 1부터 i-1까지의 범위를 가진 j에 대해,\nS[j]와 S[i-j]의 사칙연산 결과를 S[i]에 추가하고 해당 집합이 number를 포함하는지 검증 Time Complexity DP: O(1) Data Size N: 1 \u0026lt;= int \u0026lt;= 9 number: 1 \u0026lt;= int \u0026lt;= 32,000 answer: int \u0026lt;= 8 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from itertools import product def solution(N, number): S = [set() for _ in range(9)] if N == number: return 1 else: S[1].","title":"[프로그래머스 42895] N으로 표현 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/21318 문제 해설 Idea Prefix Sum 실수한 곡에 대한 누적합을 구하고 인덱싱을 통해 특정 구간에 대한 누적합 출력 마지막 곡은 항상 성공하기 때문에 y에 대한 누적합과 y-1에 대한 누적합이 다르면 1 감소 Time Complexity Prefix Sum: O(N) = 100,000 Data Size N: 1 \u0026lt;= int \u0026lt;= 100,000 scores: int(10^9) * N Q: 1 \u0026lt;= int \u0026lt;= 100,000 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import sys input = sys.stdin.readline N = int(input()) scores = list(map(int, input().split())) Q = int(input()) fails = [0]*(N+1) for i in range(1,N+1): fails[i] = fails[i-1] + int(scores[i-1] \u0026gt; scores[min(i,N-1)]) for _ in range(Q): x, y = map(int, input().split()) answer = fails[y] - fails[x-1] if fails[y] != fails[y-1]: answer -= 1 print(answer) ","permalink":"https://minyeamer.github.io/blog/boj-problems-21318/","summary":"문제 링크 https://www.acmicpc.net/problem/21318 문제 해설 Idea Prefix Sum 실수한 곡에 대한 누적합을 구하고 인덱싱을 통해 특정 구간에 대한 누적합 출력 마지막 곡은 항상 성공하기 때문에 y에 대한 누적합과 y-1에 대한 누적합이 다르면 1 감소 Time Complexity Prefix Sum: O(N) = 100,000 Data Size N: 1 \u0026lt;= int \u0026lt;= 100,000 scores: int(10^9) * N Q: 1 \u0026lt;= int \u0026lt;= 100,000 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import sys input = sys.","title":"[백준 21318] 피아노 체조 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/16987 문제 해설 Idea Backtracking 0번째 계란부터 마지막 계란까지의 모든 경우의 수를 탐색 시간 단축을 위해 현재 계란이 깨진 경우 또는 나머지 모든 계란이 깨진 경우를 예외로 처리 한 번에 두 개 이상의 계란을 치는 경우를 방지하기 위해 계란을 친 후 원상복구 수행 Time Complexity Backtracking: O(N^N) = 16,777,216 Data Size N: 1 \u0026lt;= int \u0026lt;= 8 S, W: 1 \u0026lt;= int \u0026lt;= 300 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 N = int(input()) eggs = list() for _ in range(N): eggs.append(list(map(int, input().split()))) answer = 0 def dfs(eggs, idx): global answer if idx == N: answer = max(answer, len([s for s,w in eggs if s \u0026lt; 1])) return if eggs[idx][0] \u0026lt; 1: dfs(eggs, idx+1) return if len([s for s,w in eggs if s \u0026lt; 1]) \u0026gt;= N-1: answer = max(answer, N-1) return for target in range(N): if target != idx and eggs[target][0] \u0026gt; 0: eggs[target][0] -= eggs[idx][1] eggs[idx][0] -= eggs[target][1] dfs(eggs, idx+1) eggs[target][0] += eggs[idx][1] eggs[idx][0] += eggs[target][1] dfs(eggs, 0) print(answer) ","permalink":"https://minyeamer.github.io/blog/boj-problems-16987/","summary":"문제 링크 https://www.acmicpc.net/problem/16987 문제 해설 Idea Backtracking 0번째 계란부터 마지막 계란까지의 모든 경우의 수를 탐색 시간 단축을 위해 현재 계란이 깨진 경우 또는 나머지 모든 계란이 깨진 경우를 예외로 처리 한 번에 두 개 이상의 계란을 치는 경우를 방지하기 위해 계란을 친 후 원상복구 수행 Time Complexity Backtracking: O(N^N) = 16,777,216 Data Size N: 1 \u0026lt;= int \u0026lt;= 8 S, W: 1 \u0026lt;= int \u0026lt;= 300 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 N = int(input()) eggs = list() for _ in range(N): eggs.","title":"[백준 16987] 계란으로 계란치기 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/16918 문제 해설 Idea Simulation (or BFS) 초기에 빈 칸(.)을 0, 폭탄이 있는 칸(O)을 1로 설정 처음에 폭탄이 있는 칸의 상태를 우선 1 증가시키고, 이후 모든 칸의 상태를 1씩 증가시키는 과정 반복 매번 각 칸의 상태를 점검하면서 3을 초과할 경우 해당 위치 및 이웃 위치를 폭발 대상에 추가 폭발 대상이 존재할 경우 격자의 범위를 벗어나지 않는 범위 내에서 상태를 0으로 변환 위 과정을 N초 동안 반복하고, 0은 빈칸으로, 나머지는 O로 표시하여 출력 Time Complexity Simulation: O(N^3) = 8,000,000 Data Size R, C, N: 1 \u0026lt;= int \u0026lt;= 200 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import sys input = sys.stdin.readline R, C, N = map(int, input().split()) board = list() char2id = lambda x: 0 if x == \u0026#39;.\u0026#39; else 1 id2char = lambda x: \u0026#39;.\u0026#39; if x == 0 else \u0026#39;O\u0026#39; for _ in range(R): board.append(list(map(char2id, input().strip()))) board = [[state+1 if state \u0026gt; 0 else 0 for state in row] for row in board] for s in range(2,N+1): board = [[state+1 for state in row] for row in board] bomb = set() for i in range(R): for j in range(C): if board[i][j] \u0026gt; 3: neighbor = [(0,0),(1,0),(-1,0),(0,1),(0,-1)] [bomb.add((i+dy,j+dx)) for dy,dx in neighbor] for i,j in bomb: if 0 \u0026lt;= i \u0026lt; R and 0 \u0026lt;= j \u0026lt; C: board[i][j] = 0 for row in board: print(\u0026#39;\u0026#39;.join(list(map(id2char, row)))) ","permalink":"https://minyeamer.github.io/blog/boj-problems-16918/","summary":"문제 링크 https://www.acmicpc.net/problem/16918 문제 해설 Idea Simulation (or BFS) 초기에 빈 칸(.)을 0, 폭탄이 있는 칸(O)을 1로 설정 처음에 폭탄이 있는 칸의 상태를 우선 1 증가시키고, 이후 모든 칸의 상태를 1씩 증가시키는 과정 반복 매번 각 칸의 상태를 점검하면서 3을 초과할 경우 해당 위치 및 이웃 위치를 폭발 대상에 추가 폭발 대상이 존재할 경우 격자의 범위를 벗어나지 않는 범위 내에서 상태를 0으로 변환 위 과정을 N초 동안 반복하고, 0은 빈칸으로, 나머지는 O로 표시하여 출력 Time Complexity Simulation: O(N^3) = 8,000,000 Data Size R, C, N: 1 \u0026lt;= int \u0026lt;= 200 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import sys input = sys.","title":"[백준 16918] 봄버맨 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/2302 문제 해설 Idea Dynamic Programming 자리를 옮길 수 있는 연속되는 좌석의 수는 피보나치 수열을 따름 (S[i] = F[i+1]) VIP 좌석 번호를 기준으로 연속되는 좌석의 수를 리스트로 저장 모든 연속되는 좌석 수에 대한 피보나치 수를 곱하고 출력 Sequence S2 (1,2) -\u0026gt; (1,2), (2,1) = 2(F3)\nS3 (1,2,3) -\u0026gt; (1,2,3), (2,1,3), (1,3,2) = 3(F4)\nS4 (1,2,3,4) -\u0026gt; (1,2,3,4), (2,1,3,4), (1,2,4,3), (1,3,2,4), (2,1,4,3) = 5(F5)\nS5 (1,2,3,4,5) -\u0026gt; (1,2,3,4,5), (1,2,4,3,5), (1,2,3,5,4), (2,1,3,4,5), (2,1,4,3,5), (2,1,3,5,4), (1,3,2,4,5), (1,3,2,5,4) = 8(F6)\nS6 (1,2,3,4,5,6) -\u0026gt; (1,2,3,4,5,6), (1,2,4,3,5,6), (1,2,3,5,4,6), (1,2,3,4,6,5), (1,2,4,3,6,5), \u0026hellip;\nTime Complexity DP: O(N) = 40 Data Size N: 1 \u0026lt;= int \u0026lt;= 40 M: 0 \u0026lt;= int \u0026lt;= N answer: int \u0026lt; 2^31-1 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import sys input = sys.stdin.readline N = int(input()) M = int(input()) seats, idx = list(), 0 for _ in range(M): vip = int(input()) seats.append(vip-idx-1) idx = vip seats.append(N-idx) F = {1:1, 2:1} def fibonacci(n): if n in F: return F[n] F[n] = fibonacci(n-1) + fibonacci(n-2) return F[n] answer = 1 for seat in seats: if seat \u0026gt; 1: answer *= fibonacci(seat+1) print(answer) ","permalink":"https://minyeamer.github.io/blog/boj-problems-2302/","summary":"문제 링크 https://www.acmicpc.net/problem/2302 문제 해설 Idea Dynamic Programming 자리를 옮길 수 있는 연속되는 좌석의 수는 피보나치 수열을 따름 (S[i] = F[i+1]) VIP 좌석 번호를 기준으로 연속되는 좌석의 수를 리스트로 저장 모든 연속되는 좌석 수에 대한 피보나치 수를 곱하고 출력 Sequence S2 (1,2) -\u0026gt; (1,2), (2,1) = 2(F3)\nS3 (1,2,3) -\u0026gt; (1,2,3), (2,1,3), (1,3,2) = 3(F4)\nS4 (1,2,3,4) -\u0026gt; (1,2,3,4), (2,1,3,4), (1,2,4,3), (1,3,2,4), (2,1,4,3) = 5(F5)\nS5 (1,2,3,4,5) -\u0026gt; (1,2,3,4,5), (1,2,4,3,5), (1,2,3,5,4), (2,1,3,4,5), (2,1,4,3,5), (2,1,3,5,4), (1,3,2,4,5), (1,3,2,5,4) = 8(F6)","title":"[백준 2302] 극장 좌석 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/18352 문제 해설 Idea BFS 시작 노드 X부터 연결된 노드를 순차적으로 방문하면서 X로부터 떨어진 거리를 기록 거리가 K와 같은 노드를 출력하고, 해당하는 노드가 없을 경우 -1을 출력 거리가 K를 넘어가지 않는 노드에 대해서만 탐색하여 시간 단축 Time Complexity BFS: O(N+M) = 1,300,000 Data Size N: 2 \u0026lt;= int \u0026lt;= 300,000 M: 1 \u0026lt;= int \u0026lt;= 1,000,000 K: 1 \u0026lt;= int \u0026lt;= 300,000 X: 1 \u0026lt;= int \u0026lt;= N A, B: 1 \u0026lt;= int \u0026lt;= N 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from collections import deque import sys input = sys.stdin.readline N, M, K, X = map(int, input().split()) nodes = [[] for _ in range(N+1)] visited = [False] * (N+1) dists = [0] * (N+1) for _ in range(M): A, B = map(int, input().split()) nodes[A].append(B) queue = deque() queue.append(X) visited[X] = True while queue: city = queue.popleft() for next in nodes[city]: if not visited[next] and dists[city] \u0026lt; K: queue.append(next) visited[next] = True dists[next] = dists[city]+1 targets = [i for i,d in enumerate(dists) if d==K] if targets: for target in targets: print(target) else: print(-1) ","permalink":"https://minyeamer.github.io/blog/boj-problems-18352/","summary":"문제 링크 https://www.acmicpc.net/problem/18352 문제 해설 Idea BFS 시작 노드 X부터 연결된 노드를 순차적으로 방문하면서 X로부터 떨어진 거리를 기록 거리가 K와 같은 노드를 출력하고, 해당하는 노드가 없을 경우 -1을 출력 거리가 K를 넘어가지 않는 노드에 대해서만 탐색하여 시간 단축 Time Complexity BFS: O(N+M) = 1,300,000 Data Size N: 2 \u0026lt;= int \u0026lt;= 300,000 M: 1 \u0026lt;= int \u0026lt;= 1,000,000 K: 1 \u0026lt;= int \u0026lt;= 300,000 X: 1 \u0026lt;= int \u0026lt;= N A, B: 1 \u0026lt;= int \u0026lt;= N 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from collections import deque import sys input = sys.","title":"[백준 18352] 특정 거리의 도시 찾기 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/1495 문제 해설 Idea Dynamic Programming P[i] = max(P[i-1]+V[i-1],P[i-1]-V[i-1]), 0 \u0026lt;= P[i] \u0026lt;= M 모든 P[i-1]가 P[i+1]에 영향을 줄 수 있기 때문에 범위 내 모든 값을 집합에 저장 마지막 곡에 대한 P가 존재할 경우 최댓값을 출력하고, 없을 경우 -1을 출력 Time Complexity DP: O(NM) = 50,000 Data Size N: 1 \u0026lt;= int \u0026lt;= 50 M: 1 \u0026lt;= int \u0026lt;= 1,000 S: 0 \u0026lt;= int \u0026lt;= M V: int * N 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 N, S, M = map(int, input().split()) V = list(map(int, input().split())) P = [set() for _ in range(N+1)] P[0] = {S,} for i in range(1,N+1): for j in P[i-1]: if j+V[i-1] \u0026lt;= M: P[i].add(j+V[i-1]) if j-V[i-1] \u0026gt;= 0: P[i].add(j-V[i-1]) if P[-1]: print(max(P[-1])) else: print(-1) ","permalink":"https://minyeamer.github.io/blog/boj-problems-1495/","summary":"문제 링크 https://www.acmicpc.net/problem/1495 문제 해설 Idea Dynamic Programming P[i] = max(P[i-1]+V[i-1],P[i-1]-V[i-1]), 0 \u0026lt;= P[i] \u0026lt;= M 모든 P[i-1]가 P[i+1]에 영향을 줄 수 있기 때문에 범위 내 모든 값을 집합에 저장 마지막 곡에 대한 P가 존재할 경우 최댓값을 출력하고, 없을 경우 -1을 출력 Time Complexity DP: O(NM) = 50,000 Data Size N: 1 \u0026lt;= int \u0026lt;= 50 M: 1 \u0026lt;= int \u0026lt;= 1,000 S: 0 \u0026lt;= int \u0026lt;= M V: int * N 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 N, S, M = map(int, input().","title":"[백준 1495] 기타리스트 (Python)"},{"content":"문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/17686 문제 해설 Idea 정규표현식을 활용해 HEAD, NUMBER, TAIL을 분리 전체 파일명을 완전탐색하면서 리스트에 분리된 파일명을 저장 HEAD와 NUMBER를 기준으로 파일명을 정렬하고 정렬된 원본 파일명을 반환 Time Complexity Brute-Force + Sort: O(NM+NlogN)) = 110000 Data Size files: str(100) * 1000 해설 코드 1 2 3 4 5 6 7 8 9 10 11 import re def solution(files): answer = [] for file in files: head, number, tail = re.findall(\u0026#39;([^0-9]+)([0-9]+)(.*)\u0026#39;, file)[0] answer.append((head.lower(), int(number), tail, file)) answer.sort(key=lambda x: [x[0],x[1]]) return [file for _,_,_,file in answer] ","permalink":"https://minyeamer.github.io/blog/programmers-problems-17686/","summary":"문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/17686 문제 해설 Idea 정규표현식을 활용해 HEAD, NUMBER, TAIL을 분리 전체 파일명을 완전탐색하면서 리스트에 분리된 파일명을 저장 HEAD와 NUMBER를 기준으로 파일명을 정렬하고 정렬된 원본 파일명을 반환 Time Complexity Brute-Force + Sort: O(NM+NlogN)) = 110000 Data Size files: str(100) * 1000 해설 코드 1 2 3 4 5 6 7 8 9 10 11 import re def solution(files): answer = [] for file in files: head, number, tail = re.","title":"[프로그래머스/카카오 17686] 파일명 정렬 (Python)"},{"content":"문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/17684 문제 해설 Idea LZW 알고리즘 (List로 구현) 단어를 문자 단위로 탐색하면서 캐시에 추가 캐시가 문자 사전에 없을 경우 이전 문자까지의 인덱스를 반환하고 캐시를 문자 사전에 추가 Time Complexity Brute-Force: O(N^2) = 1000000 Data Size msg: str(1000) 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def solution(msg): answer = [] chars = [chr(x) for x in range(64,91)] cache = str() for c in msg: cache += c if cache not in chars: answer.append(chars.index(cache[:-1])) chars.append(cache) cache = c answer.append(chars.index(cache)) return answer ","permalink":"https://minyeamer.github.io/blog/programmers-problems-17684/","summary":"문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/17684 문제 해설 Idea LZW 알고리즘 (List로 구현) 단어를 문자 단위로 탐색하면서 캐시에 추가 캐시가 문자 사전에 없을 경우 이전 문자까지의 인덱스를 반환하고 캐시를 문자 사전에 추가 Time Complexity Brute-Force: O(N^2) = 1000000 Data Size msg: str(1000) 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def solution(msg): answer = [] chars = [chr(x) for x in range(64,91)] cache = str() for c in msg: cache += c if cache not in chars: answer.","title":"[프로그래머스/카카오 17684] 압축 (Python)"},{"content":"문제 링크 https://programmers.co.kr/learn/courses/30/lessons/17683 문제 해설 Idea 악보 정보에서 #이 포함된 음을 소문자로 대체하고 완전탐색 시간 계산은 timedelta 활용 (재생시간,제목)으로 구성된 리스트를 정렬 Time Complexity Brute-Force: O(NM) = 143,900 Data Size m: 1 \u0026lt;= int \u0026lt;= 1439 musicinfos: list \u0026lt;= 100 musicinfos[0,1]: HH:MM (00:00 - 23:59) musicinfos[2]: str(64) musicinfos[4]: 1 \u0026lt;= int \u0026lt;= 1439 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import datetime as dt import re import math def solution(m, musicinfos): answer = list() lower_repl = lambda match: match.group(1)[0].lower() sharp_repl = lambda s: re.sub(\u0026#39;([A-G]#)\u0026#39;, lower_repl, s) m = sharp_repl(m) strptime = lambda x: dt.timedelta(hours=int(x[0]),minutes=int(x[1])) for info in musicinfos: start, end, title, chord = info.split(\u0026#39;,\u0026#39;) plays = (strptime(end.split(\u0026#39;:\u0026#39;))-strptime(start.split(\u0026#39;:\u0026#39;))).seconds//60 chord = sharp_repl(chord) chord = (chord * math.ceil(plays/len(chord)))[:plays] if m in chord: answer.append((plays,title)) return sorted(answer, key=lambda x: x[0], reverse=True)[0][1] if len(answer) else \u0026#39;(None)\u0026#39; ","permalink":"https://minyeamer.github.io/blog/programmers-problems-17683/","summary":"문제 링크 https://programmers.co.kr/learn/courses/30/lessons/17683 문제 해설 Idea 악보 정보에서 #이 포함된 음을 소문자로 대체하고 완전탐색 시간 계산은 timedelta 활용 (재생시간,제목)으로 구성된 리스트를 정렬 Time Complexity Brute-Force: O(NM) = 143,900 Data Size m: 1 \u0026lt;= int \u0026lt;= 1439 musicinfos: list \u0026lt;= 100 musicinfos[0,1]: HH:MM (00:00 - 23:59) musicinfos[2]: str(64) musicinfos[4]: 1 \u0026lt;= int \u0026lt;= 1439 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import datetime as dt import re import math def solution(m, musicinfos): answer = list() lower_repl = lambda match: match.","title":"[프로그래머스/카카오 17683] 방금그곡 (Python)"},{"content":"문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/17680 문제 풀이 Idea LRU 알고리즘 (Deque로 구현) 도시이름이 캐시에 존재할 경우 시간에서 1 추가, 아닐 경우 5 추가 캐시에서 참고한 도시는 deque 최상단으로 재배치 캐시 사이즈를 초과할 경우 가장 오래된 도시를 제거 Time Complexity Deque: O(N) = 100,000 Data Size cacheSize: 0 \u0026lt;= int \u0026lt;= 30 cities: str(20) * 100,000 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from collections import deque def solution(cacheSize, cities): answer = 0 cache = deque(maxlen=cacheSize) for city in cities: city = city.lower() if city in cache: answer += 1 cache.remove(city) cache.append(city) else: answer += 5 cache.append(city) return answer ","permalink":"https://minyeamer.github.io/blog/programmers-problems-17680/","summary":"문제 링크 https://school.programmers.co.kr/learn/courses/30/lessons/17680 문제 풀이 Idea LRU 알고리즘 (Deque로 구현) 도시이름이 캐시에 존재할 경우 시간에서 1 추가, 아닐 경우 5 추가 캐시에서 참고한 도시는 deque 최상단으로 재배치 캐시 사이즈를 초과할 경우 가장 오래된 도시를 제거 Time Complexity Deque: O(N) = 100,000 Data Size cacheSize: 0 \u0026lt;= int \u0026lt;= 30 cities: str(20) * 100,000 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from collections import deque def solution(cacheSize, cities): answer = 0 cache = deque(maxlen=cacheSize) for city in cities: city = city.","title":"[프로그래머스/카카오 17680] 캐시 (Python)"},{"content":"Task 통계 모델: ARIMA, ARIMA 확장 모델 딥러닝 모델: DeepAR, MQRNN, N-BEATS, Informer, TFT Data Stationary: 시계열의 특징이 관측된 시간과 무관 (백색잡음 등) Non-stationary: Trend 혹은 seasonality가 존재하는 데이터 Analysis log 차분 ACF(자기 상관 그래프): 무작위 신호가 두 시점에서 갖는 상관계수를 표현하는 함수 stationary: 예측값이 무한대로 발산하지 않고, 일정한 범위 내에서 안정적으로 예측하기 위한 목적 ARIAM: stationary 데이터 예측 성능 우수 Deep-Learning non-stationary: training-validation, validation-test 간 mismatching 알고리즘 거래가 데이터의 분포를 변환 domain adaptation \u0026gt; transfer learning Self-Supervised Learning unlabeld 데이터 활용 \u0026gt; labeling 지정 (pretext task) 모델이 pretext task를 학습하여 데이터 자체에 대한 이해 (pre-training) main task에 대하여 transfer learning 수행 (downstream task) ","permalink":"https://minyeamer.github.io/blog/2022-07-21/","summary":"Self-adaptive forecasting for improved deep learning on non-stationary time-series","title":"2022-07-21 Log"},{"content":"Time Series 시계열 데이터 분석은 시간을 독립변수로 활용 시계열 분석은 시계열 데이터의 확률적 특성이 시간이 지나도 그대로 유지될 것(정상성)을 가정 AR Model: 자기 회귀 모델, 과거 시점의 자신의 데이터가 현 시점의 자신에게 영향을 미침 MA Model: 이동 평균 모델, 이전 항의 에러를 현 시점에 반영해 변화율에 맞춰 추정 ARMA Model: AR과 MA 모델을 합친 모델, 과거 시점의 자신과 추세까지 전부 반영 ARIMA Model: 현재 상태에서 바로 이전 상태를 빼주는 차분을 적용 ARIMA Model 미래를 예측하기 위해 자신의 시차로 이루어진 선형 조합과 지연된 예측 오차의 선형조합에 기반 $AR(1):Y_t=\\alpha+\\beta_1Y_{t-1}+\\epsilon_t,where|\\beta_1|\u0026lt;1$ $MA(2):Y_t=\\alpha+\\phi_1\\epsilon_{t-1}+\\phi_2\\epsilon_{t-2}+\\epsilon_t,where|\\phi_1|,|\\phi_2|\u0026lt;1$ AR(1)은 Y_t가 알려진 Y_t-1 값으로부터 얻어졌다는 것을 의미 자기상관함수(ACF)와 부분자기상관함수(PACF)를 사용해 AR과 MA항의 차수를 결정 ACF: Lag에 따른 관측치들 사이의 관련성을 측정하는 함수 PACF: k 이외의 모든 다른 시점 관측치의 영향력을 배제하고 y_t와 y_t-k 두 관측치의 관련성을 측정하는 함수 ","permalink":"https://minyeamer.github.io/blog/2022-07-19/","summary":"DL Study","title":"2022-07-19 Log"},{"content":"NNLM Word Embedding: 의미적으로 유사한 단어를 가까운 벡터 공간에 표현 one-hot vector: 단어의 개수만큼의 공간을 표현, 모든 단어간 유사도가 0에 가까움 어떤 feature vector 표현이 좋고, sequence들의 probability가 높음을 학습 chain rule에 기반해 이전 단어에 대한 다음 단어의 확률을 계산,\n단어의 개수가 많아질수록 과거로 봐야할 단어의 수가 많아지기 때문에,\nMarkov assumption을 적용해 n개의 단어만 참조 (n-grams) input이 hidden node를 거치지 않고 output으로 연결되는 skip-connection이 존재할 수 있음 (optional) NNLM의 목적은 윈도우 내에서 t번째 단어에 대해 t-1번째 단어의 확률이 극대화되는 것을 학습 Word2Vec CBOW: 주변 단어를 가지고 중심 단어를 예측, Skip-gram: 중심 단어를 가지고 주변 단어 예측 CBOW는 gradient가 각각의 주변 단어에 분산되지만,\nSkip-gram은 주변 단어의 gradient를 합산하는 방향으로 학습하여 성능이 좋음 activation function이 존재하지 않는 linear 구조 Skip-gram의 목적 함수는 t번째 단어에 대해 좌우로 n개의 단어에 대한 확률 합의 최대화 대상 단어 벡터와 특정 단어 벡터 간 내적을 하고 총합에 softmax를 취함 빈번한 단어는 한쌍으로 묶고, 빈도가 높은 단어의 학습을 제거해 계산을 간소화 negative sampling: output 단어에 대한 softmax 값을 계산하기 위해 나머지 모든 것들에 대한 내적값을 계산해야 하는데,\nk 개의 예시에 대해서만 sampling하여 계산의 효율성을 추구 FastText 형태론적 feature 정보를 한 단어의 subwork unit, 문자 단위에서 추출하는 기법 기존 방법론은 모든 단어를 각각의 vector로 표현하는 것에 한계가 있고 (OOV 문제),\n단어의 내부적 구조를 무시하여 유사한 형태의 단어를 표현하지 못함 skip-gram 기반에 문자 단위 character n-gram을 활용 기존 모델의 경우 corpus 대비 context word에 대해서만 학습해 비효율적인 연산이 많은데,\nnegative sampling을 적용해 일정한 확률값으로 뽑인 word를 참고 공통된느 형태소들에 대해 parameter sharing을 하여 embedding에 반영 (의미 공유) 단어의 시작과 끝 표현 \u0026lsquo;\u0026lt;\u0026rsquo;, \u0026lsquo;\u0026gt;\u0026lsquo;을 추가하고 n개 문자 단위의 벡터를 사용해 계산하며,\nn-gram이 커지는 문제를 방지하기 위해 hasing function으로 hash값 계산 LSTM RNN에서 새로운 가중치는 기존 가중치 W - lr * W에 대한 미분값의 chain rule인데,\nhidden state가 많아질수록 새로운 weight가 기존 weight와 거의 차이가 없어짐 memory cell을 추가해 새로운 input에 대해 과거의 정보에 대해 몇 퍼센트를 기억할지 결정하고 나머지 정보를 제거 일부가 제거된 cell state는 새로운 input에 대한 hidden state와 더해지고,\n이것이 다시 hidden state와 곱해져 다음 hidden state를 생성 Seq2Seq LSTM을 활용한 효율적인 기계 번역 아키텍쳐 전통적인 RNN 기반의 기계 번역은 입력과 출력의 크기가 같다고 가정 (입력 토큰과 출력 토큰이 대응) 위 문제를 해결하기 위해 인코더에서 고정된 크기의 context vector를 추출,\n디코더가 문맥 벡터로부터 번역 결과를 추론 (인코더와 디코더는 서로 다른 파라미터를 가짐) 시작 시점에 대한 토큰 \u0026lt;sos\u0026gt;와 종료 시점에 대한 토큰 \u0026lt;eos\u0026gt;를 추가하여,\n종료 토큰을 생성할때까지 반복문을 반복 입력 문장의 순서를 거꾸로 했을 때 더 높은 정확도를 보임 (앞쪽에 위치한 단어끼리 연관성이 높음) Attention을 적용한 Seq2Seq의 경우 모든 hidden states를 디코더로 전달하여,\n필요한 hidden state를 선택 ELMo ELMo에서의 embedding vector는 bi-directional LSTM에서 가져옴 forward 및 backward의 각 단계별 hidden state를 concatenate하고,\n각각의 벡터에 대한 가중합을 통해 embedding을 생성 특정 task에 대한 가중합 embedding과 task에 대한 scale vector를 곱해 계산 forward LM의 경우 k번째 토큰의 j번째 hidden state를 사용하며,\n마지막 hidden state가 t+1 단어를 예측하는데 사용 backward LM의 경우 역방향으로 t-1 단어를 예측 bi-directional LSTM은 forward와 backward에서의 정확도를 함께 최대화 2L+1개의 표현 R에 가중합을 취해 하나의 벡터로 만드는데, 태스크에 맞는 j번째 layer에 가중칠르 두고 전반적으로 스케일을 취함 가중합을 취하는 방식에 대해선 task별로 각각 취하는 것이 가장 좋고,\n1/(L+1)로 통합하는 것이 다음, 마지막 hidden state의 가중치를 사용하는 것이 다음으로 좋음 ELMo의 조합은 downstream task 모델의 input과 output 양쪽에 붙이는게 가장 좋고,\ninput에 붙이는 것, output에 붙이는 것 순으로 좋음 GPT unlabeled text 데이터가 labeled text 데이터보다 훨씬 많기 때문에,\n사전학습을 우선 수행하고 labeled 데이터에 대해 fine-tuning을 수행하면 더욱 도움이 될 것 transformer의 decoder block을 사용하며,\nencoder-decoder 간의 masked multi-head attention 없이 단순히 쌓아올림 ELMo는 bi-directional LSTM을 사용하는 반면, GPT는 마스크된 forward를 사용해 다음 단어를 예측 일반적인 LM은 전 단계까지의 시퀀스에 대해 i번째 토큰의 likelihood를 최대화하는 것이 목적 토큰 임베딩과 포지션 임베딩을 더해 첫번째 hidden state를 만들고,\nl-1 번째 hidden state를 decoder block을 통과시켜 l번째 hidden state 생성 GPT는 입력 단어에 대해 정방향으로 예측을 수행하기 때문에 BERT처럼 masking할 필요가 없음 BERT와 달리 각각의 토큰이 생성되면, 그 다음 토큰을 생성하기 위해 해당 토큰이 사용되는 auto-regressive 방식을 가짐 GPT-3 transformer 모델에서 log loss는 scale이 커질수록 개선되고, context도 마찬가지로 scale이 커질수록 정보가 향상됨 example이 많을수록 성능이 향상되는데 scale이 클수록 차이가 도드라짐 기존엔 각각의 example에 대한 결과를 보면서 gradient를 update하면서 fine-tuning 하는데,\nGPT-3는 zero-shot일 경우 task description과 prompt를 주고,\none-shot일 경우 하나의 example, few-shot일 경우 여러 개의 example을 줌 (fine-tuning을 다시 하지 않음) fine-tuning은 새로운 데이터셋이 필요하면서, 일반화 성능이 떨어지고 과적합 문제가 발생,\nfew-shot learning은 task-specific 데이터를 사용하지 않지만 SOTA보다는 성능이 떨어짐 기존 transformer는 이전 토큰에 대해 모두 attention이 걸리는데,\nsparse transformer를 사용해 개선 학습 데이터와 테스트 데이터 간 overlap이 존재하는데, 비용이 많이 들어 해결하지 못함 문서 레벨에서 특정 표현을 반복하는 경우, 매우 긴 문장의 경우 등 특정 task에 대해 성능이 떨어짐 bidirectional이 아닌 auto-regressive한 구조적 단점 ","permalink":"https://minyeamer.github.io/blog/2022-08-26/","summary":"Language models paper reviews","title":"2022-07-19 Log"},{"content":"기계번역 과제 인코더: 입력 문장의 표현 방법 학습 디코더: 인코더에서 학습한 표현 결과를 입력받아 사용자가 원하는 문장 생성 Encoder Self Attention 임베딩: 각각의 단어를 표현하는 벡터값, [문장 길이 x 임베딩 차원] 쿼리(Q), 키(K), 밸류(V) \u0026gt; 각각의 가중치 행렬을 입력 행렬에 곱해 Q, K, V 행렬 생성 Q, K, V의 차원 [문장 길이 x 벡터의 차원] 1단계: Q와 K^T 행렬의 내적 연산, 쿼리 벡터(I)와 키 벡터(I, am, good) 사이의 유사도 계산 2단계: QK^T 행렬을 키 벡터 차원의 제곱근값($\\sqrt{d_k}$)으로 나눈 것, 안정적인 gradient 얻음 3단계: 소프트맥스 함수를 사용해 비정규화된 형태의 유사도 값을 정규화 (score 행렬) 4단계: 스코어 행렬에 V 행렬을 곱해 어텐션(Z) 행렬 계산, 어텐션 행렬은 문장의 각 단어와 벡터값 가짐\n(단어 I의 셀프 어텐션은 각 밸류 벡터값의 가중치 합으로 계산, 단어가 문장 내에 있는 다른 단어와의 연관성) Multi-Head Attention 문장 내에서 모호한 의미를 가진 단어(it)가 있을 경우,\n문장의 의미가 잘못 해석될 수 있기 때문에 멀티 헤드 어텐션을 사용한 후 그 결괏값을 더함 다수의 어텐션 행렬을 구하기 위해 서로 다른 가중치 행렬을 입력 행렬에 곱해 Q, K, V 생성 다수의 어텐션 행렬을 concatenate하고 새로운 가중치 행렬을 곱해 멀티 헤드 어텐션 결과 도출\n(concatenate 시 [어텐션 헤드 x h] 크기가 되기 때문에 원래 크기로 만들기 위해 가중치 행렬 곱함) Positional Encoding 트랜스포머는 문장 안에 있는 모든 단어를 병렬 형태로 입력 단어의 순서 정보를 제공하기 위해 문장에서 단어의 위치를 나타내는 인코딩 제공 위치 인코딩은 사인파 함수를 사용 입력 임베딩 결과에 위치 인코딩을 합한 후 멀티 헤드 어텐션에 입력 Feed Forward Network 2개의 전결합층(Dense)과 ReLU 활성화 함수로 구성 add와 norm을 추가해 서브레이어에서 멀티 헤드 어텐션의 입력값과 출력값을 서로 연결 add와 norm은 레이어 정규화(각 레이어 값이 크게 변화하는 것을 방지해 모델 학습 빠르게)와 잔차 연결 인코더 순서 입력값은 입력 임베딩으로 변환한 다음 위치 인코딩 추가, 가장 아래 있는 인코더 1의 입력값으로 공급 인코더 1은 입력값을 받아 멀티 헤드 어텐션의 서브레이어에 값을 보냄, 어텐션 행렬을 결괏값으로 출력 어텐션 행렬의 값을 다음 서브레이어인 피드포워드 네트워크에 입력, 결괏값 출력 인코더 1의 출력값을 그 위에 있는 인코더 2에 입력값으로 제공 인코더 2에서 이전과 동일한 방법 수행, 주어진 문장에 대한 인코더 표현 결과를 출력으로 제공 Decoder 이전 디코더의 입력값과 인코더의 표현(인코더의 출력값), 2개를 입력 데이터로 받음 t=1에서 디코더의 입력값은 문장의 시작을 알리는 를 입력 \u0026gt; 타깃 문장의 첫 번째 단어(Je) 생성 t=2에서 t-1 디코더에서 생성한 단어(, Je)를 추가해 문장의 다음 단어 생성 t=3에서도 동일하게 (, Je, vais)를 입력받아 다음 단어 생성 디코더에서 토큰을 생성할 때 타깃 문장의 생성이 완료 디코더도 입력값을 바로 입력하지 않고 위치 인코딩을 추가한 값을 출력 임베딩에 더해 입력값으로 사용 Masked Multi-Head Attention 디코더에서 문장을 생성할 때 이전 단계에서 생성한 단어만 입력으로 넣기 때문에,\n아직 예측하지 않은 오른쪽의 모든 단어를 마스킹해 학습을 진행 소프트맥스 함수를 적용한 정규화 작업 전에 오른쪽의 모든 단어를 $-{\\infty}$로 마스킹 수행\n($-{\\infty}$는 학습 도중 발산하는 경우가 있기 때문에 실제로는 작은 값 $e^{-9}$으로 지정) Encoder-Decoder Attention Layer 디코더의 멀티 헤드 어텐션의 입력으로 인코더의 표현값 R과 마스크된 멀티 헤드 어텐션의 결과 M을 받을 때 상호작용 발생 쿼리, 키, 밸류 행렬을 생성할 때, M을 사용해 Q를 생성, R을 활용해 K, V를 생성\n(쿼리 행렬은 타깃 문장의 표현을 포함하기 때문에 M을 참조, 키와 밸류 행렬은 입력 문장의 표현을 참조) 쿼리, 키 행렬 간의 내적 시 타깃 단어 가 입력 문장의 모든 단어(I, am, good)와 얼마나 유사한지 계산\n(두 번째 행에서 Je에 대해, 나머지 행에서도 동일한 방법을 적용해 유사도 계산) Linear and Sofmax Layer 최상위 디코더에서 얻은 출력 값을 선형 및 소프트맥스 레이어에 전달 선형 레이어는 vocab 크기와 같은 logit 형태 logit 값을 확률값으로 변환하고, 디코더에서 가장 높은 확률값을 갖는 인덱스의 단어로 출력 디코더 순서 디코더에 대한 입력 문장을 임베딩 행렬로 변환하고 위치 인코딩 정보를 추가해 디코더 1에 입력 입력을 가져와서 마스크된 멀티 헤드 어텐션 레이어에 보내고, 출력으로 어텐션 행렬 M 반환 어텐션 행렬 M, 인코딩 표현 R을 입력받아 멀티 헤드 어텐션 레이어에 값을 입력, 새로운 어텐션 행렬 생성 인코더-디코더 어텐션 레이어에서 출력한 어텐션 행렬을 피드포워드 네트워크에 입력, 디코더의 표현으로 값 출력 디코더 1의 출력값을 다음 디코더 2의 입력값으로 사용 디코더 2는 이전과 동일한 방법 수행, 타깃 문장에 대한 디코더 표현 반환 타깃 문장의 디코더 표현을 선형 및 소프트맥스 레이어에 입력해 최종으로 예측된 단어 얻음 학습 손실 함수로 cross-entropy를 사용해 분포의 차이를 확인 옵티마이저로 Adam 사용 과적합을 방지하기 위해 각 서브레이어 출력에 dropout 적용 (임베딩 및 위치 인코딩 합을 구할 때도 포함) BERT Word2Vec: 문맥 독립 임베딩, BERT: 문맥 기반 임베딩 BERT는 인코더-디코더가 있는 트랜스포머 모델에서 인코더만 사용 BERT-base: L(인코더 레이어)=12, A(어텐션 헤드)=12, H(은닉 유닛)=768 BERT-large: L=24, A=16, H=1024 BERT-tiny(L=2, A=2, H=128), BERT-mini(L=4, A=4, H=256) 등 사전 학습 대규모 데이터셋으로 학습된 가중치를 활용해 새로운 태스크에 적용 (find-tuning) BERT는 MLM과 NSP 태스크를 이용해 거대한 말뭉치를 기반으로 사전 학습 Token Embedding 첫 번째 문장의 시작 부분에 [CLS] 토큰 추가 모든 문장 끝에 [SEP] 토큰 추가 Segment Embedding 두 문장을 구별하는데 사용 [SEP] 토큰과 별도로 두 문장을 구분하기 위해 입력 토큰($E_A,E_B$) 제공 Position Embedding 단어(토큰)의 위치에 대한 정보 제공 입력 데이터 토큰 임베딩 + 세그먼트 임베딩 + 위치 임베딩 으로 표현 WordPiece Tokenizer 하위 단어 토큰화 알고리즘 기반 Let us start pretraining the model \u0026gt; [let, us, start, pre, ##train, ##ing, the, model] 단어가 어휘 사전에 있으면 토큰으로 사용, 없으면 하위 단어로 분할해 하위 단어가 어휘 사전에 있는지 확인 (OOV 처리에 효과적) Language Modeling 임의의 문장이 주어지고 단어를 순서대로 보면서 다음 단어를 예측하도록 모델 학습 자동 회귀 언어 모델링: 전방(좌\u0026gt;우) 예측, 후방(좌\u0026lt;우) 예측, 각 방향(단방향)으로 공백까지 모든 단어를 읽음 자동 인코딩 언어 모델링: 예측하면서 양방향으로 문장을 읽음 Masked Language Modeling (MLM) 주어진 입력 문장에서 전체 단어의 15%를 무작위로 마스킹, 마스크된 단어를 예측 (빈칸 채우기 태스크) [MASK] 토큰을 사전 학습시킬 경우 파인 튜닝 시 입력에 [MASK] 토큰이 없어 불일치가 발생 15% 토큰에 대해 80%만 [MASK] 토큰으로 교체, 10%는 임의의 토큰(단어)로 교체, 10%는 변경하지 않음\n(사전 학습과 파인 튜닝 태스크의 차이를 줄이기 위한 일종의 정규화 작업) 역전파를 통한 반복 학습을 거치며 최적의 가중치 학습 Whole Word Masking (WWM) WWM 방법에서는 하위 단어가 마스킹되면 해당 하위 단어와 관련된 모든 단어를 마스킹 하위 단어와 관련된 모든 단어의 마스크 비율이 15%를 초과하면 다른 단어의 마스킹을 무시 Next Sentence Prediction (NSP) BERT에 두 문장을 입력하고 두 번째 문장이 첫 번째 문장의 다음 문장인지 예측 B 문장이 A 문장에 이어지만 isNext를 반환하고, 그렇지 않으면 notNext를 반환 두 문장 사이의 관계를 파악해 질문-응답 및 유사문장탐지와 같은 downstream 태스크에서 유용 한 문서에서 연속된 두 문장을 isNext로 표시하고, 두 문서에서 각각 문장을 가져와 notNext로 표시 [CLS] 토큰 표현에 소프트맥스 함수를 사용하고 피드포워드 네트워크에 입력해 두 클래스에 대한 확률값 반환 [CLS] 토큰은 모든 토큰의 집계 표현을 보유하고 있기 때문에 문장 전체에 대한 표현을 담고 있음 사전 학습 절차 lr = 1e-4, b1 = 0.9, b2 = 0.999\n초기 모델의 큰 변화를 유도하기 위해 웜업으로 1만 스텝 학습\n(0에서 1e-4로 선형적으로 학습률 증가, 1만 스탭 후 수렴에 가까워짐에 따라 학습률을 선형적으로 감소) dropout 0.1, GELU(가우시안 오차 선형 유닛) 활성화 함수 사용 하위 단어 토큰화 알고리즘 Byte Pair Encoding (BPE) 모든 단어를 문자로 나누고 문자 시퀀스로 만듦 우선 문자 시퀀스에 있는 고유 문자를 어휘 사전에 추가 어휘 사전 크기에 도달할 때까지 가장 빈도수가 큰 기호 쌍을 반복적으로 병합해 어휘 사전에 추가 토큰화 시 어휘 사전에 존재하지 않는 단어는 하위 단어로 나눔, 사전에 없는 개별 문자는 토큰으로 교체 Byte-Level Byte Pair Encoding (BBPE) 문자 수준 시퀀스 대신 바이트 수준 시퀀스를 사용 유니코드 문자가 바이트로 변환되어 단일 문자 크기는 1~4 바이트가 됨 바이트 수준에서 빈번한 쌍을 구분해 어휘 사전을 구축 다국어 설정에서 유용, OOV 단어 처리에 효과적 WordPiece BPE랑 다르게 빈도수 대신 likelihood를 기준으로 기호 쌍을 병합 모든 기호 쌍에 대해 언어 모델의 가능도를 확인, 가능도가 가장 높은 기호 쌍을 병합 ","permalink":"https://minyeamer.github.io/blog/2022-07-17/","summary":"구글 BERT의 정석 1","title":"2022-07-17 Log"},{"content":"PyTorch Basic PyTorch Packages torch: 메인 네임스페이스 torch.autograd: 자동 미분을 위한 함수 torch.nn: 데이터 구조나 레이어 등 정의 torch.optim: 경사 하강법 등 파라미터 최적화 알고리즘 구현 torch.utils.data: 미니 배치용 유틸리티 함수 torch.onnx: ONNX의 포맷으로 모델을 저장할 때 사용 Tensor 2D Tensor: (batch size, dim) 3D Tensor: (batch size, width, height) 3D Tensor(NLP): (batch size, length, dim) torch.FloatTensor(): 텐서 생성 행렬 곱셈(.matmul()), 원소 별 곱셈(.mul(), *) dim=0: 첫번째 차원(행) 제거 = 같은 열끼리 연산, dim=1: 두번째 차원(열) 제거 = 같은 행끼리 연산 view(): reshape, squeeze(): 1인 차원 제거, unsequeeze(): 특정 위치에 1인 차원 추가 cat(), stack(), ones_like(), zeros_like() Linear Regression 선형 회귀 구현 기본 셋팅 및 변수 선언: seed 설정, x_train 및 y_train 선언 가중치와 편향의 초기화: W = torch.zeros(1, requires_grad=True) 가설 세우기: hypothesis = x_train * W + b 비용 함수 선언: cost = torch.mean((hypothesis - y_train) ** 2) 경사 하강법 구현: 1 2 3 4 5 optimizer = optim.SGD([W, b], lr=0.01) optimizer.zero_grad() # gradient를 0으로 초기화 # 파이토치가 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값을 누적시키기 때문에 미분값을 초기화 cost.backword() # 비용 함수를 미분하여 gradient 계산 optimizer.step() # W와 b를 업데이트 Autograd requires_grad=True, backward() 등 다중 선형 회귀 x의 계수를 행렬로 변환해 W와 내적 5x3 크기의 x_train과 3x1 크기의 W를 내적해 5x1 크기의 y_train 계산 nn.Module 1 2 3 4 5 6 from torch import nn import torch.nn.functional as F model = nn.Linear(input_dim, output_dim) cost = F.mse_loss(prediction, y_train) optimizer = torch.optim.SGD(model.parameters(), lr=0.01) Class 1 2 3 4 5 6 7 class LinearRegressionModel(nn.Module): def __init__(self): # super().__init__() self.linear = nn.Linear(1, 1) def forward(self, x): return self.linear(x) Custom Dataset 1 2 3 4 5 6 7 8 9 class CustomDataset(torch.utils.data.Dataset): def __init__(self): # 데이터셋의 전처리를 해주는 부분 def __len__(self): # 데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분 def __getitem__(self, idx): # 데이터셋에서 특정 1개의 샘플을 가져오는 함수 Logistic Regression hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b))) cost = F.binary_cross_entropy(hypothesis, y_train) 1 2 3 4 5 6 7 8 class BinaryClassifier(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(2, 1) self.sigmoid = nn.Sigmoid() def forward(self, x): return self.sigmoid(self.linear(x)) Softmax Regression F.softmax(z, dim=1) F.softmax() + torch.log() = F.log_softmax() F.log_softmax() + F.nll_loss() = F.cross_entropy() 1 2 3 # One-Hot Encoding y_one_hot = torch.zeros_like(hypothesis) y_one_hot.scatter_(1, y_train.unsqueeze(1), 1) Artificial Neural Network 1 2 3 4 5 6 7 model = nn.Sequential( nn.Linear(64, 32), nn.ReLU(), nn.Linear(32, 16), nn.ReLU(), nn.Linear(16, 10) ) Convolutional Neural Network 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class CNN(torch.nn.Module): def __init__(self): super(CNN, self).__init__() # 첫번째층 # ImgIn shape=(?, 28, 28, 1) # Conv -\u0026gt; (?, 28, 28, 32) # Pool -\u0026gt; (?, 14, 14, 32) self.layer1 = torch.nn.Sequential( torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(), torch.nn.MaxPool2d(kernel_size=2, stride=2)) # 두번째층 # ImgIn shape=(?, 14, 14, 32) # Conv -\u0026gt;(?, 14, 14, 64) # Pool -\u0026gt;(?, 7, 7, 64) self.layer2 = torch.nn.Sequential( torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), torch.nn.ReLU(), torch.nn.MaxPool2d(kernel_size=2, stride=2)) # 전결합층 7x7x64 inputs -\u0026gt; 10 outputs self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True) # 전결합층 한정으로 가중치 초기화 torch.nn.init.xavier_uniform_(self.fc.weight) def forward(self, x): out = self.layer1(x) out = self.layer2(out) out = out.view(out.size(0), -1) # 전결합층을 위해서 Flatten out = self.fc(out) return out NLP 1 2 3 4 import torch.nn as nn embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=3, padding_idx=1) torchtext Error cannot import name \u0026rsquo;load_state_dict_from_url'\napply this change in _download_hooks.py 1 2 3 4 try: from torch.hub import load_state_dict_from_url except ImportError: from torch.utils.model_zoo import load_url as load_state_dict_from_url Make Vocabulary 1 2 3 4 from torchtext.vocab import build_vocab_from_iterator vocab = build_vocab_from_iterator(sequences) vocab.get_stoi() # 각 단어의 정수 인덱스가 저장된 딕셔너리 Recurrent Neural Network 1 nn.RNN(input_dim, hidden_size, num_layers, batch_fisrt=True) 1 nn.LSTM(input_dim, hidden_size, num_layers, batch_fisrt=True) Char RNN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Net(nn.Module): def __init__(self, vocab_size, input_size, hidden_size, batch_first=True): super(Net, self).__init__() self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, # 워드 임베딩 embedding_dim=input_size) self.rnn_layer = nn.RNN(input_size, hidden_size, # 입력 차원, 은닉 상태의 크기 정의 batch_first=batch_first) self.linear = nn.Linear(hidden_size, vocab_size) # 출력은 원-핫 벡터의 크기를 가져야함. 또는 단어 집합의 크기만큼 가져야함. def forward(self, x): # 1. 임베딩 층 # 크기변화: (배치 크기, 시퀀스 길이) =\u0026gt; (배치 크기, 시퀀스 길이, 임베딩 차원) output = self.embedding_layer(x) # 2. RNN 층 # 크기변화: (배치 크기, 시퀀스 길이, 임베딩 차원) # =\u0026gt; output (배치 크기, 시퀀스 길이, 은닉층 크기), hidden (1, 배치 크기, 은닉층 크기) output, hidden = self.rnn_layer(output) # 3. 최종 출력층 # 크기변화: (배치 크기, 시퀀스 길이, 은닉층 크기) =\u0026gt; (배치 크기, 시퀀스 길이, 단어장 크기) output = self.linear(output) # 4. view를 통해서 배치 차원 제거 # 크기변화: (배치 크기, 시퀀스 길이, 단어장 크기) =\u0026gt; (배치 크기*시퀀스 길이, 단어장 크기) return output.view(-1, output.size(2)) # 모델 생성 model = Net(vocab_size, input_size, hidden_size, batch_first=True) # 손실함수 정의 loss_function = nn.CrossEntropyLoss() # 소프트맥스 함수 포함이며 실제값은 원-핫 인코딩 안 해도 됨. # 옵티마이저 정의 optimizer = optim.Adam(params=model.parameters()) Classification with GRU 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class GRU(nn.Module): def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2): super(GRU, self).__init__() self.n_layers = n_layers self.hidden_dim = hidden_dim self.embed = nn.Embedding(n_vocab, embed_dim) self.dropout = nn.Dropout(dropout_p) self.gru = nn.GRU(embed_dim, self.hidden_dim, num_layers=self.n_layers, batch_first=True) self.out = nn.Linear(self.hidden_dim, n_classes) def forward(self, x): x = self.embed(x) h_0 = self._init_state(batch_size=x.size(0)) # 첫번째 히든 스테이트를 0벡터로 초기화 x, _ = self.gru(x, h_0) # GRU의 리턴값은 (배치 크기, 시퀀스 길이, 은닉 상태의 크기) h_t = x[:,-1,:] # (배치 크기, 은닉 상태의 크기)의 텐서로 크기가 변경됨. 즉, 마지막 time-step의 은닉 상태만 가져온다. self.dropout(h_t) logit = self.out(h_t) # (배치 크기, 은닉 상태의 크기) -\u0026gt; (배치 크기, 출력층의 크기) return logit def _init_state(self, batch_size=1): weight = next(self.parameters()).data return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_() ","permalink":"https://minyeamer.github.io/blog/2022-07-13/","summary":"PyTorch로 시작하는 딥 러닝 입문","title":"2022-07-13 Log"},{"content":"Optimization minimize(최소화) f(x) subject to(제약조건) h(x)=0 glaobal minimum, local minimum, local maximum 목적함수: 주어진 점과 임의의 선의 간격(error)를 모두 더함 Gradient 경사도 벡터: n개의 변수에 대한 함수 f(x)의 $x^*$에서의 편미분 계수 (열 벡터) 경사도 벡터는 $f(x^*)=c$인 표면의 초접평면에 수직, 함수의 최대 증가 방향 Gradient Descent Method $$x^{(t+1)}=x^{(t)}-\\eta\\frac{dy(x^{(t)})}{dx}$$\nminimize 문제를 풀기 위해 gradient 벡터의 반대 방향으로 이동 (최대 하강 방향) 강하방향(descent direction): gradient 벡터와의 내적이 0보다 작은 경우 ($c^{(k)}ㆍd^{(k)}\u0026lt;0$) $f(x)$ 정식화 가능할 경우 최적성 기준법, 간접법 등, 정식화 할 수 없는 경우 탐색법, 직접법(경사하강법) 등 경사도 벡터 계산 2) 강하 방향 선택 3) 이동거리($\\alpha$) 결정 step size($\\alpha$)를 결정하기 위해 선탐색(황금분할 탐색) 사용 최속 강하법: 경사도 벡터의 반대 반향을 강하 방향으로 선택 켤레 경사법: 경사도 벡터의 반대 방향에 이전의 강하 방향을 더함 목적 함수로 booth 함수, rosenbrock 함수 사용 ","permalink":"https://minyeamer.github.io/blog/2022-07-08/","summary":"꼼꼼한 딥러닝 2","title":"2022-07-08 Log"},{"content":"Function scalar: 양만으로 표시할 수 있는 물리량 vector: 양과 방향으로 표현할 수 있는 물리량 일변수-스칼라함수 다항함수, 분수함수, 지수함수, 로그함수, 삼각함수 등 입력: 스칼라, 출력: 스칼라 독립변수: 함수의 출력 결정, 매개변수: 함수의 모양 결정 시그모이드 함수: 모든 실수를 0~1 사이로 찌그러트림, 출력을 확률로 해석 다변수-스칼라함수 곡면과 등고선 그래프 입력: 벡터, 출력: 스칼라 평면: 이변수 스칼라 함수, 직선: 우변이 0으로 고정 퍼셉트론: $\\ f(x,y)=ax+by+c$ 일변수-벡터함수 평면 또는 공간에 존재하는 곡선 입력: 스칼라, 출력: 벡터 다변수-벡터함수 입력: 벡터, 출력: 벡터 인공신경망과 유사 Composite Function: 함수의 합성, $\\ g∘f=g(f(x))$ 신경망은 매우 많은 함수가 겹겹이 합성된 것 (목적함수, 비용함수, 손실함수) 머신러닝 분류 문제 평가 함수로 지수,로그함수 활용 Matrix 행렬의 차원(크기): 행 개수 x 열 개수 (${m}\\times{n}$) 행렬의 전치: $\\ A^T$ 행렬의 덧셈: 두 행렬의 크기가 같을 때 같은 위치 요소 덧셈 행렬의 곱셈: 행과 열의 요소를 곱해서 더함 단위행렬: 대각 성분이 모두 1, 나머지는 0인 정사각 행렬 대각행렬: 대각 성분이 아닌 모든 성분이 0인 정사각 행렬 대칭행렬: 정사각 행렬에 대해서 $\\ S^T=S$ 직교행렬: 전치된 것이 자신인 역행렬 $\\ A^T=A^{-1}$ 이미지 표현: (C, H, W)=PyTorch 또는 (H, W, C)=TensorFlow 열 결합: 뒤에서 곱하는 벡터의 요소를 계수로 한 모든 열의 선형결합 행 결합: 앞에서 곱하는 행렬의 행 요소를 계수로 뒷 행렬의 행을 조합 (열 결합 전치) 외적: 행렬 반환, 내적: 스칼라 반환 인공신경망 $a^{(1)}=f(W^{(1)}x+b^{(1)})$ (W=weight, b=bias) 활성 함수: Sigmoid, ReLU, Tanh MNIST 데이터: (28,28)인 손글씨 이미지가 (1,781)인 어레이로 저장 로지스틱 함수: 각각의 y에 대한 확률 계산 (다중 분류에서 정확도가 낮음) softmax: 실수 k개가 0에서 1사이의 숫자 k개로 맵핑 회귀: 입력 x에 대응하는 실수 y값을 출력 Differentiation 에러를 줄이 위한 목적 (미분 정보를 이용해 탐색 방향 결정) 평균변화율: 함수의 변화를 보는 구간이 중요, 자세한 정보를 위해 간격을 좁힐 필요 순간변화율: 평균변화율의 분모를 순간에 이를 정도로 작게 만듦, 극한값이 미분계수 순간변화율은 그 위치에서 접선의 기울기 sympy 패키지를 사용해 파이썬에서 기호 연산 미분법 상수의 미분, 덧셈/뺄셈의 미분, 곱셈의 미분법, 나눗셈의 미분법 함성함수 미분: 라이프니츠 미분법, $\\ \\sqrt{ax^2+bx+c}=\\sqrt{y}$ $z=\\sqrt{x^2+3x},\\quad y=x^2+3x,\\quad z=\\sqrt{y}$ 연쇄법칙(Chain Rule): $\\ \\frac{dz}{dx}=\\frac{dz}{dy}ㆍ\\frac{dy}{dx}$ 연쇄법칙은 인공신경망(합성함수) 역전파 알고리즘의 기본 아이디어 편미분 다변수 함수에 구할 수 있는 변화율은 무수히 많음 다변수 함수에서 변수 고정, 한번에 변수 하나만 변화 (윤곽선, 1차원 함수) 편도함수: 다변수 함수일 때 하나의 변수에 대해서만 미분한 도함수, 기호: $\\partial$ Saliency Map: 이미지 분류에서 y에 대한 다변수-스칼라함수의 편미분 계수를 이미지화 Guided Back Propagation: saliency map보다 더 확실한 특징을 보임 다변수 함수의 연쇄법칙은 모든 노드의 미분계수를 더함 Jacobian 야코비안: $\\ w=f_1(x,y),\\ z=f_2(x,y)$일 때 편도함수 4개를 블록 형태로 표시 로지스틱 함수의 미분: 나눗셈의 미분과 함성함수의 미분을 사용, $\\ \\frac{d}{dz}\\sigma(z)=\\sigma(z)(1-\\sigma(z))$ 소프트맥스 함수의 미분: 벡터함수의 편미분 (인데스 별로 나눠서), ${K}\\times{K}$개의 미분계수 인덱스가 같은 경우 $\\ \\frac{\\partial}{\\partial{z_j}}s_i(z)=s_j(z)(1-s_j(z))$ 인덱스가 다른 경우 $\\ \\frac{\\partial}{\\partial{z_j}}s_i(z)=-s_i(z)s_j(z)$ 직접 미분 직접 미분하여 결과를 코딩 정확한 결과, 빠른 속도 장점, 미분을 직접 해야하나는 단점 수치 미분 수치적 계산으로 미분 계수를 근사 구현 간단, 변수가 많으면 매우 느리고 수치적으로 불안정 자동 미분 계산이 정확한지 확인할 목적으로 사용 전방 차분법 diff = (f(x+h) - f(x)) / h 중앙 차분법 diff = (f(x+(h/2)) - f(x-(h/2))) / h 식을 모를 경우 brute force \u0026gt; 2변수 이상일 때 이동할 수 있는 방향이 무한대가 되는 단점 gradient descent 방식을 사용해 반복 횟수를 줄임 ","permalink":"https://minyeamer.github.io/blog/2022-07-07/","summary":"꼼꼼한 딥러닝 1","title":"2022-07-07 Log"},{"content":"Regular Expression regexpal.com [wW]oodchuck = Woodchuck, woodchuck ranges: [A-z], [a-z], [0-9] negations: [^Ss], [^A-Z], a^b disjunction: pipe(|), yours|mine ?,,+,.: colou?r(= color, colour), ooh!=o+h!, beg.n(= begin, began) ^(start), $(end): ^[A-Z], \\.$(\\: escape) subset: ([0-9]+), \\1er (= [fast]er) non-capturing groups: /(?:some|a few) (people|cats) ?= (exact match), ?! (does not match) Tokenization Type vs Token N = number of tokens V = vocabulary (set of types) NLP task: 1)tokenizing words 2)normalizing word formats 3)segmenting sentences simpe way to tokenize: use space chracters issue: punctuation (like Ph.D), clitic (like we\u0026rsquo;re), multiword (like New York) chinese tokenization: don\u0026rsquo;t use spaces, count as a word is complex, so treat each character as token Byte Pair Encoding subword tokenization (tokens can be parts of words) BPE, Unigram language modeling tokenization, WordPiece add most frequent pair of adjancent tokens in characters er -\u0026gt; er_ -\u0026gt; ne -\u0026gt; new -\u0026gt; lo -\u0026gt; low -\u0026gt; \u0026hellip; frequent subwords are most morphemes like -est or -er Word Normalization put tokens in a standard format all letters to lower case (but, US vs us) all words to lemma (he is reading -\u0026gt; he be read) morphemes: the smallest meaningful units (stems, affixes) porter stemmer: ational -\u0026gt; ate, sses -\u0026gt; ss abbreviation dictionary can help ","permalink":"https://minyeamer.github.io/blog/2022-07-06/","summary":"CS 124","title":"2022-07-06 Log"},{"content":"4. 랭크, 차원 4-4. 고유 벡터 고윳값, 고유 벡터 = 특성 값, 특성 벡터 = 행렬의 특성 고유 벡터(eigenvector): 벡터에 선형 변환을 취했을 때, 방향은 변하지 않고 크기만 변하는 벡터 고윳값(eigenvalue): 선형 변환 이후 변한 크기, 고유 벡터가 변환되는 크기의 정도 4-5. 특이값 분해 닮음(similar): $P^{-1}AP=B$를 만족하는 가역 행렬 $P$가 존재 시, 정사각 행렬 $A, B$는 서로 닮음 직교 닮음(orthogonally similar): $B=P^{-1}AP$를 만족하는 직교 행렬 $P$가 존재 시, $B$는 $A$에 직교 닮음 직교 대각화(orthogonal diagonalization): 직교 닮음의 경우에서 정사각 행렬 $B$가 대각 행렬 $D$일 경우 직교 대각화가 가능하기 위해 $A$는 반드시 대칭 행렬 ($A^T=A$) 이어야 함 (공분산 행렬 등) 4-6. 고윳값 분해 행렬을 고유 벡터, 고윳값의 곱으로 분해하는 것 직교 벡터 $P$를 고유 벡터를 이용해 만들고 대각 행렬의 원소에 해당하는 것이 고윳값 $A=PDP^T$ 4-7. 특이값 분해 정사각 행렬을 대상으로 하는 고윳값 분해와 달리 대상 행렬을 ${m}\\times{n}$ 행렬로 일반화 인수 분해처럼 행렬의 차원 축소를 위한 도구로 사용 차원 축소를 $n$개의 점을 표현할 수 있는 기존 $p$보다 작은 차원인 $d$ 차원인 부분 공간(subspace)을 찾는 문제 데이터와 부분 공간으로부터의 수직 거리를 최소화(제곱합 $A^TA,AA^T$ 사용)하여 부분 공간을 찾음 특이값(singular value): 행렬 $A$를 제곱한 행렬의 고윳값에 루트를 씌운 값, $\\sigma_1=\\sqrt{\\lambda_1}$ $A=U\\Sigma{V^T}$ 행렬 U의 열벡터는 $AA^T$의 고유 벡터로 구성되는 left singular vector 행렬 V의 열벡터는 $A^TA$의 고유 벡터로 구성되는 right singular vector $\\Sigma$의 대각 원소는 행렬 A의 특이값 4-8. 이차식 표현 다항식을 벡터 형태로 나타낼 때 사용하는 방법 대칭 행렬 $W$에 대해 $x^TWx$ 형태로 표현한 식 양정치(positive definite): $x^TWx\u0026gt;0, \\text{ for all }x\\neq{0}$ (행렬 W의 고윳값이 모두 0보다 큼) 음정치(negative definite): $x^TWx\u0026lt;0, \\text{ for all }x\\neq{0}$ (행렬 W의 고윳값이 모두 0보다 작음) 4-9. 벡터의 미분 타깃 $y=w^Tx=x^Tw$를 데이터 벡터 x에 대해 미분하면 w가 나옴 5. 확률 변수와 확률 분포 5-1. 확률 변수 확률(probability): 어떤 사건이 일어날 가능성을 수치화시킨 것 모든 확률은 0에서 1 사이에 있으며, 모든 경우인 표본 공간(sample space)의 $P(S)=1$ 동시에 발생할 수 없는 사건들에 대해 각 사건의 합의 확률은 개별 확률이 일어날 확률의 합과 같음 확률 변수(random variable): 확률적으로 정해지는 변수, 동전 던지기에서 확률 변수 $X$는 0 또는 1의 값을 가짐 상수(constant): 변수와 다르게 항상 값이 고정된 수, $\\pi=3.14$ 등 함수(function): 한 집합의 임의의 한 원소를 다른 집합의 한 원소에 대응시키는 관계 5-2. 확률 분포 확률 변수가 특정값을 가질 확률의 함수 이산 확률 변수: 확률 변수가 가질 수 있는 값을 셀 수 있음 확률 질량 함수: 이산 확률 변수에서 특정값에 대한 확률을 나타내는 함수, $p_X(x)=P(X=x)$ 연속 확률 변수: 확률 변수가 가질 수 있는 값의 개수를 셀 수 없음 확률 밀도 함수: 연속 확률 변수의 분포를 나타내는 함수, $P(a\\lt{X}\\lt{b})=\\int_a^bf_X(x)dx$ 누적 분포 함수: 주어진 확률 변수가 특정값보다 작거나 같은 확률, $F_X(x)=P(X\\in{-\\infty,x}$ 결합 확률 밀도 함수: 확률 변수 여러 개를 함께 고려하는 확률 분포, $P_{X,Y}(x,y)=P(X=x,Y=y)$ 독립 항등 분포: 두 개 이상의 확률 변수를 고려할 때, 각 확률 변수가 통계적으로 독립이고 동일한 확률 분포(iid)를 따름 5-3. 모집단과 표본 모집단(population)은 관심이 있는 대상 전체, 표본(sample)은 모집단의 일부 모집단의 특성을 나타내는 대푯값을 모수(population parameter), 표본의 대푯값(sample statistic)을 표본 통계량 5-4. 평균과 분산 산술 평균: 모든 데이터값을 덧셈한 후 데이터 개수로 나누는 것 모평균: 모집단의 평균, $E(X)=\\mu$ 표본 평균: 모평균의 추정량, $\\bar{X}=\\frac{1}{n}\\Sigma^n_{i=1}{x_i}$ Location parameter: 평균의 변화로, 그래프의 위치 변화를 나타냄 분산: 데이터가 얼마나 퍼져 있는지를 수치화, 평균에 대한 편차 제곱의 평균 모분산: $Var(X)=E[(X-\\mu)^2]=\\sigma^2=E(X^2)-\\mu^2$ 표본 분산: $\\sigma^2=s^2=\\frac{1}{n-1}\\Sigma^n_{i=1}(x_i-\\bar{x})^2$ $x_i-\\bar{x}$는 평균에 대한 편차를 의미하며, 편차 제곱의 합을 n-1로 나누는 것은 자유도와 관련 자유도는 변수가 얼마나 자유로운지 나타내는 것으로,\n분산을 구하는 시점에서 이미 표본 평균이 정해져 있어 자유롭게 정할 수 있는 데이터가 n-1개인 것을 의미 Scale parameter: 분산과 같이 데이터의 흩어짐 정도를 결정하는 파라미터 표준 편차: 분산의 양의 제곱근으로 정의, 분산 계산 중 제곱으로 커진 결과를 다시 원래 단위로 조정하는 과정 $$E(\\Sigma^n_{i=1}X_i)=n\\mu_X\\text{, }Var(\\Sigma^n_{i=1}X_i)=n\\sigma^2$$\n5-5. 상관관계 공분산(covariance): 두 확률 변수의 상관관계를 나타내는 값, 같은 방향으로 움직이면 양수, 반대의 경우 음수 공분산은 변수 X의 편차와 변수 Y의 편차를 곱한 값의 평균, $Cov(X,Y)=E[(X-\\mu_X)(Y-\\mu_Y)]$ 공분산 행렬: 확률 변수 간 분산, 공분산을 행렬로 표현한 것, 차원 축소 등에서 자주 사용 상관 계수: 공분산을 각 변수의 표준 편차로 나누어 계산 $$Corr(X,Y)=\\frac{Cov(X,Y)}{\\sqrt{Var(X)}\\sqrt{Var(Y)}}=\\frac{Cov(X,Y)}{\\sigma_X\\sigma_Y}$$\n5-6. 균일 분포 특정 범위 내에서 확률 분포가 균일한 분포 이산형 균일 분포라면 모든 확률 변수의 확률값이 동일, $X~U(1,N)$ 연속형 균일 분포는 확률 변수의 범위가 연속형, $X~U(a,b)$ 5-7. 정규 분포 정규 분포 또는 가우시안 분포는 평균을 중심으로 대칭 형태를 띠는 종 모양 분포 $$f_X(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2} \\text{, } E(X)=\\mu \\text{, } Var(X)=\\sigma^2$$\n$\\frac{x-\\mu}{\\sigma}$는 머신러닝에서 쓰이는 데이터 표준화와 일치 표준 정규 분포: 평균이 0, 분산이 1인 정규 분포 5-8. 이항 분포 베르누이 분포, 베르누이 시행: 한 가지 실험에서 결과가 오직 2개인 시행 베르누이 시행의 성공 확률이 p일 때, 실패 확률은 1-p 이항 분포: 성공 확률이 p인 독립적인 베르누이 시행을 n회 했을 때, 성공 횟수 X가 따르는 이산형 확률 분포 다항 분포: 이항 분포를 일반화한 분포, 각 시행에서 나올 수 있는 결과가 m개로 확장 5-9. 최대 가능도 추정 가능도, 우도(likelihood): 파라미터가 주어질 때 해당 표본이 수집될 확률 가능도가 높다는 것은 해당 파라미터가 실젯값일 확률이 높다는 뜻 가능도 함수 $L(\\theta|x)=\\Pi^n_{i=1}{f(x_i|\\theta)}$ 로그 함수가 1대1 함수이기 때문에 가능도 함수에 로그 함수를 취할 수 있음 (log-likelihood function) 많은 확률을 곱할 경우 0에 가까워지기 때문에 계산상의 오류를 해결하기 위해 로그를 취함 최대 가능도 추정량(MLE): 파라미터별 가능도를 구해 가장 높은 가능도를 파라미터 추정값으로 사용 5-10. 최대 사후 추정 조건부 확률: 조건이 주어질 때의 확률, $P(A|B)=\\frac{P({A}\\bigcap{B})}{P(B)}$ 두 사건이 독립일 경우, 두 사건이 동시에 발생할 확률($P({A}\\bigcap{B}$)은 각 사건이 일어날 확률의 곱과 같음 Bayesian: 확률 분포의 파라미터를 상수로 보는 일반적인 빈도주의(Frequentist)와 달리 파라미터를 확률 변수로 보는 방법 베이즈 추정: 파라미터 $\\theta$가 확률 변수이므로 사전 확률 밀도 함수 $P(\\theta)$를 구할 수 있음 $P(\\theta,x)=P(x|\\theta)P(\\theta)$ 사후 확률 밀도 함수 $P(\\theta|x)\\propto{P(x|\\theta)P(\\theta)}$ 최대 사후 추정(MAP): 사후 확률 밀도 함수 $P(\\theta|x)$를 최대화하는 파라미터 $\\theta$ 6. 최적화 6-1. 컨벡스 셋 직선은 시작과 끝이 존재하지 않지만, 선분은 시작과 끝 지점이 존재 아핀 셋(affine set): $wx_1+(1-w)x_2\\in{C}$를 만족하는 집합 C 함수 $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}^m$가 존재할 때,\n선형 함수 $f(x)=Wx$,\n아핀 함수 $f(x)=Wx+b$ 컨벡스 셋(convex set): 두 점 $x_1,x_2\\in{C}$에 대해 아래 조건을 만족하는 집합 C $$wx_1+(1-w)x_2\\in{C}\\text{ }(0\\le{w}\\le{1})$$\n컨벡스 셋은 두 점을 잇는 직선을 포함하는 아핀 셋과 달리 두 점 사이의 선분을 포함 (집합의 경계가 존재, 컨벡스 셋 $\\subset$ 아핀 셋) 컨벡스 헐(convex hull): 선분이 아닌, 주어진 점들을 포함하는 컨벡스 셋 초평면(hyperplane): 서포트 벡터 머신 알고리즘의 핵심 개념, ${x|w^Tx=b}$ 내적값 b가 0일 경우 벡터 w와 벡터 $x-x_0$는 수직 반공간(halfspace): 초평면으로 나뉜 공간의 일부, ${w^Tx\\le{b}}$ 6-2. 컨벡스 함수 컨벡스 함수: $$f(wx_1+(1-w)x_2 \\le wf(x_1)+(1-w)f(x_2)$$ 컨벡스 함수에서 등호가 없고 $0 \\le w \\le 1$이면 strictly 컨벡스라고 말함 콘케이브(concave): 컨벡스의 반대되는 개념 (-f가 컨벡스할 경우의 f) 컨벡스 함수의 예로 지수 함수, 절댓값 함수, 멱함수, 지시 함수, 최대 함수 등이 있음 미분이 가능하다는 말은 그래디언트(gradient) $\\nabla f$가 존재한다는 뜻 1차 미분 조건: 최적값 탐색에 사용, $f(x_2) \\ge f(x_1)+\\nabla{f(x_1)^T}(x_2-x_1)$ 그래디언트 값이 0일 때, $x_1$은 함수 f에 대한 전역 최솟값(global minimizer) 2차 미분 조건: 함수 f가 두 번 미분 가능할 경우, $\\nabla^2f(x) \\ge 0$ 얀센의 부등식: $f(wx_1+(1-w)x_2) \\le wf(x_1)+(1-w)f(x_2)$ ","permalink":"https://minyeamer.github.io/blog/2022-07-04/","summary":"선형대수와 통계학으로 배우는 머신러닝 with 파이썬 2","title":"2022-07-04 Log"},{"content":"15-01. Attention Mechanism seq2seq 모델은 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 해서 정보 손실이 발생하며,\nRNN의 고질적인 문제인 기울기 소실 문제도 존재 기계 번역 분야에서 입력 문장이 길면 번역 품질이 떨어지는 것을 보정하기 위해 어텐션 기법 활용 어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시접마다,\n인코더에서 전체 입력 문장을 다시 한 번 참고한다는 점 Attention Function Attention(Q, K, V) = Attention Value 어텐션 함수는 주어진 Query에 대해서 모든 Key와의 유사도를 각각 구하고,\n유사도를 키와 맵핑되어있는 각각의 Value에 반영, 이후 유사도가 반영된 Value를 모두 더해 리턴 Q(Query): t 시점의 디코더 셀에서의 은닉 상태\nK(Keys): 모든 시점의 인코더 셀의 은닉 상태들\nV(Values): 모든 시점의 인코더 셀의 은닉 상태들\nDot-Product Attention 어텐션 메커니즘에서 출력 단어를 예측하기 위해 디코더 셀은\nt-1의 hidden state, t-1에 나온 출력 단어, Attention Value $a_t$를 필요 제안자의 이름을 따서 루옹(Luong) 어텐션이라고도 함 1. Attention Score $a_t$를 구하기 위해서는 Attention Score를 구해야 함\n(인코더의 모든 은닉 상태 각각의 디코더의 현 시점의 은닉 상태 $s_t$와 얼마나 유사한지 판단하는 스코어) Dot-Product Attention에서는 스코어 값을 구하기 위해 $s_t$를 전치하고 각 은닉 상태와 내적을 수행 스코어 함수 $score(s_t,h_i)={s^T_t}{h_i}$ $s_t$와 인코더의 모든 은닉 상태의 어텐션 스코어 모음값 $e^t=[{s^T_t}{h_1},\u0026hellip;,{s^T_t}{h_N}]$ 스코어 함수에 따라 scaled dot, general, concat, location-base 어텐션 등이 존재 2. Attention Distribution $e^t$에 softmax 함수를 적용하여 모든 값을 합하면 1이 되는 확률 분포,\nAttention Distribution을 얻으며, 분포 각각의 값을 Attention Weight라 함 어텐션 분포 $\\alpha^t=softmax(e^t)$ 3. Attention Value 어텐션의 최종 결과값을 얻기 위해서 각 인코더의 은닉 상태와 어텐션 가중치들을 곱하고,\n최종적으로 모두 더하는 Weighted Sum을 진행 어텐션 값이 구해지면 어텐션 메커니즘은 $a_t$를 $s_t$와 결합(concatenate)하여 하나의 벡터 $v_t$를 생성 $v_t\\text{를 }\\hat{y}$ 예측 연산의 입력으로 사용해 인코더로부터 얻은 정보를 활용하여 $\\hat{y}$를 예측 $$a_t=\\Sigma^N_{i=1}{\\alpha^t_i}{h_i}$$\n15-02. Bahdanau Attention 바다나우 어텐션 함수의 Query는 t 시점의 은닉 상태가 아니라 t-1 시점의 은닉 상태를 사용 $score(s_{t-1},h_i)={W^T_\\alpha}\\tanh{({W_b}{s_{t-1}}+{W_c}{h_i})}$ $W_a,W_b,W_c$는 학습 가능한 가중치 행렬을 의미하며, $s_{t-1}$와 $h_1,h_2,h_3,h_4$의 어텐션 스코어를\n각각 구하는 병렬 연산을 위해 $h_1,h_2,h_3,h_4$를 하나의 행렬 $H$로 변환 $e^t={W^T_\\alpha}\\tanh{({W_b}{s_{t-1}}+{W_c}H)}$ 컨텍스트 벡터를 구하면, 현재 시점의 입력인 단어의 임베딩 벡터와 연결(concatenate)하고,\n현재 시점의 새로운 입력으로 사용 16-01. Transformer 어텐션을 RNN의 보정을 위한 용도로 사용하지 않고, 어텐션만으로 인코더와 디코더를 생성 Transformer Hyperparameter $d_{model}=152$\n트랜스 포머의 인코더와 디코더에서의 정해진 입력과 출력의 크기, 임베딩 벡터의 차원\n$num_{layers}=6$\n트랜스포머 모델에서 인코더와 디코더가 총 몇 층으로 구성되었는지\n$num_{heads}=8$\n어텐션을 여러 개로 분할해서 병렬로 수행하고 결과값을 다시 하나로 합치는데, 이때 이 병렬의 개수\n$d_{ff}=2048$\n피드 포워드 신경망의 은닉층의 크기, 입력층과 출력층의 크기는 $d_{model}$\nPositional Encoding RNN은 단어의 위치에 따라 단어를 순차적으로 입력받아 처리하는 특성으로 인해 각 단어의 위치 정보를 가짐 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니기 때문에\n단어의 위치 정보를 얻기 위해 각 단어의 임베딩 벡터에 위치 정보들을 더해 모델의 입력으로 사용(포지셔널 인코딩) 트랜스포머는 위치 정보를 가진 값을 만들기 위해 아래 함수를 사용 $$PE_{(pos,2i)}=\\sin{(pos/10000^{2i/d_{model}})}$$ $$PE_{(pos,2i+1)}=\\cos{(pos/10000^{2i/d_{model}})}$$\n사인 함수와 코사인 함수의 값을 임베딩 벡터에 더해 단어에 순서 정보를 부여 $pos$는 입력 문장에서의 임베딩 벡터의 위치, $i$는 임데빙 벡터 내의 차원의 인덱스 의미 $d_{model}$은 트랜스포머의 모든 층의 출력 차원을 의미하는 트랜스포머의 하이퍼파라미터 Self-Attention 어텐션을 자기 자신에게 수행하는 것 Q, K, V는 모두 입력 문장의 모든 단어 벡터들을 의미 셀프 어텐션은 입력 문장 내의 단어들끼리 유사도를 구해 it이 어떤 단어와 연관되었는지 확률을 찾아냄 셀프 어텐션은 인코더의 초기 입력인 $d_{model}$의 차원을 가지는 단어 벡터들을 사용하여 수행하지 않고,\n각 단어 벡터들로부터 Q벡터, K벡터, V벡터를 얻는 작업을 거침 $d_{model}=512$의 차원을 가졌던 각 단어 벡터들은 Q벡터, K벡터, V벡터로 변환되면서\n$d_{model}$을 $num_{heads}$로 나눈 값 64를 차원으로 갖게 됨 Scaled dot-product Attention 트랜스포머에서는 스케일드 닷-프로적트 어텐션을 사용 벡터마다 일일히 컨텍스트 벡터를 구하는 벡터 연산을 하지 않고,\n문장 행렬에 가중치 행렬을 곱하여 구한 Q행렬, K행렬, V행렬에 행렬 연산을 수행 행렬 연산에서 어텐션 스코어는 행렬의 값에 전체적으로 $\\sqrt{d_k}$를 나누어 스코어 값을 가지는 행렬을 구함 $$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\nMulti-head Attention 한 번의 어텐션을 하는 것보다 여러번의 어텐션을 병렬로 사용하는 것이 더 효과적이기 때문에\n$d_{model}$의 차원을 $num_{heads}$개로 나누어 Q, K, V에 대해서 $num_{heads}$개의 병렬 어텐션을 수행 각각의 어텐션 값 행렬을 어텐션 헤드라고 불는데, 이때 가중치 행렬 $W^Q, W^K, W^V$의 값은 어텐션 헤드마다 전부 다름 멀티 헤드 어텐션은 어텐션을 병렬로 수행하여 다른 시각으로 정보들을 수집 벙렬 어텐션 수행 후 모든 어텐션 헤드를 연결(concatenate)하여 $(seq_{len}, d_{model})$ 크기의 행렬 생성 연결한 행렬에 가중치 행렬 $W^O$를 곱한 것이 멀티 헤드 어텐션의 최종 결과물이며, 인코더의 입력이었던 문장 행렬과 동일\n(인코더에서의 입력의 킉가 출력에서도 동일 크기로 계쏙 유지되어야만 다음 인코더에서도 다시 입력이 될 수 있음) Padding Mask 입력 문장에 토큰이 있을 경우 어텐션에서 제외하기 위해 -1e9라는 아주 작은 음수 값을 곱함 Masking: 어텐션에서 제외하기 위해 값을 가리는 것 어텐션 스코어 행렬에서 행에 해당하는 문장은 Query, 열에 해당하는 문장은 Key이며,\nKey에 가 있는 경우 해당 열 전체를 마스킹 Position-wise FFNN 포지션 와이즈 FFNN은 인코더와 디코더에서 공통적으로 가지고 있는 서브층으로, 완전 연결 FFNN을 의미 $FFNN(x)=MAX(0,x{W_1}+b_1){W_2}+{b_2}$ $x$는 멀티 헤드 어텐션의 결과로 나온 $(seq_{len}, d_{model})$ 크기의 행렬을 의미,\n가중치 행렬 $W_1\\text{은 }(d_{model},d_{ff})\\text{, }W_2\\text{은 }(d_{ff},d_{model})$의 크기를 가짐 서브층을 지난 인코더의 최종 출력은 여전히 인코더의 입력의 크기였던 $(seq_{len}, d_{model})$의 크기가 보존 Residual Connection 트랜스포머에서 두 개의 서브층을 가진 인코더에 추가적으로 사용하는 Add \u0026amp; Norm 기법 중 Add에 해당 잔차 연결은 서브층의 입력과 출력을 더하는 것으로, 컴퓨터 비전 분야에서 주로 사용되는 모델의 학습을 돕는 기법 $x+Sublayer(x)$로 표현할 수 있으며, 멀티 헤드 어텐션이라면 $H(x)=x+Multi\\text{-}head\\ Attention(x)$과 같음 Layer Normalization 잔차 연결의 입력을 $x$, 잔차 연결과 층 정규화 두 가지 연산을 모두 수행한 후의 결과 행렬을 $LN$이라 할 때,\n잔차 연결 후 정규화 연산을 수식으로 표현하면 $LN=LayerNorm(x+Sublayer(x))$와 같음 총 정규화는 텐서의 마지막 차원에 대해서 평균과 분산을 구하고, 이를 가지고 어떤 수식을 통해 값을 정규화하여 학습을 도움 총 정규화를 평균과 분산을 통한 정규화, 감마와 베타를 도입하는 것으로 나누었을 때,\n우선, 평균과 분산을 통해 벡터 $x_i$를 정규화 $x_i$는 벡터인 반면, 평균 $\\mu_i\\text{과 분산 }\\sigma^2_i$은 스칼라이기 때문에,\n벡터 $x_i$의 각 $k$차원의 값은 다음과 같이 정규화 ($\\epsilon$은 분모가 0이 되는 것을 방지하는 값) $$\\hat{x_{i,k}}=\\frac{x_{i,k}-\\mu_i}{\\sqrt{\\sigma^2_i+\\epsilon}}$$\nLook-ahead Mask 입력 단어를 매 시점마다 순차적으로 입력받는 RNN 계열의 신경망에 반해,\n트랜스포머는 문장 행렬을 한 번에 받기 때문에 미래 시점의 단어까지 참고할 수 있는 현상 발생 룩-어헤드 마스크는 디코더의 첫번째 서브층에서 이루어지며,\n자기 자신보다 미래에 있는 단어들은 참고하지 못하도록 마스킹함 룩-어헤드 마스크를 한다고해서 패딩 마스크가 불필요한 것이 아니므로 패딩 마스크를 포함하도록 구현 Endocer-Decoder Attention 디코더의 두번째 서브층은 멀티 헤드 어텐션을 수행한다는 점에서 이전의 어텐션들과 공통점이 있지만,\nQuery와 Key, Value가 달라 셀프 어텐션이 아님 인코더의 첫번째 서브층 Query = Key = Value\n디코더의 첫번째 서브층 Query = Key = Value\n디코더의 두번째 서브층 Query: 디코더 행렬 / Key = Value: 인코더 행렬\n16-02. Transformer Chatbot 트랜스포머를 이용한 한국어 챗봇 참고 챗본 데이터 사용 17-01. Pre-training 사전 훈련된 워드 임베딩 Word2Vec나 GloVe 등의 워드 임베딩은 하나의 단어가 하나의 벡터값으로 맵핑되므로,\n문맥을 고려하지 못하여 다의어나 동음이의어를 구분하지 못하는 문제 사전 훈련된 언어 모델 언어 모델은 주어진 텍스트로부터 다음 단어를 예측하도록 학습하여 별도의 라벨 없이 학습 가능 다의어를 구분할 수 없었던 문제점을 해결하고, RNN 계열의 신경망에서 탈피하기 위해 트랜스포머로 학습 시도 트랜스포머로 데이터를 학습시킨 언어 모델 GPT-1 등은 다양한 태스크에서 높은 성능을 얻음 이전 단어들로부터 다음 단어를 예측하는 언어 모델의 특성으로는 양방향 언어 모델을 사용할 수 없으므로,\nELMo에서는 두 개의 단방향 언어 모델을 따로 준비해 학습하는 방법을 사용했는데 여기서 발전된 마스크드 언어 모델이 등장 Masked Language Model 마스크드 언어 모델은 입력 텍스트의 단어 집합의 15%의 단어를 랜덤으로 Masking 빈칸 채우기 형식으로 마스킹된 단어들을 예측하게 함 17-02. BERT 구글이 공개한 사전 훈련된 모델로 수많은 NLP 태스크에서 최고 성능을 보임 트랜스포머를 이용하여 구현되었으며, 위키피디아와 BooksCorpus 같이 라벨이 없는 텍스트 데이터로 사전 훈련된 언어 모델 사전 훈련된 모델을 라벨이 있는 다른 작업에서 추가 훈련과 함께 하이퍼파라미터를 재조정,\n다른 작업에 대해서 파라미터 재조정을 위한 추가 훈련 과정을 Fine-tuning이라고 함 BERT-Base: L=12, D=768, A=12: 110M개의 파라미터 BERT-Large: L=24, D=1024, A=16: 340M개의 파라미터 Contextual Embedding BERT의 입력은 기존 모델들과 마찬가지로 임베딩 층을 지난 임베딩 벡터 BERT의 연산을 거친 후의 출력 임베딩은 문장의 문맥을 모두 참고한 문맥을 반영한 임베딩이 됨 BERT는 트랜스포머 인코더를 12번 쌓은 구조로, 셀프 어텐션을 통해 문맥을 반영 Subword Tokenizer BERT는 서브워드 토크나이저로 WordPiece 토크나이저를 사용 자주 등장하는 단어는 그대로 단어 집합에 추가하지만, 자주 등장하지 않는 단어는 서브워드로 분리해 집합에 추가하며,\n집합이 만들어지고 나면 단어 집합을 기반으로 토큰화 수행 BERT에서 토큰이 단어 집합에 존재할 경우 해당 토큰을 분리하지 않지만,\n존재하지 않으면 토큰을 서브워드로 분리하고, 첫번째 서브워드를 제외한 나머지 서브워드들은 앞에 #를 붙인 것을 토큰으로 함 #은 서브워드들이 단어의 중간부터 등장하는 것을 알려주기 위해 표시해둔 기호 1 2 3 from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained(\u0026#34;bert-base-uncased\u0026#34;) # BERT-Base의 토크나이저 Position Embedding 포지셔널 인코딩과 유사하지만, 위치 정보를 사인 함수와 코사인 함수가 아닌 학습을 통해서 얻는 방법 위치 정보를 위한 임베딩 층을 하나 더 사용하고, 입력마다 포지션 임베딩 벡터를 더해줌 MLM (Pre-training) BERT는 사전 훈련을 위해서 입력 텍스트의 15%의 단어를 랜덤으로 마스킹 [MASK] 토큰이 파인 튜닝 단계에서 나타나지 않아 사전 학습 단계와 파인 튜닝 단계에서의 불일치 문제가 생기는데,\n이를 완화하기 위해 마스킹 단어 중 80%는 [MASK]로 변경, 10%는 랜덤으로 단어가 변경, 10%는 동일하게 둠 NSP (Pre-training) BERT는 두 개의 문장을 준 후에 이어지는 문장인지 아닌지를 맞추는 방식으로 훈련 BERT의 입력에서 [SEP]라는 특별 토큰을 사용해서 문장을 구분 두 문장이 실제 이어지는 문장인지 아닌지에 대한 이진 분류 문제를 [CLS] 토큰의 위치로 결정 Segment Embedding WordPiece Embedding, Position Embedding과 함께 두 개의 문장을 구분하기 위한 목적으로 사용되는 임베딩 층 세그먼트 임베딩으로 구분되는 BERT의 입력에서 두 개의 문장은 두 종류의 텍스트, 두 개의 문서일 수 있음 Find-tuning 영화 리뷰 감성 분류, 로이터 뉴스 분류 등 Single Text Classification을 위해,\n문서의 시작에 [CLS] 토큰을 입력해 분류에 대해 예측 태깅 작업을 위해 각 토큰의 위치에 밀집층을 사용하여 분류에 대해 예측 자연어 추론 등의 Text Pair Classification을 위해,\n텍스트 사이에 [SEP] 토큰을 집어넣고 두 종류의 세그먼트 임베딩을 모두 사용하여 문서를 구분 QA(Question Answering)를 풀기 위해 질문과 본문이라는 두 개의 텍스트의 쌍을 입력 (SQuAD v1.1) Attention Mask BERT가 어텐션 연산을 할 때, 불필요하게 패딩 토큰에 대해서 어텐션을 하지 않도록 실제 단어와 패딩 토큰을 구분하는 입력 숫자 1은 실제 단어로 마스킹을 하지 않고, 숫자 0은 패딩 토큰으로 마스킹을 함 17-03. Pre-training 실습 구글 BERT의 마스크드 언어 모델 실습 참고 한국어 BERT의 마스크드 언어 모델 실습 참고 구글 BERT의 다음 문장 예측 참고 한국어 BERT의 다음 문장 예측 참고 17-07. Sentence BERT(SBERT) BERT의 문장 임베딩의 성능을 우수하게 개선시킨 모델 문장 쌍 분류 태스크 또는 문장 쌍 회귀 태스크로 파인 튜닝 Sentence Embedding [CLS] 토큰은 입력된 문장에 대한 총체적 표현으로, [CLS] 토큰 자체를 입력 문장의 벡터로 간주 문장 벡터를 얻기 위해 [CLS] 토큰뿐 아니라, BERT의 모든 출력 벡터들을 평균냄 출력 벡터들의 평균을 pooling이라 하며, mean pooling, max pooling 등이 있음 18. BERT 실습 Colab에서 TPU 사용하기 Transformers의 모델 클래스 불러오기 KoBERT를 이용한 네이버 영화 리뷰 분류하기 TFBertForSequenceClassification KoBERT를 이용한 KorNLI 풀어보기 (다중 클래스 분류) KoBERT를 이용한 개체명 인식 KoBERT를 이용한 기계 독해 BERT의 문장 임베딩(SBERT)을 이용한 한국어 챗봇 Faiss와 SBERT를 이용한 시맨틱 검색기 19. Topic Modelling 토픽이라는 문서 집합의 추상적인 주제를 발견하기 위한 통계적 모델 중 하나 텍스트 본문의 숨겨진 의미 구조를 발견하기 위해 사용되는 텍스트 마이닝 기법 19-01. LSA SVD 특이값 분해(Singular Value Decomposition)는 $A$가 ${m}\\times{m}$ 행렬일 때,\n다음과 같이 3개의 행렬의 곱으로 분해(decomposition)하는 것 $$A={U}\\Sigma{V^T}$$\n각 3개 행렬은 다음과 같은 조건을 만족\n${U}\\text{: }{m}\\times{m}\\text{ 직교행렬 }(AA^T=U(\\Sigma\\Sigma^T)U^T)$\n$V\\text{: }{n}\\times{n}\\text{ 직교행렬 }(A^TA=U(\\Sigma^T\\Sigma)V^T)$\n$\\Sigma\\text{: }{m}\\times{n}\\text{ 직사각 대각행렬}$ Truncated SVD Full SVD에서 나온 3개의 행렬에서 일부 벡터들을 삭제시킨 절단된 SVD 대각 행렬 $\\Sigma$의 대각 원소의 값 중에서 상위값 t개만 남기고,\nU행렬과 V행렬의 t열까지만 남김 일부 벡터들을 삭제해 데이터의 차원을 줄이는 것으로 계산 비용이 낮아지는 효과를 얻음 또한 상대적으로 중요하지 않은 정보(노이즈)를 삭제해 기존의 행렬에서 드러나지 않았던 심층적인 의미 확인 Latent Semantic Analysis(LSA) BoW에 기반한 DTM이나 TF-IDF는 단어의 빈도 수를 이용한 수치화 방법으로 단어의 의미를 고려하지 못함 LSA는 DTM이나 TF-IDF 행렬에 절단된 SVD를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌어냄 문서의 유사도 계산 등에서 좋은 성능을 보여줌 SVD의 특성상 이미 계산된 LSA에 새로운 데이터를 추가하려면 처음부터 다시 계산해야해 업데이트가 어려움 19-02. LDA Latent Dirichlet Allocation(LDA) 문서의 집합으로부터 어떤 토픽이 존재하는지를 알아내기 위한 알고리즘 문서들은 토픽들의 혼합으로 구성되어져 있으며, 토픽들은 확률 분포에 기반하여 단어들을 생성한다 가정 문서가 작성되었다는 가정 하에 토픽을 뽑아내기 위해 아래 과정을 역으로 추적하는 역공학을 수행 LDA의 가정 문서에 사용할 단어의 개수 N을 정합 문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정 문서에 사용할 각 단어를 (아래와 같이) 정함\n3-1. 토픽 분포에서 토픽 T를 확률적으로 고름\n3-2. 선택한 토픽 T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어를 고름 LDA 수행 사용자는 알고리즘에게 토픽의 개수 k를 알려줌 모든 단어를 k개 중 하나의 토픽에 할당 모든 문서의 모든 단어에 대해서 아래의 사항을 반복 진행\n3-1. 어떤 문서의 단어 w는 자신의 잘못된 토픽에 할당되어 있지만,\n다른 단어들은 올바른 토픽에 할당되어 있다는 가정 하에 단어 w의 토픽을 재할당 LSA DTM을 차원 축소하여 축소 차원에서 근접 단어들을 토픽으로 묶음 LDA 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽 추출 19-08. BERTopic BERT embeddings과 클래스 기반 TF-IDF를 활용하여\n주제 설명에서 중요한 단어를 유지하면서도 쉽게 해석할 수 있는 조밀한 클러스터를 만드는 토픽 모델링 기술 텍스트 데이터를 SBERT로 임베딩 문서를 군집화 (UMAP을 사용해 임베딩의 차원을 줄이고 HDBSCAN 기술을 사용해 클러스터링) 토픽 표현을 생성 (클래스 기반 TF-IDF 토픽 추출) 20. Text Summarization Extractive Summarization 원문에서 중요한 핵심 문장 또는 단어구 몇 개를 뽑아서 이들로 구성된 요약문을 만드는 방법 추출적 요약의 결과로 나온 요약문의 문장이나 단어구들은 전부 원문에 있는 문장들 대표적인 알고리즘으로 머신 러닝 알고리즘 TextRank가 있음 Abstractive Summarization 원문에 없던 문장이라도 핵심 문맥을 반영한 새로운 문장을 생성해서 원문을 요약하는 방법 주로 seq2seq 같은 인공 신경망을 이용하지만, 지도 학습이기 때문에 라벨 데이터가 있어야함 추상적 요약 구현 아마존 리뷰 데이터 사용 TextRank 페이지랭크를 기반으로 한 텍스트 요약 알고리즘으로,\n텍스트랭크에서 그래프의 노드들은 문장들이며 각 간선의 가중치는 문장들 간의 유사도를 의미 사전 훈련된 GloVe 및 테니스 관련 기사 데이터 사용 21. Question Answering(QA) Babi 데이터셋 ID는 각 문장의 번호를 의미, 스토리가 시작될 때는 1번으로 시작 라벨의 supporting fact는 실제 정답이 주어진 스토리에서 몇 번 ID 문장에 있었는지를 알려줌 메모리 네트워크 구조 두 개의 문장, 스토리 문장과 질문 문장이 입력으로 들어오며, 두 문장은 각각 임베딩 과정을 거침 Embedding C를 통해서 임베딩 된 스토리 문장과 Embedding B를 통해서 임베딩 된 질문 문장은\n내적을 통해 각 단어 간 유사도를 구하고, 그 결과가 softmax 함수를 지나서\nEmbedding A로 임베딩이 된 스토리 문장에 더해짐\n(Embedding A, B, C는 각각 별개의 임베딩 층) Query(질문 문장)와 Key(스토리 문장)의 유사도를 구하고 softmax 함수를 통해 정규화해\nValue(스토리 문장)에 더하는 어텐션 메커니즘과 유사 어텐션 메커니즘을 통해서 질문 문장과의 유사도를 반영한 스토리 문장 표현을 얻고,\n이를 질문 표현과 연결(concatenate)하여 LSTM과 밀집층의 입력으로 사용 QA 태스크 풀기 Babi 데이터셋 사용 MeaN으로 한국어 QA 한국어 Babi 데이터셋 사용 (훈련 데이터, 테스트 데이터) 22. GPT Generative Pre-trained Transformer(GPT) GPT 설명 참고 GPT 실습 KoGPT-2를 이용한 문장 생성 KoGPT-2 텍스트 생성을 이용한 한국어 챗봇 KoGPT-2를 이용한 네이버 영화 리뷰 분류 KoGPT-2를 이용한 KorNLI 분류 ","permalink":"https://minyeamer.github.io/blog/2022-06-30/","summary":"딥 러닝을 이용한 자연어 처리 입문 3","title":"2022-06-30 Log"},{"content":"09-01. Word Embedding Sparse Representation 벡터 또는 행렬의 값이 대부분 0으로 표현되는 방법, one-hot vector 등 단어의 개수가 늘어나면 벡터의 차원이 한없이 커지는 문제, 공간적 낭비 발생 Dense Representation 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤 (0과 1뿐 아니라 실수 포함) Word Embedding 단어를 밀집 벡터의 형태로 표현하는 방법 Embedding Vector: 워드 임베딩 과정을 통해 나온 결과 LSA, Word2Vec, FastText, Glove 등 09-02. Word2Vec Distributed Representation 희소 표현을 다차원 공간에 벡터화하는 방법 분산 표현 방법은 분포 가설이라는 가정 하에 만들어진 표현 방법 분포 가설: 비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다 (귀엽다, 예쁘다, 애교 등의 단어가 주로 함께 등장할 경우 벡터화 시 유사한 벡터값을 가짐) 저차원에 단어의 의미를 여러 차원에다가 분산하여 표현하기 대문에 단어 벡터 간 유의미한 유사도 계산 가능 Word2Vec의 학습 방식으로 CBOW(Continuous Bag of Words)와 Skip-Gram이 존재 CBOW 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법 중심 단어를 기준으로 window size만큼 앞뒤로 단어를 확인 (2n개의 단어 확인) Sliding Window: window를 옆으로 움직여서 주변 단어와 중심 단어의 선택을 변경해가며 학습 데이터셋 생성 CBOW의 인공 신경망은 주변 단어들의 one-hot vector를 입력으로 중간 단어의 one-hot vector를 예측 Word2Vec은 은닉층이 1개이며, 활성화 함수 없이 룩업 테이블이라는 연산을 담당하는 projection layer로 불림 입력층과 투사층 사이의 가중치 W는 ${V}\\times{M}$ 행렬, 투사층에서 출력층 사이의 가중치 W\u0026rsquo;는 ${M}\\times{V}$ 행렬 W와 W\u0026rsquo;는 동일한 행렬의 전치가 아니라 서로 다른 행렬이기 대문에 CBOW는 W와 W\u0026rsquo;를 학습해가는 구조를 가짐 입력 벡터와 가중치 W 행렬의 곱은 W 행렬의 i번째 행을 그대로 읽어오는 것(lookup)과 동일 주변 단어의 one-hot vector와 가중치 W를 곱한 결과 벡터들은 투사층에서 만나 평균인 벡터를 구함 평균 벡터는 두 번째 가중치 행렬 W\u0026rsquo;와 곱해져서 one-hot vector들과 차원이 V로 동일한 벡터가 나옴 해당 벡터에 softmax 함수를 거쳐 다중 클래스 분류 문제를 위한 score vector를 생성 score vector의 j번째 인덱스가 가진 값은 j번째 단어가 중심 단어일 확률로,\nscore vector $\\hat{y}$와 중심 단어의 one-hot vector $y$의 오차를 줄이기 위해 cross-entropy 함수 사용 Skip-gram CBOW와 반대로 중심 단어에서 주변 단어를 예측 중심 단어만을 입력으로 받기 때문에 투사층에서 벡터들의 평균을 구하는 과정은 없음 전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려짐 NNLM vs Word2Vec NNLM은 다음 단어를 예측하는 목적이지만, Word2Vec(CBOW)은 중심 단어를 예측하는 목적, 때문에 NNLM이 이전 단어들만 참고한다면, Word2Vec은 예측 단어의 전후 단어들을 모두 참고 Word2Vec은 NNLM에 존재하던 활성화 함수가 있는 은닉층을 제거하여 학습 속도에서 강점을 가짐 09-03. Word2Vec 실습 위키피디아 실습 1 2 3 4 from gensim.models import Word2Vec from gensim.models import KeyedVectors model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0) size: 워드 벡터의 특징 값, 임베딩된 벡터의 차원 window: context window size min_count: 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 무시) workers: 학습을 위한 프로세스 수 sg: 0은 CBOW, 1은 Skip-gram 1 2 3 4 5 # 입력한 단어에 대해서 가장 유사한 단어들을 출력 model_result = model.wv.most_similar(\u0026#34;man\u0026#34;) print(model_result) [(\u0026#39;woman\u0026#39;, 0.842622697353363), (\u0026#39;guy\u0026#39;, 0.8178728818893433), (\u0026#39;boy\u0026#39;, 0.7774451375007629), (\u0026#39;lady\u0026#39;, 0.7767927646636963), (\u0026#39;girl\u0026#39;, 0.7583760023117065), (\u0026#39;gentleman\u0026#39;, 0.7437191009521484), (\u0026#39;soldier\u0026#39;, 0.7413754463195801), (\u0026#39;poet\u0026#39;, 0.7060446739196777), (\u0026#39;kid\u0026#39;, 0.6925194263458252), (\u0026#39;friend\u0026#39;, 0.6572611331939697)] 1 2 model.wv.save_word2vec_format(\u0026#39;eng_w2v\u0026#39;) # 모델 저장 loaded_model = KeyedVectors.load_word2vec_format(\u0026#34;eng_w2v\u0026#34;) # 모델 로드 1 2 3 4 5 # 사전 훈련된 Word2Vec 임베딩 urllib.request.urlretrieve(\u0026#34;https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\u0026#34;, filename=\u0026#34;GoogleNews-vectors-negative300.bin.gz\u0026#34;) word2vec_model = KeyedVectors.load_word2vec_format(\u0026#39;GoogleNews-vectors-negative300.bin.gz\u0026#39;, binary=True) # shape(3000000, 300) 1 2 # 두 단어의 유사도 계산 print(word2vec_model.similarity(\u0026#39;this\u0026#39;, \u0026#39;is\u0026#39;)) # 0.407970363878 09-04. Negative Sampling Word2Vec이 학습 과정에서 전체 단어 집합이 아니라 일부 단어 집합에만 집중할 수 있도록 하는 방법 중심 단어에 대해 무작위로 선택된 일부 단어 집합에 대해 긍정 또는 부정을 예측하는 이진 분류 수행 전체 단어 집합의 크기만큼 선택지를 두고 다중 클래스 분류 문제를 푸는 Word2Vec보다 효율적인 연산 SGNS 네거티브 샘플링을 사용하는 Skip-gram은 중심 단어와 주변 단어가 모두 입력이 되고,\n두 단어가 실제로 윈도우 크개 내에 존재하는 이웃 관계인지 확률을 예측 중심 단어에 대한 라벨로 주변 단어를 사용하지 않고,\n중심 단어와 주변 단어에 대한 이웃 관계를 표시하기 위한 라벨로 1 또는 0을 사용 SGNS 구현 20newsgroups 데이터 사용 1 2 3 4 5 6 7 8 # 네거티브 샘플링 데이터셋 생성 from tensorflow.keras.preprocessing.sequence import skipgrams skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded] # (commited (7837), badar (34572)) -\u0026gt; 0 # (whole (217), realize (1036)) -\u0026gt; 1 # (reason (149), commited (7837)) -\u0026gt; 1 1 2 3 from tensorflow.keras.models import Sequential, Model from tensorflow.keras.layers import Embedding, Reshape, Activation, Input from tensorflow.keras.layers import Dot 1 2 3 4 5 6 7 8 9 embedding_dim = 100 # 중심 단어를 위한 임베딩 테이블 w_inputs = Input(shape=(1, ), dtype=\u0026#39;int32\u0026#39;) word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs) # 주변 단어를 위한 임베딩 테이블 c_inputs = Input(shape=(1, ), dtype=\u0026#39;int32\u0026#39;) context_embedding = Embedding(vocab_size, embedding_dim)(c_inputs) 1 2 3 4 5 6 7 8 # 두 임베딩 테이블에 대한 내적의 결과로 1 또는 0을 예측하기 위해 시그모이드 함수 사용 dot_product = Dot(axes=2)([word_embedding, context_embedding]) dot_product = Reshape((1,), input_shape=(1, 1))(dot_product) output = Activation(\u0026#39;sigmoid\u0026#39;)(dot_product) model = Model(inputs=[w_inputs, c_inputs], outputs=output) model.summary() model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;) 1 2 3 4 5 6 7 8 9 10 11 # 5epochs 학습 for epoch in range(1, 6): loss = 0 for _, elem in enumerate(skip_grams): first_elem = np.array(list(zip(*elem[0]))[0], dtype=\u0026#39;int32\u0026#39;) second_elem = np.array(list(zip(*elem[0]))[1], dtype=\u0026#39;int32\u0026#39;) labels = np.array(elem[1], dtype=\u0026#39;int32\u0026#39;) X = [first_elem, second_elem] Y = labels loss += model.train_on_batch(X,Y) print(\u0026#39;Epoch :\u0026#39;,epoch, \u0026#39;Loss :\u0026#39;,loss) 09-05. GloVe 카운트 기반과 예측 기반을 모두 사용하는 방법론으로, LSA와 Word2Vec의 단점 보완 LSA는 단어의 빈도수를 차원 축소하여 잠재된 의미를 끌어내지만, 같은 단어 의미의 유추 작업 성능은 떨어짐 Word2Vec는 예측 기반으로 단어 간 유추 작업에는 LSA보다 뛰어나지만,\nwindow size 내 주변 단어만을 고려하여 전체적인 통계 정보를 반영하지 못함 임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는 것이 목적 Window based Co-occurrence Matrix 행과 열을 전체 단어 집합의 단어들로 구성하고,\ni 단어의 window size 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬 Co-occurence Probability 동시 등장 확률 $P(k|i)$는 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고,\n특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # pip install glove_python_binary from glove import Corpus, Glove corpus = Corpus() # 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성 corpus.fit(result, window=5) glove = Glove(no_components=100, learning_rate=0.05) glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True) glove.add_dictionary(corpus.dictionary) print(glove.most_similar(\u0026#34;man\u0026#34;)) [(\u0026#39;woman\u0026#39;, 0.9621753707315267), (\u0026#39;guy\u0026#39;, 0.8860281455579162), (\u0026#39;girl\u0026#39;, 0.8609057388487154), (\u0026#39;kid\u0026#39;, 0.8383640509911114)] 09-06. FastText Word2Vec가 단어를 쪼개질 수 없는 단위로 생각한다면,\nFastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주 각 단어를 글자 단위 n-gram의 구성으로 취급하여 n에 따라 단어들이 얼마나 분리되는지 결정 tri-gram의 경우 apple에 대해서 [\u0026lt;ap, app, ppl, ple, le\u0026gt;]로 분리된 벡터 생성\n(\u0026lt;, \u0026gt;는 시작과 끝을 의미) 내부 단어들을 Word2Vec로 벡터화하고 apple의 벡터값은 내부 단어의 벡터값들의 총 합으로 구성 Out Of Vocabulary FastText는 데이터셋만 충분하다면 내부 단어를 통해 모르는 단어에 대해서도 유사도 계산 가능 birthplace를 학습하지 않은 상태라도, birth와 place라는 내부 단어가 있다면 벡터를 얻을 수 있음 Rare Word Word2Vec는 등장 빈도 수가 적은 단어에 대해서 임베딩의 정확도가 높지 않은 단점 FastText는 희귀 단어라도 n-gram이 다른 단어의 n-gram과 겹치는 경우라면,\nWord2Vec보다 비교적 높은 임베딩 벡터값을 얻음 오타와 같은 노이즈가 많은 코퍼스에서도 일정 수준의 성능을 보임 (apple, appple) 1 2 3 from gensim.models import FastText model = FastText(result, size=100, window=5, min_count=5, workers=4, sg=1) 09-08. Pre-trained Word Embedding 사전 훈련된 워드 임베딩 참고 09-09. ELMo 언어 모델로 하는 임베딩이라는 뜻으로, 사전 훈련된 언어 모델을 사용 Word2Vec는 Bank Account와 River Bank에서 Bank의 차이를 구분하지 못하지만,\nELMo는 문맥을 반영한 워드 임베딩을 수행 ELMo 표현을 기존 임베딩 벡터와 연결(concatenate)해서 입력으로 사용 가능 biLM RNN 언어 모델에서 $h_t$는 시점이 지날수록 업데이트되기 때문에,\n문장의 문맥 정보를 점차적으로 반영함 ELMo는 양쪽 방향의 언어 모델(biLM)을 학습하여 활용 biLM은 은닉층이 최소 2개 이상인 다층 구조를 전제로 함 양방향 RNN은 순방향 RNN의 hidden state와 역방향 RNN의 hidden state를 연결하는 것이지만,\nbiLM은 순방향 언어 모델과 역방향 언어 모델이라는 두 개의 언어 모델을 별개의 모델로 보고 학습 각 층(embedding, hidden state)의 출력값이 가진 정보가 서로 다른 것이므로,\n이를 모두 활용하여 순방향 언어 모델과 역방향 언어 모델의 각 층의 출력값을 연결 ELMo Representation 각 층의 출력값을 연결(concatenate) 각 층의 출력값 별로 가중치($s_1, s_2, s_3$) 부여 각 층의 출력값을 모두 더함 (2번과 3번을 요약하여 가중합이라 표현) 벡터의 크기를 결정하는 스칼라 매개변수($\\gamma$)를 곱함 ELMo 활용 스팸 메일 분류하기 데이터 사용 1 2 3 4 # 텐서플로우 1버전에서 사용 가능 %tensorflow_version 1.x pip install tensorflow-hub import tensorflow_hub as hub 1 2 3 4 5 6 7 # 텐서플로우 허브로부터 ELMo를 다운로드 elmo = hub.Module(\u0026#34;https://tfhub.dev/google/elmo/1\u0026#34;, trainable=True) sess = tf.Session() K.set_session(sess) sess.run(tf.global_variables_initializer()) sess.run(tf.tables_initializer()) 1 2 3 # 데이터의 이동이 케라스 → 텐서플로우 → 케라스가 되도록 하는 함수 def ELMoEmbedding(x): return elmo(tf.squeeze(tf.cast(x, tf.string)), as_dict=True, signature=\u0026#34;default\u0026#34;) 1 2 3 4 5 6 7 8 9 from keras.models import Model from keras.layers import Dense, Lambda, Input input_text = Input(shape=(1,), dtype=tf.string) embedding_layer = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text) hidden_layer = Dense(256, activation=\u0026#39;relu\u0026#39;)(embedding_layer) output_layer = Dense(1, activation=\u0026#39;sigmoid\u0026#39;)(hidden_layer) model = Model(inputs=[input_text], outputs=output_layer) model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) 09-10. Embedding Visualization 구글 embedding projector 시각화 도구 (논문 참고) 1 2 3 # !python -m gensim.scripts.word2vec2tensor --input 모델이름 --output 모델이름 !python -m gensim.scripts.word2vec2tensor --input eng_w2v --output eng_w2v # 임베딩 프로젝트에 사용할 metadata.tsv와 tensor.tsv 파일 생성 09-11. Document Embedding 문서 벡터를 이용한 추천 시스템 참고 문서 임베딩 : 워드 임베딩의 평균 참고 Doc2Vec으로 공시 사업보고서 유사도 계산하기 참고 10. RNN Text Classification 케라스를 이용한 텍스트 분류 개요 스팸 메일 분류하기 (RNN) 로이터 뉴스 분류하기 (LSTM) IMDB 리뷰 감성 분류하기 (GRU) 나이브 베이즈 분류기 네이버 영화 리뷰 감성 분류하기(LSTM) 네이버 쇼핑 리뷰 감성 분류하기(GRU) BiLSTM으로 한국어 스팀 리뷰 감성 분류하기 Bayes\u0026rsquo; Theorem $$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$$\nNaive Bayes Classifier 베이즈 정리를 이용한 스팸 메일 확률 표현\nP(정상 메일 | 입력 텍스트) = (P(입력 텍스트 | 정상 메일) x P(정상 메일)) / P(입력 텍스트)\nP(스팸 메일 | 입력 텍스트) = (P(입력 텍스트 | 스팸 메일) x P(스팸 메일)) / P(입력 텍스트) 나이브 베이즈 분류기에서 토큰화 이전의 단어의 순서는 중요하지 않음\n(BoW와 같이 단어의 순서를 무시하고 빈도수만 고려) 정상 메일에 입력 텍스트가 없어 확률이 0%가 되는 것을 방지하기 위해\n각 단어에 대한 확률의 분모, 분자에 전부 숫자를 더해서 분자가 0이 되는 것을 방지하는 라플라스 스무딩 사용 1 2 3 4 5 6 from sklearn.feature_extraction.text import TfidfTransformer from sklearn.naive_bayes import MultinomialNB # 다항분포 나이브 베이즈 모델 # alpha=1.0: 라플라스 스무딩 적용 model = MultinomialNB() # MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) model.fit(tfidfv, newsdata.target) 11-01. Convolutional Neural Network 이미지 처리에 탁월한 성능을 보이는 신경망 합성곱 신경망은 convolutional layer와 pooling layer로 구성 합성곱 연산(CONV)의 결과가 ReLU를 거쳐서 POOL 구간을 지나는 과정 Channel: 이미지는 (높이, 너비, 채널)이라는 3차원 텐서로 구성, 채널은 색 성분을 의미 합성곱 신경망은 이미지의 모든 픽셀이 아닌, 커널과 맵핑되는 픽셀만을 입력으로 사용하여\n다층 퍼셉트론보다 훨씬 적은 수의 가중치를 사용하여 공간적 구조 정보를 보존 편향을 추가할 경우 커널을 적용한 뒤에 더해지며, 단 하나의 편향이 커널이 적용된 결과의 모든 원소에 더해짐 다수의 채널을 가진 입력 데이터일 경우 커널의 채널 수도 입력의 채널 수만큼 존재,\n각 채널 간 합성곱 연산을 마치고 그 결과를 모두 더해서 하나의 채널을 가지는 특성 맵 생성 Convolution Operation 이미지의 특징을 추출 Kernel(filter)라는 ${n}\\times{m}$ 크기의 행렬로 각 이미지를 순차적으로 훑음 Feature map: 합성곱 연산을 통해 나온 결과 Stride: 커널의 이동 범위, 특성 맵의 크기 Padding: 합성곱 연산 이후에도 특성 맵의 크기가 입력과 동일하도록 행과 열 추가 11-02. 1D CNN 1D Convolutions LSTM과 동일하게 각 단어가 벡터로 변환된 문장 행렬을 입력으로 받음 커널의 너비는 임베딩 벡터의 차원과 동일, 커널의 높이만으로 해당 커널의 크기라 간주 커널의 너비가 임베딩 벡터의 차원이기 때문에 너비 방향으로 움직이지 못하고 높이 방향으로만 움직임 Max-pooling 1D CNN에서의 폴링 층 각 합성곱 연산으로부터 얻은 결과 벡터에서 가장 큰 값을 가진 스칼라 값을 빼내는 연산을 수행 1D CNN 구현 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint embedding_dim = 256 # 임베딩 벡터의 차원 dropout_ratio = 0.3 # 드롭아웃 비율 num_filters = 256 # 커널의 수 kernel_size = 3 # 커널의 크기 hidden_units = 128 # 뉴런의 수 model = Sequential() model.add(Embedding(vocab_size, embedding_dim)) model.add(Dropout(dropout_ratio)) model.add(Conv1D(num_filters, kernel_size, padding=\u0026#39;valid\u0026#39;, activation=\u0026#39;relu\u0026#39;)) model.add(GlobalMaxPooling1D()) model.add(Dense(hidden_units, activation=\u0026#39;relu\u0026#39;)) model.add(Dropout(dropout_ratio)) model.add(Dense(1, activation=\u0026#39;sigmoid\u0026#39;)) es = EarlyStopping(monitor=\u0026#39;val_loss\u0026#39;, mode=\u0026#39;min\u0026#39;, verbose=1, patience=3) mc = ModelCheckpoint(\u0026#39;best_model.h5\u0026#39;, monitor=\u0026#39;val_acc\u0026#39;, mode=\u0026#39;max\u0026#39;, verbose=1, save_best_only=True) model.compile(optimizer=\u0026#39;adam\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;, metrics=[\u0026#39;acc\u0026#39;]) history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[es, mc]) 11-06. Intent Classification 사전 훈련된 워드 임베딩을 이용한 의도 분류 참고 11-07. Character Embedding \u0026lsquo;misunderstand\u0026rsquo;의 의미를 \u0026lsquo;mis-\u0026lsquo;라는 접두사와 \u0026lsquo;understand\u0026rsquo;를 통해 추측하는 것과 같이,\n사람의 이해 능력을 흉내내는 알고리즘 1D CNN에서는 단어를 문자 단위로 쪼개기만하면 되기 때문에 OOV라도 벡터를 얻을 수 있음 BiLSTM에서도 문자에 대한 임베딩을 통해 얻은 벡터를 단어에 대한 벡터로 사용 12. Tagging Task 양방향 LSTM를 이용한 품사 태깅 개체명 인식 개체명 인식의 BIO 표현 이해하기 BiLSTM을 이용한 개체명 인식 BiLSTM-CRF를 이용한 개체명 인식 문자 임베딩 활용하기 BIO 표현 개체명이 시작되는 부분에 B(Begin), 개체명의 내부에 I(Inside), 나머지로 O(Outside) 태깅 개체명 태깅엔 LOC(location), ORG(organization), PER(person), MISC(miscellaneous)\n태그가 추가로 붙음 (B-ORG 등) CRF(Conditional Random Field) LSTM 위에 CRF 층을 추가하면 모델은 예측 개체명(레이블 간 의존성)을 고려 기존 양방향 LSTM 모델은 활성화 함수를 지난 시점에서 개체명을 결정했지만,\nCRF 층을 추가한 모델에서는 활성화 함수의 결과들이 CRF 층의 입력으로 전달 CRF 층은 [문장의 첫번쨰 단어에서는 I가 나오지 않는다, O-I 패턴은 나오지 않는다] 등의 제약사항을 학습 양방향 LSTM은 입력 단어에 대한 양방향 문맥을 반영하며, CRF는 출력 레이블에 대한 양방향 문맥을 반영 CRF 층은 one-hot encoding된 라벨을 지원하지 않음 13-01. Byte Pair Encoding UNK(Unknown Token) 등의 OOV 문제를 해결하기 위해 서브워드 분리 작업을 수행 BPE 알고리즘은 연속적으로 가장 많이 등장한 글자의 쌍을 찾아서 하나의 글자로 병합 BPE는 글자 단위에서 점차적으로 단어 집합을 만들어내는 Bottom up 방식의 접근 사용 WordPiece Tokenizer BPE의 변형 알고리즘으로, 코퍼스의 likelihood를 가장 높이는 쌍을 병합 모든 단어의 맨 앞에 _를 붙이고, 단어는 subword로 통계에 기반하여 띄어쓰기로 분리 WordPiece TOkenizer 겨로가를 되돌리기 위해서는 모든 띄어쓰기를 제거하고 언더바를 띄어쓰기로 바꿈 Unigram Language Model Tokenizer 각각의 서브워드들에 대해서 손실(loss)을 계산 서브 단어의 손실은 해당 서브워드가 단어 집합에서 제거되었을 경우, 코퍼스의 likelihood가 감소하는 정도 서브워드들의 손실의 정도를 정렬하여, 최악의 영향을 주는 10~20%의 토큰을 제거 13-02. SentencePiece 내부 단어 분리를 위한 구글의 패키지 사전 토큰화 작업없이 단어 분리 토큰화를 수행하여 언어에 종속적이지 않음 1 2 3 4 import sentencepiece as spm # IMDB 리뷰 데이터 사용 spm.SentencePieceTrainer.Train(\u0026#39;--input=imdb_review.txt --model_prefix=imdb --vocab_size=5000 --model_type=bpe --max_sentence_length=9999\u0026#39;) input: 학습시킬 파일 model_prefix: 만들어질 모델 이름 vocab_size: 단어 집합의 크기 model_type: 사용할 모델 (unigram(default), bpe, char, word) max_sentence_length: 문장의 최대 길이 13-03. SubwordTextEncoder Wordpiece 모델을 채택한 텐서플로우의 서브워드 토크나이저 1 2 3 4 import tensorflow_datasets as tfds tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus( train_df[\u0026#39;review\u0026#39;], target_vocab_size=2**13) 13-04. Huggingface Tokenizer BertWordPieceTokenizer: BERT에서 사용된 워드피스 토크나이저(WordPiece Tokenizer) CharBPETokenizer: 오리지널 BPE ByteLevelBPETokenizer: BPE의 바이트 레벨 버전 SentencePieceBPETokenizer: 앞서 본 패키지 센텐스피스(SentencePiece)와 호환되는 BPE 구현체 BertWordPieceTokenizer 1 2 3 from tokenizers import BertWordPieceTokenizer tokenizer = BertWordPieceTokenizer(lowercase=False, trip_accents=False) 1 2 3 4 5 6 7 8 9 data_file = \u0026#39;naver_review.txt\u0026#39; vocab_size = 30000 limit_alphabet = 6000 min_frequency = 5 tokenizer.train(files=data_file, vocab_size=vocab_size, limit_alphabet=limit_alphabet, min_frequency=min_frequency) 14-01. Sequence-to-Sequence(seq2seq) seq2seq는 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 다양한 분야에서 사용되는 모델 챗봇, 기계 번역, 내용 요약, STT(Speech to Text) 등에서 주로 사용 seq2seq는 인코더와 디코더로 나눠지며, 둘 다 LSTM 셀 또는 GRU 셀을 사용하는 RNN 아키텍처로 구성 softmax 함수를 통해 출력 시퀀스의 각 단어별 확률값을 반환하고, 디코더는 출력 단어를 결정 Encoder 입력 문장의 모든 단어들을 순차적으로 입력받고 모든 단어 정보들을 압축해서 하나의 벡터 생성 인코더 RNN 셀의 마지막 hidden state를 context vector로 디코더에 넘겨줌 Decoder 압축된 컨텍스트 벡터를 받아서 번역된 단어를 한 개씩 순차적으로 출력 기본적으로 RNNLM으로, 초기 입력으로 문장의 시작을 의미하는 심볼 가 들어감 첫번째 시점의 디코더 RNN 셀은 예측된 단어를 다음 시점의 RNN 셀 입력으로 넣으며,\n문장의 끝을 의미하는 심볼인 가 다음 단어로 예측될 때까지 반복해서 예측 훈련 과정에서는 실제 정답 상황에서 가 나와야 된다고 정답을 알려줌\n(교사 강요: 이전 시점의 디코더 셀의 예측이 틀릴 경우 연쇄 작용을 방지) seq2seq 구현 프랑스-영어 병렬 코퍼스 데이터 사용 병렬 코퍼스 데이터에서 쌍이 되는 데이터의 길이가 같지 않음에 주의 1 2 3 4 5 6 7 8 9 10 # Encoder encoder_inputs = Input(shape=(None, src_vocab_size)) encoder_lstm = LSTM(units=256, return_state=True) # encoder_outputs은 여기서는 불필요 encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs) # LSTM은 바닐라 RNN과는 달리 상태가 두 개, 은닉 상태와 셀 상태 encoder_states = [state_h, state_c] 1 2 3 4 5 6 7 8 9 10 11 12 13 # Decoder decoder_inputs = Input(shape=(None, tar_vocab_size)) decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True) # 디코더에게 인코더의 은닉 상태, 셀 상태를 전달 decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state=encoder_states) decoder_softmax_layer = Dense(tar_vocab_size, activation=\u0026#39;softmax\u0026#39;) decoder_outputs = decoder_softmax_layer(decoder_outputs) model = Model([encoder_inputs, decoder_inputs], decoder_outputs) model.compile(optimizer=\u0026#34;rmsprop\u0026#34;, loss=\u0026#34;categorical_crossentropy\u0026#34;) 1 model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=40, validation_split=0.2) seq2seq 동작 번역하고자 하는 입력 문장이 인코더에 들어가서 은닉 상태와 셀 상태를 얻음 상태와 에 해당하는 \u0026lsquo;\\t\u0026rsquo;를 디코더로 보냄 디코더가 에 해당하는 \u0026lsquo;\\n\u0026rsquo;이 나올 때까지 다음 문자를 예측하는 행동을 반복 14-02. BLEU Score Bilingual Evaluation Understudy(BLEU) 기계 번역 결과와 사람이 직접 번역한 결과가 얼마나 유사한지 비교하여 번역에 대한 성능을 측정하는 방법 측정 기준은 n-gram에 기반 언어에 구애받지 않고 사용할 수 있으며, 계산 속도가 빠른 이점 PPL과 달리 높을 수록 성능이 더 좋음을 의미 Unigram Precision 사람이 번역한 문장 중 어느 한 문장이라도 등장한 단어의 개수를 카운트하는 측정 방법 기계 번역기가 번역한 문장을 Ca, 사람이 번역한 문장을 Ref라 표현 $$\\text{Unigram Precision}=\\frac{\\text{Ref들 중에서 존재하는 Ca의 단어의 수}}{\\text{Ca의 총 단어 수}}$$\nModified Unigram Precision 하나의 단어가 여러번 반복되는 경우에서 정밀도가 1이 나오는 문제를 개선하기 위해\n유니그램이 이미 매칭된 적이 있는지를 고려 $Max_Ref_Count$: 유니그램이 하나의 Ref에서 최대 몇 번 등장했는지 카운트 $Count_{dip}=min(Count,Max_Ref_Count)$ $$\\text{Modified Unigram Precision}=\\frac{\\text{Ca의 각 유니그램에 대해 }Count_{dip}\\text{을 수행한 값의 총 합}}{\\text{Ca의 총 유니그램 수}}$$\nBLEU Score 유니그램 정밀도는 단어의 빈도수로 접근하기 때문에 단어의 순서를 고려하기 위해 n-gram 이용 BLEU 최종 식은 보정된 정밀도 $p_1,p_2,\u0026hellip;,p_n$을 모두 조합 해당 BLEU 식의 경우 문장의 길이가 짧을 때 높은 점수를 받는 문제가 있기 때문에,\n길이가 짧은 문장에게 Brevity Penalty를 줄 필요가 있음 $BP$는 Ca와 가장 길이 차이가 작은 Ref의 길이 $r$을 기준으로 $e^{(1-r/c)}$ 값을 곱하며,\n문장이 $r$보다 길어 패널티를 줄 필요가 없는 경우 1이어야 함 $$\\text{보정된 정밀도 } p_1=\\frac{\\Sigma_{{unigram}\\in{Candidate}}Count_{dip}(unigram)}{\\Sigma_{{unigram}\\in{Candidate}}Count(unigram)}$$ $$\\text{n-gram 일반화 } p_n=\\frac{\\Sigma_{{n\\text{-}gram}\\in{Candidate}}Count_{dip}(n\\text{-}gram)}{\\Sigma_{{n\\text{-}gram}\\in{Candidate}}Count(n\\text{-}gram)}$$ $$BLEU={BP}\\times{exp(\\Sigma^N_{n=1}{w_n}{\\log{p_n}})}$$\n1 2 3 import nltk.translate.bleu_score bleu_score(candidate.split(),list(map(lambda ref: ref.split(), references))) ","permalink":"https://minyeamer.github.io/blog/2022-06-29/","summary":"딥 러닝을 이용한 자연어 처리 입문 2","title":"2022-06-29 Log"},{"content":"02-01. Tokenization Corpus에서 token이라 불리는 단위로 나누는 작업 단어 토큰화에서 단순히 구두점이나 특수문자를 제거하는 것은 의미의 손실을 발생시킬 수 있기 때문에,\n사용자의 목적과 일치하는 토큰화 도구를 사용할 필요가 있음 구두점이나 특수 문자가 필요한 경우: Ph.D, AT\u0026amp;T, $45.55, 01/02/06 등 줄임말과 단어 내에 띄어쓰기가 있는 경우: what\u0026rsquo;re/what are, New York, rock \u0026rsquo;n\u0026rsquo; roll 등 문장 토큰화에서 단순히 마침표를 기준으로 문장을 잘라내는 것은 192.168.56.31, gmail.com과 같은 경우를 고려했을 때 올바르지 않음 한국어 토큰화 한국어의 경우 띄어쓰기가 가능한 단위가 어절인데,\n\u0026lsquo;그가\u0026rsquo;, \u0026lsquo;그에게\u0026rsquo;, \u0026lsquo;그를\u0026rsquo;과 같이 어절이 독립적인 단어로 구성되는 것이 아니라\n조사 등의 무언가가 붙어있는 경우가 많기 때문에 이를 전부 형태소 단위로 분리해줘야 함 자립 형태소: 접사, 어미, 조사와 상관업싱 자립하여 사용할 수 있는 형태소, [체언, 수식언, 감탄사] 등 의존 형태소: 다른 형태소와 결합하여 사용되는 형태소, [접사, 어미, 조사, 어간] 한국어의 경우 띄어쓰기가 지켜지지 않아도 글을 쉽게 이해할 수 있어 띄어쓰기가 잘 지켜지지 않음 품사 태깅: 단어의 의미를 제대로 파악하기 위해서는 해당 단어가 어떤 품사로 쓰였는지 구분할 필요 NLTK, KoNLPy 1 2 from nltk.tokenize import word_tokenize # 단어 토큰화 from nltk.tag import pos_tag # 품사 태깅 1 2 3 4 from konlpy.tag import Okt okt = Okt() okt.porphs(sentence) # 형태소 추출 okt.pos(sentence) # 품사 태깅 02-02. Cleaning and Normalization Cleaning(정제): 갖고 있는 corpus로부터 노이즈 데이터를 제거 Normalization(정규화): 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만듦 영어권 언어에서 단어의 개수를 줄이는 정규화 방법으로 대,소문자 통합을 활용 노이즈 데이터: 아무 의미 없는 특수 문자 등, 분석하고자 하는 목적에 맞지 않는 불필요한 단어들 불필요한 단어를 제거하기 위해 불용어, 등장 빈도가 적은 단어, 길이가 짧은 단어 등을 제거 노이즈 데이터의 특징을 잡아낼 수 있다면, 정규표현식을 사용해서 제거 02-03. Stemming and Lemmatization Lemmatization Lemma(표제어): 기본 사전형 단어, [am, are, is]의 뿌리 단어 be 등 Stem(어간): 단어의 의미를 담고 있는 단어의 핵심 부분, \u0026lsquo;cats\u0026rsquo;에서 \u0026lsquo;cat\u0026rsquo; Affix(접사): 단어에 추가적인 의미를 주는 부분, \u0026lsquo;cats\u0026rsquo;에서 \u0026rsquo;s\u0026rsquo; 1 2 3 4 from nltk.stem import WordNetLemmatizer lemmatizer = WordNetLemmatizer() lemmatizer.lemmatize(word) # am -\u0026gt; be, having -\u0026gt; have Stemming 1 2 3 4 from nltk.stem import PorterStemmer stemmer = PorterStemmer() stemmer.stem(word) # am -\u0026gt; am, having -\u0026gt; hav 한국어에서의 어간 추출 5언 9품사의 구조에서 용언에 해당되는 동사와 형용사는 어간과 어미의 결합으로 구성 활용: 용언의 어간이 어미를 가지는 일 규칙 활용: 어간이 어미를 취할 때 어간의 모습이 일정, 잡/어간 + 다/어미 불규칙 활용: 어간이 어미를 취할 때 어간의 모습이 바뀌거나 특수한 어미일 경우, \u0026lsquo;오르+아/어-\u0026gt;올라\u0026rsquo; 등 02-04. Stopword Stopword(불용어): 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 기여하지 않는 단어, [조사, 접미사] 등 1 2 from nltk.corpus import stopwords stop_words_list = stopwords.words(\u0026#39;english\u0026#39;) 1 2 3 okt = Okt() okt.morphs(sentence) # 조사, 접속사 등 제거 # 또는 불용어 사전을 만들어서 제거 02-05. Regular Expression 정규 표현식 참고 02-06. Integer Encoding 컴퓨터는 텍스트보다 숫자를 더 잘 처리할 수 있기 때문에 텍스트를 숫자로 변경 단어를 빈도수 순으로 정렬하고 순서대로 낮은 숫자부터 정수를 부여 dictionary, Counter, nltk.FreqDist, keras.Tokenizer 등 활용 1 2 from nltk import FreqDist FreqDist(np.hstack(preprocessed_sentences)) # np.hastack으로 문장 구분을 제거 1 2 3 4 5 6 from tensorflow.keras.preprocessing.text import Tokenizer tokenizer = Tokenizer() # num_words 파라미터로 사용할 단어 개수 지정 tokenizer.fit_on_texts(preprocessed_sentences) # 빈도수 기분으로 단어 집합 생성 print(tokenizer.word_intex) # 정수 인덱스 확인 print(tokenizer.word_counts) # 단어 빈도수 확인 print(tokenizer.texts_to_sequences(preprocessed_sentences)) # corpus를 인덱스로 변환 02-07. Padding 병렬 연산을 위해 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요 1 2 3 4 5 6 from tensorflow.keras.preprocessing.sequence import pad_sequences encoded = tokenizer.texts_to_sequences(preprocessed_sentences) padded = pad_seqences(encoded) # padding=\u0026#39;post\u0026#39;를 입력해야 뒤에서 부터 0을 채움 # maxlen으로 문장 길이 조절 # truncating=\u0026#39;post\u0026#39;를 통해 문장 길이 초과 시 뒤의 단어가 삭제되도록 설정 02-08. One-Hot Encoding Vocabulary(단어 집합): 서로 다른 단어들의 집합, book과 books과 같은 변형 형태도 다른 단어로 간주 One-Hot Encoding: 단어 집합의 크기를 벡터의 차원으로 하고,\n표현하고 싶은 단어에 1, 다른 인텍스에 0을 부여하는 단어의 벡터 표현 방식 단어의 개수가 늘어날 수록 벡터를 저장하기 위해 필요한 공간이 계속 늘어나는 단점 단어의 유사도를 표현하지 못하는 단점 (강아지, 개, 냉장고 등) 1 2 3 from tensorflow.keras.utils import to_categorical encoded = tokenizer.texts_to_sequences(preprocessed_sentences)[0] one_hot = to_categorical(encoded) 03-01. Language Model 단어 시퀀스(문장)에 확률을 할당하는 모델, 이전 단어들이 주어졌을 때 다음 단어를 예측 단어 시퀀스 W의 확률 $P(W)=P(w_1,w_2,w_3,w_4,w_5,\u0026hellip;,w_n)$ 다음 단어 등장 확률 $P(w_n|w_1,\u0026hellip;,w_{n-1})$ 03-02. Statistical Language Model 조건부 확률 남학생(A) 여학생(B) 계 중학생(C) 100 60 160 고등학생(D) 80 120 200 계 180 180 360 학생을 뽑았을 때, 고등학생이면서 남학생일 확률 $P(A \\bigcap B)=80/360$ 고등학생 중 한명을 뽑았을 때, 남학생일 확률 $P(A|D)=P(A \\bigcap D)/P(D)=(80/360)/(200/360)$ 문장에 대한 확률 \u0026lsquo;An adorable little boy is spreading smiles\u0026rsquo;의 확률\n$P(\\text{An adorable little boy is spreading smiles})=\\ P(\\text{An}) \\times P(\\text{adorable}|\\text{An}) \\times \u0026hellip; \\times P(\\text{smiles}|\\text{An adorable little boy is spreading})$ 카운트 기반 접근 An adorable little boy가 100번 등장했을 때 그 다음에 is가 등장한 경우가 30번이라면,\n$P(\\text{is}|\\text{An adorable little boy})$는 30% 카운트 기반으로 훈련할 경우 단어 시퀀스가 없어 확률이 0이 되는 경우를 방지하기 위해 방대한 양의 훈련 데이터가 필요 회소 문제: 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제 $$P(\\text{is}|\\text{An adorable little boy})= \\frac{count(\\text{An adorable little boy is})}{count(\\text{An adorable little boy})}$$\n03-03. N-gram Language Model 통계적 언어 모델의 일종이지만, 모든 단어가 아닌 일부 단어만 고려하는 접근 방법 사용 An adorable little boy에서 is가 나올 확률을 boy가 나왔을 때 is가 나올 확률로 대체\n$P(\\text{is}|\\text{An adorable little boy}) \\approx P(\\text{is}|\\text{boy})$ 뒤의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 발생 전체 문장을 고려한 언어 모델보다는 정확도가 떨어짐 몇 개의 단어를 볼지 n을 정하는 것은 trade-off 문제를 발생시킴, n은 최대 5를 넘게 잡아서는 안된다고 권장 N-gram n개의 연속적인 단어 나열 An adorable little boy에 대해\nunigrams: an, adorable, little, boy\nbigrams: an adorable, adorable little, little boy 03-05. Perplexity Perplexity(PPL): 헷갈리는 정도, 낮을수록 언어 모델의 성능이 좋음 $$PPL(W)=P(w_1,w_2,w_3,\u0026hellip;,w_N)^{-\\frac{1}{N}}=\\sqrt[N]{\\frac{1}{P(w_1,w_2,w_3,\u0026hellip;,w_N)}}$$\nBranching Factor Branching factor(분기계수): PPL이 선택할 수 있는 가능한 경우의 수 대해 PPL이 10이 나왔을 때, 언어 모델은 테스트 데이터에 대해 다음 단어를 예측할 때 평균 10개의 단어를 고려 PPL의 값이 낮다는 것은 테스트 데이터 상에서 높은 정확도를 보이는 것일뿐, 반드시 사람이 직접 느끼기에 좋은 모델인 것은 아님 $$PPL(W)=P(w_1,w_2,w_3,\u0026hellip;,w_N)^{-\\frac{1}{N}}=(\\frac{1}{10}^N)^{-\\frac{1}{N}}=\\frac{1}{10}^{-1}=10$$\n04-01. 단어의 표현 방법 국소 표현(이산 표현): 해당 단어 그 자체만 보고, 특정값을 맵핑하여 단어를 표현하는 방법 분산 표현(연속 표현): 그 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법 04-02. Bag of Words(BoW) 단어들의 순서는 고려하지 않고, 단어들의 출현 빈도에만 집중하는 텍스트 데이터 수치화 표현 방법 1 2 3 doc1 = \u0026#39;정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\u0026#39; vocabulary : {\u0026#39;정부\u0026#39;: 0, \u0026#39;가\u0026#39;: 1, \u0026#39;발표\u0026#39;: 2, \u0026#39;하는\u0026#39;: 3, \u0026#39;물가상승률\u0026#39;: 4, \u0026#39;과\u0026#39;: 5, \u0026#39;소비자\u0026#39;: 6, \u0026#39;느끼는\u0026#39;: 7, \u0026#39;은\u0026#39;: 8, \u0026#39;다르다\u0026#39;: 9} bag of words vector : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1] 1 2 3 4 from sklearn.feature_extraction.text import CounteVectorizer vector = CounterVectorizer() # stop_words 파라미터로 불용어 제거(\u0026#39;english\u0026#39; 또는 리스트 등) print(\u0026#39;bag of words vector:\u0026#39;, vector.fit_transform(corpus).toarray()) print(\u0026#39;vocabulary:\u0026#39;, vector.vocabulary_) 04-03. Document-Term Matrix(DTM) 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것 One-hot vector와 마찬가지로 대부분의 값이 0인 희소 표현의 문제 발생 불용어와 중요한 단어에 대해서 가중치를 주기 위해 TF-IDF를 사용 04-04. TF-IDF 단어의 빈도와 역 문서 빈도를 사용하여 DTM 내의 각 단어들마다 중요도를 가중치로 부여하는 방법 $tf(d,t)$: 특정 문서 $d$에서의 특정 단어 $t$의 등장 횟수, DTM에서의 각 단어들의 가진 값 $df(t)$: 특정 단어 $t$가 등장한 문서의 수, 특정 단어가 각 문서에서 등장한 횟수는 무시 $idf(d,t)$: $df(t)$에 반비례하는 수,\n총 문서의 수 n이 커질수록 기하급수적으로 증가하는 것을 방지하기 위해 $log$(일반적으로 자연 로그) 적용 $$idf(d,t)=log(\\frac{n}{1+df(t)})$$\n1 2 3 from sklearn.feature_extraction.text import TfidfVectorizer print(vector.fit_transform(corpus).toarray()) print(ve) 05. Vector Similarity Cosine Similarity 두 벡터 간의 코사인 각도를 이용하여 구할 수 있는 두 벡터의 유사도 두 벡터의 방향이 동일하면 1의 값을 가지며, 값이 1에 가까울수록 유사도가 높음 문서의 길이가 다른 상황에서 비교적 공정한 비교를 할 수 있음 Euclidean Distance 다차원 공간에서 두 개의 점 $p$와 $q$가 각각 $p=(p_1,p_2,p_3,\u0026hellip;,p_n)$과 $q=(q_1,q_2,q_3,\u0026hellip;,q_n)$의 좌표를 가질 때\n두 점 사이의 거리를 계산하는 유클리드 거리 공식 $$\\sqrt{(q_1-p_1)^2+(q_2-p_2)^2+\u0026hellip;+(q_n-p_n)^2}=\\sqrt{\\Sigma^n_{i=1}(q_i-p_i)^2}$$\nJaccard Similarity 합집합에서 교집합의 비율을 구한다면 두 집합 A와 B의 유사도를 구할 수 있음 자카드 유사도 J는 0과 1사이의 값을 가지며, 두 집합이 동일하면 1, 공통 원소가 없으면 0의 값을 가짐 $$J(A,B)=\\frac{|A \\bigcap B|}{|A \\bigcup B|}=\\frac{|A \\bigcap B|}{|A|+|B|-|A \\bigcap B|}$$\n06. Machine Learning Classification and Regression Bianry Classification: 두 개의 선택지 중 하나의 답을 선택 Multi-class Classification: 세 개 이상의 선택지 중에서 답을 선택 Regression: 연속적인 값의 범위 내에서 예측값을 도출 Learning Supervised Learning: 정답 레이블과 함께 함습 Unsupervised Learning: 데이터에 별도의 레이블이 없이 학습 Self-Supervised Learning: 레이블이 없는 데이터가 주어지면, 모델이 학습을 위해 스스로 레이블을 생성 Confusion Matrix 예측 참 예측 거짓 실제 참 TP(정답) FN(오답) 실제 거짓 FP(오답) TN(정답) Precision(정밀도): True라고 분류한 것 중 실제 True의 비율, $Precision=\\frac{TP}{TP+FP}$ Recall(재현율): 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율, $Recall=\\frac{TP}{TP+FN}$ Accuracy(정확도): 전체 예측한 데이터 중 정답을 맞춘 것에 대한 비율, $Accuracy=\\frac{TP+TN}{TP+FN+FP+TN}$ Overfitting and Underfitting Overfitting: 훈련 데이터를 과하게 학습, 훈련 데이터에 비해 테스트 데이터의 오차가 커짐 Underfitting: 테스트 데이터의 성능이 올라갈 여지가 있음에도 훈련을 덜 한 상태, 훈련 데이터에서도 정확도가 낮음 07. Deep Learning Perceptron 입력값 $x$, 가중치 $w$, 출력값 $y$ 가중치의 값이 크면 클수록 해당 입력 값이 중요하다는 것을 의미 단층 퍼셉트론: 값을 보내는 input layer와 값을 받아서 출력하는 output layer로 구성 다층 퍼셉트론(MLP): 입력층과 출력층 사이에 hidden layer를 추가 FFNN FFNN(피드 포워드 신경망): 오직 입력층에서 출력층 방향으로 연산이 전개되는 신경망 RNN(순환 신경망): 은닉층의 출력값이 다시 은닉층으로 입력되는 신경망 Activision Function 은닉층과 출력층의 뉴런에서 출력값을 결정하는 함수 Step function, Sigmoid function, ReLU 등 비선형 함수의 특성 Loss Function MSE: 연속형 변수 예측 Binary Cross-Entropy: 시그모이드 함수 출력 Categorical Cross-Entropy: 소프트맥스 함수 출력 Optimizer Momentum: 경사 하강법에 모멘텀을 더해 Local Minimum에 빠지더라도 빠져나갈 수 있게 함 Adagrad: 각 매개변수에 서로 다른 학습률을 적용 RMSprop: Adagrad가 학습을 진행할수록 학습률이 지나치게 떨어지는 단점을 개선 Adam: RMSprop과 Momentum을 합친 듯한 방법, 방향과 학습률 두 가지를 모두 잡기 위한 방법 Overfitting 방지 데이터의 양을 늘리기\n데이터의 양이 적으면 데이터의 특정 패턴이나 노이즈까지 쉽게 암기해버림, Data Augmentation 활용 모델의 복잡도 줄이기\n인공 신경망의 복잡도는 은닉층의 수나 매개변수의 수 등으로 결정 가충치 규제 적용하기\nL1 규제(가중치의 절댓값 합계를 비용 함수에 추가), L2 규제(모든 가중치들의 제곱합을 비용 함수에 추가) Dropout\n학습 시에 인공 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지 기울기 소실 기울기 소실: 역전파 과정에서 입력층으로 갈 수록 기울기가 점차적으로 작아지는 현상 기울기 폭주: 기울기가 점차 커지다가 가중치들이 비정상적으로 큰 값이 되면서 발산되는 경우 Gradient Clipping: 기울기 폭주를 막기 위해 임계값을 넘지 않도록 값의 크기를 감소 Weight Initialization Xavier Initialization: 균등 분포 또는 정규 분포로 초기화 할 때 두 가지 경우로 나뉨 He Initialization: Xavier 초기화와 다르게 다음 층의 뉴런의 수를 반영하지 않음 Batch Normalization 내부 공변량 변화: 학습 과정에서 층 별로 입력 데이터 분포가 달라지는 현상 배치 정규화: 한 번에 들어오는 배치 단위로 정규화하는 것 배치 정규화는 추가 계산을 발생시켜 모델을 복잡하게 하기 때문에 예측 시 실행 시간이 느려지는 단점 너무 작은 배치 크기에서는 잘 동작하지 않을 수 있기 때문에 미니 배치 크기에 의존적임 RNN은 각 시점마다 다른 통계치를 가지기 때문에 RNN에 적용하기 어려움 Keras API Sequential API: 단순하게 층을 쌓는 방식, 다수의 입출력 및 층 간 연산을 구현하기 어려움 Functional API: 입력의 크기(shape)를 명시한 입력층을 모델의 앞단에 정의 Subclassing API: Functional API로도 구현할 수 없는 모델들도 구현 가능 texts_to_matrix() tokenizer.texts_to_matrix(texts, mode='count'): DTM 생성 tokenizer.texts_to_matrix(texts, mode='binary'): DTM과 유사하지만 단어의 개수는 무시 tokenizer.texts_to_matrix(texts, mode='tfidf'): TF-IDF 행렬 생성 tokenizer.texts_to_matrix(texts, mode='freq'):\n각 문서에서의 단어 등장 횟수를 분자로, 문서의 크기를 분모로 하는 표현하는 방법 NNLM 피드 포워드 신경망 언어 모델, 신경망 언어 모델의 시초로, RNNLM, BiLM 등으로 발전 기존 N-gram 언어 모델은 충분한 데이터를 관측하지 못하면 언어를 정확히 모델링하지 못하는 희소 문제를 가짐 NNLM은 N-gram 언어 모델처럼 정해진 개수(window size)의 단어만을 참고 NNLM은 N개의 input layer와 projection layer, hidden layer, output layer로 구성 Projection layer의 크기가 M일 때, 각 입력 단어들은 V x M 크기의 가중치 행렬과 곱해짐 충분한 양의 훈련 코퍼스를 학습한다면 단어 간 유사도를 구할 수 있는 임베딩 벡터값을 얻을 수 있음 08-01. RNN 입력과 출력을 시퀀스 단위로 처리하는 시퀀스 모델 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 다시 은닉층 노드의 다음 계산 입력으로 보내는 특징 셀(메모리 셀, RNN 셀): 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드, 이전의 값을 기억하는 역할 Hidden state: 현재 시점을 t라 할 때, 메모리 셀이 다음 시점인 t+1의 자신에게 보내는 값 입력과 출력의 길이를 다르게 설계할 수 있어 다양한 용도로 사용 가능 one-to-many: 하나의 이미지 입력에 대해서 사진의 제목인 시퀀스를 출력하는 이미지 캡셔닝 작업에 사용 many-to-one: 단어 시퀀스에 대해서 하나의 출력을 하는 감성 분류, 스팸 메일 분류 등에 사용 many-to-many: 사용자가 문장을 입력하면 대답 문장을 출력하는 챗봇이나 번역기에 사용 RNN Parameter 현재 시점 $t$에서의 hidden state가 $h_t$라 할 때, 두 개의 가중치 $W_x$, $W_h$가 필요 $W_x$는 입력층을 위한 가중치, $W_h$는 $t-1$의 hidden state인 $h_{t-1}$을 위한 가중치 은닉층 $h_t=tanh({W_x}{x_t}+{W_h}{h_{t-1}}+b)$, 출력층 $y_t=f({W_y}{h_t}+b)$ 출력층의 활성화 함수 $f$는 이진 분류에서 시그모이드 함수, 다중 클래스 분류에서 소프트맥스 함수 등 사용 RNN의 입력 $x_t$는 단어 벤터로 간주, 단어 벡터의 차원을 $d$, hidden state의 크기를 $D_h$라 할 때,\n메모리 셀 $h_t$ = $tanh({W_h}\\times{h_{t-1}}\\times{W_x}\\times{x_t}+{b})$ $x_t$ $W_x$ $W_h$ $h_{t-1}$ $b$ $({d}\\times{1})$ $({D_h}\\times{d})$ $({D_h}\\times{D_h})$ $({D_h}\\times{1})$ $({D_h}\\times{1})$ Keras RNN hidden_units: hidden state의 크기 (output_dim) timesteps: 입력 시퀀스(문장)의 길이 (input_length) input_dim: 입력의 크기, 단어 벡터의 차원 RNN 층은 (batch_size, timesteps, input_dim) 크기의 3D 텐서를 입력으로 받음 메모리 셀의 최종 시점의 hidden state만 리턴할 경우 (batch_size, output_dim) 크기의 2D 텐서 반환 메모리 셀의 각 시점(time step)의 hidden state 값들을 모아 전체 시퀀스를 리턴할 경우 3D 텐서를 반환 return_sequences=True 옵션으로 반환값 설정 1 2 3 4 from tensorflow.keras.layers import SimpleRNN model = Sequential() model.add(SimpleRNN(hidden_units, input_shape=(timesteps, input_dim))) 메모리 셀에서 hidden state 계산은 다음과 같은 코드로 동작\n1 2 3 4 hidden_state_t = 0 # 초기 은닉 상태를 0(벡터)로 초기화 for input_t in input_length: # 각 시점마다 입력을 받는다. output_t = tanh(input_t, hidden_state_t) # 각 시점에 대해서 입력과 은닉 상태를 가지고 연산 hidden_state_t = output_t # 계산 결과는 현재 시점의 은닉 상태가 된다. Deep RNN 순환 신경망에서 은닉층이 1개 더 추가되어 은닉층이 2개인 깊은 구조 첫번째 은닉층은 다음 은닉층이 존재하기 때문에 return_sequences=True를 설정하여 모든 시점을 전달 1 2 3 4 5 from tensorflow.keras.layers import SimpleRNN model = Sequential() model.add(SimpleRNN(hidden_units, input_shape=(timesteps, input_dim), return_sequences=True)) model.add(SimpleRNN(hidden_units, return_sequences=True)) Bidirectional RNN t에서의 출력값을 예측할 때 이전 시점의 입력 뿐 아니라, 이후 시점의 입력 또한 예측 빈칸 채우기 등의 문제에서 미래 시점의 입력에 힌트가 있기 때문에 이전과 이후의 시점을 모두 고려 하나의 출력값을 예측하기 위해 두 개의 메모리 셀을 사용 첫 번째 메모리 셀은 앞 시점의 hidden state를 전달받아 현재의 hidden state를 계산 (forward) 두 번째 메모리 셀은 뒤 시점의 hidden state를 전달받아 현재의 hidden state를 계산 (backward) 은닉층을 추가하면 학습할 수 있는 양이 많아지지만, 훈련 데이터 또한 많은 양이 필요 1 2 3 4 from tensorflow.keras.layers import Bidirectional model = Sequential() model.add(Bidirectional(SimpleRNN(hidden_units, return_sequences=True), input_shape=(timesteps, input_dim))) 08-02. LSTM Valina RNN(Simple RNN)은 출력 결과가 이전의 계산 결과에 의존하여 비교적 짧은 시퀀스에서면 효과를 봄 장기 의존성 문제: time step이 길어질수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상 LSTM(장단기 메모리)은 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여\n불필요한 기억을 지우고, 기억해야할 것들을 결정 전통적인 RNN에서 cell state $C_t$를 추가하여 긴 시퀀스의 입력을 처리하는데 탁월한 성능 cell state 또한 이전 시점의 cell state가 다음 시점의 cell state를 구하기 위한 입력으로서 사용 입력 게이트: 현재 정보를 기억하기 위한 게이트 삭제 게이트: 기억을 삭제하기 위한 게이트, 시그모이드 함수의 반환값이 0에 가까울수록 많은 정보가 삭제 셀 상태: 삭제 게이트에서 일부 기억을 잃은 상태, 입력 게이트에서 선택된 기억을 삭제 게이트의 결과값과 더함 삭제 게이트는 이전 시점의 입력을 얼마나 반영할지 의미, 입력 게이트는 현재 시점의 입력을 얼마나 반영할지 결정 출력 게이트: 현재 시점 $t$의 hidden state를 결정하는 일에 사용 08-03. GRU LSTM의 장기 의존성 문제에 대한 해결책을 유지하면서, hidden state를 업데이트하는 계산을 감소 GRU는 업데이트 게이트와 리셋 게이트로 구성 데이터의 양이 적을 때는 매개 변수의 양이 적은 GRU가 유리, 반대의 경우엔 LSTM이 유리 08-04. Keras RNN and LSTM 입력 생성 train_X가 2D 텐서의 형태일 경우 batch_size 1을 추가해 3D 텐서로 변경 1 2 3 train_X = [[[0.1, 4.2, 1.5, 1.1, 2.8], [1.0, 3.1, 2.5, 0.7, 1.1], [0.3, 2.1, 1.5, 2.1, 0.1], [2.2, 1.4, 0.5, 0.9, 1.1]]] train_X = np.array(train_X, dtype=np.float32) print(train_X.shape) # (1, 4, 5) SimpeRNN return_sequences=False일 경우 2D 텐서 반환 return_sequences=True일 경우 timesteps를 포함하는 3D 텐서 반환 return_state=True일 경우 return_sequences에 관계없이 마지막 시점의 hidden state 출력 1 2 rnn = SimpleRNN(3) hidden_state = rnn(train_X) # shape(1, 3) 1 2 rnn = SimpleRNN(3, return_sequences=True) hidden_states = rnn(train_X) # shape(1, 4, 3) 1 2 rnn = SimpleRNN(3, return_sequences=True, return_state=True) hidden_states, last_state = rnn(train_X) # shape(1, 4, 3), shape(1, 3) LSTM return_state=True일 경우 마지막 cell state를 포함한 세 개의 결과를 반환 1 2 lstm = LSTM(3, return_sequences=False, return_state=True) hidden_state, last_state, last_cell_state = lstm(train_X) # each shape(1, 3) Bidirectional LSTM 정방향과 역방향에 대한 hidden state와 cell state를 반환 return_sequences=False일 경우 정방향 LSTM의 마지막 시점 hidden state와\n역방향 LSTM의 첫번째 시점 hidden state가 연결된 채 반환 return_sequences=True일 경우 각각의 순서 맞게 연결된 hidden state 반환 1 2 3 4 bilstm = Bidirectional(LSTM(3, return_sequences=False, return_state=True, \\ kernel_initializer=k_init, bias_initializer=b_init, recurrent_initializer=r_init)) hidden_states, forward_h, forward_c, backward_h, backward_c = bilstm(train_X) # shape(1, 6), shape(1, 3), shape(1, 3) 08-05. RNNLM NNLM과 달리 time step을 도입하여 입력의 길이가 고정되지 않는 언어 모델 Teaching Forcing: 테스트 과정에서 RNN 모델을 훈련시킬 때 사용하는 훈련 기법,\n모델이 t 시점에서 예측한 값을 t+1 시점에 입력으로 사용하지 않고 실제 알고 있는 정답(t 시점의 레이블)을 사용 한 번 잘못 예측하면 뒤에서의 예측가지 영향을 미쳐 훈련 시간이 느려지기 때문에 교사 강요를 사용 훈련 과정 동안 활성화 함수로는 softmax 함수, 손실 함수로는 cross-entropy 함수를 사용 one-hot vector $x_t$를 입력받으면 NNLM에서와 동일한 embedding layer를 거쳐\n${V}\\times{M}$ 크기의 embedding vector로 변환, $e_t=lookup(x_t)$ 이후 RNN과 동일한 과정을 거쳐 $\\hat{y_t}$를 반환, $h_t=\\tanh({W_x}{e_t}+{W_h}{h_{t-1}}+b)$ $\\hat{y}_t$의 각 차원 안에서의 값은 $\\hat{y}_t$의 j번째 인덱스가 가진 0과 1사이의 값이 j번째 단어가 다음 단어일 확률 08-06. Text Generation using RNN 데이터 전처리 1 2 3 4 # 원본 한국어 문장 text = \u0026#34;\u0026#34;\u0026#34;경마장에 있는 말이 뛰고 있다\\n 그의 말이 법이다\\n 가는 말이 고와야 오는 말이 곱다\\n\u0026#34;\u0026#34;\u0026#34; 1 2 3 4 5 # 단어 집합 생성 tokenizer = Tokenizer() tokenizer.fit_on_texts([text]) vocab_size = len(tokenizer.word_index) + 1 print(\u0026#39;단어 집합의 크기 : %d\u0026#39; % vocab_size) # 12 1 2 3 # 각 라인마다 texts_to_sequences() 함수를 적용해서 훈련 데이터 생성 print(sequences) [[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]] 1 2 3 4 5 # 패딩 후 라벨 분리 (가장 우측에 있는 단어, [경마장에, 있는]에서 \u0026#39;있는\u0026#39; 등을 라벨로 지정) sequences = pad_sequences(sequences, maxlen=max_len, padding=\u0026#39;pre\u0026#39;) sequences = np.array(sequences) X = sequences[:,:-1] y = sequences[:,-1] 1 2 # 라벨에 대해서 one-hot encoding 수행 y = to_categorical(y, num_classes=vocab_size) RNN 모델 설계 many-to-one 구조의 RNN을 사용 모든 가능한 단어 중 마지막 시점에서 하나의 단어를 예측하는 다중 클래스 분류 문제 수행 활성화 함수로는 softmax 함수, 손실 함수로는 cross-entropy 함수 사용 첫 단어가 주어졌을 때, n번 동안 예측을 반복하면서 현재 단어와 문장에 예측 단어를 저장 1 2 3 4 5 6 7 8 9 embedding_dim = 10 hidden_units = 32 model = Sequential() model.add(Embedding(vocab_size, embedding_dim)) model.add(SimpleRNN(hidden_units)) model.add(Dense(vocab_size, activation=\u0026#39;softmax\u0026#39;)) model.compile(loss=\u0026#39;categorical_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) model.fit(X, y, epochs=200, verbose=2) LSTM 모델 설계 뉴욕 타임즈 기사 제목 데이터 전처리 (단어 집합 크기 3494, 샘플 최대 길이 24) RNN과 동일한 작업을 수행할 LSTM 모델 설계, 예측 과정 또한 동일 1 2 3 4 5 6 7 8 9 embedding_dim = 10 hidden_units = 128 model = Sequential() model.add(Embedding(vocab_size, embedding_dim)) model.add(LSTM(hidden_units)) model.add(Dense(vocab_size, activation=\u0026#39;softmax\u0026#39;)) model.compile(loss=\u0026#39;categorical_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) model.fit(X, y, epochs=200, verbose=2) 08-07. Char RNN 입출력의 단위를 word-level에서 character-level로 변경한 RNN 문자 단위를 입출력으로 사용하기 때문에 embedding layer를 사용하지 않음 이상한 나라의 앨리스 데이터 사용 (문자열 길이 159484, 문자 집합 크기 56) 훈련 데이터에 apple이라는 시퀀스가 있고 입력의 길이가 4일 때, \u0026lsquo;appl\u0026rsquo;을 입력하면 \u0026lsquo;pple\u0026rsquo;을 예측할 것으로 기대 train_X.shape(2658, 60, 56), train_y.shape(2658, 60, 56) Char RNN 모델 설계 1 2 3 4 5 6 7 8 9 hidden_units = 256 model = Sequential() model.add(LSTM(hidden_units, input_shape=(None, train_X.shape[2]), return_sequences=True)) model.add(LSTM(hidden_units, return_sequences=True)) model.add(TimeDistributed(Dense(vocab_size, activation=\u0026#39;softmax\u0026#39;))) model.compile(loss=\u0026#39;categorical_crossentropy\u0026#39;, optimizer=\u0026#39;adam\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) model.fit(train_X, train_y, epochs=80, verbose=2) ","permalink":"https://minyeamer.github.io/blog/2022-06-28/","summary":"딥 러닝을 이용한 자연어 처리 입문 1","title":"2022-06-28 Log"},{"content":"1. 행렬 feature 1 feature 2 1 5 3 4 5 2 1-1. 행렬의 요소 행(row)과 열(column)로 구성 스칼라(scalar): 행렬을 구성하는 요소인 각 숫자, $a_{ij}$ ($i$: 행 번호, $j$: 열 번호) 벡터(vector): 스칼라의 집합, 크기와 방향을 모두 가짐, 행벡터 또는 열벡터로 표시 행렬(matrix): 행벡터의 집합 (또는 열벡터의 집합) 텐서(tensor): 2차원으로 구성된 행렬이 아닌 n차원으로 일반화한 행렬 1-2. 행렬의 종류 대각 행렬(diagonal matrix): 대각 원소 이외의 모든 성분이 0인 행렬 단위 행렬(identity matrix): 주 대각선의 원소가 모두 1이며 나머지는 0인 정사각 행렬 행렬을 대각 행렬($D$)이나 단위 행렬($I$)로 변환 시 연산량을 크게 줄이는 효과 전치 행렬(transposed matrix): 기존 행렬의 행과 열을 바꾸는 행렬, $a_{ij} \\rightarrow a_{ji}$ 1-3. 행렬의 연산 행렬의 덧셈, 뺄셈: 연산 대상이 되는 행렬의 행 번호와 열 번호가 일치하는 원소끼리 계산 행렬의 스칼라곱: 행렬을 구성하는 모든 원소에 스칼라를 곱함 행렬곱: $AB_{ij}=a_{i1}b_{1j}+a_{i2}b_{2j}+\u0026hellip;+a_{ir}b_{rj}$, 행렬 $A$의 열 크기와 행렬 $B$의 행 크기가 같아야 가능 행렬의 원소곱: 차원이 동일한 두 행렬의 동일 위치 원소를 서로 곱함, 딥러닝 최적화 관련 알고리즘에 자주 사용 행렬식(determinant): 행렬의 특성을 하나의 숫자로 표현, 행렬을 구성하고 있는 벡터로 만들 수 있는 도형의 부피를 계산 역행렬(inverse matrix): 행렬 $A$에 대해서 $AB=I$를 만족하는 행렬 $B$ ($A^{-1}$로 표기) 2. 내적 $\u0026lt;u,v\u0026gt;=uㆍv=u_1v_1+u_2v_2+\u0026hellip;+u_nv_n$ 각 벡터의 요소를 서로 곱한 후 더함 벡터의 길이(norm)를 구하거나 벡터 사이의 관계를 파악할 수 있음 내적을 통해 두 벡터 사이의 각도를 추정 가능 (내적이 양수면 예각, 내적이 음수면 둔각) 2-1. 벡터의 길이 벡터 $u$의 길이는 $||u||$로 표기 $u=(u_1,u_2,⋯,u_n)$일 때, $||u||=\\sqrt{u_1^2+u_2^2+⋯+u_n^2}$ 위 식은 피타고라스의 정리를 일반화한 것 벡터의 길이를 통해 $x$ 좌표를 $||u||\\cos(\\theta)$, $y$ 좌표를 $||u||\\sin(\\theta)$로 표시 내적값 $uㆍv=||u||||v||\\cos(\\theta)$ 벡터 $u$를 벡터 $v$에 정사영한 벡터의 길이 $||proj_vu||=||u||\\cos(\\theta)$ $uㆍv=||u||||v||\\cos(\\theta)=(||v||)\\times(||u||\\cos(\\theta))$ 3. 선형 변환 두 벡터 공간 사이의 함수 좌표 평면 상 벡터를 확대, 축소, 회전, 반사하는 것은 모두 변환 4. 랭크, 차원 4-1. 벡터 공간, 기저 벡터 공간(vector space): 벡터 집합이 존재할 때, 해당 벡터들로 구성할 수 있는 공간 기저(basis): 벡터 공간을 생성하는 선형 독립인 벡터들 부분 공간(subspace): 벡터 공간의 일부분, 전체 벡터 집합의 부분 집합 2개의 기저 벡터 집합 $S$에 속하는 부분 공간을 $W$라 할 때, $W=span(S)$ 의 관계를 가짐 4-2. 랭크와 차원 차원(dimension): 기저 벡터의 개수, 벡터 공간을 구성하는데 필요한 최소한의 벡터 개수 랭크(rank): 열벡터에 의해 span된 벡터 공간의 차원 영공간(numm space): 행렬 $A$가 주어질 때 $Ax=0$을 만족하는 모든 벡터 $x$의 집합 4-3. 직교 행렬 직교 행렬(orthogonal matrix): 어떤 행렬의 행벡터와 열벡터가 유클리드 공간의 정규 직교 기저를 이루는 행렬 $AA^T=A^TA=I$ 직교 행렬 $A$의 역행렬은 자신의 전치 행렬, $A^T=A^{-1}$ ","permalink":"https://minyeamer.github.io/blog/2022-06-19/","summary":"선형대수와 통계학으로 배우는 머신러닝 with 파이썬 1","title":"2022-06-19 Log"},{"content":"문제 링크 https://programmers.co.kr/learn/courses/30/lessons/60059 개요 numpy 라이브러리와 중복 순열을 사용해 해결할 수 있는 문제다. 문제 조건 2차원 배열인 열쇠(M)를 회전하거나 이동해 2차원 배열인 자물쇠(N)에 맞는지 여부를 반환하는 문제다. 문제 해설 2차원 배열을 numpy.ndarray 형식으로 변환하면 회전 및 이동 연산을 쉽게 처리할 수 있다. 90도 단위로 4번 회전된 각각의 목록을 구하고 상하좌우 이동을 위해 바깥쪽에 0으로 채워진 padding을 추가한다. padding이 채워진 N+M-1크기의 2차원 배열에 대해 자물쇠 크기만큼의 부분만 잘라내어 자물쇠의 구멍과 비교한다. OR 연산 시 자물쇠가 1로 채워지고 열쇠와 자물쇠 사이에 겹치는 부분이 없다면 열쇠가 자물쇠에 맞다고 판단한다. 시행착오 열쇠의 크기가 자물쇠의 크기보다 작을 경우를 고려하지 못하고 둘의 사이즈를 맞추는 과정을 무시해 에러가 생겼다. 처음엔 0으로 채워진 단일 행 또는 열과 concatenate 연산을 진행해 열쇠를 아래쪽과 오른쪽으로만 이동했는데,\n그 반대의 경우를 고려하지 못해서 에러가 생겼다. 이후 padding을 사용하는 코드로 변경했다. 통계학적 지식이 부족해 인덱스 목록에 대해 중복 조합 연산을 했었는데 잘못됨을 인지하고 중복 순열로 변경했다. 열쇠와 자물쇠의 돌기가 만나선 안된다는 부분을 처리하지 않아 특정 케이스에 대해 실패가 발생하는 이유를 인지하지 못했다.\nXOR 연산도 하나의 방법일 수 있지만 대신에 두 배열의 합을 사용해 중복되는 부분을 판단했다. 프로그래머스가 백준처럼 NumPy 라이브러리를 지원하지 않았다면 매우 난해한 문제였을테지만,\n다행히 NumPy 라이브러리를 활용해 비교적 단순한 방법으로 풀 수 있었다. 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import numpy as np from itertools import product def solution(key, lock): key, lock = np.array(key), np.array(lock) rotated_keys = [np.pad(np.rot90(key, i),len(lock)-1) for i in range(4)] index_list = list(range(len(rotated_keys[0])-len(lock)+1)) for rotated_key in rotated_keys: for i, j in list(product(index_list, index_list)): key = rotated_key[i:i+len(lock),j:j+len(lock)] if (0 not in np.logical_or(key,lock)) and (2 not in (key + lock)): return True return False ","permalink":"https://minyeamer.github.io/blog/programmers-problems-60059/","summary":"문제 링크 https://programmers.co.kr/learn/courses/30/lessons/60059 개요 numpy 라이브러리와 중복 순열을 사용해 해결할 수 있는 문제다. 문제 조건 2차원 배열인 열쇠(M)를 회전하거나 이동해 2차원 배열인 자물쇠(N)에 맞는지 여부를 반환하는 문제다. 문제 해설 2차원 배열을 numpy.ndarray 형식으로 변환하면 회전 및 이동 연산을 쉽게 처리할 수 있다. 90도 단위로 4번 회전된 각각의 목록을 구하고 상하좌우 이동을 위해 바깥쪽에 0으로 채워진 padding을 추가한다. padding이 채워진 N+M-1크기의 2차원 배열에 대해 자물쇠 크기만큼의 부분만 잘라내어 자물쇠의 구멍과 비교한다. OR 연산 시 자물쇠가 1로 채워지고 열쇠와 자물쇠 사이에 겹치는 부분이 없다면 열쇠가 자물쇠에 맞다고 판단한다.","title":"[프로그래머스/카카오 60059] 자물쇠와 열쇠 (Python)"},{"content":"문제 링크 https://programmers.co.kr/learn/courses/30/lessons/81301 개요 딕셔너리를 사용해 해결할 수 있는 문제다. 문제 조건 일부 숫자가 영단어로 변환된 문자열을 원래의 숫자로 되돌려 반환하는 문제다. 문제 해설 각각의 영단어에 대한 숫자 맵과 문자열의 replace 함수를 사용하면 쉽게 해결할 수 있다. 해설 코드 1 2 3 4 5 6 7 8 def solution(s): answer = s word_dict = {\u0026#39;zero\u0026#39;:\u0026#39;0\u0026#39;,\u0026#39;one\u0026#39;:\u0026#39;1\u0026#39;,\u0026#39;two\u0026#39;:\u0026#39;2\u0026#39;,\u0026#39;three\u0026#39;:\u0026#39;3\u0026#39;, \u0026#39;four\u0026#39;:\u0026#39;4\u0026#39;,\u0026#39;five\u0026#39;:\u0026#39;5\u0026#39;,\u0026#39;six\u0026#39;:\u0026#39;6\u0026#39;,\u0026#39;seven\u0026#39;:\u0026#39;7\u0026#39;, \u0026#39;eight\u0026#39;:\u0026#39;8\u0026#39;,\u0026#39;nine\u0026#39;:\u0026#39;9\u0026#39;} for key, value in word_dict.items(): answer = answer.replace(key, value) return int(answer) ","permalink":"https://minyeamer.github.io/blog/programmers-problems-81301/","summary":"문제 링크 https://programmers.co.kr/learn/courses/30/lessons/81301 개요 딕셔너리를 사용해 해결할 수 있는 문제다. 문제 조건 일부 숫자가 영단어로 변환된 문자열을 원래의 숫자로 되돌려 반환하는 문제다. 문제 해설 각각의 영단어에 대한 숫자 맵과 문자열의 replace 함수를 사용하면 쉽게 해결할 수 있다. 해설 코드 1 2 3 4 5 6 7 8 def solution(s): answer = s word_dict = {\u0026#39;zero\u0026#39;:\u0026#39;0\u0026#39;,\u0026#39;one\u0026#39;:\u0026#39;1\u0026#39;,\u0026#39;two\u0026#39;:\u0026#39;2\u0026#39;,\u0026#39;three\u0026#39;:\u0026#39;3\u0026#39;, \u0026#39;four\u0026#39;:\u0026#39;4\u0026#39;,\u0026#39;five\u0026#39;:\u0026#39;5\u0026#39;,\u0026#39;six\u0026#39;:\u0026#39;6\u0026#39;,\u0026#39;seven\u0026#39;:\u0026#39;7\u0026#39;, \u0026#39;eight\u0026#39;:\u0026#39;8\u0026#39;,\u0026#39;nine\u0026#39;:\u0026#39;9\u0026#39;} for key, value in word_dict.items(): answer = answer.replace(key, value) return int(answer) ","title":"[프로그래머스/카카오 81301] 숫자 문자열과 영단어 (Python)"},{"content":"문제 링크 https://programmers.co.kr/learn/courses/30/lessons/17676 개요 datetime 라이브러리를 사용해 해결할 수 있는 문제다. 문제 조건 트래픽 처리 종료 시간 및 처리 시간이 짝지어진 로그 문자열을 해석하여 초당 최대 처리량을 반환하는 문제다. 문제 해설 datetime과 timedelta 모듈을 활용하여 각각의 트래픽 로그에 대한 시작과 끝 시간을 계산한다. 트래픽의 시작 또는 끝을 1초 구간의 시작으로 정의하고 해당 구간에서 시작됐거나 진행 중인 트래픽 수를 합산한다. 합산된 트래픽 수 중에서 최댓값을 초당 최대 처리량으로 판단하여 반환한다. 한계 트래픽 로그를 시작 시간과 끝 시간으로 분리하지 않고 시간 범위로 변환할 수 있다면,\n굳이 2N 길이의 반복문을 사용하지 않고 교집합 연산을 사용해서 시간 복잡도를 개선할 수 있었을 것이다. 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from datetime import datetime, timedelta def solution(lines): answer = 0 lines = sorted(map(interpret_log, lines)) delta = timedelta(seconds=1) for start in sum(lines, []): t = sum([check_time_range(time_range, start, start+delta) for time_range in lines]) answer = max(t, answer) start += delta return answer def interpret_log(line): line = line.split() line = [word.split(s) for word, s in zip(line, [\u0026#39;-\u0026#39;,\u0026#39;:\u0026#39;,\u0026#39;s\u0026#39;])] Y,m,d,H,M,S,ms = list(map(int,line[0]+line[1][:-1]+line[1][-1].split(\u0026#39;.\u0026#39;))) end_date = datetime(Y,m,d,H,M,S,ms*1000) duration = line[2][0] if \u0026#39;.\u0026#39; in line[2][0] else line[2][0]+\u0026#39;.0\u0026#39; S,ms = list(map(int,duration.split(\u0026#39;.\u0026#39;))) start_date = end_date - timedelta(seconds=S,milliseconds=ms-1) return [start_date, end_date] def check_time_range(time_range, start, end): con1 = start \u0026lt;= time_range[0] \u0026lt; end con2 = time_range[0] \u0026lt;= start \u0026lt;= time_range[1] return con1 or con2 ","permalink":"https://minyeamer.github.io/blog/programmers-problems-17676/","summary":"문제 링크 https://programmers.co.kr/learn/courses/30/lessons/17676 개요 datetime 라이브러리를 사용해 해결할 수 있는 문제다. 문제 조건 트래픽 처리 종료 시간 및 처리 시간이 짝지어진 로그 문자열을 해석하여 초당 최대 처리량을 반환하는 문제다. 문제 해설 datetime과 timedelta 모듈을 활용하여 각각의 트래픽 로그에 대한 시작과 끝 시간을 계산한다. 트래픽의 시작 또는 끝을 1초 구간의 시작으로 정의하고 해당 구간에서 시작됐거나 진행 중인 트래픽 수를 합산한다. 합산된 트래픽 수 중에서 최댓값을 초당 최대 처리량으로 판단하여 반환한다. 한계 트래픽 로그를 시작 시간과 끝 시간으로 분리하지 않고 시간 범위로 변환할 수 있다면,","title":"[프로그래머스/카카오 17676] 추석 트래픽 (Python)"},{"content":"문제 링크 https://programmers.co.kr/learn/courses/30/lessons/42888 개요 딕셔너리를 사용해 해결할 수 있는 문제다. 문제 조건 채팅방 상태 메시지에 대해 닉네임 변경 사항을 적용하여\n최종적으로 UI 상에서 보여지는 메시지를 목록을 반환하는 문제다. 문제 해설 uid에 대한 닉네임이 짝지어진 딕셔너리(name_dict)를 기반으로 최종적인 닉네임 목록을 기록한다. 메시지가 Enter와 Change로 시작하는 경우 닉네임이 설정 또는 변경된 것이라 인지하여 딕셔너리를 수정한다. name_dict에서 uid에 대한 닉네임을 참조하여 상태 메시지를 조건에 맞는 형식으로 변환한다. 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 def solution(record): answer = [] record = [rec.split() for rec in record] name_dict = {rec[1]:rec[2] for rec in record if rec[0] in {\u0026#39;Enter\u0026#39;,\u0026#39;Change\u0026#39;}} msg_dict = {\u0026#39;Enter\u0026#39;:\u0026#39;들어왔습니다.\u0026#39;,\u0026#39;Leave\u0026#39;:\u0026#39;나갔습니다.\u0026#39;} for rec in record: if rec[0] in {\u0026#39;Enter\u0026#39;,\u0026#39;Leave\u0026#39;}: answer.append(name_dict[rec[1]]+\u0026#39;님이 \u0026#39;+msg_dict[rec[0]]) return answer ","permalink":"https://minyeamer.github.io/blog/programmers-problems-42888/","summary":"문제 링크 https://programmers.co.kr/learn/courses/30/lessons/42888 개요 딕셔너리를 사용해 해결할 수 있는 문제다. 문제 조건 채팅방 상태 메시지에 대해 닉네임 변경 사항을 적용하여\n최종적으로 UI 상에서 보여지는 메시지를 목록을 반환하는 문제다. 문제 해설 uid에 대한 닉네임이 짝지어진 딕셔너리(name_dict)를 기반으로 최종적인 닉네임 목록을 기록한다. 메시지가 Enter와 Change로 시작하는 경우 닉네임이 설정 또는 변경된 것이라 인지하여 딕셔너리를 수정한다. name_dict에서 uid에 대한 닉네임을 참조하여 상태 메시지를 조건에 맞는 형식으로 변환한다. 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 def solution(record): answer = [] record = [rec.","title":"[프로그래머스/카카오 42888] 오픈채팅방 (Python)"},{"content":"문제 링크 https://programmers.co.kr/learn/courses/30/lessons/60057 개요 문자열 처리 능력이 요구되는 문제다. 문제 조건 문자열에서 반복되는 문자 또는 단어를 압축하고 가장 짧게 압축된 길이를 반환한다. 문제 해설 문자열을 단일 문자부터 2등분이 될 때까지 한 단위씩 늘려가면서 분리된 문자들에 대한 압축 과정을 진행한다. 분리된 문자들을 순회하면서 반복되는 문자열을 무시하고 남은 문자열의 길이를 세는 방법도 있지만,\n여기선 문자열을 형식에 맞게 압축시키고 그 길이를 구한다. 이전 문자열이 담길 메모리와 해당 문자열이 반복된 횟수를 기록하는 변수를 각각 선언한다. 분리된 문자들을 순회하면서 현재 문자와 메모리가 다르면(반복되지 않으면)\n압축된 문자열에 메모리를 추가하고 초기화한다. 모든 과정에 대한 최소 길이 값을 기록하고 반환한다. 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def solution(s): answer = len(s) for unit in range(1,len(s)//2+1): s_range = list(range(0, len(s), unit))+[None] s_split = [s[s_range[i]:s_range[i+1]] for i in range(len(s_range)-1)]+[\u0026#39;\u0026#39;] new_s, memory, cnt = str(), s_split[0], 1 for s_unit in s_split[1:]: if memory != s_unit: new_s += ((str(cnt) if cnt \u0026gt; 1 else str()) + memory) memory, cnt = s_unit, 1 else: cnt += 1 answer = min(answer, len(new_s)) return answer ","permalink":"https://minyeamer.github.io/blog/programmers-problems-60057/","summary":"문제 링크 https://programmers.co.kr/learn/courses/30/lessons/60057 개요 문자열 처리 능력이 요구되는 문제다. 문제 조건 문자열에서 반복되는 문자 또는 단어를 압축하고 가장 짧게 압축된 길이를 반환한다. 문제 해설 문자열을 단일 문자부터 2등분이 될 때까지 한 단위씩 늘려가면서 분리된 문자들에 대한 압축 과정을 진행한다. 분리된 문자들을 순회하면서 반복되는 문자열을 무시하고 남은 문자열의 길이를 세는 방법도 있지만,\n여기선 문자열을 형식에 맞게 압축시키고 그 길이를 구한다. 이전 문자열이 담길 메모리와 해당 문자열이 반복된 횟수를 기록하는 변수를 각각 선언한다.","title":"[프로그래머스/카카오 60057] 문자열 압축 (Python)"},{"content":"문제 링크 https://programmers.co.kr/learn/courses/30/lessons/72410 개요 정규식을 사용해 해결할 수 있는 문제다. 문제 조건 유저가 제시한 아이디 문자열을 규칙에 맞게 변경하여 반환하는 문제다. 문제 해설 제시된 조건에 대해 정규식을 구현하여 문자열에 적용하면 된다. 정규식 활용 능력에 따라 더욱 간단한 코드로 구현할 수도 있다. 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 import re def solution(new_id): answer = new_id.lower() answer = re.sub(r\u0026#34;[^a-z0-9-_\\.]\u0026#34;,\u0026#34;\u0026#34;,answer) answer = re.sub(r\u0026#34;\\.+\u0026#34;,\u0026#34;.\u0026#34;,answer) answer = re.sub(r\u0026#34;^\\.\u0026#34;,\u0026#34;\u0026#34;,answer) answer = re.sub(r\u0026#34;\\.$\u0026#34;,\u0026#34;\u0026#34;,answer) answer = \u0026#39;a\u0026#39; if not answer else answer answer = answer[:15] answer = answer[:-1] if answer[-1] == \u0026#39;.\u0026#39; else answer answer += answer[-1]*(3-len(answer)) return answer ","permalink":"https://minyeamer.github.io/blog/programmers-problems-72410/","summary":"문제 링크 https://programmers.co.kr/learn/courses/30/lessons/72410 개요 정규식을 사용해 해결할 수 있는 문제다. 문제 조건 유저가 제시한 아이디 문자열을 규칙에 맞게 변경하여 반환하는 문제다. 문제 해설 제시된 조건에 대해 정규식을 구현하여 문자열에 적용하면 된다. 정규식 활용 능력에 따라 더욱 간단한 코드로 구현할 수도 있다. 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 import re def solution(new_id): answer = new_id.lower() answer = re.sub(r\u0026#34;[^a-z0-9-_\\.]\u0026#34;,\u0026#34;\u0026#34;,answer) answer = re.sub(r\u0026#34;\\.+\u0026#34;,\u0026#34;.\u0026#34;,answer) answer = re.","title":"[프로그래머스/카카오 72410] 신규 아이디 추천 (Python)"},{"content":"문제 링크 https://programmers.co.kr/learn/courses/30/lessons/92334 개요 딕셔너리를 사용해 해결할 수 있는 문제다. 문제 조건 일정 횟수 이상 신고당한 불량 이용자를 신고한 이용자들에게 발송되는 메일의 횟수를 리스트로 반환하는 문제다. 문제 해설 이용자 자신이 신고당한 횟수(report_dict)와 이용자가 신고한 대상 목록(mail_dict)을 각각 기록할 필요가 있다. 각각의 신고 건수에 대해 반복하며 두 가지 딕셔너리에 기록한다. 이용자id를 key로 참고하여 각각의 이용자마다 자신이 신고한 대상 중 정지된 대상의 수를 계산한다. 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def solution(id_list, report, k): report_dict = {id: 0 for id in id_list} mail_dict = {id: set() for id in id_list} for rep in set(report): user, target = rep.split() report_dict[target] += 1 mail_dict[user].add(target) answer = [] for user, targets in mail_dict.items(): answer.append(sum([1 if report_dict[target] \u0026gt;= k else 0 for target in targets])) return answer ","permalink":"https://minyeamer.github.io/blog/programmers-problems-92334/","summary":"문제 링크 https://programmers.co.kr/learn/courses/30/lessons/92334 개요 딕셔너리를 사용해 해결할 수 있는 문제다. 문제 조건 일정 횟수 이상 신고당한 불량 이용자를 신고한 이용자들에게 발송되는 메일의 횟수를 리스트로 반환하는 문제다. 문제 해설 이용자 자신이 신고당한 횟수(report_dict)와 이용자가 신고한 대상 목록(mail_dict)을 각각 기록할 필요가 있다. 각각의 신고 건수에 대해 반복하며 두 가지 딕셔너리에 기록한다. 이용자id를 key로 참고하여 각각의 이용자마다 자신이 신고한 대상 중 정지된 대상의 수를 계산한다. 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def solution(id_list, report, k): report_dict = {id: 0 for id in id_list} mail_dict = {id: set() for id in id_list} for rep in set(report): user, target = rep.","title":"[프로그래머스/카카오 92334] 신고 결과 받기 (Python)"},{"content":"Feature Transformer Import Libraries 1 2 3 from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline ColumnTransformer 1 2 3 4 5 6 7 8 9 10 numeric_features = [\u0026#39;CRIM\u0026#39;, \u0026#39;ZN\u0026#39;, \u0026#39;INDUS\u0026#39;, \u0026#39;NOX\u0026#39;, \u0026#39;RM\u0026#39;, \u0026#39;AGE\u0026#39;, \u0026#39;DIS\u0026#39;, \u0026#39;TAX\u0026#39;, \u0026#39;PTRATIO\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;LSTAT\u0026#39;] numeric_transformer = StandardScaler() categorical_features = [\u0026#39;CHAS\u0026#39;, \u0026#39;RAD\u0026#39;] categorical_transformer = OneHotEncoder(categories=\u0026#39;auto\u0026#39;) preprocessor = ColumnTransformer( transformers=[ (\u0026#39;num\u0026#39;, numeric_transformer, numeric_features), (\u0026#39;cat\u0026#39;, categorical_transformer, categorical_features)]) OneHotEncoder()의 handle_unknown 설정\nerror: 숫자로 변환된 분류형 범주에 새로운 문자열 데이터가 들어올 경우 에러를 발생시킴 ignore: 카테고리에 해당되는 번호가 없으면 자동으로 0으로 바꿈 Preprocessing-Only 1 preprocessor_pipe = Pipeline(steps=[(\u0026#39;preprocessor\u0026#39;, preprocessor)]) steps: 전처리 도구를 순서대로 적용 (모델도 입력 가능) Model Fitting 1 2 3 4 preprocessor_pipe.fit(x_train) x_train_transformed = preprocessor_pipe.transform(x_train) x_test_transformed = preprocessor_pipe.transform(x_test) Numeric Variables에 대한 11개의 열,\nCategorical Variables에 대한 2개의 열,\n카테고리 별 One-Hot Encoding이 적용된 9개의 열을 함께 표시 Pipeline을 통해 전처리를 진행할 경우 데이터를 원래대로 되돌리는 inverse_trasnform 불가능 Preprocessing + Training 1 2 3 4 from sklearn.ensemble import GradientBoostingClassifier model = Pipeline(steps=[(\u0026#39;preprocessor\u0026#39;, preprocessor), (\u0026#39;classifier\u0026#39;, GradientBoostingClassifier(n_estimators=200, random_state=0))]) Preprocessing과 Training을 같이 묶을 경우 다른 모델을 끼워넣기 어려움 위 단점 떄문에 전처리만을 사용하는 것을 권장 Preprocessing + Training + HPO 1 2 3 4 5 6 7 8 9 10 11 12 13 14 model = Pipeline(steps=[(\u0026#39;preprocessor\u0026#39;, preprocessor), (\u0026#39;classifier\u0026#39;, GradientBoostingClassifier())]) param_grid = { \u0026#39;classifier__loss\u0026#39;: [\u0026#39;deviance\u0026#39;, \u0026#39;exponential\u0026#39;], \u0026#39;classifier__learning_rate\u0026#39;: [0.01, 0.001], \u0026#39;classifier__n_estimators\u0026#39;: [200, 400], \u0026#39;classifier__min_samples_split\u0026#39;: [2, 4], \u0026#39;classifier__max_depth\u0026#39;: [2, 4], \u0026#39;classifier__random_state\u0026#39;: [0] } grid_search = GridSearchCV(model, param_grid, refit=True, cv=3, n_jobs=1, verbose=1, scoring= \u0026#39;accuracy\u0026#39;) ","permalink":"https://minyeamer.github.io/blog/aischool-06-09-pipeline/","summary":"Feature Transformer Import Libraries 1 2 3 from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline ColumnTransformer 1 2 3 4 5 6 7 8 9 10 numeric_features = [\u0026#39;CRIM\u0026#39;, \u0026#39;ZN\u0026#39;, \u0026#39;INDUS\u0026#39;, \u0026#39;NOX\u0026#39;, \u0026#39;RM\u0026#39;, \u0026#39;AGE\u0026#39;, \u0026#39;DIS\u0026#39;, \u0026#39;TAX\u0026#39;, \u0026#39;PTRATIO\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;LSTAT\u0026#39;] numeric_transformer = StandardScaler() categorical_features = [\u0026#39;CHAS\u0026#39;, \u0026#39;RAD\u0026#39;] categorical_transformer = OneHotEncoder(categories=\u0026#39;auto\u0026#39;) preprocessor = ColumnTransformer( transformers=[ (\u0026#39;num\u0026#39;, numeric_transformer, numeric_features), (\u0026#39;cat\u0026#39;, categorical_transformer, categorical_features)]) OneHotEncoder()의 handle_unknown 설정\nerror: 숫자로 변환된 분류형 범주에 새로운 문자열 데이터가 들어올 경우 에러를 발생시킴 ignore: 카테고리에 해당되는 번호가 없으면 자동으로 0으로 바꿈 Preprocessing-Only 1 preprocessor_pipe = Pipeline(steps=[(\u0026#39;preprocessor\u0026#39;, preprocessor)]) steps: 전처리 도구를 순서대로 적용 (모델도 입력 가능) Model Fitting 1 2 3 4 preprocessor_pipe.","title":"[AI SCHOOL 5기] 머신 러닝 실습 - Pipeline"},{"content":"Model Stacking 서로 다른 모델들을 모으고 Ensemble 기법을 사용해 개선된 모델을 만드는 것 기존 모델들로부터 예측 결과를 도출하는 1st Stage와\n이를 기반으로 추가적인 판단을 진행하는 2nd Stage로 나뉨 1st Stage train_X를 가지고 1번 모델을 Training Training을 거친 1번 모델에 train_X를 넣었을 때 결과(예측값)을 저장 다른 모델에도 동일한 작업을 했을 때 나온 1열의 예측값들을 묶어 S_train을 생성 (기존 Ensemble은 S_train을 행별로 투표해서 분류함) 2nd Stage 새로운 모델 생성 (1st Stage에서 사용한 것과 다른 모델 사용 가능) S_train_X, train_Y를 가지고 새로운 모델을 Training Test Model test_X를 1st Stage 모델에 넣고 결과로 나온 예측값들의 묶음 S_test를 생성 (2nd Stage 모델의 학습 데이터는 원본 데이터와 다르기 때문에 test_X를 바로 넣으면 안됨) S_train_X, train_Y를 2nd Stage 모델에 넣었을 때 결과를 가지고 Accuracy 계산 Functional API Import Library 1 from vecstack import stacking 1st Level Models 1 2 3 4 models = [ ExtraTreesClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3), RandomForestClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3), XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.1, n_estimators = 100, max_depth = 3)] Stacking 1 2 3 4 5 6 S_train, S_test = stacking(models, X_train, y_train, X_test, regression = False, metric = accuracy_score, n_folds = 4, stratified = True, shuffle = True, random_state = 0, verbose = 2) S_train과 S_test를 같이 생성 (y_test는 2차 모델 성능 평가에서만 사용) metric: Focus를 맞출 대상 2nd Level Model 1 2 3 4 5 model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.1, n_estimators = 100, max_depth = 3, eval_metric=\u0026#39;mlogloss\u0026#39;) model = model.fit(S_train, y_train) y_pred = model.predict(S_test) accuracy_score(y_test, y_pred)를 확인하여 모델의 성능 평가 Scikit-learn API Import Library 1 from vecstack import StackingTransformer 1st Level Estimators 1 2 3 4 estimators = [ (\u0026#39;ExtraTrees\u0026#39;, ExtraTreesClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3)), (\u0026#39;RandomForest\u0026#39;, RandomForestClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3)), (\u0026#39;XGB\u0026#39;, XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.1, n_estimators = 100, max_depth = 3, eval_metric=\u0026#39;mlogloss\u0026#39;))] stacking과 다르게 모델 이름과 모델 객체를 같이 튜플로 묶음 StackingTransformer 1 2 3 4 5 stack = StackingTransformer(estimators, regression = False, metric = accuracy_score, n_folds = 4, stratified = True, shuffle = True, random_state = 0, verbose = 2) stacking과 다르게 x data와 y data를 입력하지 않고 객체 자체를 모델처럼 사용 Model Fitting 1 2 3 4 stack = stack.fit(X_train, y_train) S_train = stack.transform(X_train) S_test = stack.transform(X_test) stacking은 새로운 데이터를 넣을 때 어려움이 있지만,\nStackingTransformer는 전처리 도구처럼 사용 가능 2nd Level Estimator 1 2 3 4 model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.1,n_estimators = 100, max_depth = 3, eval_metric=\u0026#39;mlogloss\u0026#39;) model = model.fit(S_train, y_train) y_pred = model.predict(S_test) ","permalink":"https://minyeamer.github.io/blog/aischool-06-08-model-stacking/","summary":"Model Stacking 서로 다른 모델들을 모으고 Ensemble 기법을 사용해 개선된 모델을 만드는 것 기존 모델들로부터 예측 결과를 도출하는 1st Stage와\n이를 기반으로 추가적인 판단을 진행하는 2nd Stage로 나뉨 1st Stage train_X를 가지고 1번 모델을 Training Training을 거친 1번 모델에 train_X를 넣었을 때 결과(예측값)을 저장 다른 모델에도 동일한 작업을 했을 때 나온 1열의 예측값들을 묶어 S_train을 생성 (기존 Ensemble은 S_train을 행별로 투표해서 분류함) 2nd Stage 새로운 모델 생성 (1st Stage에서 사용한 것과 다른 모델 사용 가능) S_train_X, train_Y를 가지고 새로운 모델을 Training Test Model test_X를 1st Stage 모델에 넣고 결과로 나온 예측값들의 묶음 S_test를 생성 (2nd Stage 모델의 학습 데이터는 원본 데이터와 다르기 때문에 test_X를 바로 넣으면 안됨) S_train_X, train_Y를 2nd Stage 모델에 넣었을 때 결과를 가지고 Accuracy 계산 Functional API Import Library 1 from vecstack import stacking 1st Level Models 1 2 3 4 models = [ ExtraTreesClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3), RandomForestClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3), XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.","title":"[AI SCHOOL 5기] 머신 러닝 실습 - Model Stacking"},{"content":"Principal Component Analysis 차원 축소를 통해 최소 차원의 정보로 원래 차원의 정보를 모사하는 알고리즘 데이터의 열의 수가 많아 학습 속도가 느려질 때 열의 수를 줄이기 위해 사용 Dimension Reduction: 고차원 벡터에서 일부 차원의 값을 모두 0으로 만들어 차원을 줄임 원래의 고차원 벡터의 특성을 최대한 살리기 위해 가장 분산이 높은 방향으로 회전 변환 진행 전체 데이터를 기반으로 분산이 가장 큰 축을 찾아 PC 1으로 만들고,\nPC 1에 직교하는 축 중에서 분산이 가장 큰 축을 PC 2로 만드는 과정 반복 정보의 누락이 있기 때문에 경우에 따라 모델의 성능 하락 발생 Feature Selection: 기존에 존재하는 열 중에 n개를 선택 Feature Extraction: 기존에 있는 열들을 바탕으로 새로운 열들을 만들어냄 (차원 축소) Learning Process Import Libraries 1 2 from sklearn import decomposition from sklearn import datasets Load Model 1 2 3 iris = datasets.load_iris() x = iris.data y = iris.target Create Model 1 model = decomposition.PCA(n_components=1) component의 개수에 상관없이 PC 1은 언제나 동일 Model Fitting 1 2 model.fit(x) x1 = model.transform(x) Plot Model Histogram (components=1) 1 2 3 4 5 6 7 8 9 import seaborn as sns sns.distplot(x1[y==0], color=\u0026#34;b\u0026#34;, bins=20, kde=False) sns.distplot(x1[y==1], color=\u0026#34;g\u0026#34;, bins=20, kde=False) sns.distplot(x1[y==2], color=\u0026#34;r\u0026#34;, bins=20, kde=False) plt.xlim(-6, 6) plt.show() Scatter (components=3) 1 2 3 4 plt.scatter(x[:, 0], x[:, 1], c=iris.target) plt.xlabel(\u0026#39;PC1\u0026#39;) plt.ylabel(\u0026#39;PC2\u0026#39;) plt.show() 최적의 PCA 개수 데이터셋의 분산 정도 확인 model.explained_variance_ratio_ components가 전체 분산 정도 중 몇 퍼센트인지 확인 합쳐서 95퍼센트를 넘는 PCA 개수가 최적의 개수 최적의 PCA 개수 확인 1 np.argmax(np.cumsum(model.explained_variance_ratio_) \u0026gt;= 0.95 ) + 1 np.cumsum: 값의 누적된 합계 계산 np.argmax: 주어진 값들 중 가장 큰 값의 인덱스 번호 최적의 PCA 개수 적용 1 2 3 model = decomposition.PCA(n_components=0.95) model.fit(x) x = model.transform(x) ","permalink":"https://minyeamer.github.io/blog/aischool-06-07-pca/","summary":"Principal Component Analysis 차원 축소를 통해 최소 차원의 정보로 원래 차원의 정보를 모사하는 알고리즘 데이터의 열의 수가 많아 학습 속도가 느려질 때 열의 수를 줄이기 위해 사용 Dimension Reduction: 고차원 벡터에서 일부 차원의 값을 모두 0으로 만들어 차원을 줄임 원래의 고차원 벡터의 특성을 최대한 살리기 위해 가장 분산이 높은 방향으로 회전 변환 진행 전체 데이터를 기반으로 분산이 가장 큰 축을 찾아 PC 1으로 만들고,\nPC 1에 직교하는 축 중에서 분산이 가장 큰 축을 PC 2로 만드는 과정 반복 정보의 누락이 있기 때문에 경우에 따라 모델의 성능 하락 발생 Feature Selection: 기존에 존재하는 열 중에 n개를 선택 Feature Extraction: 기존에 있는 열들을 바탕으로 새로운 열들을 만들어냄 (차원 축소) Learning Process Import Libraries 1 2 from sklearn import decomposition from sklearn import datasets Load Model 1 2 3 iris = datasets.","title":"[AI SCHOOL 5기] 머신 러닝 실습 - PCA"},{"content":"1. K-Means Algorithm K는 전체 데이터를 몇 개의 그룹으로 묶어낼 것인지 결정하는 상수 어떤 K 값이 적절한 것인지 파악하는 것이 중요 각각의 데이터마다 중심값까지의 거리를 계속 물어보기 때문에 계산량이 많음 클러스터링 성능을 향상시키기 위해 GPU Accelerated t-SNE for CUDA 활용 Clustering Process K개의 임의의 중심값을 선택 각 데이터마다 중심값까지의 거리를 계산하여 가까운 중심값의 클러스터에 할당 각 클러스터에 속한 데이터들의 평균값으로 각 중심값을 이동 데이터에 대한 클러스터 할당이 변하지 않을 때까지 2와 3을 반복 2. Learning Process Model Fitting 1 2 3 from sklearn import cluster kmeans = cluster.KMeans(n_clusters=2, random_state=0).fit(X) kmeans.labels_: 클러스터 번호 kmeans.cluster_centers_: 학습이 끝난 중심값 Model Predict 1 kmeans.predict([[0, 0], [8, 4]])) 각각의 번호가 어떤 클러스터에 속하는지 예측 3. K-Means for Iris Data Import Libraries 1 2 3 4 5 6 7 import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from sklearn import cluster from sklearn import datasets from sklearn import metrics Axes3D: 3D 공간에서 시각화하는 함수 Model Fitting 1 2 3 estimators = [(\u0026#39;k=8\u0026#39;, cluster.KMeans(n_clusters=8)), (\u0026#39;k=3\u0026#39;, cluster.KMeans(n_clusters=3)), (\u0026#39;k=3(r)\u0026#39;, cluster.KMeans(n_clusters=3, n_init=1, init=\u0026#39;random\u0026#39;))] Plot Model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 fignum = 1 titles = [\u0026#39;8 clusters\u0026#39;, \u0026#39;3 clusters\u0026#39;, \u0026#39;3 clusters, bad initialization\u0026#39;] for name, est in estimators: fig = plt.figure(fignum, figsize=(7, 7)) ax = Axes3D(fig, elev=48, azim=134) est.fit(X) labels = est.labels_ ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(np.float), edgecolor=\u0026#39;w\u0026#39;, s=100) ... fignum = fignum + 1 plt.show() plt.figure(fignum): plot을 여러 개 생성 (subplot()은 하나의 plot을 분리) Axes3D(elev, azim): elevation (축의 고도), azimuth (방위각) astype: 색깔 칠해주는 옵션 edgecolor: 테두리 1번 모델은 8개의 클러스터로 나눈 모델 (불필요하게 세분화시킴) 3번 모델은 초기 중앙값을 랜덤으로 잡아서 특정 클러스터에 데이터가 몰림 Ground Truth (원본) 1 2 3 4 5 6 7 8 9 10 11 12 fig = plt.figure(figsize=(7, 7)) ax = Axes3D(fig, elev=48, azim=134) for name, label in [(\u0026#39;Setosa\u0026#39;, 0), (\u0026#39;Versicolour\u0026#39;, 1), (\u0026#39;Virginica\u0026#39;, 2)]: ax.text3D(X[y == label, 3].mean(), X[y == label, 0].mean(), X[y == label, 2].mean()+2, name, horizontalalignment=\u0026#39;center\u0026#39;) ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor=\u0026#39;w\u0026#39;, s=100) ... plt.show() 4. 최적의 클러스터 개수 최적의 클러스터 기준 같은 클러스터에 있는 데이터끼리 뭉쳐 있음 서로 다른 클러스터에 있는 데이터끼리 멀리 떨어져 있음 Elbow 기법 SSE(Sum of Squared Errors)의 값이 점점 줄어들다가 어느 순간\n줄어드는 비율이 급격하게 작아지는 부분이 발생 결과물인 그래프 모양을 보면 팔꿈치에 해당하는 부분이 최적의 클러스터 개수가 됨 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def elbow(X): total_distance = [] for i in range(1, 11): model = cluster.KMeans(n_clusters=i, random_state=0) model.fit(X) total_distance.append(model.inertia_) plt.plot(range(1, 11), total_distance, marker=\u0026#39;o\u0026#39;) plt.xlabel(\u0026#39;# of clusters\u0026#39;) plt.ylabel(\u0026#39;Total distance (SSE)\u0026#39;) plt.show() elbow(X) model.inertia_: 샘플에 대해 가장 가까운 클러스터와의 거리 제곱의 합 inertia 값은 클러스터 수가 늘어날수록 감소 같은 클러스터에 있는 데이터끼리 뭉쳐있는 정도만 확인 가능 Silhouette 클러스터링의 품질을 정량적으로 계산해주는 방법 (모든 클러스터링 기법에 적용 가능) i번째 데이터 x(i)에 대한 실루엣 계수 s(i) 값은 아래의 식으로 정의 a(i): 클러스터 내 데이터 응집도(cohesion) 를 나타내는 값\n== 데이터 x(i)와 동일한 클러스터 내의 나머지 데이터들과의 평균 거리 b(i): 클러스터 간 분리도(separation) 를 나타내는 값\n== 데이터 x(i)와 가장 가까운 클러스터 내의 모든 데이터들과의 평균 거리 클러스터의 개수가 최적화되어 있으면 실루엣 계수의 값은 1에 가까운 값이 됨 실루엣 계수의 평균이 0.7 이상이면 안정적 1 2 3 from sklearn.metrics import silhouette_score silhouette_avg = silhouette_score(X, y_fitted) silhouette_avg: 실루엣 계수의 평균 클러스터링의 기준이 이론적으로는 맞을 수 있어도 실용적으로는 다를 수 있음\n(판단 기준이 없을 때 활용) ","permalink":"https://minyeamer.github.io/blog/aischool-06-06-k-means/","summary":"1. K-Means Algorithm K는 전체 데이터를 몇 개의 그룹으로 묶어낼 것인지 결정하는 상수 어떤 K 값이 적절한 것인지 파악하는 것이 중요 각각의 데이터마다 중심값까지의 거리를 계속 물어보기 때문에 계산량이 많음 클러스터링 성능을 향상시키기 위해 GPU Accelerated t-SNE for CUDA 활용 Clustering Process K개의 임의의 중심값을 선택 각 데이터마다 중심값까지의 거리를 계산하여 가까운 중심값의 클러스터에 할당 각 클러스터에 속한 데이터들의 평균값으로 각 중심값을 이동 데이터에 대한 클러스터 할당이 변하지 않을 때까지 2와 3을 반복 2.","title":"[AI SCHOOL 5기] 머신 러닝 실습 - K-Means"},{"content":"Support Vector Machine 패턴 인식을 위한 지도 학습 모델 데이터를 분류하는 Margin을 최대화하는 결정 경계(Decision Boundary)를 찾는 기법 결정 경계와 가장 가까운 데이터를 가로지르는 선을 기준으로 Plus \u0026amp; Minus Plane 설정 Support Vector: 결정 경계와 가장 가까운 데이터의 좌표 Margin: b11(plus-plane)과 b12(minus-plane) 사이의 거리, 2/w 기존의 Hard Margin SVM은 소수의 Noise로 인해 결정 경계를 찾지 못할 수 있음 Plus \u0026amp; Minus Plane에 약간의 여유 변수를 두어 에러를 무시하는 Soft Margin SVM로 발전 arg min $$arg\\ min\\lbrace\\frac{1}{2}{||w||}^2+C\\Sigma^n_{i=1}\\xi_i\\rbrace$$ $$\\text{단, }y_i({w}\\cdot{x_i}-b)\\ge{1-\\xi_i},\\quad{\\xi_i\\ge{0}},\\quad{\\text{for all }1\\le{i}\\le{n}}$$\n중괄호 안의 값(w, ξ, b)을 최소화하는 값을 찾는 것 Margin을 최대화하는 목적 함수(Objective Function) Margin(2/w)의 최대화는 w/2의 최소화와 같음 (제곱은 미분 편의성) Margin을 침범한 에러(ξ)를 모두 더하고(Σ), Hyper-Parameter인 C를 곱함 C: 얼마 만큼 여유를 가지고 오류를 허용할 것인지 판단해주는 값 C가 작을수록 에러를 무시, C가 크면 에러에 민감 Kernel Support Vector Machine 데이터가 선형적으로 분리되지 않을 경우(Lineaerly Unseparable)에 결정 경계를 찾는 기법 원본 데이터가 놓여있는 차원을 비선형 매핑을 통해 고차원 공간으로 변환 Hyper-Parameter인 커널 함수를 컴퓨터가 스스로 찾아내는 것이 Deep Learning 커널 함수는 인공신경망의 레이어와 비슷 (Learnable Kernel) Feature Crosses 데이터를 인공신경망에 밀어넣기 이전에, 기존 열들의 입력값을 조합해서 새로운 데이터 생성 @ http://j.mp/2p5CbO2 SVC Models Linear SVC 1 2 3 from sklearn.svm import LinearSVC linear_svm = LinearSVC().fit(X, y) Kernelized SVC 1 2 3 4 from sklearn.svm import SVC X, y = custom_mglearn.make_handcrafted_dataset() svm = SVC(kernel=\u0026#39;rbf\u0026#39;, C=10, gamma=0.1).fit(X, y) rbf: 대표적인 커널 함수 (Radial Basis Function, 가우시안 커널 함수) gamma: 가우시안 함수의 단면을 r 이라 할 때, 반지름의 역수(1/r) gamma가 작아질수록 반지름이 커져 선과 같은 형태가 됨 Learning Process 1 2 3 4 5 6 7 cancer = load_breast_cancer() # 유방암 데이터 (569행, 30열) X_train, X_test, y_train, y_test = train_test_split( cancer.data, cancer.target, random_state=0) svc = SVC(gamma=\u0026#39;auto\u0026#39;) svc.fit(X_train, y_train) train_test_split()에 test_size가 없으면 기본값 0.25를 Test Data로 지정 SVC(gamma='auto'): gamma를 알아서 지정 분류 모델의 경우, model.score(X, Y)를 실행하면 Accuracy Score 계산 svc.score(X_train, y_train): Training Data의 Accuracy svc.score(X_test, y_test): Test Data의 Accuracy 두 데이터의 Accuracy 차이가 심한 것은 열마다 Scale이 달라서 발생하는 문제 Scaling 후 Hyper-Parameter를 바꿨을 때 두 데이터의 차이가 심하면 Overfitting Feature Normalization (Scaling) min, max, mean, std를 계산할 때 Training Data만 사용 Min-Max Normalization min(열) = 0, max(열) = 1 new_X = (old_X - min) / (max - min) Standardization mean(열) = 0, std(열) = 1 new_X = (old_X - mean) / std Scaler Model 1 2 3 4 sc = StandardScaler() # 또는 MinMaxScaler() sc.fit(train_X) train_X_scaled = sc.transform(train_X) test_X_scaled = sc.transform(test_X) 새로운 데이터가 들어오면 2차원 행렬 상태로 Scaler를 통과시키고 모델에 입력 모델을 저장하고 불러올 때 Scaler도 같이 가져와야 함 Stanardization이 Min-Max 알고리즘보다 성능이 좋음 (예외 있음) Grid-Search 1 2 3 4 5 6 7 8 9 from sklearn.model_selection import GridSearchCV param_grid = {\u0026#39;C\u0026#39; : [0.1, 1, 10, 100, 1000], \u0026#39;gamma\u0026#39; : [1, 0.1, 0.01, 0.001, 0.0001], \u0026#39;kernel\u0026#39; : [\u0026#39;rbf\u0026#39;]} grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=1) grid.fit(X_train_scaled, y_train) C와 gamma의 후보군을 조합 내부적으로 K-Fold로 검증하기 때문에 후보군이 많아질수록 시간이 오래걸림 refit=True: GridSearchCV를 다시 트레이닝 시킴 verbose: 값이 커질수록 설명을 상세하게 적어줌 GridSearch가 SVC 모델이 되어 Model Fitting, Model Predict 등 과정 진행 grid.best_params_: 최적의 C와 gamma의 조합 반환 내부적으로 검증을 거친 상태이기 때문에 Training Data를 한 번 돌린 것과는 정확도가 다름 Model Tuning (HPO) Grid-Search (Machine Learning) Randomized-Search (Deep Learning) Bayesian-Search (ML/DL) Model Predict 1 2 3 4 5 from sklearn.metrics import classification_report grid_predictions = grid.predict(X_test_scaled) print(classification_report(y_test, grid_predictions)) precision: 모델이 양성으로 분류한 것 중 진짜 걸린 것 recall: 양성인 것 중 모델이 양성이라 맞춘 것 f1-score: Precision과 Recall의 조합 support: Test Data confusion_matrix도 import해서 사용 가능 precision recall f1-score support 0 0.98 0.94 0.96 53 1 0.97 0.99 0.98 90 accuracy 0.97 143 macro avg 0.97 0.97 0.97 143 weighted avg 0.97 0.97 0.97 143 ","permalink":"https://minyeamer.github.io/blog/aischool-06-05-kernelized-svm/","summary":"Support Vector Machine 패턴 인식을 위한 지도 학습 모델 데이터를 분류하는 Margin을 최대화하는 결정 경계(Decision Boundary)를 찾는 기법 결정 경계와 가장 가까운 데이터를 가로지르는 선을 기준으로 Plus \u0026amp; Minus Plane 설정 Support Vector: 결정 경계와 가장 가까운 데이터의 좌표 Margin: b11(plus-plane)과 b12(minus-plane) 사이의 거리, 2/w 기존의 Hard Margin SVM은 소수의 Noise로 인해 결정 경계를 찾지 못할 수 있음 Plus \u0026amp; Minus Plane에 약간의 여유 변수를 두어 에러를 무시하는 Soft Margin SVM로 발전 arg min $$arg\\ min\\lbrace\\frac{1}{2}{||w||}^2+C\\Sigma^n_{i=1}\\xi_i\\rbrace$$ $$\\text{단, }y_i({w}\\cdot{x_i}-b)\\ge{1-\\xi_i},\\quad{\\xi_i\\ge{0}},\\quad{\\text{for all }1\\le{i}\\le{n}}$$","title":"[AI SCHOOL 5기] 머신 러닝 실습 - Kernelized SVM"},{"content":"K-Nearest Neightbor Algorithm 기존의 가까운 이웃 데이터를 살펴 새로운 데이터를 분류하는 알고리즘 K=3일 경우, 가장 가까운 나머지 3개 중 2개가 Red면 Red로 판단 K 값이 작아질수록 아주 작은 영향에로 판단이 바뀌는 Overfitting 발생 K 값이 커질수록 멀리보고 결정이 느려져 Overfitting 감소 Learning Process Load Data 1 iris = datasets.load_iris() # 붓꽃 데이터 (150행, 4열) Select Feature 1 2 x = iris.data[:, :2] # [꽃받침 길이, 꽃받침 넓이] y = iris.target Create Model 1 model = neighbors.KNeighborsClassifier(6) Model Fitting 1 model.fit(x, y) Model Predict 1 model.predict([[9, 2.5], [3.5, 11]]) # 각각의 분류 표시 Plot Model Data Points 1 2 3 4 plt.figure(figsize=(10,5)) plt.scatter(x[:, 0], x[:, 1]) plt.title(\u0026#34;Data points\u0026#34;) plt.show() Plot KNN 1 2 3 4 5 6 7 x_min, x_max = x[:,0].min() - 1, x[:,0].max() + 1 y_min, y_max = x[:,1].min() - 1, x[:,1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) 1 2 3 4 5 6 7 8 9 10 11 cmap_light = ListedColormap([\u0026#39;#FFAAAA\u0026#39;, \u0026#39;#AAFFAA\u0026#39;,\u0026#39;#00AAFF\u0026#39;]) cmap_bold = ListedColormap([\u0026#39;#FF0000\u0026#39;, \u0026#39;#00FF00\u0026#39;,\u0026#39;#0000FF\u0026#39;]) plt.figure(figsize=(10,5)) plt.pcolormesh(xx, yy, Z, cmap=cmap_light) plt.scatter(x[:, 0], x[:, 1], c=y, cmap=cmap_bold, edgecolors=\u0026#39;gray\u0026#39;) plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.title(\u0026#34;3-Class classification (k = 1)\u0026#34;) plt.show() K를 높일수록 결정 경계가 부드러워짐 ","permalink":"https://minyeamer.github.io/blog/aischool-06-04-knn/","summary":"K-Nearest Neightbor Algorithm 기존의 가까운 이웃 데이터를 살펴 새로운 데이터를 분류하는 알고리즘 K=3일 경우, 가장 가까운 나머지 3개 중 2개가 Red면 Red로 판단 K 값이 작아질수록 아주 작은 영향에로 판단이 바뀌는 Overfitting 발생 K 값이 커질수록 멀리보고 결정이 느려져 Overfitting 감소 Learning Process Load Data 1 iris = datasets.load_iris() # 붓꽃 데이터 (150행, 4열) Select Feature 1 2 x = iris.data[:, :2] # [꽃받침 길이, 꽃받침 넓이] y = iris.target Create Model 1 model = neighbors.","title":"[AI SCHOOL 5기] 머신 러닝 실습 - KNN"},{"content":"XG Boost Extreme Gradient Boosting 대용량 분산 처리를 위한 Gradient Boosting 라이브러리 Decision Tree(의사결정나무) 에 Boosting 기법을 적용한 알고리즘 AdaBoost는 학습 성능은 좋으나, 모델의 학습 시간이 오래 걸리는 단점 병렬 처리 기법을 적용하여 Gradient Boost보다 학습 속도를 끌어올림 Hyper-Parameter가 너무 많기 때문에 권장 세팅 사용 @ http://j.mp/2PukeTS Decision Tree 이해하기 쉽고 해석도 용이함 입력 데이터의 작은 변동에도 Tree의 구성이 크게 달라짐 과적합이 쉽게 발생 (중간에 멈추지 않으면 Leaf 노드에 하나의 데이터만 남게 됨) 의사결정나무의 문제를 해결하기 위해 Boosting 기법 활용 ex) 테니스를 쳤던 과거 데이터를 보고 날씨 정보를 이용해 의사결정 AdaBoost Adaptive Boosting 데이터를 바탕으로 여러 weak learner 들을 반복적으로 생성 앞선 learner가 잘못 예측한 데이터에 가중치를 부여하고 학습 최종적으로 만들어진 strong learner를 이용하여 실제 예측 진행 에러를 최소화하는 weight를 매기기 위해 경사 하강법 사용 ex) Regression: 평균/가중평균, Classification: 투표 XG Boost References NGBoost Explained (Comparison to LightGBM and XGBoost) Gradient Boosting Interactive Playground Gradient Boosting explained Comparison for hyperparams of XGBoost \u0026amp; LightGBM XGBoost Parameters XG Boost 하이퍼 파라미터 상세 설명 Complete Guide to Parameter Tuning in XGBoost (with python codes) Microsoft EBM (Explainable Boosting Machine) 정형데이터를 위한 인공신경망 모델, TabNet Ensemble 주어진 데이터를 이용하여 여러 개의 서로 다른 예측 모형을 생성한 후,\n예측 모형의 예측 결과를 종합하여 하나의 최종 예측결과를 도출해내는 방법 다양한 모델이 문제 공간의 다른 측면을 보면서 각기 다른 방식으로 오점이 있다고 가정\n(모델 별로 약점을 보완) Boosting weak learner들을 strong learner로 변환시키는 알고리즘\n(약한 학습기를 여러개 사용해서 하나의 강건한 학습기를 만들어내는 것) 의사결정나무 모델을 합리적인 수준(60~70% 성능)에서 여러 종류 생성 ex) AdaBoost Gradient Boosting 경사 하강법을 사용해서 AdaBoost보다 성능을 개선한 Boosting 기법 AdaBoost는 높은 가중치를 가진 지점이 존재하게 되면 성능이 크게 떨어지는 단점\n(높은 가중치를 가진 지점과 가까운 다른 데이터들이 잘못 분류될 가능성이 높음) Gradient Boosting 기법은 이전 모델에 종속적이기 때문에 병렬 처리가 불가능 Bagging Bootstrap Aggregating 가중치를 매기지 않고 각각의 모델이 서로 독립적 x 데이터 열들의 서로 다른 조합으로 독립적인 모델을 여러 종류 생성 ex) Random Forest Random Forest 각 모델은 서로 다른 샘플 데이터셋을 활용 (Bootstrap Sampling \u0026amp; Bagging) 각 데이터셋은 복원추출을 통해 원래 데이터셋 만큼의 크기로 샘플링 (누락 \u0026amp; 중복 발생) 위 서로 다른 샘플로 각 모델 생성 시, 각 노드 지점마다 x열 n개 중 랜덤하게 m개 중 분기 선택 Classification에서는 root n을 m으로 사용 Regression에서는 n/3을 m으로 사용 @ https://j.mp/3rZ05bN \u0026amp; https://j.mp/3GJ7QqH Learning Process @ https://j.mp/3jRJH6n Import Libraries 1 2 3 4 5 6 7 import numpy as np import matplotlib.pyplot as plt from sklearn import ensemble from sklearn import datasets from sklearn.utils import shuffle from sklearn.metrics import mean_squared_error Load Data 1 2 3 4 5 6 7 8 boston = datasets.load_boston() X, y = shuffle(boston.data, boston.target, random_state=13) X = X.astype(np.float32) offset = int(X.shape[0] * 0.9) X_train, y_train = X[:offset], y[:offset] X_test, y_test = X[offset:], y[offset:] load 부분에서 다른 데이터도 사용 가능 boston.data: x data boston.target: y data int(X.shape[0] * 0.9): 전체 행 중 90 퍼센트 의미 boston.feature_names: 데이터셋에서 열 이름 boston.DESCR: 데이터에 대한 상세 설명 Model Fitting 1 2 3 4 5 6 7 params = {\u0026#39;n_estimators\u0026#39;: 500, \u0026#39;max_depth\u0026#39;: 4, \u0026#39;min_samples_split\u0026#39;: 2, \u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;loss\u0026#39;: \u0026#39;ls\u0026#39;} clf = ensemble.GradientBoostingRegressor(**params) clf.fit(X_train, y_train) mse = mean_squared_error(y_test, clf.predict(X_test)) print(\u0026#34;MSE: %.4f\u0026#34; % mse) **params: 딕셔너리를 파라미터로 변환 @ https://j.mp/2IPuJzY clf: Classifier Plot Deviance 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 test_score = np.zeros((params[\u0026#39;n_estimators\u0026#39;],), dtype=np.float64) for i, y_pred in enumerate(clf.staged_predict(X_test)): test_score[i] = clf.loss_(y_test, y_pred) plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\u0026#39;Deviance\u0026#39;) plt.plot(np.arange(params[\u0026#39;n_estimators\u0026#39;]) + 1, clf.train_score_, \u0026#39;b-\u0026#39;, label=\u0026#39;Training Set Deviance\u0026#39;) plt.plot(np.arange(params[\u0026#39;n_estimators\u0026#39;]) + 1, test_score, \u0026#39;r-\u0026#39;, label=\u0026#39;Test Set Deviance\u0026#39;) plt.legend(loc=\u0026#39;upper right\u0026#39;) plt.xlabel(\u0026#39;Boosting Iterations\u0026#39;) plt.ylabel(\u0026#39;Deviance\u0026#39;) Deviance: 편차값, 에러 n_estimators: 의사결정나무 모델을 몇 개 만들었는지 Test Data에 대한 에러 라인이 튕겨올라가는 지점이 Overfitting Plot Feature Importance 1 2 3 4 5 6 7 8 9 10 11 feature_importance = clf.feature_importances_ feature_importance = 100.0 * (feature_importance / feature_importance.max()) sorted_idx = np.argsort(feature_importance) pos = np.arange(sorted_idx.shape[0]) + .5 plt.subplot(1, 2, 2) plt.barh(pos, feature_importance[sorted_idx], align=\u0026#39;center\u0026#39;) plt.yticks(pos, boston.feature_names[sorted_idx]) plt.xlabel(\u0026#39;Relative Importance\u0026#39;) plt.title(\u0026#39;Variable Importance\u0026#39;) plt.show() feature_importances_: 트리 기반 모델이 가지고 있는 변수, 각각의 열마다의 중요도 Feature Importance는 상대적인 중요도이기 때문에 합계가 1 LSTAT(인구 중 하위 계층 비율)이 집값을 예측할 때 가장 중요함 Feature Importance References Feature Importance Analysis (LIME) Permutation importance 한글 설명 Permutation importance with Pipeline ","permalink":"https://minyeamer.github.io/blog/aischool-06-03-gradient-boosting/","summary":"XG Boost Extreme Gradient Boosting 대용량 분산 처리를 위한 Gradient Boosting 라이브러리 Decision Tree(의사결정나무) 에 Boosting 기법을 적용한 알고리즘 AdaBoost는 학습 성능은 좋으나, 모델의 학습 시간이 오래 걸리는 단점 병렬 처리 기법을 적용하여 Gradient Boost보다 학습 속도를 끌어올림 Hyper-Parameter가 너무 많기 때문에 권장 세팅 사용 @ http://j.mp/2PukeTS Decision Tree 이해하기 쉽고 해석도 용이함 입력 데이터의 작은 변동에도 Tree의 구성이 크게 달라짐 과적합이 쉽게 발생 (중간에 멈추지 않으면 Leaf 노드에 하나의 데이터만 남게 됨) 의사결정나무의 문제를 해결하기 위해 Boosting 기법 활용 ex) 테니스를 쳤던 과거 데이터를 보고 날씨 정보를 이용해 의사결정 AdaBoost Adaptive Boosting 데이터를 바탕으로 여러 weak learner 들을 반복적으로 생성 앞선 learner가 잘못 예측한 데이터에 가중치를 부여하고 학습 최종적으로 만들어진 strong learner를 이용하여 실제 예측 진행 에러를 최소화하는 weight를 매기기 위해 경사 하강법 사용 ex) Regression: 평균/가중평균, Classification: 투표 XG Boost References NGBoost Explained (Comparison to LightGBM and XGBoost) Gradient Boosting Interactive Playground Gradient Boosting explained Comparison for hyperparams of XGBoost \u0026amp; LightGBM XGBoost Parameters XG Boost 하이퍼 파라미터 상세 설명 Complete Guide to Parameter Tuning in XGBoost (with python codes) Microsoft EBM (Explainable Boosting Machine) 정형데이터를 위한 인공신경망 모델, TabNet Ensemble 주어진 데이터를 이용하여 여러 개의 서로 다른 예측 모형을 생성한 후,","title":"[AI SCHOOL 5기] 머신 러닝 실습 - Gradient Boosting"},{"content":"Logistic Regression 이진 분류(0 또는 1) 문제를 해결하기 위한 모델 다항 로지스틱 회귀(k-class), 서수 로지스틱 회귀(k-class \u0026amp; ordinal)도 존재 Sigmoid Function을 이용하여 입력값이 양성 클래스에 속할 확률을 계산 로지스틱 회귀를 MSE 식에 넣으면 지수 함정의 특징 때문에 함정이 많은 그래프가 나옴 분류를 위한 Cost Function인 Cross-Entropy 활용 성능 지표로는 Cross-Entropy 외에 Accuracy 등을 같이 사용 ex) 스팸 메일 분류, 질병 양성/음성 분류 등 양성/음성 분류 모델 선형 모델은 새로운 데이터가 들어오면 양성/음성 판단 기준이 크게 바뀜 모델을 지수 함수인 Sigmoid Function으로 변경 Sigmoid Function θ 값에 따라 기울기나 x축의 위치가 바뀌는 지수 함수 y축을 이동하는 선형 함수와 다르게 x축을 이동 y가 0.5가 되는 지점을 기준으로 대칭되는 형태 y값은 조건부 확률로 해석 (X가 있을 때 양성 클래스일 확률값) Cross-Entropy Function 예측값의 분포와 실제값의 분포를 비교하여 그 차이를 Cost로 결정 인공신경망에 각 행을 열단위로 쪼개 입력으로 넣었을 때 마지막에 카테고리 개수만큼의 수를 반환 Softmax Function을 통과시키면 개수는 그대로지만 합쳤을 때 1이되는 숫자로 변경 인공신경망 결과로 뱉어낸 카테고리별 확률을 정답과 비교해 Cross-Entropy 계산 One-Hot Encoding이 적용된 정답을 One-Hot Label이라 부름 정답 확률이 높고 오답 확률이 낮은 모델이 나은 모델 Cross-Entropy는 하나의 분포를 다른 분포로 옮겨내는 거리라고도 불림 Softmax Algorithm $$S(y_i)=\\frac{e^{y_i}}{\\Sigma_j{e^{y_i}}}$$\n다중 클래스 분류 문제를 위한 알고리즘 모델의 결과에 해당하는 점수를 각 클래스에 소속될 확률에 해당하는 값들의 벡터로 변환\n(클래스 개수만큼의 숫자를 입력 받으면 합이 1이 되는 확률값으로 변환) ROC Curve Receiver Operating Characteristic Curve 얼마나 신호에 민감하게 반응할지를 그려낸 곡선 Threshold를 끌어올리거나 끌어내리는 과정에서 발생 Threshold를 끌어롤리면 양성 기준이 엄격해지고 끌어내리면 양성 기준이 너그러워짐 모델이 엄격할 때는 양성율이 낮지만 이를 억지로 끌어올리는 과정에서 위양성율이 발생 선이 직각일수록 이상적인 모델, 반면 모델의 성능이 안좋아 실수가 많으면 선이 아래로 내려옴 Confusion Matrix 분류 모델이 학습 결과를 뱉어낸 것을 바탕으로 만든 표 모델이 얼마나 혼동하고 있는지를 나타냄 Accuracy(정확성): (참긍정 + 참부정) / 총 예시 수 Recall(재현율): 정답에서 참인 것을 골라냄, TP / (FN + TP) Precision(정밀도): 분류한 것들 중 정답을 골라냄, TP / (TP + FP) 스팸 메일 분류의 경우 정밀도를 높일 필요가 있음 F1-Score: Recall과 Precision의 조화평균, 2RP / (R + P) F-beta score: Recall과 Precision에 가중치 부여 AUC Area Under the ROC Curve ROC 커브 밑에 있는 영역의 크기 0.5에서 1 사이의 값이 나오며, 0.7 후반을 쓸만한 모델로 판단 Learning Process Load Data pd.read_excel()로 엑셀 데이터 불러오기 엑셀 데이터를 np.array() 안에 넣어 Numpy Array 형태로 변경 Select Feature 떨어진 열들을 꺼낼 때 data[:, (5, 12)] 형식으로 열을 꺼냄 Training \u0026amp; Test Set 1 2 3 4 from sklearn import model_selection x_train, x_test, y_train, y_test = \\ model_selection.train_test_split(boston_X, boston_Y, test_size=0.3, random_state=0) Create Model 1 2 3 from sklearn import linear_model model = linear_model.LogisticRegression() Model Fitting 1 model.fit(x_train, y_train) Model Predict 1 model.predict(x_train) Accuracy 1 2 3 from sklearn.metrics import accuracy_score print(accuracy_score(model.predict(x_test), y_test)) 양성/음성 확률 1 model.predict_proba(x_test) Plot ROC Curve 1 2 3 4 from sklearn.metrics import roc_curve, auc fpr, tpr, _ = roc_curve(y_true=y_test, y_score=pred_test[:,1]) roc_auc = auc(fpr, tpr) y_true가 정답, y_score가 양성이 나올 확률 fpr: ROC 커브를 그리기 위한 x좌표 tpr: ROC 커브를 그리기 위한 y좌표 1 2 3 4 5 6 7 8 9 10 11 12 13 plt.figure(figsize=(10, 10)) plt.plot(fpr, tpr, color=\u0026#39;darkorange\u0026#39;, lw=2, label=\u0026#39;ROC curve (area = %0.2f)\u0026#39; % roc_auc) plt.plot([0, 1], [0, 1], color=\u0026#39;navy\u0026#39;, lw=2, linestyle=\u0026#39;--\u0026#39;) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(\u0026#39;False Positive Rate\u0026#39;) plt.ylabel(\u0026#39;True Positive Rate\u0026#39;) plt.legend(loc=\u0026#34;lower right\u0026#34;) plt.title(\u0026#34;ROC curve\u0026#34;) plt.show() ","permalink":"https://minyeamer.github.io/blog/aischool-06-02-logistic-regression/","summary":"Logistic Regression 이진 분류(0 또는 1) 문제를 해결하기 위한 모델 다항 로지스틱 회귀(k-class), 서수 로지스틱 회귀(k-class \u0026amp; ordinal)도 존재 Sigmoid Function을 이용하여 입력값이 양성 클래스에 속할 확률을 계산 로지스틱 회귀를 MSE 식에 넣으면 지수 함정의 특징 때문에 함정이 많은 그래프가 나옴 분류를 위한 Cost Function인 Cross-Entropy 활용 성능 지표로는 Cross-Entropy 외에 Accuracy 등을 같이 사용 ex) 스팸 메일 분류, 질병 양성/음성 분류 등 양성/음성 분류 모델 선형 모델은 새로운 데이터가 들어오면 양성/음성 판단 기준이 크게 바뀜 모델을 지수 함수인 Sigmoid Function으로 변경 Sigmoid Function θ 값에 따라 기울기나 x축의 위치가 바뀌는 지수 함수 y축을 이동하는 선형 함수와 다르게 x축을 이동 y가 0.","title":"[AI SCHOOL 5기] 머신 러닝 실습 - 로지스틱 회귀"},{"content":"인공지능 Intelligent Agents를 만드는 것 주변 환경들을 인식하고 원하는 행동을 취하여 목표를 성취하는 것 Artificial Narrow Intelligence 제한된 기능만 수행할 수 있는 인공지능 weak AI Artificial General Intelligence 사람만큼 다양한 분야에서 기능을 수행할 수 있는 인공지능 strong AI Artificial Super Intelligence 모든 분야에서 사람보다 뛰어난 인공지능 모델 데이터를 가장 잘 설명할 수 있는 함수 (y = ax + b) 모델에서 θ는 Parameter(가중치, Weight) 의미 모델에서 h(x)는 Hypotheses(가설) 의미 모델에서 b는 Bias(편향, 보정치) 의미 머신러닝 어떠한 과제를 해결하는 과정에서 특정한 평가 기준을 바탕으로 학습의 경험을 쌓는 프로그램 머신러닝 분류 Supervised 입력값에 대한 정답을 예측하기 위해 학습 데이터와 정답이 같이 존재 회귀(Regression): 결과가 실수 영역 전체에서 나타남 분류(Classification): 결과가 특정 분류에 해당하는 불연속값으로 나타남 ex) 주식 가격 예측, 이미지 인식 등 Unsupervised 입력값 속에 숨어있는 규칙성을 찾기 위해 학습 정답이 없는 데이터를 주고 비슷한 집단을 분류 ex) 고객군 분류, 장바구니 분석(Association Rule) 등 Reinforcement Trial \u0026amp; Error를 통한 학습 최종적으로 얻게 될 기대 보상을 최대화하기 위한 행동 선택 정책 학습 각 상태에 대해 결정한 행동을 통해 환경으로부터 받는 보상을 학습 ex) 로봇 제어, 공정 최적화 등 Automated ML 어떤 모델(함수, 알고리즘)을 써야할지를 컴퓨터가 알아서 정하게 함 인공신경망 레이어의 범위, 후보 등을 정해놓고 그 안에서 가장 좋은 조합을 찾음 ex) AutoML Tables (행의 수가 1000건이 넘어야하는 제약) 학습 데이터를 가장 잘 설명하는 방법을 찾는 과정 데이터에 맞는 모델을 찾는 과정 (= Model Fitting) 실제 정답과 예측 결과 사이의 오차(Loss, Cost, Error)를 줄여나가는 최적화 과정 학습 과정 초기 모델에 데이터를 입력 결과를 평가 (예측/분류의 정확도 등) 결과를 개선하기 위해 모델을 수정 (모델 내부 Parameter 수정 등) Model\u0026rsquo;s Capacity 2번 모델은 3번 모델보다 오차가 크지만 새로운 데이터가 생겼을 때 비슷하게 예측 가능 3번 모델은 오차가 가장 적지만 새로운 데이터가 생겼을 때 오차가 매우 커질 수 있음 3번 모델과 같은 Overfitting(과적합)이 발생하기 전에 학습을 멈춤 Cross Validation 새로운 데이터들에 대해서도 좋은 결과를 내게 하기 위해 데이터를 3개 그룹으로 나눠 학습 60%의 Training Data로 모델을 학습 20%의 Validation Data로 모델을 최적화/선택 20%의 Test Data로 모델을 평가 데이터를 분리하는 비율은 모델에 따라 달라짐 K-Fold Cross Validation 후보 모델 간 비교 및 선택을 위한 알고리즘 Training Data를 K 등분하고 그 중 하나를 Validation Data로 설정 K 값은 자체적으로 결정하며 보통 10-Fold 사용 (시간이 없으면 5-Fold) 머신러닝에서 K는 주로 사용자가 결정하는 상수 Stratified: 층화 표집 방법, 데이터의 분류 별 비율이 다르면 K-Fold 조각 안에서 비율을 유지시킴 10-Fold 학습 과정 데이터를 80%의 Training Data와 20%의 Test Data로 나누고 Training Data를 10등분\nPhase 1. Training Data(0:9) + Validation Data(TD 9)를 사용해 점수 측정\nPhase 2. Training Data(0:8,9) + Validation Data(TD 8)를 사용해 점수 측정\n. . .\nPhase 10. Training Data(1:10) + Validation Data(TD 0)를 사용해 점수 측정\n마지막으로 Training Data 전체를 학습하고 Test Data로 검증\nScikit-learn 파이썬으로 전통적인 머신 러닝 알고리즘들을 구현한 오픈 소스 라이브러리 다른 라이브러리들과의 호환성이 좋음 (Numpy, Pandas, Matplotlib 등) 머신러닝 학습 과정 데이터셋 불러오기 1 sklearn.load_[DATA]() Train/Test set으로 데이터 나눔 1 sklearn.model_selection.train_test_split(X, Y, test_size) 모델 객체 생성 1 2 3 4 5 6 sklearn.linear_model.LinearRegression() sklearn.linear_model.LogisticRegression() sklearn.neighbors.KNeighborsClassifier(n_neighbors) sklearn.cluster.KMeans(n_clusters) sklearn.decomposition.PCA(n_components) sklearn.svm.SVC(kernel, C, gamma) 모델 학습 시키기 1 model.fit(train_X, train_Y) 모델로 새로운 데이터 예측 (Scaler를 적용했으면 새로운 데이터에도 적용) 1 2 3 4 5 6 model.predict(test_X) model.predict_proba(test_X) sklearn.metrics.mean_squared_error(predicted_Y, test_Y) sklearn.metrics.accuracy_score(predicted_Y, test_Y) sklearn.metrics.precision_score sklearn.metrics.recall_score 머신러닝 분류 기준 Choosing the right estimator Feature Normalization Numeric Column Min-Max Algorithm Standardization Categorical Column One-Hot Encoding, One-Hot Vector 열과 목록 개수만큼 0으로 채워진 행렬을 만들고 값이 해당하는 위치에 1을 표시 범주형 데이터(카테고리)의 숫자가 크고 작음에 관계없이 카테고리의 위치값만을 판단 Supervised Algorithm Linear Regression (선형 회귀) 종속 변수 y와 독립 변수 x 사이의 선형 상관 관계를 모델링하는 회귀분석 기법 Logistic Regression (로지스틱 회귀) 이진 분류(0 또는 1) 문제를 해결하기 위한 모델 ex) 스팸 메일 분류, 질병 양성/음성 분류 등 Gradient Boosting Regression (XG Boost) 대용량 분산 처리를 위한 Gradient Boosting 라이브러리 ex) 테니스를 쳤던 과거 데이터를 보고 날씨 정보를 이용해 의사결정 K-Nearest Neightbor Algorithm (KNN) 기존의 가까운 이웃 데이터를 살펴 새로운 데이터를 분류하는 알고리즘 Kernel Support Vector Machine (KSVM) 데이터를 분류하는 Margin을 최대화하는 결정 경계를 찾는 기법 Unsupervised Algorithm K-Means Algorithm 데이터를 K개의 클러스터로 분류하는 알고리즘 Principal Component Analysis 차원 축소를 통해 최소 차원의 정보로 원래 차원의 정보를 모사하는 알고리즘 Model Saving \u0026amp; Loading Model Saving 1 2 3 import joblib joblib.dump(model, \u0026#39;model_v1.pkl\u0026#39;, compress=True) Model Loading 1 2 3 import joblib model_loaded = joblib.load(\u0026#39;model_v1.pkl\u0026#39;) ","permalink":"https://minyeamer.github.io/blog/aischool-06-00-machine-learning/","summary":"인공지능 Intelligent Agents를 만드는 것 주변 환경들을 인식하고 원하는 행동을 취하여 목표를 성취하는 것 Artificial Narrow Intelligence 제한된 기능만 수행할 수 있는 인공지능 weak AI Artificial General Intelligence 사람만큼 다양한 분야에서 기능을 수행할 수 있는 인공지능 strong AI Artificial Super Intelligence 모든 분야에서 사람보다 뛰어난 인공지능 모델 데이터를 가장 잘 설명할 수 있는 함수 (y = ax + b) 모델에서 θ는 Parameter(가중치, Weight) 의미 모델에서 h(x)는 Hypotheses(가설) 의미 모델에서 b는 Bias(편향, 보정치) 의미 머신러닝 어떠한 과제를 해결하는 과정에서 특정한 평가 기준을 바탕으로 학습의 경험을 쌓는 프로그램 머신러닝 분류 Supervised 입력값에 대한 정답을 예측하기 위해 학습 데이터와 정답이 같이 존재 회귀(Regression): 결과가 실수 영역 전체에서 나타남 분류(Classification): 결과가 특정 분류에 해당하는 불연속값으로 나타남 ex) 주식 가격 예측, 이미지 인식 등 Unsupervised 입력값 속에 숨어있는 규칙성을 찾기 위해 학습 정답이 없는 데이터를 주고 비슷한 집단을 분류 ex) 고객군 분류, 장바구니 분석(Association Rule) 등 Reinforcement Trial \u0026amp; Error를 통한 학습 최종적으로 얻게 될 기대 보상을 최대화하기 위한 행동 선택 정책 학습 각 상태에 대해 결정한 행동을 통해 환경으로부터 받는 보상을 학습 ex) 로봇 제어, 공정 최적화 등 Automated ML 어떤 모델(함수, 알고리즘)을 써야할지를 컴퓨터가 알아서 정하게 함 인공신경망 레이어의 범위, 후보 등을 정해놓고 그 안에서 가장 좋은 조합을 찾음 ex) AutoML Tables (행의 수가 1000건이 넘어야하는 제약) 학습 데이터를 가장 잘 설명하는 방법을 찾는 과정 데이터에 맞는 모델을 찾는 과정 (= Model Fitting) 실제 정답과 예측 결과 사이의 오차(Loss, Cost, Error)를 줄여나가는 최적화 과정 학습 과정 초기 모델에 데이터를 입력 결과를 평가 (예측/분류의 정확도 등) 결과를 개선하기 위해 모델을 수정 (모델 내부 Parameter 수정 등) Model\u0026rsquo;s Capacity 2번 모델은 3번 모델보다 오차가 크지만 새로운 데이터가 생겼을 때 비슷하게 예측 가능 3번 모델은 오차가 가장 적지만 새로운 데이터가 생겼을 때 오차가 매우 커질 수 있음 3번 모델과 같은 Overfitting(과적합)이 발생하기 전에 학습을 멈춤 Cross Validation 새로운 데이터들에 대해서도 좋은 결과를 내게 하기 위해 데이터를 3개 그룹으로 나눠 학습 60%의 Training Data로 모델을 학습 20%의 Validation Data로 모델을 최적화/선택 20%의 Test Data로 모델을 평가 데이터를 분리하는 비율은 모델에 따라 달라짐 K-Fold Cross Validation 후보 모델 간 비교 및 선택을 위한 알고리즘 Training Data를 K 등분하고 그 중 하나를 Validation Data로 설정 K 값은 자체적으로 결정하며 보통 10-Fold 사용 (시간이 없으면 5-Fold) 머신러닝에서 K는 주로 사용자가 결정하는 상수 Stratified: 층화 표집 방법, 데이터의 분류 별 비율이 다르면 K-Fold 조각 안에서 비율을 유지시킴 10-Fold 학습 과정 데이터를 80%의 Training Data와 20%의 Test Data로 나누고 Training Data를 10등분","title":"[AI SCHOOL 5기] 머신 러닝"},{"content":"Linear Regression 종속 변수 y와 독립 변수 x 사이의 선형 상관 관계를 모델링하는 회귀분석 기법 정답이 있는 데이터의 추세를 잘 설명하는 선형 함수를 찾아 x에 대한 y를 예측 Linear Combination (선형 결합): 더하기와 곱하기로만 이루어진 식 단순 회귀분석: 1개의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때 다중 회귀분석: 2개 이상의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때 선형 회귀는 가장 적합한 θ들의 집합을 찾는 것이 목표 Cost Function 예측 값과 실제 값의 차이를 기반으로 모델의 성능(정확도)을 판단하기 위한 함수 Objective (MIN or MAX) 함수 안에 Cost Function이 존재 선형 회귀에서는 Mean Squre(d) Error Function (평균 제곱 오차 함수) 활용 MSE(Cost)가 최소가 되는 θ(a \u0026amp; b)를 찾아야하며,\n이를 위한 최적화 기법으로 Gradient Descent Algorithm (경사하강법) 활용 Mean Squre Error Function 회귀 분석을 위한 Cost Function y축 방향의 차이를 에러로 판단하는데 전체 에러를 단순하게 합칠 경우\n양 에러와 음 에러가 상쇄되어 올바른 판단을 할 수 없음 부호를 제거하기 위해 모든 에러에 제곱을 취하고 그 평균을 구한 것이 MSE MSE(Cost)가 0에 가까울수록 에러가 적다고 판단 값에 제곱을 취하기 때문에 이상치가 있으면 영향을 많이 받아 이상치를 찾아내기 쉬움 제곱 대신에 절댓값을 사용하는 MAE Function은 이상치에 영향을 덜 받음 Gradient Descent Algorithm Cost Function의 값을 최소로 만드는 θ를 찾아나가는 방법 Cost Function의 Gradient(기울기)에 상수를 곱한 값을 빼서 θ를 조정 어느 방향으로 θ를 움직이면 Cost가 작아지는지 현재 위치에서 함수를 미분하여 판단 변수(θ)를 움직이면서 전체 Cost 값이 변하지 않거나 매우 느리게 변할 때까지 접근 MSE를 미분했을 때 0이 나오는 지점을 찾아도 되지만, 빅데이터에서 x 데이터 역행렬이 오래걸림 그래프 중간에 함정처럼 페인 부분을 Local Minima라 부름 (목표점은 Global Minima) 가던 방향에서 조금 더 가는 발전된 Gradient Descent 기법을 통해 함정을 빠져나감 Local Minima도 Global Minima와 비슷하게 떨어지기 때문에 에러가 적음 $$\\text{repeat until convergence}\\ { \\theta_j:=\\theta_j-{\\alpha}\\frac{\\delta}{\\delta\\theta_j}J(\\theta_0,\\theta_1)\\quad(\\text{for}j=0\\text{and}j=1) }$$\n계산식에서 J(θ0, θ1)는 MSE를 의미하며 이를 미분한 것에 ⍺를 곱함 ⍺는 한 번 이동하는 길이를 결정하는 상수 (Step Size, 보폭, Learning Rate) ⍺와 같이 사람이 결정해야 하는 값을 Hyper-Parameter라 부름 Hyper-Parameter 사람이 결정하는 파라미터, 모델 클래스 생성 시 집어 넣는 파라미터 모델을 선택하는 것, 인공신경망의 층을 몇개로 구성할 것인지 등 Hyper-Parameter를 설정하는 것을 Model Tuning, Hyper-Parmas Tuning,\n또는 Hyper-Parameter Optimizator(HPO)라 부름 AutoML Hyper-Parameter Tuning을 컴퓨터에게 맡김 Automated FE (Feature Engineering): 결측치 채움, x열(feature) 생성 Automated MS (Model Selection) Automated HPO (Hyper Parameter Optimization) Learning Process Load Data pd.read_excel()로 엑셀 데이터 불러오기 엑셀 데이터를 np.array() 안에 넣어 Numpy Array 형태로 변경 Select Feature Numpy Array는 2차원 행렬이어야 하기 때문에 data[:, 1:2] 형식으로 열을 꺼냄 (data[:, 1]는 1차원 행렬을 반환) Training \u0026amp; Test Set 1 2 3 4 from sklearn import model_selection x_train, x_test, y_train, y_test = \\ model_selection.train_test_split(boston_X, boston_Y, test_size=0.3, random_state=0) Create Model 1 2 3 from sklearn import linear_model model = linear_model.LinearRegression() Model Fitting 1 model.fit(x_train, y_train) model.coef_: a에 해당하는 θ 값 model.intercept_: b에 해당하는 θ 값 (y 절편) Model Predict 1 model.predict(x_train) MSE 1 print(np.mean((model.predict(x_train) - y_train) ** 2)) 1 2 3 from sklearn.metrics import mean_squared_error print(mean_squared_error(model.predict(x_train), y_train)) Training Data와 Test Data의 MSE 차이를 원본 데이터와 함께 비교하여 Overfitting 판단 선형회귀는 성능을 기대하기 어려움 Plot Linear Model 1 2 3 4 5 6 7 8 plt.figure(figsize=(10, 10)) plt.scatter(x_test, y_test, color=\u0026#34;black\u0026#34;) # Test data plt.scatter(x_train, y_train, color=\u0026#34;red\u0026#34;, s=1) # Train data plt.plot(x_test, model.predict(x_test), color=\u0026#34;blue\u0026#34;, linewidth=3) plt.show() ","permalink":"https://minyeamer.github.io/blog/aischool-06-01-linear-regression/","summary":"Linear Regression 종속 변수 y와 독립 변수 x 사이의 선형 상관 관계를 모델링하는 회귀분석 기법 정답이 있는 데이터의 추세를 잘 설명하는 선형 함수를 찾아 x에 대한 y를 예측 Linear Combination (선형 결합): 더하기와 곱하기로만 이루어진 식 단순 회귀분석: 1개의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때 다중 회귀분석: 2개 이상의 독립변수(x)가 1개의 종속변수(y)에 영향을 미칠 때 선형 회귀는 가장 적합한 θ들의 집합을 찾는 것이 목표 Cost Function 예측 값과 실제 값의 차이를 기반으로 모델의 성능(정확도)을 판단하기 위한 함수 Objective (MIN or MAX) 함수 안에 Cost Function이 존재 선형 회귀에서는 Mean Squre(d) Error Function (평균 제곱 오차 함수) 활용 MSE(Cost)가 최소가 되는 θ(a \u0026amp; b)를 찾아야하며,","title":"[AI SCHOOL 5기] 머신 러닝 실습 - 선형 회귀"},{"content":"INNER JOIN 1 2 3 4 5 6 7 8 9 SELECT l.Title, r.Name FROM albums AS l INNER JOIN artists AS r ON r.ArtistId = l.ArtistId; 1 2 3 4 5 6 SELECT Title, Name FROM albums INNER JOIN artists USING(ArtistId); LEFT JOIN 1 2 3 4 5 6 7 8 SELECT Name, Title FROM artists LEFT JOIN albums ON artists.ArtistId = albums.ArtistId ORDER BY Name; SELF JOIN 1 2 3 4 5 6 7 8 9 10 SELECT m.firstname || \u0026#39; \u0026#39; || m.lastname AS \u0026#39;Manager\u0026#39;, e.firstname || \u0026#39; \u0026#39; || e.lastname AS \u0026#39;Receives reports from\u0026#39; FROM employees e INNER JOIN employees m ON m.employeeid = e.reportsto ORDER BY manager; \u0026lsquo;A 테이블\u0026rsquo;과 A 테이블의 복사본인 \u0026lsquo;B 테이블\u0026rsquo;을 합치기 Grouping Data 1 2 3 4 5 6 7 8 9 SELECT albumid, COUNT(trackid) FROM tracks GROUP BY albumid ORDER BY COUNT(trackid) DESC; 1 2 3 4 5 6 7 8 9 SELECT albumid, COUNT(trackid) FROM tracks GROUP BY albumid HAVING albumid = 1; 1 2 3 4 5 6 7 8 9 SELECT albumid, COUNT(trackid) FROM tracks WHERE COUNT(trackid) BETWEEN 18 AND 20 GROUP BY albumid; 에러발생: WHERE문에는 집계함수 사용 불가 WHERE가 집계함수보다 우선적으로 실행되기 때문 1 2 3 4 5 6 7 8 9 10 11 SELECT tracks.albumid, title, MIN(tracks.milliseconds), MAX(tracks.milliseconds), ROUND(AVG(tracks.milliseconds), 2) FROM tracks INNER JOIN albums ON albums.albumid = tracks.albumid GROUP BY tracks.albumid; ROUND는 n번째 자리까지 나타나도록 반올림 Subquery 1 2 3 SELECT AVG(SUM(bytes)) FROM tracks GROUP BY albumid; SELECT 문에서 집계함수의 결과 값에 바로 중첩하여 집계함수 적용 불가 1 2 3 4 5 6 7 8 9 SELECT AVG(SIZE) FROM (SELECT SUM(bytes) AS SIZE FROM tracks GROUP BY albumid); ","permalink":"https://minyeamer.github.io/blog/aischool-05-03-merge/","summary":"INNER JOIN 1 2 3 4 5 6 7 8 9 SELECT l.Title, r.Name FROM albums AS l INNER JOIN artists AS r ON r.ArtistId = l.ArtistId; 1 2 3 4 5 6 SELECT Title, Name FROM albums INNER JOIN artists USING(ArtistId); LEFT JOIN 1 2 3 4 5 6 7 8 SELECT Name, Title FROM artists LEFT JOIN albums ON artists.ArtistId = albums.ArtistId ORDER BY Name; SELF JOIN 1 2 3 4 5 6 7 8 9 10 SELECT m.","title":"[AI SCHOOL 5기] SQL 프로그래밍 실습 - Merge"},{"content":"SELECT 1 SELECT 10 / 5, 2 * 4; 1 SELECT trackid, name FROM tracks; 1 SELECT * FROM tracks; INSERT 1 INSERT INTO artists (name) VALUES(\u0026#39;Bud Powell\u0026#39;); 1 2 3 4 5 6 7 8 9 10 11 script = \u0026#34;\u0026#34;\u0026#34; INSERT INTO artists (name) VALUES (\u0026#34;?\u0026#34;); \u0026#34;\u0026#34;\u0026#34; data = [ (\u0026#34;Buddy Rich\u0026#34;), (\u0026#34;Candido\u0026#34;), (\u0026#34;Charlie Byrd\u0026#34;) ] cur.executemany(script, data) 1 2 3 4 5 6 7 SELECT ArtistId, Name FROM Artists ORDER BY ArtistId DESC; UPDATE 1 UPDATE employees SET lastname = \u0026#39;Smith\u0026#39; WHERE employeeid = 3; 1 2 3 4 5 6 UPDATE employees SET city = \u0026#39;Toronto\u0026#39;, state = \u0026#39;ON\u0026#39;, postalcode = \u0026#39;M5P 2N7\u0026#39; WHERE employeeid = 4; 1 2 UPDATE employees SET email = UPPER(firstname || \u0026#34;.\u0026#34; || lastname || \u0026#34;@corp.co.kr\u0026#34;); Sorting 1 2 3 4 5 6 7 8 SELECT TrackId, Name, Composer FROM tracks ORDER BY Composer; NULL Data인 None은 SQLite3에서 가장 작은 값으로 인식 Filtering DISTINCT 1 SELECT DISTINCT city FROM customers; NULL을 포함한 중복값을 하나만 남기고 제외 1 SELECT DISTINCT city, country FROM customers; 2개 열의 값이 모두 동일한 행들을 제외 WHERE 1 2 3 4 5 6 7 8 9 SELECT name, milliseconds, bytes, albumid FROM tracks WHERE (albumid = 10) AND (milliseconds \u0026gt; 250000); WHERE \u0026amp; LIKE 1 2 3 4 5 6 7 SELECT trackid, name FROM tracks WHERE name LIKE \u0026#39;Wild%\u0026#39;; WHERE \u0026amp; IN 1 2 3 4 5 6 7 8 SELECT TrackId, Name, MediaTypeId FROM Tracks WHERE MediaTypeId IN (1, 2) WHERE \u0026amp; LIMIT/OFFSET 1 2 3 4 5 6 SELECT trackId, name FROM tracks LIMIT 10 OFFSET 7; LIMIT: 불러오는 값의 수 OFFSET: OFFSET에 해당하는 수만큼 떼어내 그 이후의 데이터 불러옴 WHERE \u0026amp; BETWEEN 1 2 3 4 5 6 7 8 9 10 SELECT InvoiceId, BillingAddress, Total FROM invoices WHERE Total BETWEEN 14.91 AND 18.86 ORDER BY Total; WHERE \u0026amp; IS NULL 1 2 3 4 5 6 7 8 9 SELECT Name, Composer FROM tracks WHERE Composer IS NULL ORDER BY Name; ","permalink":"https://minyeamer.github.io/blog/aischool-05-02-sql-crud/","summary":"SELECT 1 SELECT 10 / 5, 2 * 4; 1 SELECT trackid, name FROM tracks; 1 SELECT * FROM tracks; INSERT 1 INSERT INTO artists (name) VALUES(\u0026#39;Bud Powell\u0026#39;); 1 2 3 4 5 6 7 8 9 10 11 script = \u0026#34;\u0026#34;\u0026#34; INSERT INTO artists (name) VALUES (\u0026#34;?\u0026#34;); \u0026#34;\u0026#34;\u0026#34; data = [ (\u0026#34;Buddy Rich\u0026#34;), (\u0026#34;Candido\u0026#34;), (\u0026#34;Charlie Byrd\u0026#34;) ] cur.executemany(script, data) 1 2 3 4 5 6 7 SELECT ArtistId, Name FROM Artists ORDER BY ArtistId DESC; UPDATE 1 UPDATE employees SET lastname = \u0026#39;Smith\u0026#39; WHERE employeeid = 3; 1 2 3 4 5 6 UPDATE employees SET city = \u0026#39;Toronto\u0026#39;, state = \u0026#39;ON\u0026#39;, postalcode = \u0026#39;M5P 2N7\u0026#39; WHERE employeeid = 4; 1 2 UPDATE employees SET email = UPPER(firstname || \u0026#34;.","title":"[AI SCHOOL 5기] SQL 프로그래밍 실습 - SQL CRUD"},{"content":"DBMS DataBase Management System 하드웨어에 저장된 데이터베이스를 관리해주는 소프트웨어 관계형 데이터베이스(RDBMS)가 주로 사용 Oracle, MySQL(MariaDB), SQLite, MS SQL, PstgreSQL 데이터 모델링 현실 세계 E-R 다이어그램 (개념 스키마) Relation 모델 (논리적 스키마) 물리적인 SQL 코드 (데이터베이스 스키마) 개념적 데이터 모델링 현실 세계로부터 개체를 추출, 개체들의 관계를 정의, E-R 다이어그램 생성 개체(Entity): 회원, 제품 등 저장할 가치가 있는 데이터를 포함한 개체 속성(Attribute): 이름, 이메일 등 의미 있는 데이터의 가장 작은 논리적 단위 관계(Relationship): 구매 등 개체와 개체 사이의 연관성 및 개체 집합 간 대응 관계 논리적 데이터 모델링 E-R 다이어그램을 바탕으로 논리적인 구조를 Relation 모델로 표현 릴레이션(Relation): 개체에 대한 데이터를 2차원 테이블 구조로 표현한 것 속성(Attribute): 열, 필드 튜플(Tuble): 행, 레코드, 인스턴스 차수(Degree): 릴레이션 내 속성(Column)의 총 개수 카디널리티(Cardinality): 릴레이션 내 튜플(Row)의 총 개수 물리적 데이터 모델링 Relation 모델을 물리 저장 장치에 저장할 수 있는 물리적 구조로 구현 SQL Structured Query Language RDBMS에서 데이터를 관리 및 처리하기 위해 만들어진 언어 DDL(Data Definition Language): CREATE, ALTER, DROP DML(Data Manipulation Language): SELECT, INSERT, UPDATE, DELETE DCL(Data Control Language): GRANT, REVOKE NoSQL 관계형 모델을 사용하지 않음, 명시적인 스키마가 없음 대용량 데이터 분산 저장에 특화 Kye-Value, Document, Wide Column, Graph 등 ","permalink":"https://minyeamer.github.io/blog/aischool-05-00-sql-programming/","summary":"DBMS DataBase Management System 하드웨어에 저장된 데이터베이스를 관리해주는 소프트웨어 관계형 데이터베이스(RDBMS)가 주로 사용 Oracle, MySQL(MariaDB), SQLite, MS SQL, PstgreSQL 데이터 모델링 현실 세계 E-R 다이어그램 (개념 스키마) Relation 모델 (논리적 스키마) 물리적인 SQL 코드 (데이터베이스 스키마) 개념적 데이터 모델링 현실 세계로부터 개체를 추출, 개체들의 관계를 정의, E-R 다이어그램 생성 개체(Entity): 회원, 제품 등 저장할 가치가 있는 데이터를 포함한 개체 속성(Attribute): 이름, 이메일 등 의미 있는 데이터의 가장 작은 논리적 단위 관계(Relationship): 구매 등 개체와 개체 사이의 연관성 및 개체 집합 간 대응 관계 논리적 데이터 모델링 E-R 다이어그램을 바탕으로 논리적인 구조를 Relation 모델로 표현 릴레이션(Relation): 개체에 대한 데이터를 2차원 테이블 구조로 표현한 것 속성(Attribute): 열, 필드 튜플(Tuble): 행, 레코드, 인스턴스 차수(Degree): 릴레이션 내 속성(Column)의 총 개수 카디널리티(Cardinality): 릴레이션 내 튜플(Row)의 총 개수 물리적 데이터 모델링 Relation 모델을 물리 저장 장치에 저장할 수 있는 물리적 구조로 구현 SQL Structured Query Language RDBMS에서 데이터를 관리 및 처리하기 위해 만들어진 언어 DDL(Data Definition Language): CREATE, ALTER, DROP DML(Data Manipulation Language): SELECT, INSERT, UPDATE, DELETE DCL(Data Control Language): GRANT, REVOKE NoSQL 관계형 모델을 사용하지 않음, 명시적인 스키마가 없음 대용량 데이터 분산 저장에 특화 Kye-Value, Document, Wide Column, Graph 등 ","title":"[AI SCHOOL 5기] SQL 프로그래밍"},{"content":"Connect SQLite3 1 2 3 4 5 import sqlite3 dbpath = \u0026#34;maindb.db\u0026#34; conn = sqlite3.connect(dbpath) cur = conn.cursor() connnect(): DBMS와 연결 conn.commit(): 현재 변경사항 저장 conn.rollback(): 마지막 commit 시점으로 되돌리기 cursor(): DB에서 SQL문을 실행하는 객체 Execute Scripts Datatypes NULL: 결측치 INTEGER (or INT): 정수 (양수 또는 음수), int 값 REAL: 실수, float 값 TEXT (or VARCHAR): 텍스트, string 값 BLOB: 모든 종류의 파일을 저장하는 바이너리 객체 Scripts DROP TABLE IF EXISTS: 테이블이 이미 있으면 제거 CREATE TABLE: 테이블 생성 AUTOINCREMENT: 값을 따로 입력하지 않으면 자동 증가 숫자 부여 NOT NULL: 빈 값이 저장되는 것을 허용하지 않음 INSERT INTO TABLE(FIELD, \u0026hellip;) VALUES(VALUE, \u0026hellip;):\n테이블에 데이터 추가, 전체 필드에 값 추가 시 필드명 생략 가능 --: 한 줄 주석, /* ... */: 여러 줄 주석 Excecute conn.executescript(): 스크립트 구문 실행 cur.executemany(): 많은 데이터를 한번에 INSERT/UPDATE/DELETE\n(\u0026quot;INSERT INTO ... VALUES(?, ?, ?, ?, ?);\u0026quot;, date) cur.execute(): 하나의 SQL문 실행 cur.fetchall(): SQL문 실행 결과를 모두 반환 (튜플 형태) cur.description: 테이블 정보 conn.close(): DB 연결 해제 To Dataframe pd.read_sql_query(query, conn) CREATE Table 1 2 3 4 CREATE TABLE devices ( name TEXT NOT NULL, model TEXT NOT NULL, Serial INTEGER NOT NULL UNIQUE 1 2 3 4 5 6 7 8 9 10 11 CREATE TABLE contact_groups( contact_id INTEGER, group_id INTEGER, PRIMARY KEY (contact_id, group_id), FOREIGN KEY (contact_id) REFERENCES contacts(contact_id) ON DELETE CASCADE, FOREIGN KEY (group_id) REFERENCES groups(group_id) ON DELETE CASCADE ); CASCADE: css cascade와 동일 ALTER Table 1 ALTER TABLE devices RENAME TO equipment; 1 2 ALTER TABLE equipment ADD COLUMN location text; 1 2 ALTER TABLE equipment RENAME COLUMN location TO loc; DROP Table 1 DROP TABLE equipment ; Pandas로 삭제된 테이블 요청 시 no such table 에러 발생 DB 내 테이블 목록/구조 확인 1 2 3 4 5 6 7 SELECT name FROM sqlite_master WHERE type =\u0026#39;table\u0026#39; AND name NOT LIKE \u0026#39;sqlite_%\u0026#39;; sqlite_master는 기본적으로 생성되는 테이블 sqlite_master 테이블에서 생성된 모든 테이블 목록/구조 확인 가능 ","permalink":"https://minyeamer.github.io/blog/aischool-05-01-sqlite3/","summary":"Connect SQLite3 1 2 3 4 5 import sqlite3 dbpath = \u0026#34;maindb.db\u0026#34; conn = sqlite3.connect(dbpath) cur = conn.cursor() connnect(): DBMS와 연결 conn.commit(): 현재 변경사항 저장 conn.rollback(): 마지막 commit 시점으로 되돌리기 cursor(): DB에서 SQL문을 실행하는 객체 Execute Scripts Datatypes NULL: 결측치 INTEGER (or INT): 정수 (양수 또는 음수), int 값 REAL: 실수, float 값 TEXT (or VARCHAR): 텍스트, string 값 BLOB: 모든 종류의 파일을 저장하는 바이너리 객체 Scripts DROP TABLE IF EXISTS: 테이블이 이미 있으면 제거 CREATE TABLE: 테이블 생성 AUTOINCREMENT: 값을 따로 입력하지 않으면 자동 증가 숫자 부여 NOT NULL: 빈 값이 저장되는 것을 허용하지 않음 INSERT INTO TABLE(FIELD, \u0026hellip;) VALUES(VALUE, \u0026hellip;):","title":"[AI SCHOOL 5기] SQL 프로그래밍 실습 - SQLite3"},{"content":"마케팅 비용 분석 매월 유튜브에 광고 비용을 지출하여 신규 유저(구매 고객 or 회원가입 고객)를 획득 월별로 10,000원 단위의 유튜브 광고 비용과 해당 월에 신규로 획득된 유저 수가 측정되었다고 가정 비교 데이터 단순 CAC 계산 CAC(Customer Acquisition Cost, 신규고객 유치 비용) @ https://j.mp/35O5NRe 1 2 3 4 5 cac = ad_df[\u0026#39;Marketing_Costs\u0026#39;].sum() / ad_df[\u0026#39;User_Acquired\u0026#39;].sum() print(cac * 10000) # Output 446원 위의 금액에 추가로 획득하기를 원하는 유저 수를 곱한 금액을\n유튜브 광고 비용으로 쓰면 그만큼 유저가 늘어날까?\n== 위의 금액 만큼 유튜브 광고에 쓰면 정말로 유저가 1명 늘어날까?\n피어슨 상관계수 1 2 3 4 5 stats.pearsonr(ad_df[\u0026#39;Marketing_Costs\u0026#39;], ad_df[\u0026#39;User_Acquired\u0026#39;]) # Output 피어슨 상관계수 : 0.8035775069546849 p-value : 0.0016386012345537505 p-value가 0.0016(\u0026lt;0.05)이므로,\n월별 유튜브 광고 비용과 신규 유저 수가 통계적으로 유의미한 상관관계가 없다 월별 유튜브 광고 비용과 신규 유저 수 사이에는\n통계적으로 유의미한 강한 상관관계(+0.8)가 있다 피어슨 상관계수 값에 대한 해석 기준 (Strong/Moderate/Weak) @ https://j.mp/3mH8FWN 파이썬 프로그래밍 없이 상관관계 분석을 진행할 수 있는 도구 @ https://j.mp/324551c A/B Test (독립표본) 페이지 구성과 세부 디자인을 다르게 만든 2개의 웹사이트 시안을 기반으로 A/B Test 진행 웹사이트 시안 A와 B 각각에 유입된 유저들이\n실제로 각 웹사이트 내에서 이탈하기까지의 시간(체류시간, Duration time)을 측정 T-Test를 진행하기 이전에 Null인 데이터들은 제외 비교 데이터 독립표본 T-Test 1 2 3 4 5 6 stats.ttest_ind(web_a.Duration_A.values, web_b.Duration_B.values, equal_var=False) # Output Ttest_indResult(statistic=3.0165632092150694, pvalue=0.008734970056646718) equal_var=True : 두 개의 열의 분산값이 동일할거라 가정 equal_var=False: 분산값이 동일하지 않기 때문에 False로 설정 2개의 측정 그룹이 동일한 수가 아니거나 유사한 분산값을 갖지 않을 경우\nWelch\u0026rsquo;s t-test를 사용 @ https://j.mp/3kLFwcE p-value가 0.0087(\u0026lt;0.05)이므로,\n웹 시안 A와 B에 대한 체류시간의 평균값이 통계적으로 유의미한 차이가 없다 이러한 전제 하에 위와 같은 체류시간 측정 결과가 나올 확률이 0.87%라고 이해할 수 있음 웹 시안 A와 B에 대한 유저들의 체류 시간 사이에는 통계적으로 유의미한 차이가 있다 A/B Test (카이제곱 검정) 최종 구매를 위한 버튼을 2개의 서로 다른 시안으로 제작해 각기 다른 유저들에게 노출 해당 버튼을 누르면 구매가 확정된다고 가정 비교 데이터 Conversion Rate (전환율) 전체 웹사이트 사용자 중에서 얼마나 많은 사용자들이 동작을 마치는지에 대한 지표 1 2 3 4 5 6 conversion_rate = click_df[\u0026#39;Clicked\u0026#39;] / (click_df[\u0026#39;Clicked\u0026#39;] + click_df[\u0026#39;Unclicked\u0026#39;]) * 100 # Output Button_A 5.746209 Button_B 7.737226 dtype: float64 Click-Through Rate (CTR, 클릭율) 얼마나 많은 사용자들이 웹사이트에 접근하기 위해 광고를 클릭하는지에 대한 지표 Button_A는 5.75% Button_B는 7.73% 카이제곱 검정 버튼 A/B에 대한 클릭 여부가 정리된 위 테이블 == Contingency Table(분할표)\n@ https://j.mp/384CcFR chi2_contingency 함수 활용 시 Contingency Table을 기반으로 카이제곱 검정\n@ https://j.mp/3mH1Nsr 1 2 3 4 stats.chi2_contingency([click_df[\u0026#39;Clicked\u0026#39;], click_df[\u0026#39;Unclicked\u0026#39;]])[1] # Output 0.004968535119697213 p-value가 0.0049(\u0026lt;0.05)이므로,\n버튼 A/B에 대한 클릭 여부가 통계적으로 유의미한 연관성이 없다는 전제 하에,\n이러한 클릭 수 측정 결과가 나올 확률이 0.49%라고 이해할 수 있음 배너 버튼 시안 A와 B에 대한 유저들의 클릭 수 사이에는 통계적으로 유의미한 차이가 있다 A/B Test References A/B Testing에 대한 기초적인 정보들 @ https://j.mp/3eeo2TA 데이터를 활용한 디지털 마케팅 효과분석 @ https://j.mp/2P7YHCO P-hacking에 대하여 @ https://j.mp/3mLMOgP / https://j.mp/31XJBTF p-value p-value의 높고 낮음과 별개로 실제 실험의 효과 크기 역시도 중요하게 고려해야한다.\n예를 들어 어떤 웹사이트의 구매 버튼의 디자인을 변경하여 구매 수가 n 만큼 증가되었고,\n디자인 변경 전/후에 대한 구매 버튼 클릭 수 사이의 관계를 대상으로 통계 검정 후 p-value가 0.05보다 낮게 나왔더라도,\n정작 증가된 구매 수에 해당하는 n이 미미하다면 낮은 p-value에도 불구하고 디자인 변경의 실질적인 효용이 적기 때문이다.\n(통계적으로만 유의미할 뿐 독립변수의 변화에 따른 종속변수의 변화값이 실질적/실용적인 의미를 갖지 않음)\n","permalink":"https://minyeamer.github.io/blog/aischool-04-04-ab-test/","summary":"마케팅 비용 분석 매월 유튜브에 광고 비용을 지출하여 신규 유저(구매 고객 or 회원가입 고객)를 획득 월별로 10,000원 단위의 유튜브 광고 비용과 해당 월에 신규로 획득된 유저 수가 측정되었다고 가정 비교 데이터 단순 CAC 계산 CAC(Customer Acquisition Cost, 신규고객 유치 비용) @ https://j.mp/35O5NRe 1 2 3 4 5 cac = ad_df[\u0026#39;Marketing_Costs\u0026#39;].sum() / ad_df[\u0026#39;User_Acquired\u0026#39;].sum() print(cac * 10000) # Output 446원 위의 금액에 추가로 획득하기를 원하는 유저 수를 곱한 금액을\n유튜브 광고 비용으로 쓰면 그만큼 유저가 늘어날까?","title":"[AI SCHOOL 5기] 통계분석 실습 - A/B Test"},{"content":"Import Libraries 1 2 3 4 5 6 7 8 import pandas as pd import seaborn as sns import scipy as sp from scipy import stats import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) 교차분석 교차표 (Cross-Table) 1 2 3 4 crosstab = pd.crosstab(df.propensity, df.skin, margins=True) crosstab.columns=[] crosstab.index=[] margins: 합계(All) 추가 여부 normalize: Normalization 여부 Chi-square 검정 두 범주형 변수 사이의 관계가 있는지 없는지를 검정 (독립성 검정) 귀무가설: Indepedent (vice versa) 대립가설: Not Independent 1 2 3 4 stats.chisquare(df.column1, df.column2) # Output Power_divergenceResult(statistic=291.8166666666667, pvalue=0.023890557260065975) p-value 관찰 데이터의 검정 통계량이 귀무가설을 지지하는 정도 귀무가설이 참이라는 전제 하에, 관찰이 완료된 값이 표본을 통해 나타날 확률 p-value가 0.05(5%) 미만일 경우, 관측치가 나타날 확률이 매우 낮다고 판단하여 귀무가설 기각 p-value가 0.05(5%) 이상일 경우, 관측치가 나타날 확률이 충분하다고 판단하여 귀무가설 지지 p-value가 0.05 이하라는 것이 항상 대립가설을 의미하는 것은 아님 (5%만큼 귀무가설이 참일 가능성) 1 crosstab.plot.bar(stacked=True) 독립표본 T-test 분석 시각화 서로 다른 집단에서 같은 열 비교 두 집단 간의 평균 차이를 검정 ex) \u0026ldquo;서로 다른\u0026rdquo; 성별 간에 전반적인 만족도의 평균값 사이에 유의미한 차이가 \u0026ldquo;없다\u0026rdquo; 1 2 3 4 stats.ttest_ind(column1.values, column2.values) # Output Ttest_indResult(statistic=-0.494589803056421, pvalue=0.6213329051985961) Box-Plot Histogram 1 2 3 4 5 sns.distplot(male, kde=False, fit=stats.norm, hist_kws={\u0026#39;color\u0026#39;: \u0026#39;r\u0026#39;, \u0026#39;alpha\u0026#39;: 0.2}, fit_kws={\u0026#39;color\u0026#39;: \u0026#39;r\u0026#39;}) sns.distplot(female, kde=False, fit=stats.norm, hist_kws={\u0026#39;color\u0026#39;: \u0026#39;g\u0026#39;, \u0026#39;alpha\u0026#39;: 0.2}, fit_kws={\u0026#39;color\u0026#39;: \u0026#39;g\u0026#39;}) 대응표본 T-test 분석 시각화 동일한 집단에서 서로 다른 열 비교 동일한 모집단으로부터 추출된 두 변수의 평균값을 비교 분석 \u0026ldquo;동일한\u0026rdquo; 고객 집단이 평가한 구매 가격에 대한\n만족도와 구매 문의에 대한 만족도의 평균값 사이에 유의미한 차이가 있다. 1 2 3 4 stats.ttest_rel(df[\u0026#34;satisf_b\u0026#34;], df[\u0026#34;satisf_i\u0026#34;]) # Output Ttest_relResult(statistic=-7.155916401026872, pvalue=9.518854506666398e-12) Histogram 1 2 3 4 5 sns.distplot(df[\u0026#34;satisf_b\u0026#34;], kde=False, fit=stats.norm, hist_kws={\u0026#39;color\u0026#39;: \u0026#39;r\u0026#39;, \u0026#39;alpha\u0026#39;: 0.2}, fit_kws={\u0026#39;color\u0026#39;: \u0026#39;r\u0026#39;}) sns.distplot(df[\u0026#34;satisf_i\u0026#34;], kde=False, fit=stats.norm, hist_kws={\u0026#39;color\u0026#39;: \u0026#39;g\u0026#39;, \u0026#39;alpha\u0026#39;: 0.2}, fit_kws={\u0026#39;color\u0026#39;: \u0026#39;g\u0026#39;}) 분산분석 시각화 분산분석(ANalysis Of VAriance, ANOVA) 세 개의 집단에서 적어도 하나의 유의미한 차이가 있는가 ex) 3가지 구매 동기에 따른 전반적인 만족도의 평균값 중 적어도 하나는 유의미한 차이가 있다. 1 2 3 4 stats.f_oneway(anova1, anova2, anova3) # Output F_onewayResult(statistic=4.732129410493065, pvalue=0.009632034309915485) Histogram 1 2 3 sns.distplot(anova1, kde=False, fit=sp.stats.norm, hist_kws={\u0026#39;color\u0026#39;: \u0026#39;r\u0026#39;, \u0026#39;alpha\u0026#39;: 0.2}, fit_kws={\u0026#39;color\u0026#39;: \u0026#39;r\u0026#39;}) sns.distplot(anova2, kde=False, fit=sp.stats.norm, hist_kws={\u0026#39;color\u0026#39;: \u0026#39;g\u0026#39;, \u0026#39;alpha\u0026#39;: 0.2}, fit_kws={\u0026#39;color\u0026#39;: \u0026#39;g\u0026#39;}) sns.distplot(anova3, kde=False, fit=sp.stats.norm, hist_kws={\u0026#39;color\u0026#39;: \u0026#39;b\u0026#39;, \u0026#39;alpha\u0026#39;: 0.2}, fit_kws={\u0026#39;color\u0026#39;: \u0026#39;b\u0026#39;}) 상관관계 분석 시각화 연속형 변수열들끼리 비교 피어슨 상관계수 1 df.corr.corr() -1에 가까울수록 음의 상관관계 1에 가까울수록 양의 상관관계 0에 가까울수록 상관관계가 적음 PairPlot 1 sns.parilpot(df_corr) Iris Data 붓꽃의 품종에 대한 데이터 1 2 iris = sns.load_dataset(\u0026#34;iris\u0026#34;) sns.pairplot(iris, kind=\u0026#34;reg\u0026#34;) reg: 추세선 sepal_width \u0026lt;-\u0026gt; petal_length\n두 그룹으로 나눠져 있어 음의 상관관계라 보기 어려움 그룹을 나눠서 비교 (1. 양의 상관관계, 2. 관계 없음) Simpson\u0026rsquo;s paradox 심슨의 역설 @ https://j.mp/31Kd6v7 \u0026amp; https://j.mp/3IswbTj 사례로 알아보는 심슨의 역설 @ https://j.mp/3ICKS6q 전체를 봤을 때와 일부를 나눠서 봤을 때 다른 결과가 나옴 신장 결석 크기를 치료법 a, b를 사용해서 얼마나 빨리 치료하는지 작은 결석/큰 결석 인원수가 달라서 b 치료법 확률이 낮아짐 \u0026gt; 합계는 높음 ","permalink":"https://minyeamer.github.io/blog/aischool-04-03-test-statistics/","summary":"Import Libraries 1 2 3 4 5 6 7 8 import pandas as pd import seaborn as sns import scipy as sp from scipy import stats import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) 교차분석 교차표 (Cross-Table) 1 2 3 4 crosstab = pd.crosstab(df.propensity, df.skin, margins=True) crosstab.columns=[] crosstab.index=[] margins: 합계(All) 추가 여부 normalize: Normalization 여부 Chi-square 검정 두 범주형 변수 사이의 관계가 있는지 없는지를 검정 (독립성 검정) 귀무가설: Indepedent (vice versa) 대립가설: Not Independent 1 2 3 4 stats.","title":"[AI SCHOOL 5기] 통계분석 실습 - T-Test \u0026 상관관계 분석"},{"content":"Chart Pie Chart 1 df[\u0026#39;column\u0026#39;].value_counts().plot(kind = \u0026#39;pie\u0026#39;) Bar Chart 1 df[\u0026#39;column\u0026#39;].value_counts().plot(kind = \u0026#39;bar\u0026#39;) Descriptive Statistics df['column'].max(): 최댓값 (행방향 기준: axis=1) df['column'].min(): 최솟값 df['column'].sum(): 합계 df['column'].mean(): 평균 df['column'].variance(): 분산 df['column'].std(): 표준편차 df['column'].describe(): 기술통계량 분포의 왜도와 첨도 df['column'].hist(): 히스토그램 df['column'].skew(): 왜도 (분포가 좌우로 치우쳐진 정도) 왜도(Skewness): 0에 가까울수록 정규분포 (절대값 기준 3 미초과)\n우측으로 치우치면 음(negative)의 왜도, 좌측으로 치우치면 양(positive)의 왜도 df['column'].kurtosis(): 첨도 (분포가 뾰족한 정도) 첨도(Kurtosis): 1에 가까울수록 정규분포 (절대값 기준 8 또는 10 미초과) 왜도가 0, 정도가 1일 때 완전한 정규분포로 가정 sns.distplot(df['column'], rug=True): distribution plot\nrug: 막대 그래프를 표시할지 여부 sns.jointplot(x='column1', y='column2', data=df): 산점도와 히스토그램 한번에 표시 sns.jointplot(..., kind=\u0026quot;kde\u0026quot;): 밀집된 분포 곡선을 표시 Outlier 탐지 및 제거 df.boxplot(column='column'): 데이터 전체에 걸쳐서 분포 밀집도를 표시 IQR 활용 IQR(Inter-Quantile Range): 바닥부터 75% 지점의 값 - 바닥부터 25% 지점의 값 상한치: 바닥부터 75% 지점의 값 + IQR의 1.5배 하한치: 바닥부터 25% 지점의 값 - IQR의 1.5배 상한/하한치를 넘으면 Outlier로 판단 1 2 3 4 5 6 7 Q1 = df[\u0026#39;column\u0026#39;].quantile(0.25) Q3 = df[\u0026#39;column\u0026#39;].quantile(0.75) IQR = Q3 - Q1 df_IQR = df[ (df[\u0026#39;column\u0026#39;] \u0026lt; Q3 + IQR * 1.5) \u0026amp; (df[\u0026#39;column\u0026#39;] \u0026gt; Q1 - IQR * 1.5) ] df_IQR.boxplot(column=\u0026#39;column\u0026#39;) Outlier 제거 전후 분포 비교 Histogram Before After Joint-Plot Before After Log 함수를 활용한 데이터 스케일링 왜도 혹은 첨도가 너무 큰 경우, Log 함수를 적용해 왜도/첨도를 낮춰주는 전처리를 적용 processed_df['log_column'] = np.log(processed_df['column']) Before After ","permalink":"https://minyeamer.github.io/blog/aischool-04-02-descriptive-statistics/","summary":"Chart Pie Chart 1 df[\u0026#39;column\u0026#39;].value_counts().plot(kind = \u0026#39;pie\u0026#39;) Bar Chart 1 df[\u0026#39;column\u0026#39;].value_counts().plot(kind = \u0026#39;bar\u0026#39;) Descriptive Statistics df['column'].max(): 최댓값 (행방향 기준: axis=1) df['column'].min(): 최솟값 df['column'].sum(): 합계 df['column'].mean(): 평균 df['column'].variance(): 분산 df['column'].std(): 표준편차 df['column'].describe(): 기술통계량 분포의 왜도와 첨도 df['column'].hist(): 히스토그램 df['column'].skew(): 왜도 (분포가 좌우로 치우쳐진 정도) 왜도(Skewness): 0에 가까울수록 정규분포 (절대값 기준 3 미초과)\n우측으로 치우치면 음(negative)의 왜도, 좌측으로 치우치면 양(positive)의 왜도 df['column'].kurtosis(): 첨도 (분포가 뾰족한 정도) 첨도(Kurtosis): 1에 가까울수록 정규분포 (절대값 기준 8 또는 10 미초과) 왜도가 0, 정도가 1일 때 완전한 정규분포로 가정 sns.","title":"[AI SCHOOL 5기] 통계분석 실습 - 빈도 분석 \u0026 기술통계량 분석"},{"content":"Numpy Numpy Array 내부의 데이터는 하나의 자료형으로 통일 Numpy Array에 값을 곱하면 전체 데이터 그대로 복사되는 리스트와 달리 데이터에 각각 곱해짐 np.array([]): Numpy Array 생성 np.dtype: Numpy Array의 Data Type np.shape: Numpy Array 모양(차원) np.arange(): range를 바탕으로 Numpy Array 생성 np.reshape(): Numpy Array 모양을 변경, 열에 -1을 입력하면 자동 계산 np.dot(): 행렬곱 Pandas pd.Series([], index=[]): Key가 있는 리스트(Series) 생성 Series.values: Series의 값 Series.index: Series의 키 값 df.ammount: 띄어쓰기 없이 영단어로 구성된 열은 변수처럼 꺼내 쓸 수 있음 df.insert(column, 'key', 'value'): index 기준으로 특정 위치에 새로운 열 삽입 df[(con1) \u0026amp; (con2)]: 여러 개의 조건을 사용할 땐 각각의 조건을 괄호 안에 묶어야 함 df['key'].value_counts(): 값의 출현 빈도 합계 (sort=False로 정렬 해제) df['key'].value_counts().plot(kind='pie'): 빈도수를 기준으로 원형차트 생성 df['key'].apply(): 조건에 따라 변환된 값을 가진 열 반환 df['key'].replace(): 변환값이 1대1 대응 시 apply() 대신 replace() 사용 가능\ndf['gender'].replace([1, 2], ['male', 'female']) ","permalink":"https://minyeamer.github.io/blog/aischool-04-01-numpy-pandas/","summary":"Numpy Numpy Array 내부의 데이터는 하나의 자료형으로 통일 Numpy Array에 값을 곱하면 전체 데이터 그대로 복사되는 리스트와 달리 데이터에 각각 곱해짐 np.array([]): Numpy Array 생성 np.dtype: Numpy Array의 Data Type np.shape: Numpy Array 모양(차원) np.arange(): range를 바탕으로 Numpy Array 생성 np.reshape(): Numpy Array 모양을 변경, 열에 -1을 입력하면 자동 계산 np.dot(): 행렬곱 Pandas pd.Series([], index=[]): Key가 있는 리스트(Series) 생성 Series.values: Series의 값 Series.index: Series의 키 값 df.ammount: 띄어쓰기 없이 영단어로 구성된 열은 변수처럼 꺼내 쓸 수 있음 df.","title":"[AI SCHOOL 5기] 통계분석 실습 - Numpy \u0026 Pandas"},{"content":"Wadis 마감 상품 재고 체크 Google 메일 설정 1 2 3 4 5 6 7 8 9 10 11 12 import smtplib from email.mime.text import MIMEText def sendMail(sender, receiver, msg): smtp = smtplib.SMTP_SSL(\u0026#39;smtp.gmail.com\u0026#39;, 465) smtp.login(sender, \u0026#39;your google app password\u0026#39;) msg = MIMEText(msg) msg[\u0026#39;Subject\u0026#39;] = \u0026#39;Product is available!\u0026#39; smtp.sendmail(sender, receiver, msg.as_string()) smtp.quit() Wadis 상품 재고 체크 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 라이브러리 선언 check_status = 1 url = \u0026#39;https://www.wadiz.kr/web/campaign/detail/{item_number}\u0026#39; # 상품 재고가 확인되어 메일이 발송되면 종료 while check_status: webpage = urlopen(url) source = BeautifulSoup(webpage, \u0026#39;html.parser\u0026#39;) target = source.find_all(\u0026#39;button\u0026#39;, {\u0026#39;class\u0026#39;:\u0026#39;rightinfo-reward-list\u0026#39;}) for item in target: # 가격이 \u0026#39;179,000\u0026#39;원 상품 중 if \u0026#39;179,000\u0026#39; in item.find(\u0026#39;dt\u0026#39;).get_text().strip(): # \u0026#39;블루\u0026#39; 색상인 상품에 대하여 if \u0026#39;블루\u0026#39; in item.find(\u0026#39;p\u0026#39;).get_text().strip(): # 판매 중인 상태가 되면 (마감된 상품엔 \u0026#34;soldout\u0026#34; 클래스가 추가) if len(item.attrs[\u0026#39;class\u0026#39;]) == 2: sendMail(sender, receiver, msg) check_status = 0 서울상권분석서비스 웹 스크래핑 시도 1 2 3 4 url = \u0026#39;https://golmok.seoul.go.kr/regionAreaAnalysis.do\u0026#39; response = requests.get(url).content web_page = BeautifulSoup(response, \u0026#39;html.parser\u0026#39;) 해당 웹 페이지는 POST 요청으로 데이터를 주고 받기 때문에 GET 방식으로는 접근 불가 개발자 도구의 Network 탭을 확인하면 JSON 데이터 확인 가능 POST 요청할 때 Payload를 변경하여 JSON 파일 종류 변경 가능 POST 요청 1 2 3 4 5 6 # Payload 설정 data = {\u0026#39;stdrYyCd\u0026#39;: \u0026#39;2021\u0026#39;, \u0026#39;stdrQuCd\u0026#39;: \u0026#39;4\u0026#39;, \u0026#39;stdrSlctQu\u0026#39;: \u0026#39;sameQu\u0026#39;, \u0026#39;svcIndutyCdL\u0026#39;: \u0026#39;CS000000\u0026#39;, \u0026#39;svcIndutyCdM\u0026#39;: \u0026#39;all\u0026#39;} response = requests.post(\u0026#39;https://golmok.seoul.go.kr/region/selectRentalPrice.json\u0026#39;, data=data).content result = json.loads(response) Output\n1 2 3 4 5 [{\u0026#39;GBN_CD\u0026#39;: \u0026#39;11\u0026#39;, \u0026#39;NM\u0026#39;: \u0026#39;서울시 전체\u0026#39;, \u0026#39;GUBUN\u0026#39;: \u0026#39;si\u0026#39;, \u0026#39;BF1_FST_FLOOR\u0026#39;: \u0026#39;132504\u0026#39;, ... 네이버 금융 Top 종목 TOP 종목 테이블 값 추출 1 2 3 4 5 6 url = \u0026#39;http://finance.naver.com\u0026#39; response = requests.get(url).content web_page = BeautifulSoup(response, \u0026#39;html.parser\u0026#39;) top_items = web_page.find(\u0026#39;tbody\u0026#39;, {\u0026#39;id\u0026#39;:\u0026#39;_topItems1\u0026#39;}) item_rows = top_items.find_all(\u0026#39;tr\u0026#39;) TOP 종목 테이블 값 표시 1 2 3 4 5 6 7 8 9 10 for item in item_rows: item_name = item.find(\u0026#39;th\u0026#39;).get_text() item_price = item.find_all(\u0026#39;td\u0026#39;)[0].get_text() item_delta_price = item.find_all(\u0026#39;td\u0026#39;)[1].get_text() item_delta_percent = item.find_all(\u0026#39;td\u0026#39;)[2].get_text().strip() print(\u0026#39;{} : 현재가 {}, 어제보다 {} {}, 백분율 변환 시 {}\u0026#39;.format( item_name, item_price, item_delta_price[3:], item_delta_price[:2], item_delta_percent)) 부동산 매매 내역 공공데이터포털 API 발급 https://www.data.go.kr/data/15057267/openapi.do 상업업무용 부동산 매매 신고 자료 활용신청 상세 정보는 API 기술문서 참조 Python3 샘플 코드 1 2 3 4 5 6 7 import requests url = \u0026#39;http://openapi.molit.go.kr/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcNrgTrade\u0026#39; params ={\u0026#39;serviceKey\u0026#39; : \u0026#39;서비스키\u0026#39;, \u0026#39;LAWD_CD\u0026#39; : \u0026#39;11110\u0026#39;, \u0026#39;DEAL_YMD\u0026#39; : \u0026#39;201512\u0026#39; } response = requests.get(url, params=params) print(response.content) request.get 요청 시 params를 사용해서 파라미터 한번에 입력 LAWD_CD: 법정동코드 10자리 중 앞 5자리 @ https://www.code.go.kr/index.do 부동산 매매 신고 자료 XML 요청 1 2 response = requests.get(url, params=params).content web_page = BeautifulSoup(response, \u0026#39;lxml-xml\u0026#39;) 매매 내역을 DataFrame에 저장 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 loc_code = [] loc = [] date = [] price = [] building_usage = [] for item in items: try: loc_code.append(item.find(\u0026#39;지역코드\u0026#39;).get_text()) loc.append(item.find(\u0026#39;시군구\u0026#39;).get_text() + item.find(\u0026#39;법정동\u0026#39;).get_text()) date.append(item.find(\u0026#39;년\u0026#39;).get_text() + item.find(\u0026#39;월\u0026#39;).get_text() + item.find(\u0026#39;일\u0026#39;).get_text()) price.append(item.find(\u0026#39;거래금액\u0026#39;).get_text()) building_usage.append(item.find(\u0026#39;건물주용도\u0026#39;).get_text()) except: pass import pandas as pd df = pd.DataFrame({\u0026#39;지역코드\u0026#39;:loc_code, \u0026#39;부동산 위치\u0026#39;:loc, \u0026#39;거래 일자\u0026#39;:date, \u0026#39;거래 금액\u0026#39;:price, \u0026#39;부동산 용도\u0026#39;:building_usage}) ","permalink":"https://minyeamer.github.io/blog/aischool-03-04-web-crawling/","summary":"Wadis 마감 상품 재고 체크 Google 메일 설정 1 2 3 4 5 6 7 8 9 10 11 12 import smtplib from email.mime.text import MIMEText def sendMail(sender, receiver, msg): smtp = smtplib.SMTP_SSL(\u0026#39;smtp.gmail.com\u0026#39;, 465) smtp.login(sender, \u0026#39;your google app password\u0026#39;) msg = MIMEText(msg) msg[\u0026#39;Subject\u0026#39;] = \u0026#39;Product is available!\u0026#39; smtp.sendmail(sender, receiver, msg.as_string()) smtp.quit() Wadis 상품 재고 체크 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 라이브러리 선언 check_status = 1 url = \u0026#39;https://www.","title":"[AI SCHOOL 5기] 웹 크롤링 실습 - 웹 크롤링"},{"content":"Selenium 브라우저의 기능을 체크할 때 사용하는 도구 브라우저를 조종해야할 때도 사용 Import Libraries 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 크롬 드라이버 파일 자동 다운로드 from webdriver_manager.chrome import ChromeDriverManager # 크롬 드라이버를 파일에 연결 from selenium.webdriver.chrome.service import Service from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys from bs4 import BeautifulSoup import time import pandas as pd import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) # 불필요한 Warning 메시지 무시 Virtual Browser 1 2 3 4 5 # 크롬 드라이버 파일을 다운로드 후 세팅 service = Service(executable_path=ChromeDriverManager().install()) # 세팅된 크롬 드라이버를 연결해 가상 브라우저 실행 driver = webdriver.Chrome(service=service) driver.maximize_window(): 가상 브라우저 크기 최대화 올바른 실행을 위해 가상 브라우저의 내부는 건들지 않아야 함 Google Translation Google 번역 페이지 접속 1 2 3 translate_url = \u0026#39;https://translate.google.co.kr/?sl=auto\u0026amp;tl=en\u0026amp;op=translate\u0026amp;hl=ko\u0026#39; driver.get(translate_url) driver.current_url: 가상 브라우저가 접속한 페이지의 URL 주소 반환 driver.page_source: 가상 브라우저가 접속한 페이지의 소스코드 반환 driver.find_element: BeautifulSoup의 find와 같음 driver.find_elements: BeautifulSoup의 find_all과 같음 원본 텍스트 입력 클래스나 ID를 통한 접근이 어려울 경우 XPath를 통해 접근 개발자 도구에서 full XPath를 복사 1 2 3 4 origin_xpath = \u0026#39;원본 텍스트 부분에 해당하는 XPath\u0026#39; driver.find_element_by_xpath(origin_xpath).clear() driver.find_element_by_xpath(origin_xpath).send_keys(\u0026#39;원본 텍스트\u0026#39;) .click(): 특정 부분 클릭 .clear(): 특정 부분에 입력된 값 지우기 .send_keys(): 특정 부분에 값 입력 번역된 텍스트 가져오기 1 2 3 translation_xpath = \u0026#39;번역된 텍스트 부분에 해당하는 XPath\u0026#39; translated_contents = driver.find_element_by_xpath(translation_xpath).text .text: 번역된 텍스트 가상 브라우저 종료 1 2 driver.close() driver.quit() Translated Word Cloud Translated Word Cloud 생성 방법 기사글 전체를 번역하고 단어를 빈도수 순으로 정렬 빈도수를 기반으로 단어를 선정하고 해당 단어들만을 번역 * 선정된 단어들을 번역 1 2 3 4 5 6 7 8 9 10 11 for key in translation_target: # key를 원본 텍스트 부분에 입력 time.sleep(3) # translated_contents 변수에 번역된 텍스트를 가져와서 저장 translation_result[translated_contents] = translation_target[key] driver.close() driver.quit() Translated Word Cloud 파파고 번역 파파고 번역 페이지는 키워드 입력 후 번역된 결과를 보이는 시간 간격이 김 딜레이를 지정하고 반복문을 수행할 경우 번역이 끝나지 않아 잘못된 결과를 가져올 가능성 번역이 완료될 경우 나타나는 태그를 기준으로 대기 시간 설정 파파고에서는 번역된 단어의 발음에 해당하는 \u0026lt;p\u0026gt; 태그가 나타날 때를 번역 완료로 판단 expected_conditions를 활용해 특정한 태그의 로딩이 완료될 때까지 대기 선정된 단어들을 번역 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions from selenium.webdriver.common.by import By # 가상 브라우저 실행 # 파파고 번역 페이지(https://papago.naver.com/?sk=ko\u0026amp;tk=en) 이동 for key in translation_target: driver.find_element_by_id(\u0026#39;txtSource\u0026#39;).clear() driver.find_element_by_id(\u0026#39;txtSource\u0026#39;).send_keys(key) time.sleep(3) wait = WebDriverWait(driver, timeout=10) wait.until(expected_conditions.presence_of_element_located((By.CSS_SELECTOR, \u0026#34;#targetEditArea \u0026gt; p\u0026#34;))) translated_contents = driver.find_element_by_id(\u0026#39;txtTarget\u0026#39;).text translation_result_papago[translated_contents] = translation_target[key] # 가상 브라우저 종료 WebDriverWait(): 가상 브라우저가 timeout을 초과하면 에러 발생 wait.until(expected_conditions): 지정한 Tag가 포착될 때까지 대기 expected_conditions Documentation @ https://j.mp/3mCnc5G 인터파크 투어 여행지 검색 1 2 3 4 5 # 가상 브라우저 실행 # 인터파크 투어 페이지(http://tour.interpark.com/) 이동 driver.find_element_by_id(\u0026#39;SearchGNBText\u0026#39;).send_keys(\u0026#39;보라카이\u0026#39;) driver.find_element_by_class_name(\u0026#39;search-btn\u0026#39;).click() 여행지 검색 결과 크롤링 1 2 3 4 5 # 더보기 버튼 클릭 driver.find_element_by_class_name(\u0026#39;moreBtn\u0026#39;).click() # 2페이지로 변경 driver.find_element_by_xpath(\u0026#39;/html/body/div[3]/div/div[1]/div[2]/div[4]/div[3]/ul/li[2]\u0026#39;).click() ","permalink":"https://minyeamer.github.io/blog/aischool-03-03-selenium/","summary":"Selenium 브라우저의 기능을 체크할 때 사용하는 도구 브라우저를 조종해야할 때도 사용 Import Libraries 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 크롬 드라이버 파일 자동 다운로드 from webdriver_manager.chrome import ChromeDriverManager # 크롬 드라이버를 파일에 연결 from selenium.webdriver.chrome.service import Service from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys from bs4 import BeautifulSoup import time import pandas as pd import warnings warnings.","title":"[AI SCHOOL 5기] 웹 크롤링 실습 - 셀레니움"},{"content":"Okt Library 한국어 형태소 분석기 KoNLPy 패키지에 속한 라이브러리 KoNLPy 테스트 1 2 3 4 5 from konlpy.tag import Okt tokenizer = Okt() tokens = tokenizer.pos(\u0026#34;아버지 가방에 들어가신다.\u0026#34;, norm=True, stem=True) print(tokens) norm: 정규화(Normalization), \u0026lsquo;안녕하세욯\u0026rsquo; -\u0026gt; \u0026lsquo;안녕하세요\u0026rsquo; stem: 어근화(Stemming, Lemmatization), (\u0026lsquo;한국어\u0026rsquo;, \u0026lsquo;Noun\u0026rsquo;) Pickle Library (Extra) 파이썬 변수를 pickle 파일로 저장/불러오기 1 2 3 4 5 with open(\u0026#39;raw_pos_tagged.pkl\u0026#39;, \u0026#39;wb\u0026#39;) as f: pickle.dump(raw_pos_tagged, f) with open(\u0026#39;raw_pos_tagged.pkl\u0026#39;,\u0026#39;rb\u0026#39;) as f: data = pickle.load(f) 크롤링 데이터 전처리 크롤링 데이터 불러오기 1 2 3 df = pd.read_excel(\u0026#39;result_220328_1314.xlsx\u0026#39;) articles = df[\u0026#39;Article\u0026#39;].tolist() articles = \u0026#39;\u0026#39;.join(articles) Article 데이터를 불러와서 리스트화 시키고 다시 하나의 문자열로 변환 형태소 단위 분해 1 2 3 4 from konlpy.tag import Okt tokenizer = Okt() raw_pos_tagged = tokenizer.pos(articles, norm=True, stem=True) 단어 등장 빈도 시각화 1 2 3 4 5 6 word_cleaned = [\u0026#39;불용어가 제거된 단어 목록\u0026#39;] # NLTK의 Text() 클래스에서 matplotlib의 plot 기능 제공 word_counted = nltk.Text(word_cleaned) plt.figure(figsize=(15, 7)) word_counted.plot(50) 단어 등장 빈도 시각화 (막대그래프) 1 2 3 4 5 6 7 # NLTK의 FreqDist() 클래스를 선언하면 인덱스 열이 지정된 객체 생성 word_frequency = nltk.FreqDist(word_cleaned) df = pd.DataFrame(list(word_frequency.values()), word_frequency.keys()) result = df.sort_values([0], ascending=False) result.plot(kind=\u0026#39;bar\u0026#39;, legend=False, figsize=(15,5)) plt.show() Word Cloud Import Libraries 1 2 3 4 5 from wordcloud import WordCloud import matplotlib.pyplot as plt from PIL import Image import numpy as np import matplotlib.pyplot as plt Create WordCloud 1 2 3 word_cloud = WordCloud(font_path=\u0026#34;malgun.ttf\u0026#34;, width=2000, height=1000, background_color=\u0026#39;white\u0026#39;).generate_from_frequencies(word_dic) width, height: 워드클라우드 해상도 background_color: 배경색 max_words: 단어 최대 갯수 (default: 200) max_font_size: 최대 글자 크기 prefer_horizontal: 가로로 보여주는 정도, 가로로만 그리려면 1.0 설정 Show WordCloud 1 2 3 4 5 plt.figure(figsize=(15,15)) # 화면에 보여지는 크기 plt.imshow(word_cloud) plt.axis(\u0026#34;off\u0026#34;) plt.tight_layout(pad=0) plt.show() Masking 1 2 3 4 5 6 python_coloring = np.array(Image.open(\u0026#34;python_mask.jpg\u0026#34;)) word_cloud = WordCloud(font_path=\u0026#34;malgun.ttf\u0026#34;, width=2000, height=1000, mask=python_coloring, background_color=\u0026#39;white\u0026#39;).generate_from_frequencies(word_dic) np.array로 이미지 파일을 열면 픽셀 단위의 행렬 생성 mask 파라미터에 Numpy Array 전달 WordCloud의 해상도는 원본 이미지의 해상도에 영향을 받음 Coloring 1 2 3 4 5 6 7 from wordcloud import ImageColorGenerator image_colors = ImageColorGenerator(python_coloring) ... plt.imshow(word_cloud.recolor(color_func=image_colors), interpolation=\u0026#39;bilinear\u0026#39;) ImageColorGenerator 객체를 통해 이미지로부터 색상을 추출 recolor 함수를 통해 이미지 컬러 다시 칠하기 interpolation: 비어있는 픽셀 값을 칠하는 방법, bilinear(보간법) colormap: 임의로 색상 지정 ('Reds', 'Blues' 등) Save to Image File 1 word_cloud.to_file(\u0026#34;word_cloud_completed.png\u0026#34;) ","permalink":"https://minyeamer.github.io/blog/aischool-02-04-word-cloud/","summary":"Okt Library 한국어 형태소 분석기 KoNLPy 패키지에 속한 라이브러리 KoNLPy 테스트 1 2 3 4 5 from konlpy.tag import Okt tokenizer = Okt() tokens = tokenizer.pos(\u0026#34;아버지 가방에 들어가신다.\u0026#34;, norm=True, stem=True) print(tokens) norm: 정규화(Normalization), \u0026lsquo;안녕하세욯\u0026rsquo; -\u0026gt; \u0026lsquo;안녕하세요\u0026rsquo; stem: 어근화(Stemming, Lemmatization), (\u0026lsquo;한국어\u0026rsquo;, \u0026lsquo;Noun\u0026rsquo;) Pickle Library (Extra) 파이썬 변수를 pickle 파일로 저장/불러오기 1 2 3 4 5 with open(\u0026#39;raw_pos_tagged.pkl\u0026#39;, \u0026#39;wb\u0026#39;) as f: pickle.dump(raw_pos_tagged, f) with open(\u0026#39;raw_pos_tagged.pkl\u0026#39;,\u0026#39;rb\u0026#39;) as f: data = pickle.load(f) 크롤링 데이터 전처리 크롤링 데이터 불러오기 1 2 3 df = pd.","title":"[AI SCHOOL 5기] 텍스트 분석 실습 - 워드클라우드"},{"content":"Import Libraries 1 2 3 4 5 6 7 import requests from bs4 import BeautifulSoup import pandas as pd from datetime import datetime import time # time.sleep() import re 뉴스 검색 결과에서 네이버 뉴스 추출 네이버 뉴스 검색 결과 URL 분석 1 2 3 4 https://search.naver.com/search.naver? where=news\u0026amp; sm=tab_jum\u0026amp; \u0026lt;!-- 불필요 --\u0026gt; query=데이터분석 네이버 뉴스 검색 URL 불러오기 1 2 3 4 5 query = input() # 데이터분석 url = f\u0026#39;https://search.naver.com/search.naver?where=news\u0026amp;query={query}\u0026#39; web = requests.get(url).content source = BeautifulSoup(web, \u0026#39;html.parser\u0026#39;) 네이버 뉴스 기사 주제 가져오기 1 2 3 4 5 6 news_subjects = source.find_all(\u0026#39;a\u0026#39;, {\u0026#39;class\u0026#39; : \u0026#39;news_tit\u0026#39;}) subject_list = [] for subject in news_subjects: subject_list.append(subject.get_text()) 네이버 뉴스 기사 링크 가져오기 1 2 3 4 5 urls_list = [] for urls in source.find_all(\u0026#39;a\u0026#39;, {\u0026#39;class\u0026#39; : \u0026#39;info\u0026#39;}): if urls.attrs[\u0026#39;href\u0026#39;].startswith(\u0026#39;https://news.naver.com\u0026#39;): urls_list.append(urls.attrs[\u0026#39;href\u0026#39;]) 단일 뉴스 페이지 분석 ConnectionError 1 2 web_news = requests.get(urls_list[0]).content source_news = BeautifulSoup(web_news, \u0026#39;html.parser\u0026#39;) 1 ConnectionError: (\u0026#39;Connection aborted.\u0026#39;, RemoteDisconnected(\u0026#39;Remote end closed connection without response\u0026#39;)) 브라우저를 거치지 않고 HTML 코드를 요청하면 ConnectionError 발생 사용자임을 알리는 헤더 추가 1 2 3 4 5 6 7 headers = {\u0026#39;User-Agent\u0026#39;:\u0026#39;Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36\u0026#39;} web_news = requests.get(urls_list[0], headers=headers).content source_news = BeautifulSoup(web_news, \u0026#39;html.parser\u0026#39;) 기사 제목 / 발행 날짜 추출 1 2 title = source_news.find(\u0026#39;h3\u0026#39;, {\u0026#39;id\u0026#39; : \u0026#39;articleTitle\u0026#39;}).get_text() date = source_news.find(\u0026#39;span\u0026#39;, {\u0026#39;class\u0026#39; : \u0026#39;t11\u0026#39;}).get_text() Pandas Timestamp 1 2 3 4 5 # 2022.03.25. 오전 10:18 date = source_news.find(\u0026#39;span\u0026#39;, {\u0026#39;class\u0026#39; : \u0026#39;t11\u0026#39;}).get_text() # 2022.03.25.10:18am pd_date = pd.Timestamp(reformatted_date) 기사 본문 추출 1 2 3 4 5 6 7 article = source_news.find(\u0026#39;div\u0026#39;, {\u0026#39;id\u0026#39; : \u0026#39;articleBodyContents\u0026#39;}).get_text() article = article.replace(\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;) article = article.replace(\u0026#34;// flash 오류를 우회하기 위한 함수 추가function _flash_removeCallback() {}\u0026#34;, \u0026#34;\u0026#34;) article = article.replace(\u0026#34;동영상 뉴스 \u0026#34;, \u0026#34;\u0026#34;) article = article.replace(\u0026#34;동영상 뉴스\u0026#34;, \u0026#34;\u0026#34;) article = article.strip() 기사 발행 언론사 추출 1 2 press_company = source_news.find(\u0026#39;address\u0026#39;, {\u0026#39;class\u0026#39; : \u0026#39;address_cp\u0026#39;}).find(\u0026#39;a\u0026#39;).get_text() print(press_company) 여러 뉴스 데이터 수집 각 기사들의 데이터를 수집해 리스트에 추가 1 2 3 4 5 6 7 8 for url in urls_list: ... titles.append(title) dates.append(date) articles.append(article) article_urls.append(url) press_companies.append(press_company) 데이터에 대한 DataFrame 생성 1 2 3 4 5 article_df = pd.DataFrame({\u0026#39;Title\u0026#39;:titles, \u0026#39;Date\u0026#39;:dates, \u0026#39;Article\u0026#39;:articles, \u0026#39;URL\u0026#39;:article_urls, \u0026#39;PressCompany\u0026#39;:press_companies}) 여러 페이지의 뉴스 데이터 수집 각각의 페이지에 해당하는 쿼리 리스트 생성 1 2 3 4 5 6 max_page = int(input()) # 5 query = input() # 데이터분석 start_points = [] for point in range(1, max_page*10+1, 10): start_points.append(str(point)) 각각의 페이지에 대한 반복문 실행 1 2 3 4 5 6 7 8 9 10 11 12 13 14 current_call = 1 last_call = (max_page - 1) * 10 + 1 while current_call \u0026lt;= last_call: url = \u0026#34;https://search.naver.com/search.naver?where=news\u0026amp;query=\u0026#34; + query + \\ \u0026#34;\u0026amp;start=\u0026#34; + str(current_call) web = requests.get(url).content source = BeautifulSoup(web, \u0026#39;html.parser\u0026#39;) ... # 대량의 데이터를 크롤링할 때는 요청 사이에 딜레이 생성 time.sleep(5) current_call += 10 날짜 지정하여 크롤링 네이버 뉴스 날짜 지정 검색 결과 URL 분석 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 https://search.naver.com/search.naver?where=news \u0026amp;query=데이터분석 \u0026amp;sm=tab_opt \u0026amp;sort=0 \u0026amp;photo=0 \u0026amp;field=0 \u0026amp;pd=4 \u0026amp;ds= \u0026amp;de= \u0026amp;docid= \u0026amp;related=0 \u0026amp;mynews=0 \u0026amp;office_type=0 \u0026amp;office_section_code=0 \u0026amp;news_office_checked= \u0026lt;!-- 날짜 지정 (from{YYYYMMDD}to{YYYYMMDD}) --\u0026gt; \u0026amp;nso=so%3Ar%2Cp%3Afrom20220101to20220301 \u0026amp;is_sug_officeid=0 날짜에 해당하는 쿼리 생성 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 start_date = input() # 2022.01.01 end_date = input() # 2022.03.01 start_date = start_date.replace(\u0026#34;.\u0026#34;, \u0026#34;\u0026#34;) end_date = end_date.replace(\u0026#34;.\u0026#34;, \u0026#34;\u0026#34;) ... while current_call \u0026lt;= last_call: url = \u0026#34;https://search.naver.com/search.naver?where=news\u0026amp;query=\u0026#34; + query \\ + \u0026#34;\u0026amp;nso=so%3Ar%2Cp%3Afrom\u0026#34; + start_date \\ + \u0026#34;to\u0026#34; + end_date \\ + \u0026#34;%2Ca%3A\u0026amp;start=\u0026#34; + str(current_call) web = requests.get(url).content source = BeautifulSoup(web, \u0026#39;html.parser\u0026#39;) ... 기사 정렬 순서 지정하여 크롤링 네이버 뉴스 기사 정렬 순서 검색 결과 URL 분석 1 2 3 4 5 6 https://search.naver.com/search.naver?where=news \u0026amp;query=데이터분석 \u0026amp;sm=tab_opt \u0026lt;!-- 관련도순: 0, 최신순: 1, 오래된순: 2 --\u0026gt; \u0026amp;sort=0 ... 정렬 순서에 해당하는 쿼리 생성 1 2 query = input() # \u0026#34;데이터분석\u0026#34; \u0026lt; 정확한 검색 sort_type = int(input()) # 1 데이터를 엑셀 파일로 저장 1 article_df.to_excel(\u0026#39;result_{}.xlsx\u0026#39;.format(datetime.now().strftime(\u0026#39;%y%m%d_%H%M\u0026#39;)), index=False, encoding=\u0026#39;utf-8\u0026#39;) ","permalink":"https://minyeamer.github.io/blog/aischool-03-02-web-scraping-advanced/","summary":"Import Libraries 1 2 3 4 5 6 7 import requests from bs4 import BeautifulSoup import pandas as pd from datetime import datetime import time # time.sleep() import re 뉴스 검색 결과에서 네이버 뉴스 추출 네이버 뉴스 검색 결과 URL 분석 1 2 3 4 https://search.naver.com/search.naver? where=news\u0026amp; sm=tab_jum\u0026amp; \u0026lt;!-- 불필요 --\u0026gt; query=데이터분석 네이버 뉴스 검색 URL 불러오기 1 2 3 4 5 query = input() # 데이터분석 url = f\u0026#39;https://search.naver.com/search.naver?where=news\u0026amp;query={query}\u0026#39; web = requests.","title":"[AI SCHOOL 5기] 웹 크롤링 실습 - 웹 스크래핑 심화"},{"content":"문제 링크 https://leetcode.com/problems/the-k-weakest-rows-in-a-matrix/ 개요 2차원 배열에 대해 각각의 리스트의 합을 기준으로 정렬을 하고 그 순서를 반환하는 문제이다. 파이썬에서는 내장함수 sort()를 사용하면 쉽게 풀 수 있다. 문제 해설 입력으로 2차원 배열 mat과 출력값의 개수를 의미하는 정수 k가 주어진다. mat에 있는 각각의 리스트는 0과 1의 조합으로 이루어져 있으며 1의 개수가 많은 리스트가 강한 리스트이다. 문제에서 요구하는 것은 1. 리스트를 약한 순으로 정렬하고\n2. 정렬하기 전의 인덱스 번호를 정렬된 순서대로 반환하는 것이다. 이를 위해 리스트의 인덱스 번호와 리스트의 합을 따로 저장할 필요가 있으므로 for문을 통해 mat을 순회한다. 순회하면서 mat의 각 리스트 내용을 [인덱스 번호, 리스트의 합]으로 덮어쓰고\n이후에 1번 원소(리스트의 합)을 기준으로 mat를 정렬한다. 마지막에 정렬된 mat의 0번 원소(인덱스 번호)를 k개 만큼만 추출해서 반환하면 된다. 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Solution(object): def kWeakestRows(self, mat, k): \u0026#34;\u0026#34;\u0026#34; :type mat: List[List[int]] :type k: int :rtype: List[int] \u0026#34;\u0026#34;\u0026#34; for i in range(len(mat)): mat[i] = [i, sum(mat[i])] mat.sort(key=lambda x: x[1]) return [mat[i][0] for i in range(k)] ","permalink":"https://minyeamer.github.io/blog/leetcode-problems-1337/","summary":"문제 링크 https://leetcode.com/problems/the-k-weakest-rows-in-a-matrix/ 개요 2차원 배열에 대해 각각의 리스트의 합을 기준으로 정렬을 하고 그 순서를 반환하는 문제이다. 파이썬에서는 내장함수 sort()를 사용하면 쉽게 풀 수 있다. 문제 해설 입력으로 2차원 배열 mat과 출력값의 개수를 의미하는 정수 k가 주어진다. mat에 있는 각각의 리스트는 0과 1의 조합으로 이루어져 있으며 1의 개수가 많은 리스트가 강한 리스트이다. 문제에서 요구하는 것은 1. 리스트를 약한 순으로 정렬하고\n2. 정렬하기 전의 인덱스 번호를 정렬된 순서대로 반환하는 것이다. 이를 위해 리스트의 인덱스 번호와 리스트의 합을 따로 저장할 필요가 있으므로 for문을 통해 mat을 순회한다.","title":"[LeetCode 1337] The K Weakest Rows in a Matrix (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/2805 개요 이분 탐색으로 해결할 수 있는 문제이다. Python3을 사용하면 시간초과가 발생하므로 PyPy3를 사용한다. 문제 조건 일정 높이에 대해 모든 나무를 잘랐을 때, 조건을 만족하는 절단기의 최대 높이(H)를 구하는 문제이다. 잘린 나무의 길이의 합은 상근이가 필요로 하는 나무의 길이(M)보다 크거나 같아야 한다. 문제 해설 나무의 수(N)의 최댓값이 1,000,000이므로 모든 범위에 대해 반복하는 순차 탐색을 이용할 경우 시간초과가 발생한다. 시간 복잡도가 O(log n)인 이분 탐색을 이용하면 시간 복잡도가 O(n)인 순차 탐색을 쓰는 것보다 훨씬 빠르다. 이분 탐색은 중간값(md)을 기준으로 시작하여 조건에 따라\n최대/최솟값의 포인터(mx/mn)를 조정하는 탐색 알고리즘이다. 잘린 나무의 길이의 합(total)을 구할 때 잘리지 않은 나무에 대한 음수값을 포함하지 않도록 주의한다. 조건을 만족할 경우 최솟값(mn)을 중간값(md)보다 크게 맞추며,\n반대의 경우 최댓값(mx)을 중간값(md)보다 작게 조정한다. 이분 탐색을 마치면 최댓값(mx)에 조건을 만족하는 최대 높이(H)의 값이 남게 된다. 시간 복잡도 시간 복잡도가 O(log N)인 이분 탐색의 매 반복마다 시간 복잡도가 O(n)인 for문을 실행하므로\n시간 복잡도는 O(N log N) 이상이 된다. N의 최댓값 1,000,000에 대해 20,000,000번이 넘는 연산이 실행되므로 Python3으로는 시간 제한 1초를 초과한다. PyPy3에 대한 이해가 깊은 편이 아니라 자세한 설명은 어렵지만,\n메모리를 더 사용하는 대신 코드를 캐싱하는 PyPy3를 사용하면 시간 제한 안에 해결할 수 있다. 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 N, M = map(int, input().split()) trees = list(map(int, input().split())) mn, md, mx = 0, 0, max(trees) while mn \u0026lt;= mx: md = (mx + mn) // 2 total = 0 for tree in trees: total += tree - md if tree \u0026gt; md else 0 if total \u0026gt;= M: mn = md + 1 else: mx = md - 1 print(mx) ","permalink":"https://minyeamer.github.io/blog/boj-problems-2805/","summary":"문제 링크 https://www.acmicpc.net/problem/2805 개요 이분 탐색으로 해결할 수 있는 문제이다. Python3을 사용하면 시간초과가 발생하므로 PyPy3를 사용한다. 문제 조건 일정 높이에 대해 모든 나무를 잘랐을 때, 조건을 만족하는 절단기의 최대 높이(H)를 구하는 문제이다. 잘린 나무의 길이의 합은 상근이가 필요로 하는 나무의 길이(M)보다 크거나 같아야 한다. 문제 해설 나무의 수(N)의 최댓값이 1,000,000이므로 모든 범위에 대해 반복하는 순차 탐색을 이용할 경우 시간초과가 발생한다. 시간 복잡도가 O(log n)인 이분 탐색을 이용하면 시간 복잡도가 O(n)인 순차 탐색을 쓰는 것보다 훨씬 빠르다.","title":"[백준 2805] 나무 자르기 (PyPy3)"},{"content":"Scikit-learn Library Traditional Machine Learning (vs DL, 인공신경을 썼는지의 여부) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from sklearn import datasets, linear_model, model_selection, metrics data_total = datasets.load_boston() x = data_total.data y = data_total.target train_x, test_x, train_y, test_y = model_selection.train_test_split(x, y, test_size=0.3) # 학습 전의 모델 생성 model = linear_model.LinearRegression() # 모델에 학습 데이터를 넣으면서 학습 진행 model.fit(train_x, train_y) # 모델에게 새로운 데이터를 주면서 예측 요구 predictions = model.predict(test_x) # 예측 결과를 바탕으로 성능 점수 확인 metrics.mean_squared_error(predictions, test_y) 실습에서 사용할 패키지 1 2 from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics.pairwise import cosine_similarity Vectorizer 1 2 3 4 5 6 7 corpus = [doc1, doc2, doc3] vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(corpus).todense() # vertorizer.fit(corpus) # X = vectorizer.trasnform(corpus) Vectorizer는 리스트로 묶인 다수의 데이터에 대한 벡터 생성 fit_transform()은 기본적으로 메모리를 차지하는 \u0026lsquo;0\u0026rsquo;이라는 값들을 제외하고\n좌표에 대한 값의 형태로 표시 .todense(): \u0026lsquo;0\u0026rsquo;이라는 값들을 포함하여 행렬의 형태로 표시 X.shape: Vectorizer의 모양 확인 (row, column) CountVectorizer(): 단어의 출연 횟수만으로 벡터 생성 Print Output 1 2 [[0.0071001 0.00332632 0. ... 0. 0.00166316 0. ] [0.00889703 0. 0.00138938 ... 0.00138938 0. 0.00138938]] Pandas Output 3. Cosine Similarity 1 cosine_similarity(X[0], X[1]) [하나의 행 vs 전체 행] 구도로 표시 1 2 3 4 similarity = cosine_similarity(X[0], X) # 위에서부터 순서대로 보기 위해 전치 행렬(Transpose)로 표시 pd.DataFrame(similarity.T) [각 행 vs 전체 행] 구도로 표시 1 2 3 4 5 similarity = cosine_similarity(X, X) result = pd.DataFrame(similarity) result.columns = [\u0026#39;Shawshank\u0026#39;, \u0026#39;Godfather\u0026#39;, \u0026#39;Inception\u0026#39;] result.index = [\u0026#39;Shawshank\u0026#39;, \u0026#39;Godfather\u0026#39;, \u0026#39;Inception\u0026#39;] 유사도 행렬 시각화 1 2 3 4 5 6 7 8 9 10 import seaborn as sns import matplotlib.pyplot as plt plt.figure(figsize=(10, 10)) sns.heatmap(result, annot=True, fmt=\u0026#39;f\u0026#39;, linewidths=5, cmap=\u0026#39;RdYlBu\u0026#39;) sns.set(font_scale=1.5) plt.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False) plt.show() ","permalink":"https://minyeamer.github.io/blog/aischool-02-03-text-analysis/","summary":"Scikit-learn Library Traditional Machine Learning (vs DL, 인공신경을 썼는지의 여부) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from sklearn import datasets, linear_model, model_selection, metrics data_total = datasets.load_boston() x = data_total.data y = data_total.target train_x, test_x, train_y, test_y = model_selection.train_test_split(x, y, test_size=0.3) # 학습 전의 모델 생성 model = linear_model.LinearRegression() # 모델에 학습 데이터를 넣으면서 학습 진행 model.fit(train_x, train_y) # 모델에게 새로운 데이터를 주면서 예측 요구 predictions = model.","title":"[AI SCHOOL 5기] 텍스트 분석 실습 - 텍스트 분석"},{"content":"Tokenizing Text Data Import Libraries 1 2 3 import nltk from nltk.corpus import stopwords from collections import Counter Set Stopwords 1 2 3 4 5 6 stop_words = stopwords.words(\u0026#34;english\u0026#34;) stop_words.append(\u0026#39;,\u0026#39;) stop_words.append(\u0026#39;.\u0026#39;) stop_words.append(\u0026#39;’\u0026#39;) stop_words.append(\u0026#39;”\u0026#39;) stop_words.append(\u0026#39;—\u0026#39;) Open Text Data 1 2 file = open(\u0026#39;movie_review.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#34;utf-8\u0026#34;) lines = file.readlines() Tokenize 1 2 3 4 5 6 tokens = [] for line in lines: tokenized = nltk.word_tokenize(line) for token in tokenized: if token.lower() not in stop_words: tokens.append(token) Counting Nouns POS Tagging 1 2 3 4 5 6 tags = nltk.pos_tag(tokens) word_list = [] for word, tag in tags: if tag.startswith(\u0026#39;N\u0026#39;): word_list.append(word.lower()) Counting Nouns 1 2 counts = Counter(word_list) print(counts.most_common(10)) Output\n1 [(\u0026#39;movie\u0026#39;, 406), (\u0026#39;batman\u0026#39;, 303), (\u0026#39;film\u0026#39;, 284), (\u0026#39;joker\u0026#39;, 219), (\u0026#39;dark\u0026#39;, 136), (\u0026#39;ledger\u0026#39;, 131), (\u0026#39;knight\u0026#39;, 124), (\u0026#39;time\u0026#39;, 112), (\u0026#39;heath\u0026#39;, 110), (\u0026#39;performance\u0026#39;, 87)] Counting Adjectives POS Tagging 1 2 3 4 5 6 tags = nltk.pos_tag(tokens) word_list = [] for word, tag in tags: if tag.startswith(\u0026#39;J\u0026#39;): word_list.append(word.lower()) Counting Adjectives 1 2 counts = Counter(word_list) print(counts.most_common(10)) Output\n1 [(\u0026#39;good\u0026#39;, 141), (\u0026#39;best\u0026#39;, 102), (\u0026#39;great\u0026#39;, 78), (\u0026#39;many\u0026#39;, 54), (\u0026#39;much\u0026#39;, 52), (\u0026#39;comic\u0026#39;, 43), (\u0026#39;real\u0026#39;, 29), (\u0026#39;bad\u0026#39;, 28), (\u0026#39;little\u0026#39;, 26), (\u0026#39;new\u0026#39;, 25)] Counting Verbs POS Tagging 1 2 3 4 5 6 tags = nltk.pos_tag(tokens) word_list = [] for word, tag in tags: if tag.startswith(\u0026#39;V\u0026#39;): word_list.append(word.lower()) Counting Verbs 1 2 counts = Counter(word_list) print(counts.most_common(10)) Output\n1 [(\u0026#39;see\u0026#39;, 59), (\u0026#39;get\u0026#39;, 54), (\u0026#39;made\u0026#39;, 49), (\u0026#39;think\u0026#39;, 46), (\u0026#39;seen\u0026#39;, 45), (\u0026#39;make\u0026#39;, 45), (\u0026#39;say\u0026#39;, 41), (\u0026#34;\u0026#39;ve\u0026#34;, 37), (\u0026#34;\u0026#39;m\u0026#34;, 32), (\u0026#39;going\u0026#39;, 31)] Visualizing Tokens Import Libraries 1 2 import matplotlib.pyplot as plt import re 정규표현식으로 토큰 분류 1 2 3 4 5 6 7 8 tokens = [] for line in lines: tokenized = nltk.word_tokenize(line) for token in tokenized: if token.lower() not in stop_words: if re.match(\u0026#39;^[a-zA-Z]+\u0026#39;, token): tokens.append(token) 정규표현식 개념 소개 @ https://j.mp/3bJQJHg 정규표현식 기본 문법 정리 @ https://j.mp/3bLXSqB 상세한 정규표현식 설명 @ http://j.mp/2PzgFO8 상세한 정규표현식 예제 @ https://hamait.tistory.com/342 점프 투 파이썬 정규표현식 @ https://wikidocs.net/4308 Visualizing Tokens 1 2 3 4 plt.figure(figsize=(10, 3)) plt.title(\u0026#39;Top 25 Words\u0026#39;,fontsize=30) corpus.plot(25) Top 25 Words Chart Similar Words 1 2 print(\u0026#39;Similar words : \u0026#39;) corpus.similar(\u0026#39;batman\u0026#39;) Output\n1 2 Similar words : superhero film action movie character better iconic seen acting actor heath performance modern difficult villain second end good come best Collocation 1 2 print(\u0026#39;Collocation\u0026#39;) corpus.collocation() Output\n1 2 Collocation Dark Knight; Heath Ledger; Christian Bale; comic book; Harvey Dent; Christopher Nolan; Bruce Wayne; Aaron Eckhart; Morgan Freeman; Gary Oldman; Batman Begins; Two Face; Gotham City; Maggie Gyllenhaal; Rachel Dawes; Michael Caine; special effect; Tim Burton; Jack Nicholson; dark knight ","permalink":"https://minyeamer.github.io/blog/aischool-02-02-text-data-exploration/","summary":"Tokenizing Text Data Import Libraries 1 2 3 import nltk from nltk.corpus import stopwords from collections import Counter Set Stopwords 1 2 3 4 5 6 stop_words = stopwords.words(\u0026#34;english\u0026#34;) stop_words.append(\u0026#39;,\u0026#39;) stop_words.append(\u0026#39;.\u0026#39;) stop_words.append(\u0026#39;’\u0026#39;) stop_words.append(\u0026#39;”\u0026#39;) stop_words.append(\u0026#39;—\u0026#39;) Open Text Data 1 2 file = open(\u0026#39;movie_review.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#34;utf-8\u0026#34;) lines = file.readlines() Tokenize 1 2 3 4 5 6 tokens = [] for line in lines: tokenized = nltk.word_tokenize(line) for token in tokenized: if token.lower() not in stop_words: tokens.","title":"[AI SCHOOL 5기] 텍스트 분석 실습 - 텍스트 데이터 분석"},{"content":"NLTK Library NLTK(Natural Language Toolkit)은 자연어 처리를 위한 라이브러리 1 2 3 import nltk nltk.download() 문장을 단어 수준에서 토큰화 1 2 3 sentence = \u0026#39;NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\u0026#39; nltk.word_tokenize(sentence) Output\n1 2 3 4 5 6 [\u0026#39;NLTK\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;leading\u0026#39;, \u0026#39;platform\u0026#39;, ... POS Tagging 1 2 tokens = nltk.word_tokenize(sentence) nltk.pos_tag(tokens) Output\n1 2 3 4 5 6 [(\u0026#39;NLTK\u0026#39;, \u0026#39;NNP\u0026#39;), (\u0026#39;is\u0026#39;, \u0026#39;VBZ\u0026#39;), (\u0026#39;a\u0026#39;, \u0026#39;DT\u0026#39;), (\u0026#39;leading\u0026#39;, \u0026#39;VBG\u0026#39;), (\u0026#39;platform\u0026#39;, \u0026#39;NN\u0026#39;), ... NLTK POS Tags List Stopwords 제거 1 2 3 4 5 6 7 8 9 10 11 from nltk.corpus import stopwords stop_words = stopwords.words(\u0026#39;english\u0026#39;) stop_words.append(\u0026#39;,\u0026#39;) stop_words.append(\u0026#39;.\u0026#39;) result = [] for token in tokens: if token.lower() not in stopWords: result.append(token) Output\n1 [\u0026#39;NLTK\u0026#39;, \u0026#39;leading\u0026#39;, \u0026#39;platform\u0026#39;, \u0026#39;building\u0026#39;, \u0026#39;Python\u0026#39;, \u0026#39;programs\u0026#39;, \u0026#39;work\u0026#39;, \u0026#39;human\u0026#39;, \u0026#39;language\u0026#39;, \u0026#39;data\u0026#39;, \u0026#39;provides\u0026#39;, \u0026#39;easy-to-use\u0026#39;, \u0026#39;interfaces\u0026#39;, \u0026#39;50\u0026#39;, \u0026#39;corpora\u0026#39;, \u0026#39;lexical\u0026#39;, \u0026#39;resources\u0026#39;, \u0026#39;WordNet\u0026#39;, \u0026#39;along\u0026#39;, \u0026#39;suite\u0026#39;, \u0026#39;text\u0026#39;, \u0026#39;processing\u0026#39;, \u0026#39;libraries\u0026#39;, \u0026#39;classification\u0026#39;, \u0026#39;tokenization\u0026#39;, \u0026#39;stemming\u0026#39;, \u0026#39;tagging\u0026#39;, \u0026#39;parsing\u0026#39;, \u0026#39;semantic\u0026#39;, \u0026#39;reasoning\u0026#39;, \u0026#39;wrappers\u0026#39;, \u0026#39;industrial-strength\u0026#39;, \u0026#39;NLP\u0026#39;, \u0026#39;libraries\u0026#39;, \u0026#39;active\u0026#39;, \u0026#39;discussion\u0026#39;, \u0026#39;forum\u0026#39;] Lemmatizing Lemmatization: 단어의 형태소적/사전적 분석을 통해 파생적 의미를 제거하고,\n어근에 기반하여 기본 사전형인 lemma를 찾는 것 1 2 3 4 5 6 7 8 9 10 lemmatizer = nltk.wordnet.WordNetLemmatizer() print(lemmatizer.lemmatize(\u0026#34;cats\u0026#34;)) # cat print(lemmatizer.lemmatize(\u0026#34;geese\u0026#34;)) # goose print(lemmatizer.lemmatize(\u0026#34;better\u0026#34;)) # better print(lemmatizer.lemmatize(\u0026#34;better\u0026#34;, pos=\u0026#34;a\u0026#34;)) # good print(lemmatizer.lemmatize(\u0026#34;ran\u0026#34;)) # ran print(lemmatizer.lemmatize(\u0026#34;ran\u0026#34;, \u0026#39;v\u0026#39;)) # run default로 n 이므로 \u0026lsquo;cats\u0026rsquo;, \u0026lsquo;geese\u0026rsquo; 들은 기본명사형을 반환 형용사 \u0026lsquo;better\u0026rsquo;는 pos에 a를 함께 입력해주어야 원형인 \u0026lsquo;good\u0026rsquo;을 반환 동사 \u0026lsquo;ran\u0026rsquo;은 pos에 v를 함께 입력해주어야 원형인 \u0026lsquo;run\u0026rsquo;을 반환 영화 리뷰 데이터 전처리 1 2 3 4 5 6 7 8 9 10 file = open(\u0026#39;moviereview.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) lines = file.readlines() sentence = lines[1] tokens = nltk.word_tokenize(sentence) lemmas = [] for token in tokens: if token.lower() not in stop_words: lemmas.append(lemmatizer.lemmatize(token)) ","permalink":"https://minyeamer.github.io/blog/aischool-02-01-processing-text-data/","summary":"NLTK Library NLTK(Natural Language Toolkit)은 자연어 처리를 위한 라이브러리 1 2 3 import nltk nltk.download() 문장을 단어 수준에서 토큰화 1 2 3 sentence = \u0026#39;NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.","title":"[AI SCHOOL 5기] 텍스트 분석 실습 - 텍스트 분석"},{"content":"BeautifulSoup Library 1 2 from bs4 import BeautifulSoup from urllib.request import urlopen 단어의 검색 결과 출력 다음 어학사전 URL 불러오기 1 2 3 4 5 6 # 찾는 단어 입력 word = \u0026#39;happiness\u0026#39; url = f\u0026#39;https://alldic.daum.net/search.do?q={word}\u0026#39; web = urlopen(url) web_page = BeautifulSoup(web, \u0026#39;html.parser\u0026#39;) 찾는 단어 출력 1 2 text_search = web_page.find(\u0026#39;span\u0026#39;, {\u0026#39;class\u0026#39;: \u0026#39;txt_emph1\u0026#39;}) print(f\u0026#39;찾는 단어: {text_search.get_text()}\u0026#39;) 단어의 뜻 출력 1 2 3 4 list_search = web_page.find(\u0026#39;ul\u0026#39;, {\u0026#39;class\u0026#39;: \u0026#39;list_search\u0026#39;}) list_text = list_search.find_all(\u0026#39;span\u0026#39;, {\u0026#39;class\u0026#39;: \u0026#39;txt_search\u0026#39;}) definitions = [definition.get_text() in list_text] print(f\u0026#39;단어의 뜻: {definitions}\u0026#39;) Output\n1 2 찾는 단어: happiness 단어의 뜻: [\u0026#39;행복\u0026#39;, \u0026#39;만족\u0026#39;, \u0026#39;기쁨\u0026#39;, \u0026#39;행운\u0026#39;] 영화 정보 출력 네이버 영화 URL 불러오기 1 2 3 4 5 6 # 찾는 영화 번호 입력 (향후 영화 제목으로 검색 구현) movie = 208077 url = f\u0026#39;https://movie.naver.com/movie/search/result.naver?query={movie}\u0026#39; web = urlopen(url) web_page = BeautifulSoup(web, \u0026#39;html.parser\u0026#39;) 영화 제목 출력 1 2 title = web_page.find(\u0026#39;h3\u0026#39;, {\u0026#39;class\u0026#39;:\u0026#39;h_movie\u0026#39;}).find(\u0026#39;a\u0026#39;) print(f\u0026#39;Movie Title: {title.get_text()}\u0026#39;) 네이버 영화 배우/제작진 URL 불러오기 1 2 3 url = f\u0026#39;https://movie.naver.com/movie/bi/mi/detail.naver?query={movie}\u0026#39; web = urlopen(url) web_page = BeautifulSoup(web, \u0026#39;html.parser\u0026#39;) 감독 이름 출력 1 2 director = web_page.find(\u0026#39;div\u0026#39;, {\u0026#39;class\u0026#39;:\u0026#39;dir_product\u0026#39;}).find(\u0026#39;a\u0026#39;) print(f\u0026#39;Director: {director.get_text()}\u0026#39;) 출연 배우들 이름 출력 1 2 3 4 actor_list = web_page.find(\u0026#39;ul\u0026#39;, {\u0026#39;class\u0026#39;:\u0026#39;lst_people\u0026#39;}) actor_names = actor_list.find_all(\u0026#39;a\u0026#39;, {\u0026#39;class\u0026#39;:\u0026#39;k_name\u0026#39;}) actors = [actor.get_text() for actor in actor_names] print(f\u0026#39;Actors: {actors}\u0026#39;) Output\n1 2 3 Movie Title: 스파이더맨: 노 웨이 홈 Director: 존 왓츠 Actors: [\u0026#39;톰 홀랜드\u0026#39;, \u0026#39;젠데이아 콜먼\u0026#39;, \u0026#39;베네딕트 컴버배치\u0026#39;, \u0026#39;존 파브로\u0026#39;, \u0026#39;제이콥 배덜런\u0026#39;, \u0026#39;마리사 토메이\u0026#39;, \u0026#39;알프리드 몰리나\u0026#39;] 티스토리 게시글 출력 및 저장 티스토리 게시글 URL 불러오기 1 2 3 4 5 6 # 찾는 글 번호 입력 post_number = 22 url = f\u0026#39;https://minyeamer.tistory.com/{post_number}\u0026#39; web = urlopen(url) source = BeautifulSoup(web, \u0026#39;html.parser\u0026#39;) 티스토리 게시글 출력 1 2 3 4 all_text = source.find(\u0026#39;article\u0026#39;,{\u0026#39;class\u0026#39;: \u0026#39;content\u0026#39;}) tags = [\u0026#39;h2\u0026#39;, \u0026#39;h3\u0026#39;, \u0026#39;h4\u0026#39;, \u0026#39;li\u0026#39;, \u0026#39;p\u0026#39;, \u0026#39;blockquote\u0026#39;, \u0026#39;code\u0026#39;] article = all_text.find_all(tags) print(article) 티스토리 게시글 저장 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from urllib.request import HTTPError for post_number in range(10): try: url = f\u0026#39;https://minyeamer.tistory.com/{post_number}\u0026#39; web = urlopen(url) source = BeautifulSoup(web, \u0026#39;html.parser\u0026#39;) except HTTPError: print(f\u0026#39;{i}번 글에서 에러가 발생했습니다.\u0026#39;) pass with open(\u0026#39;tistory_all.txt\u0026#39;, \u0026#39;a\u0026#39;, encoding = \u0026#39;utf-8\u0026#39;) as f: all_text = source.find(\u0026#39;article\u0026#39;,{\u0026#39;class\u0026#39;: \u0026#39;content\u0026#39;}) tags = [\u0026#39;h2\u0026#39;, \u0026#39;h3\u0026#39;, \u0026#39;h4\u0026#39;, \u0026#39;li\u0026#39;, \u0026#39;p\u0026#39;, \u0026#39;blockquote\u0026#39;, \u0026#39;code\u0026#39;] article = all_text.find_all(tags) for content in article: f.write(content.get_text() + \u0026#39;\\n\u0026#39;) ","permalink":"https://minyeamer.github.io/blog/aischool-03-01-web-scraping-basic/","summary":"BeautifulSoup Library 1 2 from bs4 import BeautifulSoup from urllib.request import urlopen 단어의 검색 결과 출력 다음 어학사전 URL 불러오기 1 2 3 4 5 6 # 찾는 단어 입력 word = \u0026#39;happiness\u0026#39; url = f\u0026#39;https://alldic.daum.net/search.do?q={word}\u0026#39; web = urlopen(url) web_page = BeautifulSoup(web, \u0026#39;html.parser\u0026#39;) 찾는 단어 출력 1 2 text_search = web_page.find(\u0026#39;span\u0026#39;, {\u0026#39;class\u0026#39;: \u0026#39;txt_emph1\u0026#39;}) print(f\u0026#39;찾는 단어: {text_search.get_text()}\u0026#39;) 단어의 뜻 출력 1 2 3 4 list_search = web_page.find(\u0026#39;ul\u0026#39;, {\u0026#39;class\u0026#39;: \u0026#39;list_search\u0026#39;}) list_text = list_search.","title":"[AI SCHOOL 5기] 웹 크롤링 실습 - 웹 스크래핑 기본"},{"content":"Web Crawling vs Web Scraping Web Crawling: Bot이 web을 link를 통해 돌아다니는 것 Web Scraping: Webpage에서 원하는 자료를 긇어오는 것 HTML Tags Tag\u0026rsquo;s Name: html, head, body, p, span, li, ol, ul, div Tag\u0026rsquo;s Attribute: class, id, style, href, src The Process of Web Scraping URL 분석 (query 종류 등) URL 구성 HTTP Response 얻기 (urlopen(URL) or request.get(URL).content) HTTP source 얻기 (BeautifulSoup(HTTP Response, 'html.parser')) HTML Tag 꺼내기 (.find('tag_name', {'attr_name':'attr_value'})) Tag로부터 텍스트 혹은 Attribute values 꺼내기 (Tag.get_text() or Tag.attrs) The Process of Data Analysis for Text Data 텍스트 데이터를 str 자료형으로 준비 Tokenize (형태소 분석) POS Tagging (Part-of-speech, 품사 표시) Stopwords 제거 (불용어 제거) 단어 갯수 카운팅 \u0026amp; 단어 사전 생성 단어 사전 기반 데이터 시각화 (+ 머신러닝/딥러닝 모델 적용) TF-IDF Term Frequency - Inverse Document Frequency 특정 단어가 문서에서 어떤 중요도를 가지는지를 나타내는 지표 많은 문서에 공통적으로 들어있는 단어는 문서 구별 능력이 떨어진다 판단하여 가중치 축소 Count Vectorizer 단어의 빈도수만을 사용해서 벡터 생성 Document That Nice Car John Has Red A 1 1 1 0 0 0 B 1 0 1 1 1 1 TF-IDF Vectorizer 단어의 빈도수(TF)를 TF-IDF 값으로 변경하여 가중치가 조정된 벡터 생성 Cosine Similarity 두 벡터 사이 각도의 코사인 값을 이용하여 두 벡터의 유사한 정도 측정 유사도가 -1이면 서로 완전히 반대되는 경우 유사도가 0이면 서로 독립적인 경우 유사도가 1이면 서로 완전히 같은 경우 텍스트 매칭에 적용될 경우 두 벡터에 해당 문서에서의 단어 빈도가 적용 Embedding 한국어 임베딩 @ https://j.mp/3mduiBk ","permalink":"https://minyeamer.github.io/blog/aischool-03-00-web-crawling/","summary":"Web Crawling vs Web Scraping Web Crawling: Bot이 web을 link를 통해 돌아다니는 것 Web Scraping: Webpage에서 원하는 자료를 긇어오는 것 HTML Tags Tag\u0026rsquo;s Name: html, head, body, p, span, li, ol, ul, div Tag\u0026rsquo;s Attribute: class, id, style, href, src The Process of Web Scraping URL 분석 (query 종류 등) URL 구성 HTTP Response 얻기 (urlopen(URL) or request.get(URL).content) HTTP source 얻기 (BeautifulSoup(HTTP Response, 'html.parser')) HTML Tag 꺼내기 (.find('tag_name', {'attr_name':'attr_value'})) Tag로부터 텍스트 혹은 Attribute values 꺼내기 (Tag.","title":"[AI SCHOOL 5기] 웹 크롤링"},{"content":"Visualization Libraries Plotly Altair Bokeh (Website Graph) @ https://j.mp/30772sU Data Chart Types Numeric: 숫자 자체에 의미가 있음 (온도 등), 연속형 Categoric: 숫자 너머에 의미가 있음 (성별, 강아지 품종 등), 불연속형 @ https://goo.gl/ErLHCY @ http://j.mp/2JcEENe GeoJSON Data 1 2 3 4 5 6 import json # 한국의 지도 데이터 참조 # @ https://github.com/southkorea/southkorea-maps geo_path = \u0026#39;skorea_municipalities_geo_simple.json\u0026#39; geo_str = json.load(open(geo_path, encoding=\u0026#39;utf-8\u0026#39;)) JSON(Javascript Object Notation): 데이터 교환을 위한 표준 포맷 GeoJSON: 지도 데이터 포맷 json.load: JSON 파일 불러오기 json.dump: JSON 파일 저장하기 PyPrnt Library 1 2 3 from pyprnt import prnt prnt(geo_str, truncate=True, width=80) PyPrnt: JSON 구조 파악에 용의한 도구 @ http://j.mp/2WVZuGy Folium Library 1 2 3 4 import folium # Folium 공식문서 @ https://goo.gl/5UgneX seoul_map = folium.Map(location=, zoom_start=, tiles=) Folium: 지도 데이터 시각화 라이브러리 localtion: 초기 지도 시작 위치 zoom_start: 초기 지도 확대 정도 tiles: 지도 타입 (default \u0026ldquo;Stamen Terrain\u0026rdquo; or \u0026ldquo;Stamen Toner\u0026rdquo;) 초기 좌표를 [37.5502, 126.982], 확대 정도를 11로 설정하면 서울을 표시 살인사건 발생건수 시각화 1 2 3 4 5 6 # Choropleth map @ https://goo.gl/yrTRHU seoul_map.choropleth(geo_data = geo_str, data = gu_df[\u0026#39;살인\u0026#39;], columns = [gu_df.index, gu_df[\u0026#39;살인\u0026#39;]], fill_color = \u0026#39;PuRd\u0026#39;, key_on = \u0026#39;feature.id\u0026#39;) geo_data: GeoJSON 데이터 data: 시각화의 대상이 될 데이터 columns: DataFrame의 index column을 가져와 인식 fill_color: matplolib colormap과 유사 @ http://colorbrewer2.org key_on: GeoJSON 규약을 따름, JSON 파일 feature의 id에 매칭 경찰서별 검거율 점수 계산 경찰서별 검거율에 대한 시각화 시 문제점\n경찰서별 검거율의 최대-최소 차이가 17로 매우 적음 검거율을 원형 차트로 만들었을 때 각각의 차이가 적어서 직관적이지 못함 Min-Max Algorithm을 사용하여 검거율 점수 계산\n$$z_i=\\frac{x_i-\\text{min}(x)}{\\text{max}(x)-\\text{min}(x)}$$\n1 2 3 4 5 6 def re_range(x, oldMin, old_max, new_min, new_max): return (x - old_min)*(new_max - new_min) / (old_max - old_min) + new_min df[\u0026#39;점수\u0026#39;] = re_range(df[\u0026#39;검거율\u0026#39;], min(df[\u0026#39;검거율\u0026#39;]), max(df[\u0026#39;검거율\u0026#39;]), 1, 100) 경찰서별 좌표 데이터 수집 강남경찰서 좌표 데이터 내용 확인\n1 2 3 4 import googlemaps gmaps = googlemaps.Client(key=\u0026#39;your-api-key\u0026#39;) gangnam_police_map = gmaps.geocode(\u0026#39;서울강남경찰서\u0026#39;, language=\u0026#34;ko\u0026#34;) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 [{\u0026#39;address_components\u0026#39;: [{\u0026#39;long_name\u0026#39;: \u0026#39;１１\u0026#39;, \u0026#39;short_name\u0026#39;: \u0026#39;１１\u0026#39;, \u0026#39;types\u0026#39;: [\u0026#39;premise\u0026#39;]}, {\u0026#39;long_name\u0026#39;: \u0026#39;테헤란로114길\u0026#39;, \u0026#39;short_name\u0026#39;: \u0026#39;테헤란로114길\u0026#39;, \u0026#39;types\u0026#39;: [\u0026#39;political\u0026#39;, \u0026#39;sublocality\u0026#39;, \u0026#39;sublocality_level_4\u0026#39;]}, {\u0026#39;long_name\u0026#39;: \u0026#39;강남구\u0026#39;, \u0026#39;short_name\u0026#39;: \u0026#39;강남구\u0026#39;, \u0026#39;types\u0026#39;: [\u0026#39;political\u0026#39;, \u0026#39;sublocality\u0026#39;, \u0026#39;sublocality_level_1\u0026#39;]}, {\u0026#39;long_name\u0026#39;: \u0026#39;서울특별시\u0026#39;, \u0026#39;short_name\u0026#39;: \u0026#39;서울특별시\u0026#39;, \u0026#39;types\u0026#39;: [\u0026#39;administrative_area_level_1\u0026#39;, \u0026#39;political\u0026#39;]}, {\u0026#39;long_name\u0026#39;: \u0026#39;대한민국\u0026#39;, \u0026#39;short_name\u0026#39;: \u0026#39;KR\u0026#39;, \u0026#39;types\u0026#39;: [\u0026#39;country\u0026#39;, \u0026#39;political\u0026#39;]}, {\u0026#39;long_name\u0026#39;: \u0026#39;06175\u0026#39;, \u0026#39;short_name\u0026#39;: \u0026#39;06175\u0026#39;, \u0026#39;types\u0026#39;: [\u0026#39;postal_code\u0026#39;]}], \u0026#39;formatted_address\u0026#39;: \u0026#39;대한민국 서울특별시 강남구 테헤란로114길 11\u0026#39;, \u0026#39;geometry\u0026#39;: {\u0026#39;location\u0026#39;: {\u0026#39;lat\u0026#39;: 37.5094352, \u0026#39;lng\u0026#39;: 127.0669578}, \u0026#39;location_type\u0026#39;: \u0026#39;ROOFTOP\u0026#39;, \u0026#39;viewport\u0026#39;: {\u0026#39;northeast\u0026#39;: {\u0026#39;lat\u0026#39;: 37.5107841802915, \u0026#39;lng\u0026#39;: 127.0683067802915}, \u0026#39;southwest\u0026#39;: {\u0026#39;lat\u0026#39;: 37.5080862197085, \u0026#39;lng\u0026#39;: 127.0656088197085}}}, \u0026#39;partial_match\u0026#39;: True, \u0026#39;place_id\u0026#39;: \u0026#39;ChIJcbaB0UakfDURoyy8orQOWFg\u0026#39;, \u0026#39;plus_code\u0026#39;: {\u0026#39;compound_code\u0026#39;: \u0026#39;G358+QQ 대한민국 서울특별시\u0026#39;, \u0026#39;global_code\u0026#39;: \u0026#39;8Q99G358+QQ\u0026#39;}, \u0026#39;types\u0026#39;: [\u0026#39;establishment\u0026#39;, \u0026#39;point_of_interest\u0026#39;, \u0026#39;police\u0026#39;]}] gmaps.geocode: Google Maps의 Geocoding에 대한 함수, 위도/경도 및 우편번호 등 반환 gmaps.reverse_geocode((lng, lat), lang=): 위도/경도 값으로 주소값 반환 formatted_address: 도로명 주소 반환값 geometry.location: 위도/경도 반환값 (lat / lng) 경찰서별 좌표 데이터 수집\n1 2 3 4 5 6 7 8 9 10 11 lat = [] lng = [] for name in df[\u0026#39;경찰서\u0026#39;]: police_map = gmaps.geocode(name, language=\u0026#39;ko\u0026#39;) police_loc = seoul_police_map[0].get(\u0026#39;geometry\u0026#39;) lat.append(police_loc[\u0026#39;location\u0026#39;][\u0026#39;lat\u0026#39;]) lng.append(police_loc[\u0026#39;location\u0026#39;][\u0026#39;lng\u0026#39;]) df[\u0026#39;lat\u0026#39;] = lat df[\u0026#39;lng\u0026#39;] = lng 경찰서별 검거율 데이터 시각화 1 2 3 4 5 6 7 police_map = folium.Map(location=, zoom_start=) for n in df.index: folium.CircleMarker([df.at[n, \u0026#39;lat\u0026#39;], df.at[n, \u0026#39;lng\u0026#39;]], radius=df.at[n, \u0026#39;점수\u0026#39;]*0.5, # meter 단위 color=\u0026#39;#3186cc\u0026#39;, fill=True, fill_color=\u0026#39;#3186cc\u0026#39;).add_to(map) 시각화된 데이터 종합 1 2 3 4 5 6 7 8 9 10 11 12 13 police_map = folium.Map(location=, zoom_start=) police_map.choropleth(geo_data = geo_str, data = crime_ratio[\u0026#39;전체발생비율\u0026#39;], columns = [crime_ratio.index, crime_ratio[\u0026#39;전체발생비율\u0026#39;]], fill_color = \u0026#39;PuRd\u0026#39;, key_on = \u0026#39;feature.id\u0026#39;) for n in df.index: folium.CircleMarker([df.at[n, \u0026#39;lat\u0026#39;], df.at[n, \u0026#39;lng\u0026#39;]], radius=df.at[n, \u0026#39;점수\u0026#39;]*0.7, color=\u0026#39;#3186cc\u0026#39;, fill=True, fill_color=\u0026#39;#3186cc\u0026#39;).add_to(police_map) Export DataFrame DataFrame to csv file 1 df.to_csv(\u0026#39;processed_data.csv\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) DataFrame to Excel file 1 2 3 4 5 from pandas import ExcelWriter writer = ExcelWriter(\u0026#39;file_name.xlsx\u0026#39;) df.to_excel(writer) writer.save() Saving a folium map as an HTML file 1 folium_map.save(\u0026#39;folium_map.html\u0026#39;) HTML 파일로 시각화된 지도 데이터 추출\npolice-map.html GeoJSON Data (Not Simplified) 1 2 geo_path = \u0026#39;skorea-2018-municipalities-geo.json\u0026#39; geo_str = json.load(open(geo_path, encoding=\u0026#39;utf-8\u0026#39;)) features.properties.code에서 서울 내 지역코드는 11로 시작 기존 features.id는 feature.properties.name과 매칭 choropleth 실행 시 key_on에 feature.properties.name 입력 서울 내 지역만 수집 1 2 3 4 5 in_seoul = [] for feature in geo_str[\u0026#39;features\u0026#39;]: if feature[\u0026#39;properties\u0026#39;][\u0026#39;code\u0026#39;].startswith(\u0026#39;11\u0026#39;): in_seoul.append(feature) 구체적인 데이터로 구현한 지도 ","permalink":"https://minyeamer.github.io/blog/aischool-01-03-data-visualization/","summary":"Visualization Libraries Plotly Altair Bokeh (Website Graph) @ https://j.mp/30772sU Data Chart Types Numeric: 숫자 자체에 의미가 있음 (온도 등), 연속형 Categoric: 숫자 너머에 의미가 있음 (성별, 강아지 품종 등), 불연속형 @ https://goo.gl/ErLHCY @ http://j.mp/2JcEENe GeoJSON Data 1 2 3 4 5 6 import json # 한국의 지도 데이터 참조 # @ https://github.com/southkorea/southkorea-maps geo_path = \u0026#39;skorea_municipalities_geo_simple.json\u0026#39; geo_str = json.load(open(geo_path, encoding=\u0026#39;utf-8\u0026#39;)) JSON(Javascript Object Notation): 데이터 교환을 위한 표준 포맷 GeoJSON: 지도 데이터 포맷 json.","title":"[AI SCHOOL 5기] 데이터 분석 실습 - 데이터 시각화"},{"content":"문제 링크 https://www.acmicpc.net/problem/11650 개요 배열 형태의 자료들을 정렬하는 간단한 문제이다. 파이썬에서는 내장 함수 sort()를 사용하면 쉽게 풀 수 있다. 문제 해설 문제에서 요구하는 것은 x좌표 값과 y좌표 값으로 구성된 배열들의 리스트를 x 값, y 값 순으로 정렬하는 것이다. 배열의 자료구조는 인덱싱으로 접근이 가능한 것이면 아무거나 상관없기에 좌표 표현에 직관적인 튜플을 사용한다. 정렬의 기준이 반대였으면 람다 식을 써야겠지만 좌표의 위치가 곧 정렬 순서이기 때문에 Key값은 사용하지 않는다. 해설 코드 1 2 3 4 5 6 7 8 9 10 import sys input = sys.stdin.readline points = [] for _ in range(int(input())): points.append(tuple(map(int, input().split()))) for point in sorted(points): print(point[0], point[1]) ","permalink":"https://minyeamer.github.io/blog/boj-problems-11650/","summary":"문제 링크 https://www.acmicpc.net/problem/11650 개요 배열 형태의 자료들을 정렬하는 간단한 문제이다. 파이썬에서는 내장 함수 sort()를 사용하면 쉽게 풀 수 있다. 문제 해설 문제에서 요구하는 것은 x좌표 값과 y좌표 값으로 구성된 배열들의 리스트를 x 값, y 값 순으로 정렬하는 것이다. 배열의 자료구조는 인덱싱으로 접근이 가능한 것이면 아무거나 상관없기에 좌표 표현에 직관적인 튜플을 사용한다. 정렬의 기준이 반대였으면 람다 식을 써야겠지만 좌표의 위치가 곧 정렬 순서이기 때문에 Key값은 사용하지 않는다. 해설 코드 1 2 3 4 5 6 7 8 9 10 import sys input = sys.","title":"[백준 11650] 좌표 정렬하기 (Python)"},{"content":"Visualization Library 1 2 3 import seaborn as sns sns.heatmap(gu_df[]) Visualization Issues 한글 데이터 표시 오류 서로 다른 자릿수로 구성된 열에 동일한 스케일 적용 시각화된 테이블 형태의 비직관성 문제 인구수가 고려되지 않은 부정확한 데이터 한글 데이터 시각화 1 2 3 4 5 6 7 8 matplotlib inline # Windows font_name = font_manager.FontProperties(fname=\u0026#34;C:/~/malgun.ttf\u0026#34;).get_name() rc(\u0026#39;font\u0026#39;, family=font_name) # Mac rc(\u0026#39;font\u0026#39;, family=\u0026#39;AppleGothic\u0026#39;) Feature Scaling/Normalization Min-Max Algorithm 열에 대한 최솟값(min)을 0, 열에 대한 최댓값(max)를 1로 맞춤 기존 열을 old_x, 새로운 열을 new_x라 할 때,\nnew_x = ( old_x - min(column) ) / ( max(column) - min(column) ) Standardization 열에 대한 평균값(mean)을 0, 열에 대한 표준편차 값(std)를 1로 맞춤 기존 열을 old_x, 새로운 열을 new_x라 할 때,\nnew_x = ( old_x - mean(column) ) / std(column) 표준 점수 (Z-score)와 동일 시각화 개선 전체 테이블의 사이즈 조정\npit.figure(figsize = (x, y)) 셀 형식 및 색상 등 변경\nsns.heatmap(norm, annot=, fmt=, linewidths=, cmap=) annot: 셀 내에 수치 입력 여부 (defualt False) fmt: 셀 내에 입력될 수치의 format ('f' == float) linewidths: 셀 간 거리 (내부 테두리) cmap: matplotlib colormap @https://goo.gl/YWpBES 테이블 제목 설정\nplt.tile() 시각화 설정된 테이블 표시\nplt.show() 데이터 정확성 개선 범죄 발생 횟수에 인구수 반영 1 2 # 시각화된 데이터에서 열방향(axis=0)을 기준으로 인구수 데이터를 나눔 (인구 10만 단위) crime_ratio = crime_count_norm.div(gu_df[\u0026#39;인구수\u0026#39;], axis=0) * 100000 구별 5대 범죄 발생 수치 평균 계산 1 crime_ratio[\u0026#39;전체발생비율\u0026#39;] = crime_ratio.mean(axis=1) 각 사건들의 중형도를 고려하지 못할 수 있음 \u0026lsquo;살인\u0026rsquo; 열에 스케일 적용 중 이미 큰 값이 곱해졌기 때문에 가중치가 적용되었다고도 판단 가능 Improved Visualization 1 2 3 4 5 6 plt.figure(figsize = (10,10)) sns.heatmap(crime_ratio.sort_values(by=\u0026#39;전체발생비율\u0026#39;, ascending=False), annot=True, fmt=\u0026#39;f\u0026#39;, linewidths=.5, cmap=\u0026#39;Reds\u0026#39;) plt.title(\u0026#39;범죄 발생(전체발생비율로 정렬) - 각 항목을 정규화한 후 인구로 나눔\u0026#39;) plt.show() ","permalink":"https://minyeamer.github.io/blog/aischool-01-02-data-exploration/","summary":"Visualization Library 1 2 3 import seaborn as sns sns.heatmap(gu_df[]) Visualization Issues 한글 데이터 표시 오류 서로 다른 자릿수로 구성된 열에 동일한 스케일 적용 시각화된 테이블 형태의 비직관성 문제 인구수가 고려되지 않은 부정확한 데이터 한글 데이터 시각화 1 2 3 4 5 6 7 8 matplotlib inline # Windows font_name = font_manager.FontProperties(fname=\u0026#34;C:/~/malgun.ttf\u0026#34;).get_name() rc(\u0026#39;font\u0026#39;, family=font_name) # Mac rc(\u0026#39;font\u0026#39;, family=\u0026#39;AppleGothic\u0026#39;) Feature Scaling/Normalization Min-Max Algorithm 열에 대한 최솟값(min)을 0, 열에 대한 최댓값(max)를 1로 맞춤 기존 열을 old_x, 새로운 열을 new_x라 할 때,","title":"[AI SCHOOL 5기] 데이터 분석 실습 - 데이터 탐색"},{"content":"Practice Data 서울시 범죄현황 통계자료 범죄별로 검거율 계산 1 2 3 4 5 6 7 # gu_df는 실습 자료에 서울시 경찰청의 소속 구 데이터를 추가한 DataFrame gu_df[\u0026#39;강간검거율\u0026#39;] = gu_df[\u0026#39;강간(검거)\u0026#39;]/gu_df[\u0026#39;강간(발생)\u0026#39;]*100 gu_df[\u0026#39;강도검거율\u0026#39;] = gu_df[\u0026#39;강도(검거)\u0026#39;]/gu_df[\u0026#39;강도(발생)\u0026#39;]*100 gu_df[\u0026#39;살인검거율\u0026#39;] = gu_df[\u0026#39;살인(검거)\u0026#39;]/gu_df[\u0026#39;살인(발생)\u0026#39;]*100 gu_df[\u0026#39;절도검거율\u0026#39;] = gu_df[\u0026#39;절도(검거)\u0026#39;]/gu_df[\u0026#39;절도(발생)\u0026#39;]*100 gu_df[\u0026#39;폭력검거율\u0026#39;] = gu_df[\u0026#39;폭력(검거)\u0026#39;]/gu_df[\u0026#39;폭력(발생)\u0026#39;]*100 gu_df[\u0026#39;검거율\u0026#39;] = gu_df[\u0026#39;소계(검거)\u0026#39;]/gu_df[\u0026#39;소계(발생)\u0026#39;]*100 해당 계산법의 문제:\n이전 연도에 발생한 사건이 많이 검거될 경우 검거율이 100%를 초과 발생 건수가 0인 경우 검거율에 결측치(N/A)가 발생 초과된 검거율을 최댓값으로 조정:\n1 2 # 검거율에 해당되는 열의 집합 columns columns = [\u0026#39;강간검거율\u0026#39;, \u0026#39;강도검거율\u0026#39;, \u0026#39;살인검거율\u0026#39;, \u0026#39;절도검거율\u0026#39;, \u0026#39;폭력검거율\u0026#39;] 모든 행에 대해 반복문 실행 1 2 3 4 for row_index, row in gu_df_rate.iterrows(): for column in columns: if row[column] \u0026gt; 100: gu_df.at[row_index, column] = 100 Masking 기법 활용 1 gu_df[ gu_df[columns] \u0026gt; 100 ] = 100 gu_df[columns] \u0026gt; 100은 True와 False로 이루어진 행렬을 반환 해당 행렬을 gu_df의 Key로 사용하면 True에 해당하는 값이 채로 친듯 솎아 걸러짐 조건이 두 개 이상일 경우 괄호로 감싸주어야 함 Pandas에서는 and, or, not이 동작하지 않기 때문에 \u0026amp;, |, ~ 사용 gu_df[ (gu_df['살인(발생)'] \u0026gt; 7) \u0026amp; (gu_df['폭력(발생)'] \u0026gt; 2000) ] 결측치를 의미있는 값으로 변경:\n1 gu_df[\u0026#39;살인검거율\u0026#39;] = gu_df[\u0026#39;살인검거율\u0026#39;].fillna(100) 인구 데이터 Merge 1 2 # 인구 데이터에 해당하는 csv 파일 불러오기 popul_df = pd.read_csv(\u0026#39;pop_kor.csv\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) Pandas Merge Functions Join: A.join(B) A와 B의 index 열이 동일해야 함 Merge: pd.merge(A, B, left_on=, right_on=, how=) DataFrame A의 기준 left_on과 DataFrame B의 기준 right_on을 비교 how 옵션에는 inner (교집합), full_outer (합집합),\nleft_outer (A 기준 합), right_outer (B 기준 합) 가능 Concatenate: pd.concat([A, B], axis=) 무조건 갖다 붙이기 때문에 사용에 주의 Additional Pandas Functions df.iterrow(): 반복문을 사용해 모든 행을 참조할 때 사용, (행 이름, Series) 반환 df.at[]: DataFrame에서 단일 값 추출 (단일 인덱싱에서 df.loc[]보다 빠름) df.[].fillna(): 결측치(N/A)를 의미있는 값으로 바꿈 (Series가 가지고 있는 함수) df[df[]].str.contains()]: 특정 문자가 포함된 행을 표시 Setting Index pd.read_csv('pop_kor.csv', encoding='utf-8', index_col=) pd.read_csv('pop_kor.csv', encoding='utf-8').set_index() ","permalink":"https://minyeamer.github.io/blog/aischool-01-01-data-analysis/","summary":"Practice Data 서울시 범죄현황 통계자료 범죄별로 검거율 계산 1 2 3 4 5 6 7 # gu_df는 실습 자료에 서울시 경찰청의 소속 구 데이터를 추가한 DataFrame gu_df[\u0026#39;강간검거율\u0026#39;] = gu_df[\u0026#39;강간(검거)\u0026#39;]/gu_df[\u0026#39;강간(발생)\u0026#39;]*100 gu_df[\u0026#39;강도검거율\u0026#39;] = gu_df[\u0026#39;강도(검거)\u0026#39;]/gu_df[\u0026#39;강도(발생)\u0026#39;]*100 gu_df[\u0026#39;살인검거율\u0026#39;] = gu_df[\u0026#39;살인(검거)\u0026#39;]/gu_df[\u0026#39;살인(발생)\u0026#39;]*100 gu_df[\u0026#39;절도검거율\u0026#39;] = gu_df[\u0026#39;절도(검거)\u0026#39;]/gu_df[\u0026#39;절도(발생)\u0026#39;]*100 gu_df[\u0026#39;폭력검거율\u0026#39;] = gu_df[\u0026#39;폭력(검거)\u0026#39;]/gu_df[\u0026#39;폭력(발생)\u0026#39;]*100 gu_df[\u0026#39;검거율\u0026#39;] = gu_df[\u0026#39;소계(검거)\u0026#39;]/gu_df[\u0026#39;소계(발생)\u0026#39;]*100 해당 계산법의 문제:\n이전 연도에 발생한 사건이 많이 검거될 경우 검거율이 100%를 초과 발생 건수가 0인 경우 검거율에 결측치(N/A)가 발생 초과된 검거율을 최댓값으로 조정:\n1 2 # 검거율에 해당되는 열의 집합 columns columns = [\u0026#39;강간검거율\u0026#39;, \u0026#39;강도검거율\u0026#39;, \u0026#39;살인검거율\u0026#39;, \u0026#39;절도검거율\u0026#39;, \u0026#39;폭력검거율\u0026#39;] 모든 행에 대해 반복문 실행 1 2 3 4 for row_index, row in gu_df_rate.","title":"[AI SCHOOL 5기] 데이터 분석 실습 - 데이터 분석"},{"content":"Data Types Structured Data Relational Database Spread Sheets Semi-structured Data System Logs Sensor Data HTML Unstructured Data Image / Video Sound Document Data Collection Tools Logstash: 로그 데이터 (SQL 구조화) Elasticsearch: 데이터가 자유로움 Kibana: 그래프 자동화 Elastic Stack, Zepplin API Meanings 웹 상에서의 API 라이브러리/프로그램 도구 (텐서플로우에서의 함수 등) Open API 공익적인 목적 서비스 활성화 목적 (서드파티 앱 지원) SNS에서 무분별한 크롤링으로 인한 서버 과부하 대비 Missing Data Handling 랜덤하게 채워넣기 주변 (행의) 값들로 채워넣기 열의 대푯값을 계싼해서 채워넣기 (mea, median) 전체 행들을 그룹으로 묶어낸 후 그룹 내 해당 열의 값을 예측해 채워넣기 나머지 열들로 머신러닝 예측모델을 만든 후 해당 열의 값을 예측해 채워넣기 특정 기준 비율 이상으로 빠져있을 시 해당 열 삭제 Pandas Functions Referring df = pd.read_excel(): 엑셀 파일 열기 (엑셀 파일 원본은 행과 열로 구성된 pandas.DataFrame() 타입) df.head(): 위에서부터 값을 참조 (default 5) df.tail(): 밑에서부터 값을 참조 (default 5) df.describe(): 기술 통계량 반환 (평균, 최솟값 등) df.info(): DataFrame 정보 반환 (Non-Null 행에서 유효성 확인) df.loc[row]: DataFrame에서 행 꺼내기 (추가로 column도 지정 가능) df.iloc[row]: DataFrame에서 인덱스 번호를 기준으로 행 꺼내기 df[column]: DataFrame에서 열 꺼내기 df[column].apply(lambda x: x+1): 특정 열에 속한 값에 1씩 더해서 반환 Modifiying df.drop([row]): DataFrame에서 행 삭제 del df[column]: DataFrame에서 열 삭제 df.rename(columns=, inplace=True):\nDataFrame에서 열 이름 바꾸기 (inplace 옵션은 덮어쓰기를 의미) df.sort_values(by=, inplace=True):\nDataFrame에서 열 내부의 값을 정렬 (내림차순 정렬 시 ascending=False 옵션 추가) pd.pivot_table(df, index=, aggfunc=np.mean):\n기존 DataFrame에서 특정 행을 index로 설정한 새로운 DataFrame 생성 (피벗 테이블) aggfunc 옵션에 계산식을 넣을 수 있음 (count, np.sum 등) Copying df_2 = df: 얕은 복사 (원본 변경 시 복사본도 같이 변경) df_3 = df.copy(): 깊은 복사 (원본 변경이 복사본에 영향을 미치지 않음) ","permalink":"https://minyeamer.github.io/blog/aischool-01-00-data-analysis/","summary":"Data Types Structured Data Relational Database Spread Sheets Semi-structured Data System Logs Sensor Data HTML Unstructured Data Image / Video Sound Document Data Collection Tools Logstash: 로그 데이터 (SQL 구조화) Elasticsearch: 데이터가 자유로움 Kibana: 그래프 자동화 Elastic Stack, Zepplin API Meanings 웹 상에서의 API 라이브러리/프로그램 도구 (텐서플로우에서의 함수 등) Open API 공익적인 목적 서비스 활성화 목적 (서드파티 앱 지원) SNS에서 무분별한 크롤링으로 인한 서버 과부하 대비 Missing Data Handling 랜덤하게 채워넣기 주변 (행의) 값들로 채워넣기 열의 대푯값을 계싼해서 채워넣기 (mea, median) 전체 행들을 그룹으로 묶어낸 후 그룹 내 해당 열의 값을 예측해 채워넣기 나머지 열들로 머신러닝 예측모델을 만든 후 해당 열의 값을 예측해 채워넣기 특정 기준 비율 이상으로 빠져있을 시 해당 열 삭제 Pandas Functions Referring df = pd.","title":"[AI SCHOOL 5기] 데이터 분석"},{"content":"문제 링크 https://www.acmicpc.net/problem/4949 개요 스택을 이용하여 풀 수 있는 문제이다. 문자열 처리에 관한 능력이 추가로 요구된다. 최대 입력 크기가 정해지지 않았기에 시간 복잡도는 무시한다. 문제 해설 해당 문제에서 고려해야할 문자는 종료 조건인 점(\u0026rsquo;.\u0026rsquo;)을 제외하면 소괄호와 대괄호 뿐이다. 균형잡힌 문장의 구분 여부는 1. 닫힌 괄호가 열린 괄호보다 앞에 나온 경우 2. 열린 괄호가 안 닫힌 경우로 판단했다. 문자 하나하나마다 확인하며 괄호를 골라낼 수도 있지만 이번엔 정규식을 사용해본다. 우선 정규식 라이브러리인 re에 속한 sub 메서드를 사용해 괄호를 제외한 모든 문자를 제거한다. 나머지 문자에 대해 for문을 돌려 열린 괄호면 스택에 추가, 닫힌 괄호면 스택에 남은 값을 뺀다. 단, 닫힌 괄호의 경우 스택에 열린 괄호가 없거나 스택 맨 위의 값이 다른 종류의 괄호면 균형이 깨졌다 판단한다. 코드의 중복을 발생시키지 않기 위해 균형이 깨진 경우를 IndexError의 발생으로 통일하고\n이 때 조건 변수를 재설정하고 반복문을 중지시킨다. 반복문이 종료된 후 스택과 조건 변수에 대한 NAND 결과를 통해 균형잡힌 문장의 여부를 판단하여 결과를 출력한다. 해설 코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import re match = {\u0026#39;)\u0026#39;: \u0026#39;(\u0026#39;, \u0026#39;]\u0026#39;: \u0026#39;[\u0026#39;} while True: balanced_stack = [] unbalanced = False sentence = input() if sentence == \u0026#39;.\u0026#39;: break sentence = re.sub(\u0026#39;[^\\(\\)\\[\\]]+\u0026#39;, \u0026#39;\u0026#39;, sentence) for bracket in sentence: if bracket in {\u0026#39;(\u0026#39;, \u0026#39;[\u0026#39;}: balanced_stack.append(bracket) else: try: if balanced_stack[-1] == match[bracket]: balanced_stack.pop() else: raise IndexError except IndexError: unbalanced = True break if not(balanced_stack or unbalanced): print(\u0026#39;yes\u0026#39;) else: print(\u0026#39;no\u0026#39;) ","permalink":"https://minyeamer.github.io/blog/boj-problems-4949/","summary":"문제 링크 https://www.acmicpc.net/problem/4949 개요 스택을 이용하여 풀 수 있는 문제이다. 문자열 처리에 관한 능력이 추가로 요구된다. 최대 입력 크기가 정해지지 않았기에 시간 복잡도는 무시한다. 문제 해설 해당 문제에서 고려해야할 문자는 종료 조건인 점(\u0026rsquo;.\u0026rsquo;)을 제외하면 소괄호와 대괄호 뿐이다. 균형잡힌 문장의 구분 여부는 1. 닫힌 괄호가 열린 괄호보다 앞에 나온 경우 2. 열린 괄호가 안 닫힌 경우로 판단했다. 문자 하나하나마다 확인하며 괄호를 골라낼 수도 있지만 이번엔 정규식을 사용해본다. 우선 정규식 라이브러리인 re에 속한 sub 메서드를 사용해 괄호를 제외한 모든 문자를 제거한다.","title":"[백준 4949] 균형잡힌 세상 (Python)"},{"content":"문제 링크 https://www.acmicpc.net/problem/2164 개요 큐를 이용하여 풀 수 있는 간단한 문제이다. 양쪽에서 데이터를 빼고 집어넣는 작업이 요구되기 때문에 deque의 사용을 권장한다. 1번 카드의 위치를 앞으로 하냐 뒤로 하냐는 크게 상관없기 때문에 앞에서부터 정의하겠다. 문제 해설 문제에서 제시된 행동은 1. 제일 위의 카드를 버린다 2. 제일 위에 남은 카드를 제일 아래로 옮긴다 이다. 해당 행동을 카드가 한 장이 남을 때까지 무한히 반복하면 된다. 1번 행동을 하기 위해선 1번 카드를 큐의 맨 앞으로 정했기에 큐의 왼쪽에서 값을 빼내면 된다. 큐의 왼쪽에서 값을 빼내기 위해 popleft() 함수를 사용한다. 2번 행동은 마찬가지로 큐의 왼쪽에서 값을 빼내고, 추가로 빼낸 값을 맨 뒤에 추가한다. 큐의 왼쪽에서 빼낸 값을 다시 넣어야 하기 때문에 append() 안에 popleft()를 넣어준다. 두 가지 동작을 while문 안에 넣고 카드가 1개보다 많이 남으면 반복하도록 조건을 설정한다. while문이 종료된 후 하나의 값이 남아있는 큐에서 pop()을 사용해 마지막 남은 카드를 출력한다. 시간 복잡도 문제에서 주어진 시간은 2초다. 가장 단순하게 리스트로 큐를 구현할 경우 del() 함수를 이용해 값을 삭제해야 할 것이고\n이를 N번 만큼 반복할 것이므로 이 때의 시간 복잡도는 O(N^2)를 초과한다. N의 최댓값이 500,000이므로 대충 어림잡아도 연산 횟수가 2억을 훌쩍 초과한다. 반면, deque로 큐를 구현한 해설의 경우 시간 복잡도가 O(1)인 popleft()를\nN번 만큼 반복하기 때문에 O(N)의 시간 복잡도를 가진다. 이는 제한 시간 내에 충분히 수행하고도 여유가 남을 알고리즘이다. 해설 코드 1 2 3 4 5 6 7 8 from collections import deque N = int(input()) cards = deque([i for i in range(1, N+1)]) while len(cards) \u0026gt; 1: cards.popleft() cards.append(cards.popleft()) print(cards.pop()) ","permalink":"https://minyeamer.github.io/blog/boj-problems-2164/","summary":"문제 링크 https://www.acmicpc.net/problem/2164 개요 큐를 이용하여 풀 수 있는 간단한 문제이다. 양쪽에서 데이터를 빼고 집어넣는 작업이 요구되기 때문에 deque의 사용을 권장한다. 1번 카드의 위치를 앞으로 하냐 뒤로 하냐는 크게 상관없기 때문에 앞에서부터 정의하겠다. 문제 해설 문제에서 제시된 행동은 1. 제일 위의 카드를 버린다 2. 제일 위에 남은 카드를 제일 아래로 옮긴다 이다. 해당 행동을 카드가 한 장이 남을 때까지 무한히 반복하면 된다. 1번 행동을 하기 위해선 1번 카드를 큐의 맨 앞으로 정했기에 큐의 왼쪽에서 값을 빼내면 된다.","title":"[백준 2164] 카드2 (Python)"},{"content":"References Name Link HackerRank https://www.hackerrank.com/domains/sql Leetcode https://leetcode.com/problemset/all/ Programmers https://school.programmers.co.kr/learn/challenges Baekjoon https://www.acmicpc.net/ solved.ac https://solved.ac/ Big-O List minyeamer/references/big-o.md Math Reference Number Level Problem Name Subcategory Problem Link Solution Link Programmers 60059 Lv3 🥇 자물쇠와 열쇠 Matrix Link Link Programmers 17687 Lv2 🥈 n진수 게임 Link Link Programmers 12977 Lv1 🥉 소수 만들기 Combination Link Link Baekjoon 1010 Silver 🥈 다리 놓기 Combination Link Link Baekjoon 1463 Silver 🥈 1로 만들기 Link Link Baekjoon 1676 Silver 🥈 팩토리얼 0의 개수 Link Link Baekjoon 2108 Silver 🥈 통계학 Link Link Baekjoon 1049 Silver 🥈 기타줄 Link Link Baekjoon 2475 Bronze 🥉 검증수 Link Link Baekjoon 2609 Bronze 🥉 최대공약수와 최소공배수 Combination Link Link Baekjoon 11050 Bronze 🥉 이항 계수 1 Combination Link Link String Reference Number Level Problem Name Subcategory Problem Link Solution Link Programmers 60057 Lv2 🥈 문자열 압축 Link Link Programmers 72410 Lv1 🥉 신규 아이디 추천 Regular Expression Link Link Baekjoon 22859 Gold 🥇 HTML 파싱 Regular Expression Link Link Baekjoon 15829 Bronze 🥉 Hashing Hashing Link Link Hash Table Reference Number Level Problem Name Subcategory Problem Link Solution Link Programmers 42577 Lv2 🥈 전화번호 목록 Link Link Programmers 42578 Lv2 🥈 위장 Set Link Link Programmers 42888 Lv2 🥈 오픈채팅방 Link Link Programmers 1845 Lv1 🥉 폰켓몬 Link Link Programmers 42576 Lv1 🥉 완주하지 못한 선수 Counter Link Link Programmers 92334 Lv1 🥉 신고 결과 받기 Dictionary Link Link Programmers 81301 Lv1 🥉 숫자 문자열과 영단어 Dictionary Link Link Baekjoon 1620 Silver 🥈 나는야 포켓몬 마스터 이다솜 Dictionary Link Link Baekjoon 1764 Silver 🥈 듣보잡 Set Link Link Baekjoon 1920 Silver 🥈 수 찾기 Set Link Link Baekjoon 10816 Silver 🥈 숫자 카드 2 Counter Link Link Stack/Queue Reference Number Level Problem Name Subcategory Problem Link Solution Link Programmers 17680 Lv2 🥈 캐시 Deque Link Link Baekjoon 5430 Gold 🥇 AC Deque Link Link Baekjoon 1874 Silver 🥈 스택 수열 Stack Link Link Baekjoon 1966 Silver 🥈 프린터 큐 Deque Link Link Baekjoon 2164 Silver 🥈 카드2 Queue Link Link Baekjoon 4949 Silver 🥈 균형잡힌 세상 Stack Link Link Baekjoon 9012 Silver 🥈 괄호 Stack Link Link Baekjoon 10773 Silver 🥈 제로 Stack Link Link Baekjoon 10828 Silver 🥈 스택 Stack Link Link Baekjoon 10845 Silver 🥈 큐 Queue Link Link Baekjoon 10866 Silver 🥈 덱 Deque Link Link Baekjoon 11866 Silver 🥈 요세푸스 문제 0 Stack Link Link Heap Reference Number Level Problem Name Subcategory Problem Link Solution Link Baekjoon 7662 Gold 🥇 이중 우선순위 큐 Guidance Link Link Baekjoon 1927 Silver 🥈 최소 힙 Guidance Link Link Sorting Reference Number Level Problem Name Subcategory Problem Link Solution Link Leetcode 1337 Easy 🥉 The K Weakest Rows\nin a Matrix Link Link Programmers 17686 Lv2 🥈 파일명 정렬 Link Link Programmers 42746 Lv2 🥈 가장 큰 수 Link Link Programmers 42747 Lv2 🥈 H-Index Link Link Programmers 42748 Lv1 🥉 K번째수 Link Link Baekjoon 1181 Silver 🥈 단어 정렬 Link Link Baekjoon 1431 Silver 🥈 시리얼 번호 Link Link Baekjoon 10814 Silver 🥈 나이순 정렬 Link Link Baekjoon 10816 Silver 🥈 숫자 카드 2 Link Link Baekjoon 11650 Silver 🥈 좌표 정렬하기 2 Link Link Baekjoon 18870 Silver 🥈 좌표 압축 Link Link Baekjoon 1259 Bronze 🥉 팰린드롬수 Link Link Baekjoon 2920 Bronze 🥉 음계 Link Link Brute Force Reference Number Level Problem Name Subcategory Problem Link Solution Link Programmers 17683 Lv2 🥈 방금그곡 Link Link Programmers 17684 Lv2 🥈 압축 Link Link Programmers 42842 Lv2 🥈 카펫 Link Link Programmers 42840 Lv1 🥉 모의고사 Link Link Baekjoon 1107 Gold 🥇 리모컨 Link Link Baekjoon 15686 Gold 🥇 치킨 배달 Combination Link Link Baekjoon 1182 Silver 🥈 부분수열의 합 Link Link Baekjoon 18111 Silver 🥈 마인크래프트 Link Link Greedy Reference Number Level Problem Name Subcategory Problem Link Solution Link Programmers 17676 Lv3 🥇 추석 트래픽 Sliding Window Link Link Programmers 62048 Lv2 🥈 멀쩡한 사각형 Link Link Programmers 87390 Lv2 🥈 n^2 배열 자르기 Link Link Baekjoon 1049 Silver 🥈 기타줄 Link Link Baekjoon 1105 Silver 🥈 팔 Link Link Baekjoon 1541 Silver 🥈 잃어버린 괄호 Link Link Baekjoon 1931 Silver 🥈 회의실 배정 Sliding Window Link Link Baekjoon 11399 Silver 🥈 ATM Link Link Simulation Reference Number Level Problem Name Subcategory Problem Link Solution Link Baekjoon 16918 Lv2 🥈 봄버맨 Link Link Dynamic Programming Reference Number Level Problem Name Subcategory Problem Link Solution Link Programmers 42895 Lv3 🥇 N으로 표현 Link Link Programmers 43105 Lv3 🥇 정수 삼각형 Link Link Baekjoon 1003 Silver 🥈 피보나치 함수 Link Link Baekjoon 1463 Silver 🥈 1로 만들기 Link Link Baekjoon 1495 Silver 🥈 기타리스트 Link Link Baekjoon 1697 Silver 🥈 숨바꼭질 Link Link Baekjoon 2302 Silver 🥈 극장 좌석 Link Link Baekjoon 2579 Silver 🥈 계단 오르기 Link Link Two Pointer Reference Number Level Problem Name Subcategory Problem Link Solution Link Baekjoon 20922 Silver 🥈 겹치는 건 싫어 Link Link DFS/BFS Reference Number Level Problem Name Subcategory Problem Link Solution Link Programmers 43162 Lv3 🥇 네트워크 DFS Link Link Programmers 43163 Lv3 🥇 단어 변환 BFS Link Link Programmers 1844 Lv2 🥈 게임 맵 최단거리 BFS Link Link Programmers 43165 Lv2 🥈 타겟 넘버 DFS Link Link Programmers 87946 Lv2 🥈 피로도 DFS Link Link Baekjoon 5547 Gold 🥇 일루미네이션 BFS Link Link Baekjoon 7569 Gold 🥇 토마토 BFS Link Link Baekjoon 7576 Gold 🥇 토마토 BFS Link Link Baekjoon 10026 Gold 🥇 적록색약 BFS Link Link Baekjoon 1012 Silver 🥈 유기농 배추 DFS Link Link Baekjoon 1260 Silver 🥈 DFS와 BFS Guidance Link Link Baekjoon 1389 Silver 🥈 케빈 베이컨의 6단계 법칙 BFS Link Link Baekjoon 2606 Silver 🥈 바이러스 DFS Link Link Baekjoon 11724 Silver 🥈 연결 요소의 개수 DFS Link Link Baekjoon 11725 Silver 🥈 트리의 부모 찾기 BFS Link Link Baekjoon 16987 Silver 🥈 계란으로 계란치기 Backtracking Link Link Baekjoon 18352 Silver 🥈 특정 거리의 도시 찾기 BFS Link Link Binary Search Reference Number Level Problem Name Subcategory Problem Link Solution Link Programmers 43238 Lv3 🥇 입국심사 Link Link Programmers 68936 Lv2 🥈 쿼드압축 후 개수 세기 Divide Link Link Baekjoon 1654 Silver 🥈 랜선 자르기 Link Link Baekjoon 1780 Silver 🥈 종이의 개수 Divide Link Link Baekjoon 2630 Silver 🥈 색종이 만들기 Divide Link Link Baekjoon 2805 Silver 🥈 나무 자르기 Link Link Graph Reference Number Level Problem Name Subcategory Problem Link Solution Link Programmers 49189 Lv3 🥇 가장 먼 노드 BFS Link Link Programmers 49191 Lv3 🥇 순위 Dictionary Link Link Programmers 77486 Lv3 🥇 다단계 칫솔 판매 Union-Find Link Link Baekjoon 1197 Gold 🥇 최소 스패닝 트리 Prim Link Link Tree Reference Number Level Problem Name Subcategory Problem Link Solution Link Leetcode 236 Medium 🥈 Lowest Common Ancestor\nof a Binary Tree Link Link Others Reference Number Level Problem Name Subcategory Problem Link Solution Link Programmers 92344 Lv3 🥇 파괴되지 않은 건물 Prefix Sum Link Link Programmers 92341 Lv2 🥈 주차 요금 계산 Time Link Link Baekjoon 21758 Gold 🥇 꿀 따기 Prefix Sum Link Link Baekjoon 1074 Silver 🥈 Z Recursion Link Link Baekjoon 1308 Silver 🥈 D-Day Time Link Link Baekjoon 21318 Silver 🥈 피아노 체조 Prefix Sum Link Link SQL Reference Level Problem Name Subcategory Problem Link Solution Link HackerRank Advanced 🥇 New Companies Join, Count Link Link ","permalink":"https://minyeamer.github.io/blog/solved-problems/","summary":"References Name Link HackerRank https://www.hackerrank.com/domains/sql Leetcode https://leetcode.com/problemset/all/ Programmers https://school.programmers.co.kr/learn/challenges Baekjoon https://www.acmicpc.net/ solved.ac https://solved.ac/ Big-O List minyeamer/references/big-o.md Math Reference Number Level Problem Name Subcategory Problem Link Solution Link Programmers 60059 Lv3 🥇 자물쇠와 열쇠 Matrix Link Link Programmers 17687 Lv2 🥈 n진수 게임 Link Link Programmers 12977 Lv1 🥉 소수 만들기 Combination Link Link Baekjoon 1010 Silver 🥈 다리 놓기 Combination Link Link Baekjoon 1463 Silver 🥈 1로 만들기 Link Link Baekjoon 1676 Silver 🥈 팩토리얼 0의 개수 Link Link Baekjoon 2108 Silver 🥈 통계학 Link Link Baekjoon 1049 Silver 🥈 기타줄 Link Link Baekjoon 2475 Bronze 🥉 검증수 Link Link Baekjoon 2609 Bronze 🥉 최대공약수와 최소공배수 Combination Link Link Baekjoon 11050 Bronze 🥉 이항 계수 1 Combination Link Link String Reference Number Level Problem Name Subcategory Problem Link Solution Link Programmers 60057 Lv2 🥈 문자열 압축 Link Link Programmers 72410 Lv1 🥉 신규 아이디 추천 Regular Expression Link Link Baekjoon 22859 Gold 🥇 HTML 파싱 Regular Expression Link Link Baekjoon 15829 Bronze 🥉 Hashing Hashing Link Link Hash Table Reference Number Level Problem Name Subcategory Problem Link Solution Link Programmers 42577 Lv2 🥈 전화번호 목록 Link Link Programmers 42578 Lv2 🥈 위장 Set Link Link Programmers 42888 Lv2 🥈 오픈채팅방 Link Link Programmers 1845 Lv1 🥉 폰켓몬 Link Link Programmers 42576 Lv1 🥉 완주하지 못한 선수 Counter Link Link Programmers 92334 Lv1 🥉 신고 결과 받기 Dictionary Link Link Programmers 81301 Lv1 🥉 숫자 문자열과 영단어 Dictionary Link Link Baekjoon 1620 Silver 🥈 나는야 포켓몬 마스터 이다솜 Dictionary Link Link Baekjoon 1764 Silver 🥈 듣보잡 Set Link Link Baekjoon 1920 Silver 🥈 수 찾기 Set Link Link Baekjoon 10816 Silver 🥈 숫자 카드 2 Counter Link Link Stack/Queue Reference Number Level Problem Name Subcategory Problem Link Solution Link Programmers 17680 Lv2 🥈 캐시 Deque Link Link Baekjoon 5430 Gold 🥇 AC Deque Link Link Baekjoon 1874 Silver 🥈 스택 수열 Stack Link Link Baekjoon 1966 Silver 🥈 프린터 큐 Deque Link Link Baekjoon 2164 Silver 🥈 카드2 Queue Link Link Baekjoon 4949 Silver 🥈 균형잡힌 세상 Stack Link Link Baekjoon 9012 Silver 🥈 괄호 Stack Link Link Baekjoon 10773 Silver 🥈 제로 Stack Link Link Baekjoon 10828 Silver 🥈 스택 Stack Link Link Baekjoon 10845 Silver 🥈 큐 Queue Link Link Baekjoon 10866 Silver 🥈 덱 Deque Link Link Baekjoon 11866 Silver 🥈 요세푸스 문제 0 Stack Link Link Heap Reference Number Level Problem Name Subcategory Problem Link Solution Link Baekjoon 7662 Gold 🥇 이중 우선순위 큐 Guidance Link Link Baekjoon 1927 Silver 🥈 최소 힙 Guidance Link Link Sorting Reference Number Level Problem Name Subcategory Problem Link Solution Link Leetcode 1337 Easy 🥉 The K Weakest Rows","title":"[Index] Solved Problems"},{"content":"List Operation Example Big-O Index l[i] O(1) Store l[i] = 0 O(1) Length len(l) O(1) Append l.append(x) O(1) Pop l.pop() O(1) Slice l[a:b] O(b-a) Construction list(x) O(len(x)) Check l1 == l2 O(len(n)) Insert l[a:b] = x O(n) Containment x in l O(n) Copy l.copy() O(n) Remove l.remove() O(n) Count l.count(x) O(n) Index l.index(x) O(n) Pop l.pop(i) O(n) Extreme value min(l)/max(l) O(n) Iteration for v in l: O(n) Reverse l.reverse() O(n) Sort l.sort() O(n Log n) Multiply k * l O(k n) Set Operation Example Big-O Length len(s) O(1) Add s.add(x) O(1) Containment x in s O(1) Remove s.remove() O(1) Pop s.pop() O(1) Construction set(x) O(len(x)) Check s1 == s2 O(len(s)) Union s + t O(len(s)+len(t)) Intersection s \u0026amp; t O(len(s)+len(t)) Difference s - t O(len(s)+len(t)) Symmetric Diff s ^ t O(len(s)+len(t)) Iteration for v in s: O(n) Copy s.copy() O(n) Dictionary Operation Example Big-O Index d[k] O(1) Store d[k] = v O(1) Length len(d) O(1) Pop d.pop() O(1) View d.keys() O(1) Construction dict(x) O(len(x)) Iteration for k in d: O(n) Sort Method Best Average Worst Insertion Sort O(n) O(n^2) O(n^2) Selection Sort O(n^2) O(n^2) O(n^2) Bubble Sort O(n^2) O(n^2) O(n^2) Shell Sort O(n) O(n^1.5) O(n^1.5) Quick Sort O(n log n) O(n log n) O(n^2) Heap Sort O(n log n) O(n log n) O(n log n) Merge Sort O(n log n) O(n log n) O(n log n) Radix Sort O(dn) O(dn) O(dn) Search Method Search Insert Delete Sequential O(n) O(1) O(n) Binary O(log n) O(log n + n) O(log n + n) Binary Search Tree (Balanced) O(log n) O(log n) O(log n) Binary Search Tree (Left-Associative) O(n) O(n) O(n) Hashing (Best) O(1) O(1) O(1) Hashing (Worst) O(n) O(n) O(n) Heap Operation Example Big-O Push heapq.heappush(heap, x) O(log n) Pop heapq.heappop(heap) O(log n) Construction heapq.heapify(heap) O(n) DFS/BFS N은 노드, E는 간선일 때\n인접 리스트: O(N+E) 인접 행렬: O(N^2) ","permalink":"https://minyeamer.github.io/blog/big-o-list/","summary":"List Operation Example Big-O Index l[i] O(1) Store l[i] = 0 O(1) Length len(l) O(1) Append l.append(x) O(1) Pop l.pop() O(1) Slice l[a:b] O(b-a) Construction list(x) O(len(x)) Check l1 == l2 O(len(n)) Insert l[a:b] = x O(n) Containment x in l O(n) Copy l.copy() O(n) Remove l.remove() O(n) Count l.count(x) O(n) Index l.index(x) O(n) Pop l.pop(i) O(n) Extreme value min(l)/max(l) O(n) Iteration for v in l: O(n) Reverse l.reverse() O(n) Sort l.","title":"Big-O List"},{"content":"Crawling 크롤러는 웹 페이지의 데이터를 모아주는 소프트웨어 크롤링은 크롤러를 사용해 웹 페이지의 데이터를 추출해 내는 행위 Request request 모듈의 get() 함수는 서버에게 html 정보를 요청 get() 함수는 url, 파라미터 값을 받고 request.Response를 반환 정상적인 응답을 받을 경우 Response [200] 반환 응답값을 reponse 변수에 넣고 response.text를 출력하면 html 코드 출력 BeautifulSoup bs4 모듈의 BeautifulSoup 기능은 입력값을 의미있는 데이터로 변환 1 2 3 4 5 soup = BeautifulSoup(response.text, \u0026#39;html.parser\u0026#39;) soup.title # html 코드에서 title에 해당하는 태그를 반환 soup.title.string # title 태그에서 문자열 값만 뽑아 반환 soup.findAll(\u0026#39;span\u0026#39;) # 모든 span 태그를 반환 soup.findAll(\u0026#39;a\u0026#39;, \u0026#39;link_favorsch\u0026#39;) # link_favorsch 클래스만 반환 1 2 results = soup.findAll(\u0026#39;a\u0026#39;, \u0026#39;link_favorsch\u0026#39;) result.get_text() # result에서 태그를 제외하고 텍스트만 반환 File open(file, mode): 파일을 생성, 참조, 수정할 때 사용하는 라이브러리 mode에는 r (read), w (write), a (append)가 있음 1 2 file = open(\u0026#34;rankresult.txt\u0026#34;, \u0026#34;w\u0026#34;) file.write(result.get_text()+\u0026#34;\\n\u0026#34;) 1 2 file = open(\u0026#34;rankresult.txt\u0026#34;, \u0026#34;a\u0026#34;) file.write(result.get_text()+\u0026#34;\\n\u0026#34;) API API는 누군가가 만든 프로그램을 가져와서 사용할 때 필요한 인터페이스 API Key는 API를 누가 사용하는지 알 수 있는 키 OpenWeatherMap API Current Weather Data API 사용 https://api.openweathermap.org/data/2.5/weather?q={city name}\u0026amp;appid={API key} f-string을 이용하여 city name과 API Key를 변수로 설정 requests.get(api)로 API 요청 API 파라미터에 lang을 kr로 추가하여 반환값을 한국어로 변경 API 파라미터에 units를 metric으로 추가하여 온도 단위르 섭씨로 변경 JSON 자바스크립트의 오브젝트에 따르는 문자 기반 데이터 포맷 json.loads(str)로 문자열을 JSON (딕셔너리 타입)으로 변경 Translator googletrans: 언어 감지 및 번역을 도와주는 라이브러리 googletrans의 Translator를 import하고 Translator()로 Translator 생성 1 2 3 4 5 \u0026gt;\u0026gt;\u0026gt; translator.detect(sentence) # 문장에 대한 언어 감지 결과를 반환 \u0026gt;\u0026gt;\u0026gt; Detected(lang=ko, confidence=1.0) \u0026gt;\u0026gt;\u0026gt; translator.translate(sentence, \u0026#39;en\u0026#39;) # 문장에 대한 변역 결과를 반환 \u0026gt;\u0026gt;\u0026gt; Translated(src=ko, dest=en, text=Hello, ... Mail IMAP 다른 메일 서버에서 보낸 메일을 클라이언트에게 보내기 위한 프로토콜 SMTP 간단하게 메일을 보내기 위한 프로토콜 SMTP 메일 서버를 연결한다. (smtp.gmail.com:465) 1 smtp = smtplib.SMTP_SSL(SMTP_SERVER, SMTP_PORT) SMTP 메일 서버에 로그인한다. 1 smtp.login(\u0026#39;MAIL_ADDRESS\u0026#39;, \u0026#39;PASSWORD\u0026#39;) SMTP 메일 서버에 메일을 보내고 연결을 끊는다. 1 2 smtp.send_message() smtp.quit() MIME 전자우편을 위한 인터넷 표준 포맷 email.message 모듈의 .EmailMessage기능 사용 MIME의 Header에는 Subject, To 등이 존재 이메일을 만든다. 1 message = EmailMessage() 이메일에 내용을 담는다. 1 message.set_content(\u0026#39;content\u0026#39;) 발신자, 수신자를 설정한다. 1 2 3 message[\u0026#39;Subject\u0026#39;] = \u0026#39;subject\u0026#39; message[\u0026#39;from\u0026#39;] = \u0026#39;user1@gmail.com\u0026#39; message[\u0026#39;To\u0026#39;] = \u0026#39;user2@gmail.com Attach Image Read Image 1 2 with open(\u0026#39;codelion.png\u0026#39;,\u0026#39;rb\u0026#39;) as image: image_file = image.read() Attach Image 1 2 # 이미지 파일 첨부 (메일 형식이 mixed로 바뀜) add_attachment(image, maintype=\u0026#39;image\u0026#39;, subtype=\u0026#39;png\u0026#39;) 이미지 파일의 확장자를 판단 1 imghdr.what(\u0026#39;filename\u0026#39;, image) Validation 정규표현식\n^①[a-zA-Z0-9.+_-]+@②[a-zA-Z0-9]+③\\.[a-zA-z]{2,3}$\n① [a부터 z까지, A부터 Z까지, 0부터 9까지, . , + , _ , -] | +: 1회 이상 반복\n② @ | [a부터 z까지, A부터 Z까지, 0부터 9까지] | +: 1회 이상 반복\n③ . (개행문자 ) | [a부터 z까지, A부터 Z까지] | {2,3}: 최소 2회, 최대 3번 반복 1 re.match(reg, \u0026#39;example@gmail.com\u0026#39;) 적합하지 않은 이메일 형식일 경우 None을 반환 if문을 사용해 None이 아니면 메일을 보내도록 설정 Others 함수: 입력한 값을 사용해 결과물을 만들어 반환하는 조립기 모듈: 함수들을 모아놓은 파일 type(): 객체의 타입을 반환 datetime.today().strftime(\u0026quot;%Y년 %m월 %d일\u0026quot;): 오늘의 날짜 반환 로봇이 아님을 알리는 헤더\n1 headers = {\u0026#39;User-Agent\u0026#39;:\u0026#39;Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36\u0026#39;} ","permalink":"https://minyeamer.github.io/blog/aischool-00-02-python-advanced/","summary":"Crawling 크롤러는 웹 페이지의 데이터를 모아주는 소프트웨어 크롤링은 크롤러를 사용해 웹 페이지의 데이터를 추출해 내는 행위 Request request 모듈의 get() 함수는 서버에게 html 정보를 요청 get() 함수는 url, 파라미터 값을 받고 request.Response를 반환 정상적인 응답을 받을 경우 Response [200] 반환 응답값을 reponse 변수에 넣고 response.text를 출력하면 html 코드 출력 BeautifulSoup bs4 모듈의 BeautifulSoup 기능은 입력값을 의미있는 데이터로 변환 1 2 3 4 5 soup = BeautifulSoup(response.text, \u0026#39;html.parser\u0026#39;) soup.title # html 코드에서 title에 해당하는 태그를 반환 soup.","title":"[코드라이언] 파이썬 심화"},{"content":"for문 문장을 여러 번 실행할 떄 복사 붙여넣기로 길게 늘이지 않고 단순하게 표현하기 위한 구문 for문에 적용되는 문장은 들여쓰기를 해야 함 1 2 for _ in range(30): print(random.choice([\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;])) while문 for문과 마찬가지로 문장을 반복실행할 수 있는 구문 조건을 충족할 경우 반복을 멈춤 True를 조건으로 사용 시 무한루프 발생 while True: break 명령어를 통해 반복문 종료 가능 변수 객체에 이름표를 붙이고 이름표가 불리면 내용물인 객체를 반환\nlunch = random.choice([\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;]) 딕셔너리 \u0026ldquo;xx은 xx이다\u0026quot;를 코드로 표현한 자료구조 딕셔너리의 get 명령어는 Key에 해당하는 값을 반환 값을 추가할 때는 dict[a] = b 형식으로 추가 딕셔너리의 clear 명령어는 딕셔너리 내용을 초기화 집합 중복된 값을 제거하여 표현하는 자료구조 set()으로 집합 생성 합집합: set1 | set2 교집합: set1 \u0026amp; set2 차집합: set1 - set2 조건문 상황에 따른 처리를 하기 위한 구문 if 조건:으로 조건문 선언 같은 경우를 구할 땐 a == b 나머지 경우에 대해서는 else 사용 pip/conda pip: 파이썬에서 지원받는 패키지만을 가져옴 (라이브러리만 맞으면 설치) conda: 아나콘다에서 지원받는 패키지만을 가져옴 (아나콘다에서 유리) conda의 장점: 기존 Python 및 라이브러리 버전 충돌을 체크함 conda의 단점: 설치 속도가 너무 느림 설치가 너무 느리거나 다른 라이브러리에 대한 영향이 없을 경우 pip 사용 라이브러리 참조 파일 생성 시 pip install -r requirements.txt 기타 명령어 random.choice(): 리스트 안에서 랜덤한 객체 하나를 반환 time.sleep(): 입력값만큼의 시간(초) 동안 딜레이 발생 len(): 리스트/딕셔너리의 목록 개수 반환 ","permalink":"https://minyeamer.github.io/blog/aischool-00-01-python-basic/","summary":"for문 문장을 여러 번 실행할 떄 복사 붙여넣기로 길게 늘이지 않고 단순하게 표현하기 위한 구문 for문에 적용되는 문장은 들여쓰기를 해야 함 1 2 for _ in range(30): print(random.choice([\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;])) while문 for문과 마찬가지로 문장을 반복실행할 수 있는 구문 조건을 충족할 경우 반복을 멈춤 True를 조건으로 사용 시 무한루프 발생 while True: break 명령어를 통해 반복문 종료 가능 변수 객체에 이름표를 붙이고 이름표가 불리면 내용물인 객체를 반환\nlunch = random.choice([\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;]) 딕셔너리 \u0026ldquo;xx은 xx이다\u0026quot;를 코드로 표현한 자료구조 딕셔너리의 get 명령어는 Key에 해당하는 값을 반환 값을 추가할 때는 dict[a] = b 형식으로 추가 딕셔너리의 clear 명령어는 딕셔너리 내용을 초기화 집합 중복된 값을 제거하여 표현하는 자료구조 set()으로 집합 생성 합집합: set1 | set2 교집합: set1 \u0026amp; set2 차집합: set1 - set2 조건문 상황에 따른 처리를 하기 위한 구문 if 조건:으로 조건문 선언 같은 경우를 구할 땐 a == b 나머지 경우에 대해서는 else 사용 pip/conda pip: 파이썬에서 지원받는 패키지만을 가져옴 (라이브러리만 맞으면 설치) conda: 아나콘다에서 지원받는 패키지만을 가져옴 (아나콘다에서 유리) conda의 장점: 기존 Python 및 라이브러리 버전 충돌을 체크함 conda의 단점: 설치 속도가 너무 느림 설치가 너무 느리거나 다른 라이브러리에 대한 영향이 없을 경우 pip 사용 라이브러리 참조 파일 생성 시 pip install -r requirements.","title":"[코드라이언] 파이썬 기초"},{"content":"AI SCHOOL 지원 과정 아직 군에 복무 중이던 시절, 전역한 후 바로 취업하기 위해 국비, 부트캠프 과정을 탐색하던 중 AI SCHOOL을 발견했다. 이떄 개인적으로 가격 비교, 사용자 맞춤 추천 등의 기능을 포함한 서비스를 구상하고 있었는데 AI 기술이 바로 그것이었다. AI SCHOOL과 함께 눈에 들었던 게 SW마에스트로였지만 5월까지는 군인 신분인 나와는 맞지 않아 아쉽게 포기했다. AI SCHOOL의 지원 과정은 서류(자기소개서)와 과제(영상) 순으로 진행되었다. 영상을 찍어야 할 때 아직 군대 안에 있었기에 어려웠지만 모종의 방법으로 촬영에 성공했다. 이때 처음으로 영상 편집 프로그램 중 다빈치 리졸브를 사용했는데 꽤 재미있었다. 다행히 AI SCHOOL에 합격했고 조기전역 후 곧바로 데스크 세팅에 들어갔다. 첫 주차 온라인 강의 AI SCHOOL 첫 날에 오리엔테이션을 진행하고 이 날을 포함한 4일 동안 온라인 강의를 수강했다. 일단 만드는 PYTHON, [기초] 같이 푸는 PYTHON 터미널에서 실행할 수 있는 간단한 기능을 파이썬 기초 문법만을 사용해 구현하는 강의다. 리스트, 딕셔너리, 집합 등의 자료구조와 for문, while문 등의 반복문을 배울 수 있었다. [심화] 같이 푸는 PYTHON 웹을 통해 실행할 수 있는 기능을 파이썬을 통해 구현하는 강의다. 함수나 클래스 등을 배우지 않고 바로 넘어가는 느낌은 있었지만 속도감 있어서 좋았다. 크롤링, API, 구글 번역기, 메일 전송 등의 기술을 다뤘다. 파이썬으로는 알고리즘 밖에 안해봤기에 새로운 라이브러리와 마주하여 굉장히 즐겁게 따라하면서 배웠다. 이번 주는 아직 전역 후 데스크 및 물건 정리 등으로 많이 바빠 차분히 공부하지 못했던게 아쉬웠다. 다행히 금요일 전까지 데스크 세팅은 맞춰서 정상적으로 줌에 참여할 수 있었다. 줌을 통한 첫 번째 수업은 주피터 노트북의 사용법을 익히고 앞서 온라인 강의를 통해 배웠던 파이썬 문법을 다듬었다. 1회차 챕틀리 및 특강 코드라이언 온라인 강의가 끝난 시점부터 1회차 챕틀리(도전과제)가 진행되었다. 딥러닝에 대한 이해 전무한 나로서 numpy 라이브러리를 사용하는 과제는 많이 생소했다. 어찌저찌 검색하며 해결할 순 있었고 검색 중에 슬쩍 본 퍼셉트론 등의 개념에 흥미가 생겼다. 토요일 오후에 챕틀리 해설을 위한 특별 강의가 진행되었다.\n(수강생들을 위해 주말에도 업무하시는 매니저님들께 감사를 표합니다.) 초청 강사 분을 통해 진행된 특별강의에서는 numpy 라이브러리를 다루는 법에 대해 배웠다. 이미 고생하면서 찾아본 내용을 복습하는 기분이었지만 제출한 코드에 문제가 없음을 확인할 수 있었다. ","permalink":"https://minyeamer.github.io/blog/ai-school-00-00-start/","summary":"AI SCHOOL 지원 과정 아직 군에 복무 중이던 시절, 전역한 후 바로 취업하기 위해 국비, 부트캠프 과정을 탐색하던 중 AI SCHOOL을 발견했다. 이떄 개인적으로 가격 비교, 사용자 맞춤 추천 등의 기능을 포함한 서비스를 구상하고 있었는데 AI 기술이 바로 그것이었다. AI SCHOOL과 함께 눈에 들었던 게 SW마에스트로였지만 5월까지는 군인 신분인 나와는 맞지 않아 아쉽게 포기했다. AI SCHOOL의 지원 과정은 서류(자기소개서)와 과제(영상) 순으로 진행되었다. 영상을 찍어야 할 때 아직 군대 안에 있었기에 어려웠지만 모종의 방법으로 촬영에 성공했다.","title":"[AI SCHOOL 5기] 첫 주차"},{"content":"defaultdict() collections 모듈에 포함된 dict의 서브 클래스 dict와 작동 방식은 동일하지만 인자로 주어진 객체의 기본값을 초기값으로 지정 가능 1 2 3 \u0026gt;\u0026gt;\u0026gt; int_dict = defaultdict(int) \u0026gt;\u0026gt;\u0026gt; int_dict \u0026gt;\u0026gt;\u0026gt; defaultdict(\u0026lt;class \u0026#39;int\u0026#39;\u0026gt;, {}) int를 인자로 넣을 경우 값을 지정하지 않은 키는 그 값이 0으로 지정됨 1 2 3 4 \u0026gt;\u0026gt;\u0026gt; int_dict[\u0026#39;key1\u0026#39;] 0 \u0026gt;\u0026gt;\u0026gt; int_dict defaultdict(\u0026lt;class \u0026#39;int\u0026#39;\u0026gt;, {\u0026#39;key\u0026#39;: 0}) infinite 양의 무한대 float('inf') 음의 무한대 float('-inf') Prim\u0026rsquo;s Algorithm 시작 정점을 선택한 후, 정점에 인접한 간선 중 최소 비용의 간선을 연결하여\n최소 신장 트리(MST)를 확장해가는 방식 Kruskal\u0026rsquo;s Algorithm이 비용이 가장 작은 간선부터 다음 간선을 선택하는데 반해,\nPrim\u0026rsquo;s Algorithm은 특정 정점에서부터 다음 정점을 갱신해나가며 비용이 작은 간선을 선택 Prim\u0026rsquo;s Algorithm의 시간 복잡도는 최악의 경우 O(E log E)\n(while 구문에서 모든 간선에 대해 반복하고, 최소 힙 구조를 사용) Reference: www.fun-coding.org/Chapter20-prim-live.html 파이썬 구현 코드\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def prim(edge_list: list, start_node: int) -\u0026gt; list: mst = list() adjacent_edge_list = defaultdict(list) for weight, n1, n2 in edge_list: adjacent_edge_list[n1].append((weight, n1, n2)) adjacent_edge_list[n1].append((weight, n2, n1)) connected_nodes = {start_node} candidate_edge_list = adjacent_edge_list[start_node] heapq.heapify(candidate_edge_list) while candidate_edge_list: weight, n1, n2 = heapq.heappop(candidate_edge_list) if n2 not in connected_nodes: connected_nodes.add(n2) mst.append((weight, n1, n2)) for edge in adjacent_edge_list[n2]: if edge[2] not in connected_nodes: heapq.heappush(candidate_edge_list, edge) return mst Prim\u0026rsquo;s Algorithm 개선 간선이 아닌 노드를 중심으로 우선순위 큐를 적용 노드마다 Key 값을 가지고 있고, Key 값을 우선순위 큐에 넣음 Key 값이 0인 정점의 인접한 정점들에 대해 Key 값과 연결된 비용을 비교하여\nKey 값이 작으면 해당 정점의 Key 값을 갱신 개선된 Prim\u0026rsquo;s Algorithm의 시간 복잡도는 O(E log V) 해당 알고리즘을 구현하기 위해 heapdict 라이브러리 사용\n(기존의 heap 내용을 업데이트하면 알아서 최소 힙의 구조로 업데이트됨) 파이썬 구현 코드\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from heapdict import heapdict def prim(graph: dict, start_node: int) -\u0026gt; (list, int): mst, keys, pi, total_weight = list(), heapdict(), dict(), 0 for node in graph.keys(): keys[node] = float(\u0026#39;inf\u0026#39;) pi[node] = None keys[start_node], pi[start_node] = 0, start_node while keys: current_node, current_key = keys.popitem() mst.append([pi[current_node], current_node, current_key]) total_weight += current_key for adjacent, weight in graph[current_node].items(): if adjacent in keys and weight \u0026lt; keys[adjacent]: keys[adjacent] = weight pi[adjacent] = current_node return mst, total_weight struggling with a problem 백준 골드 5를 혼자서 푼 후 기고만장해져서 골드 4의 1197번 문제에 도전해보았다. 이틀에 걸쳐 도전했지만 포기하고 정답을 보게되었음에도 문제를 해결한 것 같지 않다. 해당 문제는 n개의 정점들에 대한 간선들 중에서 가장 가중치가 작은 경로의 가중치를 찾는 것이다. 처음엔 노드하면 DFS와 BFS 밖에 몰랐기 때문에 당연하게 DFS로 접근했다:\n먼저 부모, 자식, 가중치, 인덱스를 변수로 가지는 Node 클래스를 선언하여\n간선의 정보를 노드 내 인스턴스 변수에 저장하게 한다. 전체 노드 중 자식 노드를 가진 노드에 한해 가중치 최솟값을 구하는 함수를 실행한다. 해당 함수는 root에서부터 end-point까지 순회하면서 가중치 합의 최솟값을 구하는 동작을 수행한다. 함수의 결과는 따로 반환되지 않고 root 노드의 인스턴스 변수에 저장된다. 이러한 논리를 가지고 작성한 알고리즘이 글 밑에 있는 첫 번째 코드이다. 하지만 해당 코드는 1초의 시간 제한 안에 돌아가기엔 무리가 있었다. DFS로 안된다는 것을 깨닫고 질문글을 훑어본 후 크루스칼 알고리즘을 선택하기로 했다:\n우선 고려해야될 것은 크루스칼 알고리즘이 모든 노드를 연결시키기 위한 알고리즘이라는 것이다. 해당 문제는 root 노드에서부터 시작하는 모든 경로를 고려해야 하는데 크루스칼 알고리즘을\n사용할 경우 가장 작은 가중치로 시작하는 경로만을 선택하고 나머지를 무시하게 된다. 이 경우 발생하는 반례가 다음과 같다. 1 2 3 4 5 6 3 3 1 2 2 1 3 3 2 3 9999 output: 10001 answer: 3 크루스칼 알고리즘에 의해 1 -\u0026gt; 2의 간선을 선택하고 1 -\u0026gt; 3의 간선을 무시할 경우\n최종적으로는 1 -\u0026gt; 2 -\u0026gt; 3의 경로에 대한 가중치 10001을 결과로 얻게 된다. 이에 대한 해결책으로 생각한 것이 EtherChannel의 Active/Passive 개념이다. 앞서 시도한 DFS 기반 알고리즘에 크루스칼 알고리즘을 조합해서 모든 경로를 탐색하는데\n가중치가 가장 작은 경로로 이어지는 자식 노드를 Active로, 나머지를 Passive로 분류한다. 만약 한 노드에 새로운 자식 노드가 추가되면 자식 노드들의 가중치를 비교해서 Active를 갱신하고\n해당 노드의 부모 노드를 타고 올라가며 동일한 작업을 반복한다. 해당 알고리즘은 root 노드에서부터 모든 자식 노드를 탐색해야 했던 DFS 기반 알고리즘과는 반대로\n자식 노드에서부터 root 노드까지의 경로만을 탐색하기 때문에 시간 초과를 피할 수 있었다. 하지만 여러 조건들을 고려하다보니 작성자인 나조차도 알아보기 힘들정도로 코드가 많이 복잡해졌고\nroot 노드가 기준인데 굳이 아래서부터 위를 탐색하는 방식이 마음에 들지 않았다. 그리고 가장 큰 문제는 해당 알고리즘에도 반례가 있어서 정답이 될 수 없었다는 것이다. 하루동안 고민한 끝에 크루스칼 알고리즘을 포기하고 이와 비슷하다는 프림 알고리즘을 선택하게 되었다:\n이제까지 사용했던 Node 인스턴스 내에 모든 정보를 저장하는 접근방식을 버리고\n프림 알고리즘의 기본에 집중했다. 부모 노드의 값을 자식 노드의 배열 값에 저장하는 Union-Find 알고리즘을 기반으로 그래프를 그리고\n모든 노드에 대해 프림 알고리즘을 수행하여 최소 가중치를 구하는 방식을 구상했다. 하지만 이 경우에 두 가지 문제점이 있었다. 프림 알고리즘도 결국 모든 노드를 연결하기 위한 알고리즘이기 때문에,\nroot에서 end-point까지 갔다 하더라도 거기서 멈추지 않고 다른 경로를 탐색하는 문제가 생긴다. 해당 문제에 대한 해결책으로 Find 연산을 응용한 깊이 탐색 과정을 추가했다. 매 반복마다 현재 노드에 대해 Find 연산을 수행하고 재귀한 횟수 반환하여 깊이로 지정한다. 깊이가 지속적으로 증가하지 않을 경우 end-point까지 도달했다 판단하여 반복을 멈춘다. 모든 경로의 깊이가 1일 경우 1번 조건을 무시하고 다른 경로를 탐색하는 문제가 있다. root 노드에서 시작했는데 다시 root 노드로 돌아올 경우 해당 노드 자체를 무시한다. 위 조건에 걸릴 경우 양의 무한대 값을 반환하여 가중치 판단 과정에서 제외시킬 수 있었다. 이렇게 많은 시행착오를 거쳤지만 하나를 해결하면 다른 빈틈이 생겨버려 포기할 수밖에 없었다. 심지어 백준에서는 heapdict 모듈을 지원하지 않아 해당 알고리즘을 활용할 수도 없었다. 언젠가 이 문제를 완벽하게 해결하기 위해 디버그 값을 남긴다. 1 2 3 4 5 6 7 8 9 10 3 3 1 2 2 1 3 3 2 3 9999 graph = {1: {2: 1, 3: 3}, 2: {1: 1, 3: 2}, 3: {2: 2, 1: 3}} mst1 = [[1, 1, 0], [1, 2, 1], [2, 3, 2]], weight: 3 mst2 = [[2, 2, 0], [2, 1, 1], [2, 3, 2]], weight: 3 mst3 = [[3, 3, 0], [3, 2, 2], [2, 1, 1]], weight: 3 output: 3 answer: 3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 6 8 1 3 -1 1 5 3 1 6 2 2 5 5 2 6 6 3 4 9 3 5 -1 5 6 -1 graph = {1: {3: -1, 5: 3, 6: 2}, 3: {1: -1, 4: 9, 5: -1}, 5: {1: 3, 2: 5, 3: -1, 6: -1}, 6: {1: 2, 2: 6, 5: -1}, 2: {5: 5, 6: 6}, 4: {3: 9}} mst1 = [[1, 1, 0], [1, 3, -1], [3, 5, -1], [5, 6, -1], [5, 2, 5], [3, 4, 9]], w = 11 mst2 = [[3, 3, 0], [3, 5, -1], [5, 6, -1], [3, 1, -1], [5, 2, 5], [3, 4, 9]], w = 11 mst3 = [[5, 5, 0], [5, 6, -1], [5, 3, -1], [3, 1, -1], [5, 2, 5], [3, 4, 9]] 11 mst4 = [[6, 6, 0], [6, 5, -1], [5, 3, -1], [3, 1, -1], [5, 2, 5], [3, 4, 9]] 11 mst5 = [[2, 2, 0], [2, 5, 5], [5, 6, -1], [5, 3, -1], [3, 1, -1], [3, 4, 9]] 11 mst6 = [[4, 4, 0], [4, 3, 9], [3, 5, -1], [5, 6, -1], [3, 1, -1], [5, 2, 5]] 11 output: 11 answer: -3 1 2 3 4 5 6 7 8 9 10 3 3 1 2 2 1 3 3 2 3 9999 graph = {1: {2: 2, 3: 3}, 2: {1: 2, 3: 9999}, 3: {1: 3, 2: 9999}} mst1 = [[1, 1, 0], [1, 2, 2], [1, 3, 3]], weight = 5 mst2 = [[2, 2, 0], [2, 1, 2], [1, 3, 3]], weight = 5 mst3 = [[3, 3, 0], [3, 1, 3], [1, 2, 2]], weight = 5 output: 5 answer: 3 결론:\n해당 문제에 대한 정답을 찾아본 결과 프림 알고리즘을 heapdict 없이 구현한 알고리즘을 보았는데\n노드에 대한 방문 여부를 판단하여 경로를 구하는 방식이었다. 백준에서는 해당 문제가 통과되었지만 위 세 개의 데이터를 넣었을 때 예상과 다른 값이 나왔다. 아마 내가 문제를 제대로 이해하지 못했거나 채점 데이터 자체가 적어서 그랬을 것이다. 결과적으로 다른 사람이 작성한 정답을 보게 됐지만 완전히 납득하지는 못했다. My First Algorithm (DFS) class Node: def __init__(self, index): self.index = index self.data = 2147483647 self.parent = [] self.child = [] def print_node(self): print(self.index, self.data, self.parent, self.child) def spanning_tree(nodes, check, root, parent, data): for child in parent.child: weight = data + child[1] child = nodes[child[0]] if child.child: if not check[child.index]: spanning_tree(nodes, check, root, child, weight) else: check[parent.index] = True if weight \u0026lt; root.data: root.data = weight\nV, E = map(int, input().split()) graph = [Node(i) for i in range(V+1)] visited = [False for _ in range(V+1)]\nfor _ in range(E): A, B, C = map(int, input().split()) graph[A].child.append((B,C)) graph[B].parent.append((A,C))\nmin_weight = 2147483647\nfor node in graph: if node.child and not node.parent: spanning_tree(graph, visited, node, node, 0) if node.data \u0026lt; min_weight: min_weight = node.data\nprint(min_weight) My Second Algorithm (Kruskal's Algorithm) class Node: def __init__(self, index): self.index = index self.data = 0 self.root = self self.parent = self self.active = None self.passive = [] def get_branch(self): if self.active: return self.passive + [self.active] else: return [] def set_branch(self, node, data): if self.root == node.root: if data \u0026lt; node.data: node.parent = self node.data = data else: node.root = self.root node.parent = self node.data += data if not self.active: self.active = node self.data += node.data node.data = self.data else: self.passive.append(node) self.update_data() def update_data(self): branch = self.get_branch() branch.sort(key=lambda n: n.data, reverse=True) active = branch.pop() if active != self.active: self.active = active self.passive = branch self.data = self.active.data def union_root(source: Node, target: Node, data: int) -\u0026gt; None: root = source.root if target.root in [source, source.root, target]: source.set_branch(target, data) while source != root: source = source.parent source.update_data()\nV, E = map(int, input().split())\ngraph = [Node(i) for i in range(V + 1)] edge_dict = {}\nfor _ in range(E): A, B, C = map(int, input().split()) edge_dict[(A, B)] = C\nedge_list = sorted(edge_dict.items(), key=lambda x: [x[1], x[0]])\nfor (a, b), c in edge_list: node_a, node_b = graph[a], graph[b] if node_a.parent != node_b.parent: union_root(node_a, node_b, c)\nweight = 2147483647\nfor edge_node in graph: if (edge_node.root == edge_node) and edge_node.get_branch(): if edge_node.data \u0026lt; weight: weight = edge_node.data\nprint(weight) My Third Algorithm (Prim's Algorithm) def prim(nodes: dict, start: int) -\u003e int or float: mst, keys, pi = [], heapdict(), dict() depth, total_weight = -1, 0 for n in nodes.keys(): keys[n] = float('inf') pi[n] = None keys[start], pi[start] = 0, start while keys: current_node, current_key = keys.popitem() current_depth = get_depth(pi, start, current_node, 0) if current_depth \u0026lt;= depth: if pi[current_node] == start: return float('inf') break depth = current_depth mst.append([pi[current_node], current_node, current_key]) total_weight += current_key for adjacent, weight in nodes[current_node].items(): if adjacent in keys and weight \u0026lt; keys[adjacent]: keys[adjacent] = weight pi[adjacent] = current_node return total_weight def get_depth(nodes: dict, root: int, start: int, data: int) -\u0026gt; int: if start == root: return data if nodes[start] == root: return data+1 return get_depth(nodes, root, nodes[start], data+1)\nV, E = map(int, input().split()) graph = defaultdict(dict)\nfor _ in range(E): A, B, C = map(int, input().split()) graph[A][B] = C graph[B][A] = C\nweight_list = [] for node in graph.keys(): heapq.heappush(weight_list, prim(graph, node))\nprint(heapq.heappop(weight_list)) Answer Algorithm V, E = map(int, input().split()) graph = [[] for _ in range(V+1)] visited = [False for _ in range(V+1)] heap = [[0, 1]] for _ in range(E): A, B, C = map(int, input().split()) graph[A].append([C, B]) graph[B].append([C, A]) total_weight = 0 node_cnt = 0 while heap: if node_cnt == V: break weight, node = heapq.heappop(heap) if not visited[node]: visited[node] = True total_weight += weight node_cnt += 1 for i in graph[node]: heapq.heappush(heap, i)\nprint(total_weight) Userful Reference\nGraph Editor\n","permalink":"https://minyeamer.github.io/blog/2022-03-06/","summary":"Algorithm Study","title":"2022-03-06 Log"},{"content":"Operator Overloading 연산자 오버로딩은 인스턴스 객체끼리 서로 연산을 할 수 있게 기존 연산자의 기능을 중복으로 정의하는 것 연산자 오버로딩의 예시 Method Operator Example __add__(self, other) + (Binomial) A + B, A += B __pos__(self) + (Unary) +A _sub__(self, other) - (Binomial) A - B, A -= B __neg__(self) - (Unary) -A __mul__(self, other) * A * B, A *= B __truediv__(self, other) / A / B, A /= B __floordiv__(self, other) // A // B, A //= B __mod__(self, other) % A % B, A %= B __pow__(self, other) pow(), ** pow(A, B), A ** B __eq__(self, other) == A == B __lt__(self, other) \u0026lt; A \u0026lt; B __gt__(self, other) \u0026gt; A \u0026gt; B __lshift__(self, other) \u0026laquo; A \u0026laquo; B __rshift__(self, other) \u0026raquo; A \u0026raquo; B __and__(self, other) \u0026amp; A \u0026amp; B, A \u0026amp;= B __xor__(self, other) ^ A ^ B, A ^= B __or__(self, other) | A | B, A |= B __invert__(self) ~ ~A __abs__(self) abs() abs(A) Union-Find Algorithm 두 노드가 같은 그래프에 속하는지 판별하는 알고리즘 노드를 합치는 Union 연산과 루트 노드를 찾는 Find 연산으로 이루어짐 배열에 나열된 모든 노드들은 기본적으로 자기 자신의 값을 가짐 노드를 합칠 때 자식 노드의 배열 값에 부모 노드의 배열 값을 넣음 간단한 구현 코드\n1 2 3 4 5 6 7 8 9 10 11 def find(graph: list, x: int) -\u0026gt; int: if graph[x] == x: return x graph[x] = find(graph, graph[x]) def union(graph: list, x: int, y: int) -\u0026gt; None: x = find(graph, x) y = find(graph, y) if x == y: return graph[y] = x Kruskal\u0026rsquo;s Algorithm 가장 적은 비용으로 모든 노드를 연결하기 위해 사용하는 알고리즘 (최소 비용 신장 트리) 모든 간선 정보를 오름차순으로 정렬한 뒤 비용이 적은 간선부터 그래프에 포함 Reference: https://blog.naver.com/ndb796/221230994142 간단한 구현 코드\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Edge: def __init__(self, a: int, b: int, cost: int): self.parent = a self.child = b self.cost = cost def get_parent(graph: list, x: int) -\u0026gt; int: if graph[x] == x: return x graph[x] = get_parent(graph, graph[x]) def union_parent(graph: list, a: int, b: int) -\u0026gt; None: a = get_parent(graph, a) b = get_parent(graph, b) if a \u0026lt; b: graph[b] = a else: graph[a] = b def find(graph: list, a: int, b: int) -\u0026gt; int: a = get_parent(graph, a) b = get_parent(graph, b) if a == b: return True else: return False def sort_edge(edge_list: list) -\u0026gt; list: return sorted(edge_list, key=lambda x: [x.cost, x.parent, x.child]) def union_edge(graph: list, edge_list: list) -\u0026gt; int: cost = 0 for edge in edge_list: if not find(graph, edge.parent, edge.child): cost += edge.cost union_parent(graph, edge.parent, edge.child) return cost ","permalink":"https://minyeamer.github.io/blog/2022-03-05/","summary":"Algorithm Study","title":"2022-03-05 Log"},{"content":"1. Set 백준 1107번(리모컨) 문제를 풀 때 유용하게 사용 해당 문제는 특정 길이의 문자열에 대해 가능한 모든 조합을 탐색해야 하는데\n시간복잡도를 줄이기 위해 중복이 없는 집합을 사용 빈집합은 set() 명령어로 간단하게 정의 Set은 Dictionary와 동일한 Hash Table 기반이기 때문에\nx in s 연산의 시간복잡도가 O(1)\n리스트의 x in s 연산 시간복잡도가 O(n)인 것과는 큰 차이 Set을 응용해 작성한 코드 일부\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 buttons = set([str(i) for i in range(10)]) channels = {N,} diff = {abs(int_N-100)} if M \u0026gt; 0: buttons -= set(list(input().split())) channels = set() for i in range(1, count+1): product = itertools.product(buttons, repeat=i) channels |= set(map(\u0026#39;\u0026#39;.join, product)) min_chan, max_chan = \u0026#39;0\u0026#39;, \u0026#39;1\u0026#39; for _ in range(count-1): min_chan += max(buttons) for _ in range(count): max_chan += min(buttons) if set(max_chan) \u0026amp; buttons == set(max_chan): channels.add(max_chan) if set(min_chan) \u0026amp; buttons == set(min_chan): channels.add(min_chan) 2. Dictionary 백준 1620번(나는야 포켓몬 마스터 이다솜) 문제를 풀 때 사용 해당 문제는 문자열 또는 인덱스를 입력했을 때 대칭되는 값을 출력해야 하는데\n처음엔 시간복잡도가 O(n)인 List의 index(x)를 사용하여 시간초과가 발생 문자열과 인덱스의 관계를 Dictionary로 구현해 탐색 시간복잡도를 O(1)로 개선 3. Coutner 백준 10816번(숫자카드 2) 문제를 풀 때 사용 해당 문제는 숫자 카드의 값을 입력했을 때 해당 카드의 개수를 출력해야 하는데\n처음엔 시간복잡도가 O(n)인 List의 count(x)를 사용하여 시간초과가 발생 전체 카드에 대한 Counter를 정의하여 탐색 시간복잡도를 O(1)로 개선 4. Combination 백준 1010번(다리 놓기) 문제를 풀 때 사용 해당 문제는 강에 다리를 놓는 경우의 수를 출력해야 하는데\nmath 모듈의 comb 함수를 이용해 경우의 수를 계산 5. Permutation 백준 1107번(리모컨) 문제를 풀 때 유용하게 사용 해당 문제에서 특정 길이의 문자열에 대해 가능한 모든 조합을 나열하는데,\n순서를 고려하고 중복을 허용하기 위해 중복 순열(Product)을 사용 Permutation을 응용해 작성한 코드 일부\n1 2 3 4 5 buttons = set([str(i) for i in range(10)]) ... for i in range(1, count+1): product = itertools.product(buttons, repeat=i) channels |= set(map(\u0026#39;\u0026#39;.join, product)) 6. Binary Search 백준 1654번(랜선 자르기) 문제를 풀 때 유용하게 사용 해당 문제는 서로 다른 길이의 선들을 동일한 길이로 가장 길게 잘라야 되는데\n처음엔 가장 긴 선부터 가장 짧은 선까지의 범위 내에서 완전탐색을 진행하여 시간초과가 발생 완전탐색을 이분탐색으로 대체하여 시간복잡도 개선 Binary Search를 응용해 작성한 코드 일부\n1 2 3 4 5 6 7 8 9 while mn \u0026lt; mx: md = (mx + mn) // 2 count = 0 for i in range(K): count += k[i] // md if count \u0026lt; N: mx = md else: mn = md + 1 7. Heap 백준 7662번(이중 우선순위 큐) 문제를 풀 때 유용하게 사용 해당 문제는 최솟값과 최댓값 삭제 기능을 모두 가지고 있는 이중 우선순위 큐를 구현하는 것 처음엔 List의 pop(x), index(x), max(x)/min(x)를 혼합하여 사용한 것 때문에\nO(n^3) 이상의 시간복잡도를 만들어서 시간초과가 발생 두번째 시도에선 List의 pop(x)와 heapq 모듈의 heappop(x)를 사용해 시간복잡도를 O(1()로 개선\n하지만, Heap은 이진트리 기반으로 리스트와는 구조가 다르기 때문에 인덱스로 참조 시 에러가 발생 세번째 시도에선 단일 큐를 Max Heap과 Min Heap으로 나누고 각각에서 heappop(x), heappush(x)를 수행\n하지만, Max Heap 또는 Min Heap에서 삭제된 값이 반대쪽 Heap에서 남아있는 경우가 있어 에러가 발생 해당 에러에 대한 해결책으로 Max Heap과 Min Heap을 동기화를 시키는 방법도 있지만,\n값이 유효한지 판단하는 Dictionary를 구현해 값에 대한 참/거짓 여부를 참조하는 방법을 이용\n해당 Dictionary를 heappop(x) 사용 시 한 번, 최대/최솟값 출력 시 한 번씩 참조해 에러 해결 Heap을 응용해 작성한 코드 일부\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 if cmd == \u0026#39;I\u0026#39;: n = int(n) heapq.heappush(min_Q, n) heapq.heappush(max_Q, -n) try: valid[n] += 1 except KeyError: valid[n] = 1 ins += 1 elif cmd == \u0026#39;D\u0026#39;: try: if n == \u0026#39;1\u0026#39;: max_pop = -heapq.heappop(max_Q) while not valid[max_pop]: max_pop = -heapq.heappop(max_Q) valid[max_pop] -= 1 ins -= 1 elif n == \u0026#39;-1\u0026#39;: min_pop = heapq.heappop(min_Q) while not valid[min_pop]: min_pop = heapq.heappop(min_Q) valid[min_pop] -= 1 ins -= 1 except IndexError: min_Q, max_Q = [], [] continue 1 2 3 4 5 6 7 8 9 10 max_pop, min_pop = 0, 0 while True: max_pop = -heapq.heappop(max_Q) if valid[max_pop]: break while True: min_pop = heapq.heappop(min_Q) if valid[min_pop]: break print(max_pop, min_pop) 8. DFS/BFS 백준 1260번(DFS와 BFS) 문제를 풀 때 사용 해당 문제는 DFS와 BFS로 탐색했을 때의 결과를 출력하는 기본적인 문제 DFS는 깊이를 우선적으로 탐색, BFS는 너비를 우선적으로 탐색 DFS는 경로의 특징을 저장할 때 사용, BFS는 최단거리를 구할 때 사용 DFS는 스택 또는 재귀함수로 구현, BFS는 큐(데크)를 이용해서 구현 DFS/BFS를 응용해 작성한 코드 일부\n1 2 3 4 5 6 7 8 9 def dfs(nodes, visited, node): visited[node] = True next_nodes = nodes[node] while next_nodes: next_node = heapq.heappop(next_nodes) if not visited[next_node]: print(next_node, end=\u0026#39; \u0026#39;) dfs(nodes, visited, next_node) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def bfs(nodes, visited, root): queue = deque() visited[root] = True queue.append(root) while queue: node = queue.popleft() visited[node] = True print(node, end=\u0026#39; \u0026#39;) next_nodes = nodes[node] while next_nodes: next_node = heapq.heappop(next_nodes) if not visited[next_node]: visited[next_node] = True queue.append(next_node) ","permalink":"https://minyeamer.github.io/blog/2022-03-04/","summary":"Algorithm Study","title":"2022-03-04 Log"}]